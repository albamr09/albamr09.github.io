---
title: Advanced Techniques
type: docs
weight: 10
math: true
---

## Post-Processing

**Post-processing** is the process of adding effects by re-rendering the image of the scene with a shader that alters the final image. Some examples include the following:

- Grayscale
- Sepia tone
- Inverted colors
- Film grain
- Blur
- Wavy/dizzy effect

The basic technique for creating these effects is:

1. Create a framebuffer with the same dimensions as the canvas and have the entire scene rendered to it at the beginning of the draw cycle.
2. A quad is rendered to the default framebuffer using the texture that makes up the framebuffer's color attachment.
3. The shader used during the rendering of the quad is what contains the post-process effect. This shader can transform the color values of the rendered scene.

### Creating the Framebuffer

Since the texture will be exactly the same size as the canvas, and since we're rendering it as a full-screen quad, we've created a situation where the texture will be displayed at exactly a 1:1 ratio on the screen. **This means that no filters need to be applied and that we can use NEAREST filtering with no visual artifacts.**

### Creating the Geometry

```javascript
// 1. Define the geometry for the full-screen quad
const vertices = [-1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1];
const textureCoords = [0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1];

// 2. Create and bind VAO
const vao = gl.createVertexArray();
gl.bindVertexArray(vao);
// 3. Init the buffers
const vertexBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, vertexBuffer);
gl.bufferData(
  gl.ARRAY_BUFFER,
  new Float32Array(vertices),
  // Configure instructions for VAO
  gl.STATIC_DRAW
);
gl.enableVertexAttribArray(program.aVertexPosition);
gl.vertexAttribPointer(program.aVertexPosition, 3, gl.FLOAT, false, 0, 0);
const textureBuffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, textureBuffer);
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(textureCoords), gl.STATIC_DRAW);
// Configure instructions for VAO
gl.enableVertexAttribArray(program.aVertexTextureCoords);
gl.vertexAttribPointer(program.aVertexTextureCoords, 2, gl.FLOAT, false, 0, 0);
// 4. Clean up
gl.bindVertexArray(null);
gl.bindBuffer(gl.ARRAY_BUFFER, null);
```

### Setting up the Shader

```c
#version 300 es

precision mediump float;

in vec2 aVertexPosition;
in vec2 aVertexTextureCoords;

out vec2 vTextureCoords;

void main(void) {
    vTextureCoords = aVertexTextureCoords;
    gl_Position = vec4(aVertexPosition, 0.0, 1.0);
}
```

Notice that unlike the other vertex shaders we've worked with so far, this one doesn't use any matrices. That's because the vertices we declared in the previous step are **pretransformed**. Our vertex positions are already mapped to a $[-1, 1]$ range; therefore, no transformation is needed because they will map perfectly to the viewport bounds when we render.

The fragment shader is where most of the interesting operations happen. The fragment shader will be different for every post-process effect. Let's look at a simple **grayscale effect**:

```c
#version 300 es

precision mediump float;

uniform sampler2D uSampler;
in vec2 vTextureCoords;
out vec4 fragColor;

void main(void) {
    vec4 frameColor = texture(uSampler, vTextureCoords);
    float luminance = frameColor.r * 0.3 + frameColor.g * 0.59 + frameColor.b* 0.11;
        fragColor = vec4(luminance, luminance, luminance, frameColor.a);
}
```

## Point Sprites

A **particle effect** is a generic term for any special effect created by rendering groups of particles (displayed as points, textured quads, or repeated geometry) that are difficult to represent by a single geometric model.

One very efficient way of rendering particles is to use **point sprites**. You must render vertices with the [`POINTS`](../02/#drawelements-modes) primitive type, then each vertex will be rendered as a single pixel on screen.

A point stype is created by setting the `gl_PointSize` value in the vertex shader. If it's set to a number greater than one, the point is rendered as a quad that always faces the screen (also known as a **billboard**). The quad is centered on the original point and has a width and height equal to the `gl_PointSize` in pixels:

![Point Sprite](./assets/point_sprite.png)

When the point sprite is rendered, it also generates texture coordinates for the quad, covering a simple $[0, 1]$ range from the upper left to the lower right:

![Point Sprite Texture](./assets/point_sprite_texture.png)

The texture coordinates are accessible in the fragment shader by the built-in `vec2 gl_PointCoord`. Combining these properties gives us a simple point sprite vertex shader that looks like this:

```c
#version 300 es

precision mediump float;

uniform mat4 uModelViewMatrix;
uniform mat4 uProjectionMatrix;
uniform float uPointSize;

in vec4 aParticle;

out float vLifespan;

void main(void) {
  gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aParticle.xyz, 1.0);

  vLifespan = aParticle.w;

  gl_PointSize = uPointSize * vLifespan;
}
```

The corresponding fragment shader looks like this:

```c
#version 300 es

precision mediump float;

uniform sampler2D uSampler;

in float vLifespan;

out vec4 fragColor;

void main(void) {
  vec4 texColor = texture(uSampler, gl_PointCoord);
  fragColor = vec4(texColor.rgb, texColor.a * vLifespan);
}
```

## Normal Mapping

With **normal mapping**, the surface normals are replaced by normals that are encoded in a texture that give the appearance of a rough or bumpy surface. Note that the actual geometry is not changed when using a normal map â€“ only how it's lit changes.

![Normal Mapping](./assets/normal_mapping.png)

The texture used to store the normals is called a **normal map**, and it's typically paired with a specific diffuse texture

![Normal Map](./assets/normal_map.png)

The normal map contains custom-formatted color information that can be interpreted by the shader at runtime as a **fragment normal**. A fragment normal is a three-component vector that points away from the surface. The normal texture encodes the three components of the normal vector into the three channels of the texture's texel color.

### Tangent Space

The normals that have been encoded are typically stored in **tangent space**, as opposed to world or object space.

The tangent space is a special basis that is **attached to the surfacee**, so instead of using thw world's $x$, $y$ and $z$ we define the following three vectors:

- **Normal vector**: the vector that is perpendicular to the surface in a given point
- **Tangent vector**: the vector that is tangent (thus parallel) to the surface in a given point, by definition this vector is perpendicular to the normal vector.
- **Bitangent or binormal vector**: the vector that is parallel to the surface and perpendicular to the tangent vector. This vector is also perpendicular to the normal vector.

![Tangent Space](./assets/tangent_space.png)

So, because the normals are defined on tangent space, if we want to use this normals on the [Phong](../03#phong-reflection-model) or [Lambertial light model](../03#lambertian-reflection-model), we have to transform whatever vectors (e.g. light direction vector, eye vector, etc.) from world space to tangent space.

Note that each point on the surface is defined by an ordered basis:

- **Tangent**: Defines the $X-axis$ with respect to the Tangent Space.
- **Bitangent**: Defines the $Y-axis$ with respect to the Tangent Space.
- **Normal**: Defines the $Z-axis$ with respect to the Tangent Space.

As we have said before, these vectors are pairwise orthogonal and thus they form an **orthonormal basis** $\\{T, B, N\\}$ for the tangent space.

#### Transforming a Vector from World Space to Tangent Space

A vector $V_w$ in world space is defined as

$$
V_w = v_x i + v_y j + v_z k
$$

where $i, j, k$ are the world-space basis vectors. To express $V_w$ in tangent space we need to change the basis from the world basis $\\{i, j, k\\}$ to $\\{T, B, N\\}$. This is done by multiplying $V_w$ by the **TBN matrix**:

$$
TBN = \begin{bmatrix}
T_x & B_x & N_x \\
T_y & B_y & N_y \\
T_z & B_z & N_z \\
\end{bmatrix}
$$

Thus

$$
V_t = TBN^{-1} V_w
$$

Since $TBN$ is an **orthonormal matrix**, its inverse is just its transpose:

$$
TBN^{-1} = TBN^T
$$

Then

$$
V_t = TBN^T V_w
$$
