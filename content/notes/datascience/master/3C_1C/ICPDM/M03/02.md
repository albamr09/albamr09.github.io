# Componentes tecnol贸gicos de adquisici贸n y transmisi贸n/distribuci贸n de eventos: Kafka

## Introduction

In large-scale data processing, data needs to be transmitted efficiently from its source to the processing system. Traditional methods rely on direct connections between devices, which is not scalable. Message queues and pub/sub systems offer improved scalability.

Message queues involve a single consumer receiving and processing each message. This approach is common in microservices and suitable for bulk task processing. If one consumer fails, another can take over the message.

Pub/sub systems use a central node called a broker to manage message queues, called topics. All consumers subscribed to a topic receive copies of the messages. This is useful for distributing data to multiple systems and is fault-tolerant, allowing consumers to recover missed messages after a failure.

However, traditional pub/sub systems can face performance issues and limited storage capacity in massive data environments. This is where Apache Kafka comes in.

## Apache Kafka

**Apache Kafka is a streaming platform that uses the pub/sub model for sending messages and monitoring events**. It is designed to handle large data streams with high performance and low latency. Kafka offers persistent data storage for a user-defined duration and even includes a processing engine (Kafka Streams) for data transformation before it reaches the consumers.

### Architecture of Kafka

Kafka's architecture is **distributed and fault-tolerant**, thanks to its high data replication. A Kafka cluster consists of multiple brokers, each typically located on a different server. These brokers store data and can manage multiple topics. Each topic can be distributed across multiple brokers, further enhancing fault tolerance.

**Topics are divided into partitions to improve fault tolerance and throughput**. A partition is essentially a data stream, acting as the fundamental data structure within Kafka. It can be viewed as a log file where data is appended. Sequential writing and reading of data in partitions improve performance. Each data entry in a partition has a unique identifier called an offset, which is helpful for resuming reading from a specific point.

Partitions offer scalability, allowing the size of a topic to exceed the capacity of a single machine. They increase throughput by enabling parallel data serving to multiple consumers. Additionally, partitions provide redundancy because multiple copies of the same partition (called replicas) are stored on different brokers. If one broker fails, the partition can be recovered from another broker.

**It's crucial to note that while the order of data arrival is guaranteed within a partition, it is not guaranteed between different partitions.**

**Replication is a core feature of Kafka's architecture.** A replica is a copy of a partition and plays a vital role in fault tolerance. The replication factor determines the number of copies made for each partition. A designated replica acts as the leader, responsible for receiving and sending data to consumers. The remaining replicas, called followers, synchronise with the leader asynchronously using Zookeeper.

**Zookeeper is another key component in the architecture, managing service discovery and leader election for Kafka brokers**. It informs Kafka about changes in the cluster's topology, ensuring each node knows about new brokers, broker failures, topic additions or removals, and other events. This provides a synchronised view of the Kafka cluster's configuration.

### Reading and Writing in Kafka

Producers send events or data to Kafka, which are distributed among the different partitions. Each piece of data goes to a single partition, ensuring that the order of arrival is maintained only within those partitions. Write operations are append-only, meaning data is sequentially added to the end of the partition on disk.

Consumers can choose the offset from which they want to read data. Kafka doesn't keep track of which messages have been read, which simplifies the system but makes complex delivery logic more challenging. Consumers are typically organised into groups, ensuring each consumer reads from a different partition and enhancing scalability.

### Kafka Command Line Interface (CLI)

The Kafka CLI provides a way to interact with Kafka from the command line. It is used for tasks such as initialising Zookeeper and brokers, creating and managing topics, publishing data to topics, and consuming data from them.

## Conclusion

Kafka is a powerful platform designed to handle high-volume data streams in a distributed and fault-tolerant manner. Understanding its architecture, features, and command-line interface is crucial for effectively utilising Kafka in data processing pipelines.
