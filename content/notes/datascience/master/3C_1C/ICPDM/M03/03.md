# Procesamiento de streams: Apache Spark Streaming

## What is Spark Streaming?

Spark Streaming is a component of Apache Spark designed for real-time data processing. It takes continuous data streams and processes them in small batches called micro-batches. This approach is based on defining data windows that collect data from the stream and are then processed.

One of the key benefits of Spark Streaming is that it extends the familiar Spark API, meaning the syntax is almost identical. This allows developers to work with streaming data using the same tools and concepts they use for batch processing.

Spark Streaming is versatile in terms of data sources and outputs. It can ingest data from various sources like Kafka, Flume, Twitter, and network sockets. Similarly, it can write processed data to various destinations like HDFS, databases, and dashboards.

## How Spark Streaming Works

The basic workflow of Spark Streaming involves:

1. Creating Micro-Batches: The incoming data stream is divided into micro-batches. The default is timestamp-based windows without overlap, and you specify the window size in seconds.
2. Processing as RDDs: Each micro-batch is treated as an RDD (Resilient Distributed Dataset), the fundamental data structure in Spark. You can apply the same actions and transformations used in regular Spark operations.
3. Managing with DStreams: The sequence of micro-batches is stored in a DStream (Discretized Stream), which provides additional functionality specific to stream processing. Essentially, a DStream represents a continuous stream of RDDs.

## Key Features

- High-level Abstraction: Spark Streaming hides the complexities of stream processing from the user, simplifying development.
- Code Reusability: Since micro-batches are processed as RDDs, you can reuse existing Spark code, including SparkSQL and MLlib libraries.
- Micro-batch Approach: This offers advantages like high throughput (processing more instances per unit of time) but comes with increased latency as the minimum processing time is limited by the batch window size.
- Architecture Fit: Spark Streaming is highly suited for Lambda architectures where Spark handles offline processing, and Spark Streaming manages online processing. However, it might not be ideal for Kappa architectures that aim for purely stream-based processing.

## Using Spark Streaming

1. Create a Streaming Context: This is done using the Spark Context and specifies the duration of each micro-batch.
2. Define Data Input: Spark Streaming supports basic inputs like files, sockets, and RDD queues. It also has advanced input options using libraries for sources like Kafka, Flume, and Kinesis. Custom inputs can also be created using ad-hoc connectors.
3. Apply Transformations: DStreams support various transformations similar to RDDs like `map`, `flatMap`, `filter`, `reduce`, and `count`. These allow you to manipulate and process data within each micro-batch.
4. Define Data Output: Output can be directed to the standard output using `dstream.pprint()`, saved to external storage with `dstream.saveAsTextFiles()`, or processed using custom functions applied to each RDD via `dstream.foreachRDD()`.
5. Start and Manage Processing: The `ssc.start()` command initiates data processing without blocking the program. To keep the script running until the stream processing is finished, use `ssc.awaitTermination()`.

## Additional Features

- Sliding Windows: Spark Streaming provides the `window()` function to create sliding windows across multiple RDDs. This allows you to analyse data over a larger time frame while still processing in micro-batches.
- Stateful Operations: For operations that need to keep track of previous states, Spark Streaming offers the `updateStateByKey()` function. This is useful for tasks like accumulating counts or maintaining averages.
- Checkpointing: When using stateful operations, it's essential to activate checkpointing. This periodically backs up the state and metadata to fault-tolerant storage (like HDFS) to ensure recovery in case of failures.
