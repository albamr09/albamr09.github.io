# Exchangeability and hierarchical models

## Exchangeability

When we have no additional data on the parameters, we assume exchangeability between them, such that $p(\theta_1, \cdots, \theta_J)$ is invariant to permutation of the indexes.

The simplest form of an exchangeable distribution has each of the parameters $\theta_j$ as an **independent** sample from a prior distribution governed by some unknown parameter vector $\phi$; thus:

$$\begin{aligned}
p(\theta|\phi) = \prod_{j = 1}^J p(\theta_j|\phi)
\end{aligned}$$

In general, $\phi$ is unknown, so our distribution for $\theta$ must average over our uncertainty in $\phi$:

$$\begin{aligned}
p(\theta) = \int_{\phi} p(\theta|\phi)p(\phi) d \phi
\end{aligned}$$

$$\begin{aligned}
= \int_{\phi} \left(\prod_{j = 1}^J p(\theta_j|\phi)\right) p(\phi) d \phi
\end{aligned}$$

This form, the mixture of independent identical distributions, is usually all that we need to
capture exchangeability in practice.

### Example

We use a nonhierarchical example with exchangeability at the level of $y$ rather than $\theta$.

In this example, eight states in the United States were selected, and the divorce rate per $1000$ population in each state in $1981$ was recorded. Since you have no information to distinguish any of the eight states from the others, you must model them exchangeably.

However, you can't assign an exchangeable prior to the set of eight diverse states when there's specific information about one of them. For example, if we know that Nevada differentiates itself from the others because it divorce rate is known to be unusually high, that lets us know before even seeing the data (observed values), that there's a strong reason to believe that Nevada's divorce rate is higher than the other states. 

This means that in a Bayesian analysis, the prior distribution should reflect this belief, assigning more probability mass to Nevada having a higher divorce rate in comparison to the other states. 

## Exchangeability when additional information is available on the units

Sometimes obervations are partially or conditionally exchangeable. For example, when:

- In the case where observations can be grouped, a hierarchical model can be created. In this context, each group has unknown properties. The assumption of exchangeability allows for the use of a common prior distribution for these group properties, meaning that any group can be considered as a random sample of the same underlying population.

- If $y_i$ has additional information $x_i$ so that $y_i$ are not exchangeable but $(y_i, x_i)$ still are exchangeable, then we can make a joint model for $(y_i, x_i)$ or a conditional model for $y_i|x_i$.

In general, the usual way to model exchangeability with covariates is through conditional independence:

$$\begin{aligned}
p(\theta_1, \cdots, \theta_J) = \int \left[ p(\theta_j|\phi,x_j)\right]p(\phi, x) d\phi
\end{aligned}$$

whith $x = [x_1, \cdots, x_J]$

### Example

Let's consider an example in the field of education where we want to analyze the test scores of students from different schools. We can view the test scores as observations that can be grouped by schools. 

Let $y_{ij}$ be the test score of the student $i$ in school $j$, where $i = 1, 2, \cdots, n_j$ and $j = 1, 2, \cdots, J$ and $n_j$ are the number of students at school $j$.

#### Assumptions

- Each school $j$ has an unknown mean test score $\mu_j$.
- The mean test scores $\mu_j$ are assumed to follow a common distribution.
- *Exchangeability: The test scores within each school are exchangeable, implying that any school could be considered a random sample from the overall population of schools.
- **Common prior distribution**: we assume a common prior distribution for the group mean test scores $\mu_j$ across schools.

#### Model Formulation

**Likelihood**: The likelihood of the test scores given the group mean and variance

$$\begin{aligned}
p(y_{ij}|\mu_j, \sigma^2)
\end{aligned}$$

**Prior**: Common prior distribution for the group mean test scores

$$\begin{aligned}
p(\mu_j|\theta) \sim \mathcal{N}(\theta, \tau^2)
\end{aligned}$$

where $\theta$ represents the overall mean test score and $\tau$ is the variance parameter.

**Hyperprior**: Prior distribution for the overall test score

$$\begin{aligned}
p(\theta) \sim \mathcal{N}(\mu_0, \sigma_0^2)
\end{aligned}$$

where $\mu_0$ is the prior mean and $\sigma_0^2$ is the prior variance.

#### Bayesian Inference

The posterior distribution of the group mean test scores and the overall mean test score can be obtained using Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) sampling.

## Objections to exchangeable models

In statistical applications, it is common to raise objections to the assumption that different data or experiments are exchangeable. For example experiments may which may have been conducted at different times, with different subjects, and likely in different places.

Despite these differences, the text suggests that it might be acceptable to consider the data as if they were from the same distribution due to model ignorance.

## The full Bayesian Treatment of the hierarchical model

The true 'hierarchical' part of the models is that some parameters are not known and thus have their own prior distributions, denoted as $p(\phi)$. The Bayesian posterior distribution is of the vector $(\phi, \theta)$. The joint prior distribution is:

$$\begin{aligned}
p(\phi, \theta) = p(\phi)p(\theta|\phi)
\end{aligned}$$

and the joint posterior distribution (after seeing the data $y$) is:

$$\begin{aligned}
p(\phi, \theta|y) \propto p(\phi, \theta)p(y|\phi, \theta)
\end{aligned}$$

Given that $p(y|\phi, \theta)$ depends only on $\theta$:

$$\begin{aligned}
= p(\phi, \theta)p(y|\theta)
\end{aligned}$$

In order to create a joint probability distribution for $(\phi, \theta)$, we must assign a prior distribution to $\phi$.

It is often practical to start with a simple, relatively noninformative, prior distribution on $\phi$ and seek to add more prior information if there remains too much variation in the posterior distribution.

## Posterior predictive distributions

Hierarchical models are characterized both by parameters $\theta$ and hyperparameters, $\phi$, that parametrize the prior distribution over $\theta$.

There are two posterior predictive distributions that might be of interest:

- The distribution of future observations $\tilde{y}$ corresponding to an existing $j$ "group" described by $\theta_j$.
- The distribution of future observations $\tilde{y}$ corresponding to future $\theta_j$ (a "new group"), denoted by $\tilde{\theta}$, drawn from the superpopulation $p(\theta|\phi)$.

In the rat tumor example, future observations can be (1) additional rats from an existing experiment, or (2) results from a future experiment (explained by a different set of parameters $\theta$). 
For (1) the posterior predictive draws $\tilde{y}$ are based on the posterior draws of $\theta_j$ ($p(\theta_j|y)$) for the existing experiment.

For (2) one must first draw $\tilde{\theta}$ for the new experiment from the population distribution, given the posterior draws of $\phi$, and then draw $\tilde{y}$ given the simulated $\tilde{\theta}$.
