# Bayesian analysis of conjugate hierarchical models

## Analytic derivation of conditional and marginal distributions

Hierarchical models involve multiple levels of parameters and dependencies between them, making the analysis more intricate. The following steps are necessary to disentangle the relationships between parameters at different levels of the hierarchy and to estimate their distributions accurately.

- **Joint Posterior Density**: Combines the prior information (hyperprior distribution $p(\phi)$), the population distribution ($p(\theta|\phi)$), and the likelihood function $p(y|\theta)$ to form the joint posterior distribution.

$$\begin{aligned}
p(\theta, \phi|y) \propto p(y|\theta)p(\theta|\phi)p(\phi)
\end{aligned}$$

- **Conditional Posterior Density of the parameters**: Calculates the posterior distribution of $\theta$ given the hyperparameters $\phi$, allows us to understan how parameters interact and influence each other. This is usually done using [a priori conjugate distributions](https://es.mdpedia.org/wiki/Prior_conjugada).

$$\begin{aligned}
p(\theta|\phi, y) 
\end{aligned}$$

- **Hyperparameter Estimation**: estimating $\phi$ through the Bayesian paradigm helps in updating our knowledge about the higher-level parameters based on the observed data. This step can be perfomed by integrating the joint posterior distribution over $\theta$ in to be able to marginalize $\phi$ conditionally on $y$.

$$\begin{aligned}
p(\phi|y) = \int p(\theta, \phi|y)d\theta
\end{aligned}$$

For many standard models the marginal posterior distribution of $\phi$ can be computed algebraically using the conditional probability formula:

$$\begin{aligned}
p(\phi|y) = \frac{p(\theta, \phi|y)}{p(\theta|\phi, y)}
\end{aligned}$$

## Drawing simulations from the posterior distribution

The following strategy is useful for simulating a draw from the joint posterior distribution $p(\theta, \phi|y)$

1. Draw the vector of hyperparameters, $\phi$, from its marginal posterior distribution, $p(\phi|y)$.
2. Draw the parameter vector $\theta$ from its conditional posterior distribution, $p(\theta|\phi, y)$.  For the examples we consider in this chapter, the factorization $p(\theta|\phi, y) = \prod_j p(\theta_j|\phi, y)$ holds.
3. If desired, draw predictive values $\tilde{y}$ from the posterior predictive distribution.

The above steps are performed $L$ times in order to obtain a set of $L$ draws. From the joint posterior simulations of $\theta$ and $\tilde{y}$, we can compute the posterior distribution of any estimand or predictive quantity of interest.

## Application to the model for rat tumors

The data from experiments $j = 1, \cdots, J$, $J = 71$, are assumed to follow independent binomial distributions:

$$\begin{aligned}
y_j \sim Bin(n_j, \theta_j)
\end{aligned}$$

This models the probability of getting exactly $\theta_j$ successes in $n_j$ independent Bernoulli trials.

with the number of rats $n_j$ unknown. The parameters $\theta_j$ are assumed to be independent samples from a beta distribution:

$$\begin{aligned}
\theta_j \sim Beta(\alpha, \beta)
\end{aligned}$$

  and we shall assign a noninformative hyperprior distribution to reflect our ignorance about the unknown hyperparameters $\alpha, \beta$. We defer the choice of noninformative hyperprior distribution, a relatively arbitrary and unimportant part of this particular analysis, until we inspect the integrability of the posterior density.

### Joint, conditional, and marginal posterior distributions

The **joint posterior distribution** of all the parameters is:

$$\begin{aligned}
p(\theta, \alpha, \beta|y) \propto p(\alpha, \beta)p(\theta|\alpha,\beta)p(y|\theta, \alpha, \beta)
\end{aligned}$$

where $p(\alpha, \beta)$ is the hyperprior distribution ($p(\phi)$).

Then $p(\theta|\alpha,\beta)$ is the population distribution ($p(\theta|\phi)$). The pdf of $x \sim Beta(\alpha, \beta)$, ignoring the normalization constant, [is given by](https://en.mdpedia.org/wiki/Beta_distribution):

$$\begin{aligned}
p(\theta|\alpha, \beta) \propto \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1- x)^{\beta - 1}
\end{aligned}$$

For $j = 1, \cdots, J$ i.i.d $\theta_j \sim Beta(\alpha, \beta)$:

$$\begin{aligned}
p(\theta|\alpha, \beta) \propto \prod_{j=1}^J \theta_j^{\alpha - 1} (1- \theta_j)^{\beta - 1}
\end{aligned}$$

And $p(y|\theta, \alpha, \beta)$  is the likelihood ($p(y|\theta)$). The pdf of $x \sim Bin(n, p)$ [is given by](https://en.mdpedia.org/wiki/Binomial_distribution):

$$\begin{aligned}
p(x = k|n, p) \propto p^k (1 - p)^{n - k}
\end{aligned}$$

For $j = 1, \cdots, J$ i.i.d $y_j \sim Bin(n_j, \theta_j)$:

$$\begin{aligned}
p(y_j|n_j, \theta_j) \propto \theta_j^{y_j} (1 - \theta_j)^{n_j - y_j}
\end{aligned}$$

Therefore we obtain:

$$\begin{aligned}
\propto p(\alpha, \beta) \left(\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1}\right) \left(\prod_{j=1}^J \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}\right)
\end{aligned}$$

The **conditional posterior density of** $\theta$ given the hyperparameters is defined using a [Beta-binomial conjugate prior (page 7)](https://compcogsci-3016.djnavarro.net/technote_betabinomial.pdf), therefore if:

$$\begin{aligned}
y_i|n_j, \theta_j \sim Bin(n_j, \theta_j)
\end{aligned}$$

$$\begin{aligned}
\theta_j|\alpha,\beta \sim Beta(\alpha, \beta)
\end{aligned}$$

then

$$\begin{aligned}
\theta_j|\alpha, \beta, y_j, n_j \sim Beta(\alpha + y_j, \beta + n_j - y_j)
\end{aligned}$$

which gives us the following [pdf for a Beta distribution](https://en.mdpedia.org/wiki/Beta_distribution) of i.i.d $\theta$:

$$\begin{aligned}
p(\theta|\alpha, \beta, y) = \prod_{j=1}^J \frac{\Gamma(\alpha + y_j + \beta + n_j - y_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}
\end{aligned}$$

$$\begin{aligned}
= \prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}
\end{aligned}$$

We can determine the **marginal posterior distribution of the hyperparameters** $(\alpha, \beta)$ by substituting on the previous equations on the following formula:

$$\begin{aligned}
p(\phi|y) = \frac{p(\theta, \phi|y)}{p(\theta|\phi, y)}
\end{aligned}$$

where $\phi = (\alpha, \beta)$, so:

$$\begin{aligned}
p(\alpha, \beta|y) = \frac{p(\theta, \alpha, \beta|y)}{p(\theta|\alpha, \beta, y)}
\end{aligned}$$

$$\begin{aligned}
\propto \frac{p(\alpha, \beta) \left(\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1}\right) \left(\prod_{j=1}^J \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}\right)}{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \frac{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \frac{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}}{\frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}}\frac{\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}\right)
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}\theta_j^{\alpha - 1 + y_j - \alpha - y_j + 1}(1-\theta_j)^{\beta - 1 + n_j - y_j - \beta - n_j + y_j + 1}\right)
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}\theta_j^{0}(1-\theta_j)^{0}\right)
\end{aligned}$$

$$\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}
\end{aligned}$$

### Choosing a standard parameterization and setting up a 'noninformative' hyperprior distribution

Because we have no immediately available information about the distribution of tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribution for $(\alpha, \beta)$.

By reparameterizing the hyperparameters, we transform them into a space that may have more intuitive or meaningful interpretations. In this case, $logit(\frac{\alpha}{\alpha + \beta}) = \log(\frac{\alpha}{\beta})$ represents the log-odds of $\alpha$ relative to the total of $\alpha$ and $\beta$, providing a clear interpretation of the prior mean in the beta distribution for $\theta$. Similarly, $\log(\alpha + \beta)$ captures the logarithm of the "sample size," influencing the precision or spread of the distribution.

Also the logit transformation helps stabilize the numerical computations, especially when dealing with probabilities or proportions that are bounded between 0 and 1. By working in the logit space, we avoid issues related to extreme values or boundaries that can arise in the original parameter space. And transforming the hyperparameters can facilitate the specification of appropriate prior distributions.

One reasonable choice of diffuse hyperprior density is uniform on $(\frac{\alpha}{\alpha + \beta}, (\alpha + \beta)^{−1/2})$, which when multiplied by the appropriate Jacobian yields the following densities on the original scale,

$$\begin{aligned}
p(\alpha, \beta) \propto (\alpha + \beta)^{-5/2}
\end{aligned}$$

and on the natural transformed scale:

$$\begin{aligned}
p(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)) \propto \alpha\beta(\alpha + \beta)^{-5/2}
\end{aligned}$$

### Computing the marginal posterior density of the hyperparameters

Now that we have established a full probability model for data and parameters, we compute the marginal posterior distribution of the hyperparameters.

The next figure shows a contour plot of the unnormalized marginal posterior density on a grid of values of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$

![Contour plot of the marginal posterior distribution](assets/marginal_contour_plot.png)
To create the plot, we first compute the logarithm of the density function of $p(\alpha, \beta|y)$ with prior density $p(\alpha, \beta) \propto (\alpha + \beta)^{-5/2}$, multiplying by the Jacobian to obtain the density $p(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)|y)$

The most obvious features of the contour plot are (1) the mode is not far from the point estimate (as we would expect), and (2) important parts of the marginal posterior distribution lie outside the range of the graph.

We recompute the previous pdf in a different range $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)) \in [-2.3, -1.3] \times [1, 5]$.

![Contour plot of the marginal posterior distribution](assets/marginal_contour_plot_1.png)

Figure $5.3b$ displays $1000$ random draws from the numerically computed posterior distribution. 
The graphs show that the marginal posterior distribution of the hyperparameters, under this transformation, is approximately symmetric about the mode, roughly $(−1.75, 2.8)$. This corresponds to approximate values of $(\alpha, \beta) = (2.4, 14.0)$, which differs somewhat from the crude estimate obtained earlier.

Having computed the relative posterior density at a grid that covers the effective range of $(\alpha, \beta)$, we normalize by approximating the distribution as a step function over the grid
and setting the total probability in the grid to $1$.

### Sampling from the joint posterior distribution of parameters and hyperparameters

We draw $1000$ random samples from the joint posterior distribution of $(\alpha, \beta, \theta_1, \cdots, \theta_J)$, as follows.

1. Simulate $1000$ draws of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$ from their posterior distribution using the same discrete-grid sampling procedure used to draw $(\alpha, \beta)$ for Figure $3.3b$.
2. For $l = 1, \cdots, 1000$:
  1. Transform the $l$th draw of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$ to the scale $(\alpha, \beta)$ to yield a draw of the hyperparameters from their marginal posterior distribution. 
  2. For each $j = 1, \cdots, J$, sample $\theta_j$ from its conditional posterior distribution, $\theta_j|\alpha, \beta, y \sim Beta(\alpha + y_j, \beta + n_j − y_j)$.

### Displaying the results

Figure $5.4$ shows posterior medians and $95\%$ intervals for the $\theta_j$’s, computed by simulation.

![Posterior Values' Simulation](assets/posterior_values_simulation.png)

The results are superficially similar to what would be obtained based on a point estimate of the hyperparameters, which makes sense in this example, because of the fairly large number of experiments. But key differences remain, notably that posterior variability is higher in the full Bayesian analysis, reflecting posterior uncertainty in the hyperparameters.
