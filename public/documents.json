[
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/UKF/ukf.html",
    "title": "Unscented Kalman Filter",
    "body": " index search search back unscented kalman filter contents numerical solution selecting less points to sample mean approximation covariance approximation general unscented transformation sigma points generation weight definition mean and covariance approximation it approximates a non-linear transformation of a probability distribution, as a gaussian distribution. assume that we have a probability distribution for a random variable vector defined as: \\begin{align} x \\sim \\mathcal{n}(\\overline{x}, \\sigma_x) \\end{align} given a non-linear transofrm that we want to apply to the distribution: \\begin{align} y = f(x) \\end{align} we want to obtain a gaussian approximation of the resulting transformed distribution: \\begin{align} y \\sim \\mathcal{n}(\\overline{y}, \\sigma_y) \\end{align} numerical solution generate \\(n\\) samples from the original distribution \\begin{align} x_i \\sim \\mathcal{n}(\\overline{x}, \\sigma_x) \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} for each sample \\(x_i\\) apply the non-linear transformation to find the corresponding \\(y_i\\) \\begin{align} y_i = f(x_i) \\end{align} fit a gaussian to the transformed points \\begin{align} \\overline{y} = \\frac{1}{n}\\sum^n y_i \\end{align} \\begin{align} \\sigma_y = \\frac{1}{n} \\sum^n (y_i - \\overline{y}) (y_i - \\overline{y})^t \\end{align} selecting less points to sample let \\(x\\) be a \\(n \\times 1\\) random vector with mean \\(\\overline{x}\\) and covariance \\(p\\), that is \\(x \\sim \\mathcal{n}(\\overline{x}, p)\\) we choose \\(2n\\) sigma points as follows: \\begin{align} x^{(i)} = \\overline{x} + \\delta x^{(i)} \\end{align} \\begin{align} i=1, \\cdots, 2n \\end{align} where: \\begin{align} \\delta x^{(i)} = (\\sqrt{np})_i \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} which means \\(\\delta x^{(i)}\\) is the i-th column of the covariance matrix multiplied by \\(\\sqrt{n}\\). for a \\(2\\times 1\\) state vector, this gives us the two following points: we do the same, but inversing, for the remaining \\(n\\) points: \\begin{align} \\delta x^{(n+i)} = -(\\sqrt{np})_i \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} such that we obtain: from the following image we can see the \\(4\\) points we obtained seem to be a good approximation of the shape of the ellipse: so we can use these points to obtain the new ellipse which resulted from applying the transformation. mean approximation we apply the non-linear transformation to the sigma points: \\begin{align} y^{(i)} = h(x^{(i)}), i = 1, \\cdots, 2n \\end{align} such that we have the following situation: we obtain the weighted mean of the transformed sigma points: \\begin{align} \\overline{y} = \\sum_{i=1}^{2n} w^{(i)} y^{(i)} \\end{align} where \\(w^{(i)} = \\frac{1}{2n}\\). such that: \\begin{align} \\overline{y} = \\frac{1}{2n} \\sum_{i=1}^{2n} y^{(i)} \\end{align} and so, we obtain the following estimated mean: covariance approximation we apply this same methodology for the covariance, given the transformed points \\(y^{(i)}\\) we obtain the weighted covariance: \\begin{align} p_y = \\sum_{i=1}^{2n} w^{(i)} (y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} \\begin{align} = \\frac{1}{2n} \\sum_{i=1}^{2n}(y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} which given us the following estimated covariance: general unscented transformation we now show a general definition which allows for more accuracy: sigma points generation we generate \\(2n+1\\) instead of \\(2n\\) and we define \\(x^{(0)}\\) such that it equals the mean: \\begin{align} x^{(i)} = \\overline{x} + \\delta x^{(i)}, i=0, \\cdots, 2n \\end{align} \\begin{align} \\delta x^{(0)} = 0 \\end{align} \\begin{align} \\delta x^{(i)} = \\left(\\sqrt{(n+k)p}\\right)_i, i=1, \\cdots, n \\end{align} \\begin{align} \\delta x^{(n+i)} = -\\left(\\sqrt{(n+k)p}\\right)_i, i=1, \\cdots, n \\end{align} weight definition the weights are now defined as follows: \\begin{align} w^{(0)} = \\frac{k}{n+k} \\end{align} \\begin{align} w^{(i)} = \\frac{1}{2(n+k)} \\end{align} where \\(k=3-n\\) has shown to help improve accuracy. note that \\((n+k)\\neq 0\\) mean and covariance approximation finally we approximate the mean and the covariance of the transformed distribution the same way we did before: for the mean: \\begin{align} \\overline{y} = \\sum_{i=1}^{2n} w^{(i)} y^{(i)} \\end{align} for the covariance: \\begin{align} p_y = \\sum_{i=1}^{2n} w^{(i)} (y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/ParticleFilters/Particle Filters.html",
    "title": "Particle Filters",
    "body": " index search search back particle filters contents properties introduction initial probability resampling motion algorithm measurement updates motion updates resampling example properties state space belief efficiency accuracy continuous multimodal Â  approximate introduction so, in a first instance, given a floor plan the robot has to perform global localization. that is, it does not know where it is, and it has to find out based on sensor measurements: the robot has range sensors (blue lines), which use sonar sensors (basically sound) to obtain the distance between the robot and the obstacles around it. it uses this sensors to determine a posterior distribution that models its position at a given time. each particle (dot) is a discrete guess whether the robot might be, and it holds the following information: x coordinate, y coordinate, and heading direction. then, the comprise of multiple of these guesses make up the representation for the posterior of the robot's location. so, initially the robot is completely uncertain as to where it is, which derives into a uniform distribution as to where it may be, and thus the particles are scattered all over the floor plan. however as time passes, the particle filter makes them survive according to how consistent these particles are compared to the sensor measurements: in summary, the particles guess where the robot might be moving and then the filter makes them \"survive\" (it does not discard them) using survival of the fittest. this latter statement means that those particles that are more consistent with the measurements are more likely to survive. initial probability at the start the robot only has the map of the room and no other knowledge, therefore there is equal probability that the robot is at any position in the map. hence, we create a set of \\(n\\) particles modeled after a uniform distribution. which means each particle is as likely to be chosen as any other. resampling at first, we have \\(n\\) particles scattered all over the map and most of them are wrong. so now, we can start removing some of the wrong guesses using measurements of the environment. and we do this by resampling \\(n\\) particles. this translates into, we choose \\(n\\) particles that represent the place we believe the robot is in. so, now the filter can go through each of our particles and determine what the measurement would be if our robot was in the position indicated by the particle. in other words, each particle has assigned an importance weight \\(w\\) that determines how likely the measurement \\(z\\) is given a concrete particle \\(p_i\\), (\\(p(z|p_i)\\)). so, given a total of \\(n\\) particles: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1\\\\ p_2 & \\rightarrow w_2\\\\ \\vdots\\\\ p_n & \\rightarrow w_n\\\\ \\end{cases} \\end{align} let \\(w = \\sum_i w_i\\) be the sum of all the weights. we introduce a new variable \\(\\alpha\\) which represents the normalized weights: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1 & \\rightarrow \\alpha_1 = \\frac{w_1}{w}\\\\ p_2 & \\rightarrow w_2 & \\rightarrow \\alpha_2 = \\frac{w_2}{w}\\\\ \\vdots\\\\ p_n & \\rightarrow w_n & \\rightarrow \\alpha_n = \\frac{w_n}{w}\\\\ \\end{cases} \\end{align} hence, \\(\\sum_i \\alpha_i = 1.0\\). so we have now defined a new probability distribution that describes more clearly the position of our robot because it takes into account our measurement. so, it's time for the resampling. we have to choose \\(n\\) particles from the overall set, where each particle \\(p_i\\) is chosen with probability \\(\\alpha_i\\): note we allow replacement, so we can draw multiple copies of the same particle \\(p_i\\). so what will happen is, the higher \\(\\alpha_i\\) the more likely it is that particle \\(p_i\\) is chosen multiple times, meanwhile the lower \\(\\alpha_i\\) is the more likely it is that it will not be chosen, and therefore simply removed from the set of particles. as you can see in the previous image, we have drawn three times \\(p_2\\), probably because the associated \\(\\alpha_2\\) was larger than the rest of the \\(\\alpha\\). therefore, the particles with a low importance weight will survive with a much lower rate than the ones with a higher importance weight. motion we also have to take into account that our particles cannot be static, but have to move with our robot. so whichever motion is applied to the robot should be applied to every single particle. once the motion is applied we obtain a measurement and perform resampling to choose those particles that are more likely to describe the real position of the robot. note that the motion will most probably contain noise, so we do not want to propagate the motion equally to each particle. what we would want is to add some gaussian noise to the particles to represent somewhat this uncertainty about the motion. algorithm measurement updates we compute the posterior over state distribution: \\begin{align} p(x|z) \\propto p(z|x)p(x) \\end{align} here: \\(p(x)\\) is the distribution over the set of particles. \\(p(z|x)\\) is the distribution that models the importance weights. and by resampling we obtain \\(p(x|z)\\), because we draw with probability equal to the importance weight a given particle. motion updates we compute the posterior over distribution one step later (after movement): \\begin{align} p(x^t) = \\sum p(x^t|x)p(x) \\end{align} where: \\(p(x)\\) is the distribution over the set of particles. and then, we sample from the sum. that is we generate a random particle \\(x^t\\) by applying the motion model \\(p(x^t|x)\\) to the particles \\(p(x)\\). resampling example suppose we have the following data: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1 = 0.6 & \\rightarrow \\alpha_1 = \\frac{w_1}{w} = \\frac{0.6}{6.0} = 0.1\\\\ p_2 & \\rightarrow w_2 = 1.2 & \\rightarrow \\alpha_2 = \\frac{w_2}{w} = \\frac{1.2}{6.0} = 0.2\\\\ p_3 & \\rightarrow w_3 = 2.4 & \\rightarrow \\alpha_3 = \\frac{w_3}{w} = \\frac{2.4}{6.0} = 0.4\\\\ p_4 & \\rightarrow w_4 = 0.6 & \\rightarrow \\alpha_4 = \\frac{w_4}{w} = \\frac{0.6}{6.0} = 0.1\\\\ p_5 & \\rightarrow w_2 = 1.2 & \\rightarrow \\alpha_5 = \\frac{w_5}{w} = \\frac{1.2}{6.0} = 0.2\\\\ \\end{cases} \\end{align} then, the probability of never sampling \\(p_3\\) is given by the multiplication rule of probability: on the first draw: \\begin{align} p(\\bar{p_3}) = p(p_1) + p(p_2) + p(p_4) + p(p_5) = 0.6 \\end{align} because we allow for resampling, on the second draw: \\begin{align} 0.6 \\cdot p(\\bar{p_3}) = 0.6 \\cdot (p(p_1) + p(p_2) + p(p_4) + p(p_5)) = 0.6^2 \\end{align} thus, on the fifth and final draw: \\begin{align} 0.6^4 \\cdot p(\\bar{p_3}) = 0.6^4 \\cdot (p(p_1) + p(p_2) + p(p_4) + p(p_5)) = 0.6^5 = 0.0777 \\end{align} however, the probabily of never drawing \\(p_1\\) equals: \\begin{align} p(\\bar{p_1}) = 0.9 ^ 5 = 0.59 \\end{align} therefore, the particles with a low importance weight will survive with a much lower rate than the ones with a higher importance weight. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/PIDControl/PIDControl.html",
    "title": "PID Control",
    "body": " index search search back pid control contents smoothing algorithm pid control p controller pd control systematic bias pid control twiddle up until now we have created paths that connect dots with straight lines. however this tend to become unnefficient, as you can smooth them to reduce the distance and also to represent the motion of an object is a more realistic way. smoothing algorithm initilize \\(y_i\\) to the non-smooth locations \\(x_i\\): \\(y_i = x_i\\) optimize two criteria: first minimize the distance between the non-smooth point and the smooth point \\begin{align} \\min_i (x_i - y_i)^2 \\end{align} then minimize the distance between two consecutive smooth points \\begin{align} \\min_i (y_i - y_{i+1})^2 \\end{align} to optimize both, we include a parameter \\(\\alpha\\), to minimize the weigthed sum: \\begin{align} \\min (x_i - y_i)^2 + \\alpha (y_i - y_{i+1})^2 \\end{align} we optimize both, because they are in conflict with each other: if we only optimize the first one, we obtain the same path as the original non-smoothed path if we only optimize the second one, we obtain no path pid control if we have a car that has a steering angle \\(\\alpha\\), how would we go about defining this parameter? p controller we set this angle proportional by some factor of \\(\\tau\\) to the crosstrack error. where the crosstrack error refers to the lateral error between the vehicle and the reference trajectory. thus: \\begin{align} \\alpha = \\tau cte \\end{align} note that with this approach we will eventually overshoot when reaching for the reference trajectory. that is because the car it not oriented the same as the trajectory, therefore it needs to reposition once it reaches the trajectory: pd control here the steering angle does no only take into account the \\(cte\\), but it also uses the derivative of cte. the latter will compute how much we are reducing the error in each moment \\(t\\), and use this value to counter steer this angle (reduce the angle): \\begin{align} \\alpha = - \\tau_p cte - \\tau \\frac{\\delta}{\\delta t} cte \\end{align} where: \\begin{align} \\frac{\\delta}{\\delta t} cte = \\frac{cte_t - cte_{t-1}}{\\delta t} \\end{align} systematic bias in real life there is usually some noise when it comes to the angle of the wheels, and we refer to that as systematic bias. for example the wheels might be deviated a certain angle without us knowing. pid control because of this systematic bias, the error with respect to the reference trajectory is very large. therefore if we sum it over time we obtain larger and larger values. so, if we sum this cte error weighted by a factor \\(\\tau_i\\), we can correct this error by counter steering: \\begin{align} \\alpha = - \\tau_p cte - \\tau_d \\frac{\\delta}{\\delta t} cte - \\tau_i \\sum cte \\end{align} where \\(\\sum cte\\) equals the sum of the \\(cte\\) error overtime. note: \\(- \\tau_p cte\\): represents the proportional error \\(- \\tau_d \\frac{\\delta}{\\delta t}\\): represents the differential error \\(- \\tau_i \\sum cte\\): represents the integral error twiddle we use twiddle to optimize a set of parameters. in our case what we do is optimize, that is minimize, the average cte. so, given a parameters vector \\(p = [0, 0, 0]\\) and a vector of potential changes \\(dp = [1, 1, 1]\\) we: execute run() which computes the \"optimal\" steering angle and moves the robot accordingly. it also stores this motion as a trajectory. this function will return a \"goodness\" metric, that will signify the cte. so, after executing run() we get the best error so far. we modify p to make our error smaller, to make this modification we use twiddle. the algorithm is as follows: # compute initial error best_error = run(p) # while the sum of the potential changes is bigger than a tolerance parameter while sum(dp) < tolerance: # iterate over every parameter for i in range(len(p)): # update the parameter value by the value of the corresponding potential change p[i] += dp[i] # compute the new error for this change err = run(p) # does this better the previous error? err < best_error: # make the change bigger dp[i] *= 1.1 # if the error is worse else: # we try updating the parameter by subtracting (by two because we added before) p[i] -= 2*dp[i] err = run(p) # does this better the previous error? err < best_error: # make the change bigger dp[i] *= 1.1 # if substracting does not make the error better else: # we decrease the change dp[i] *= 0.9 basically twiddle decreases/increases the parameters first a little bit, and for each time we make the error better we augment the increase or decrease. and we stop when there are no major changes being made to the parameters, that is sum(dp) < tolerance. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/index.html",
    "title": "Artificial Intelligence Robotics",
    "body": " index search search back artificial intelligence robotics histogram localization kalman filters particle filters search pid control slam $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/Search/Search.html",
    "title": "Search",
    "body": " index search search back search contents a algorithm dynamic programming the process of finding a path from a starting point to the goal location is called robot motion planning or simply planning. then given: map starting location goal location cost function the goal is to find the minimum cost path between the start and the goal. a* consider a square grid having many obstacles and we are given a starting cell and a target cell. we want to reach the target cell (if possible) from the starting cell as quickly as possible. what a* search algorithm does is that at each step it picks the node according to a value \\(f = g + h(x,y)\\), where \\(g\\) is the current cost and \\(h(x,y)\\) is the value of the heuristic function in cell \\((x,y)\\). that is h is the estimated movement cost to move from that given square on the grid to the final destination. this is often referred to as the heuristic, which is nothing but a kind of smart guess. algorithm 1. let openlist equal empty list of nodes 2. let closedlist equal empty list of nodes 3. put startnode on the openlist (leave it's f at zero) 4. while openlist is not empty 5. let currentnode equal the node with the least f value 6. remove currentnode from the openlist 7. add currentnode to the closedlist 8. if currentnode is the goal 9. you've found the exit! 10. let children of the currentnode equal the adjacent nodes 11. for each child in the children 12. if child is in the closedlist 13. continue to beginning of for loop 14. child.g = currentnode.g + distance b/w child and current 15. child.h = distance from child to end 16. child.f = child.g + child.h 17. if child.position is in the openlist's nodes positions 18. if child.g is higher than the openlist node's g 19. continue to beginning of for loop 20. add the child to the openlist dynamic programming given a grid and a goal position, dynammic programming gives you the optimal action for each cell. where the optimal action is to move to the direction that offers the lower distance to the goal. to compute this distance we calculate: \\begin{align} f(x,y) = g = min_{x',y'} f(x', y') + 1 \\end{align} that is, we obtain recursively the distance of each neighbour to the goal and we add one. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/Localization/Localization.html",
    "title": "Histogram Localization",
    "body": " index search search back histogram localization contents probability given by belief probability after sense normalize the distribution exact motion theorem of total probability inexact motion entropy bayes rule motion using total probability summary belief sense move localization algorithm probability given by belief probability after sense exact motion inexact motion bayes rule motion using total probability summary localization algorithm probability given by belief suppose every place in the world is modeled after an uniform probability distribution, then every cell the robot has the same probability. probability after sense now suppose the robot is able to sense a color, and each cell has a different color assigned. let's assume the robot senses the color red, then the cells with this color assigned should have a higher probability. therefore we define two values, a hit value and a miss value. if the cell is red, then we multiply its probability by the hit value. if the cell is not red, then we multiply its probability by the miss value. note that the hit value is a big value, and the miss value is lower. thus the probability for miss cells is lower than the probability for hit cells. also, observe that a measurement refers to what the robot senses, that is, a green cell or a red cell. normalize the distribution now that we have altered the values of the probability distribution, it is likely that they do not sum up to one, which is a requirement to every probability function. therefore we need to normalize it. so what we would do is: compute the probabilities after the robot \"senses\" a measurement normalize these probabilities by dividing each probability by the total sum of all probabilities exact motion suppose we have a world made up of a grid with 5 cells with the following probabilities: [\\(\\frac{1}{3}\\)][\\(\\frac{1}{3}\\)][\\(\\frac{1}{9}\\)][\\(\\frac{1}{9}\\)][\\(\\frac{1}{3}\\)] we also know that with 100% probability the world moves to the right. theorem of total probability to compute the probability of each cell after the movement, we use the law of total probability that states: given events a and \\(b = \\{b_1, \\cdots, b_n\\}\\) events in a sample space where \\(b\\) is pairwise disjoint, then: \\begin{align} p(a) = \\sum_n p(a \\cap b_n) \\end{align} then, by the conditional probability formula: \\begin{align} p(a) = \\sum_n p(a|b_n)p(b_n) \\end{align} so to apply this theorem what we do is sum the probabilities of ending up in cell \\(j\\) when we come from cell \\(i\\), which is expressed symbolically: \\begin{align} p(x_{j}) = \\sum_{i=1}^5 p(x_{j}|x_{i}) \\cdot p(x_{i}) = 1 \\cdot p({x_j}_{\\{j=(i+1)\\}}) \\end{align} because we know: \\begin{align} p(x_j|x_i) = \\begin{cases} 1, & j = i + 1 \\\\ 0, & \\text{ in any other case} \\end{cases} \\end{align} inexact motion however what if \\(p(x_j|x_i) < 1\\)? suppose: \\(p(x_{i+2}|x_i) = 0.8\\): the robot moved 2 positions/units with \\(0.8\\) probability \\(p(x_{i+1}|x_i) = 0.1\\): the robot moved 1 positions/units with \\(0.1\\) probability \\(p(x_{i+3}|x_i) = 0.1\\): the robot moved 3 positions/units with \\(0.1\\) probability then for each \\(i\\): \\begin{align} p(x_{j}) = \\sum_{i=1}^5 p(x_{j}|x_{i}) \\cdot p(x_{i}) \\end{align} where: \\begin{align} p(x_{j}|x_{i}) = \\begin{cases} 0.8, & j = i + 2 \\\\ 0.1, & j = i + 1 \\\\ 0.1, & j = i + 3 \\\\ 0, & \\text{ otherwhise } \\end{cases} \\end{align} entropy the entropy will decrease after the measurement update (sense) step, and the entropy will increase after the movement step (move). in general, entropy represents the amount of uncertainty in a system. since the measurement update step decreases uncertainty, entropy will decrease. the movement step increases uncertainty, so entropy will increase after this step. the entropy formula for our case is the following: \\begin{align} entropy = \\sum_{i=1}^5(-p(x_i) \\cdot \\log(p(x_i))) \\end{align} bayes rule suppose: \\(x\\) represents the grid cell \\(z\\) represents the measurements then the bayes rule states: \\begin{align} p(x_i|z) = \\frac{p(z|x_i)p(x_i)}{p(z)} \\end{align} where: \\(p(x_i|z)\\) is called the posterior \\(p(z|x_i)\\) is called the likelihood \\(p(z)\\) is known as the evidence or marginal likelihood (that is, it marginalizes \\(z\\)). to compute \\(p(z)\\) we use the theorem of total probability: \\begin{align} p(z) = \\sum_{i=1}^n p(z|x_i)p(x_i) \\end{align} so, to compute \\(p(x_i|z)\\) we follow the steps: for each \\(x_i\\) compute the non-normalized posterior: \\(\\hat{p}(x_i|z) = p(z|x_i)p(x_i)\\) sum all non-normalized posteriors to obtain the evidence: \\(p(z) = \\sum_{i=1}^n \\hat{p}(x_i|z)\\) for each \\(x_i\\) normalize the posterior with the evidence: \\(p(x_i|z) = \\frac{\\hat{p}(x_i|z)}{p(z)}\\) motion using total probability let's say we are at time \\(t\\), and \\(i\\) determines the cell, then the motion is expressed probabilistically as follows: \\begin{align} p(x_i^t) = \\sum_{j} p(x_i|x_j)p(x_j^{t-1}) \\end{align} if we break down this formula: \\(p(x_i|x_j)\\) is the probability that we end up in the cell \\(x_i\\) given we come from the cell \\(x_j\\) \\(p(x_j^{t-1})\\) is the probability of being in cell \\(x_j\\) at the previous time \\(p(x_i^t)\\) is the probability of being in cell \\(x_i\\) at time \\(t\\) summary belief represents where are possible places the robot might be, that is each cell has an associated probability value sense also known as the measurement update function. for each cell we compute the probability that the robot is in that cell, given a measurement sensed by the robot in the moment \\(t\\) (\\(p(x_k|z)\\), where \\(x_k\\) is the cell and \\(z\\) is the measurement). therefore, for each cell in the world we multiply the previous probability value (given by belief) and the probability that the robot moved to the given cell. for example, to satisfy the probability function properties, we need to normalize it, so it sums up to one. move it is a convolution, for each possible location, after the motion, we reverse engineered the situation and guessed where the world might have come from. so what we do is we compute the probability of each cell using the total probability theorem, so given a cell \\(x_k\\), we compute: \\begin{align} p(x_k) = \\sum_{l}p(x_k|x_l)p(x_l) \\end{align} where \\(p(x_k|x_l)\\) is the probabily that the robot moved to cell \\(x_k\\) from cell \\(x_l\\). usually what we do is stablish a motion using a vector (i.e. \\((0,1) \\in \\mathbb{r}^2\\) to indicate the robot moved one unit up in the two dimensional vector space). for example, this probability may refer to how likely it is that the robot moved to the exact cell, how likely it is that the robot moved to a cell \"beyond\" the goal or how likely it is that the robot moved to a cell that lies \"before\" the goal. so if we have these three probabilities, for each cell we sum the probabilities of the robot being in that cell taking into account the three scenarions: if the robot moved to cell \\(x_k\\) from cell \\(x_l\\), and that cell was the goal, then it moved to that cell with probability \\(p_{exact}\\) and maybe the robot moved to cell \\(x_k\\) from cell \\(x_i\\), however the goal was \\(x_{k+1}\\), then it moved to that cell with probability probability \\(p_{undershoot}\\) maybe the robot moved to cell \\(x_k\\) from cell \\(x_j\\), however the goal was \\(x_{k-1}\\), then it moved to that cell with probability probability \\(p_{overshoot}\\) suppose now that then only cells in the world are mentioned: \\(x_k, x_l, x_i, x_j\\). then for \\(x_k\\) we update the belief as follows: \\begin{align} p(x_k) = p(x_l) * p_{exact} + p(x_i) * p_{undershoot} + p(x_j) * p_{overshoot} \\end{align} localization algorithm next we lay out an example of the localization algorithm implemented in \\(\\mathbb{r}^2\\): # the function localize takes the following arguments: # # colors: # 2d list, each entry either 'r' (for red cell) or 'g' (for green cell) # # measurements: # list of measurements taken by the robot, each entry either 'r' or 'g' # # motions: # list of actions taken by the robot, each entry of the form [dy,dx], # where dx refers to the change in the x-direction (positive meaning # movement to the right) and dy refers to the change in the y-direction # (positive meaning movement downward) # note: the *first* coordinate is change in y; the *second* coordinate is # change in x # # sensor_right: # float between 0 and 1, giving the probability that any given # measurement is correct; the probability that the measurement is # incorrect is 1-sensor_right # # p_move: # float between 0 and 1, giving the probability that any given movement # command takes place; the probability that the movement command fails # (and the robot remains still) is 1-p_move; the robot will not overshoot # its destination in this exercise # # the function should return (not just show or print) a 2d list (of the same # dimensions as colors) that gives the probabilities that the robot occupies # each cell in the world. # # compute the probabilities by assuming the robot initially has a uniform # probability of being in any cell. # # also assume that at each step, the robot: # 1) first makes a movement, # 2) then takes a measurement. # # motion: # [0,0] - stay # [0,1] - right # [0,-1] - left # [1,0] - down # [-1,0] - up # compute the probability of \"hit\" cell and the probability of a \"miss\" cell # # :param float z value sensed by the robot (i.e. 'r' or 'g') # :param float cell_measurement value in the cell (i.e. 'r' or 'g') # :param float sensor_right probability that what the robot sensed is correct # # if the value sensed and the value in the cell are equal hit = 1 and miss = 0 # :return [sensor_right, 0] # otherwhise # :return [0, (1-sensor_right)] def probability_hit_miss(z, cell_measurement, sensor_right): hit = (z == cell_measurement) return [hit * sensor_right, (1-hit) * (1-sensor_right)] # compute the probability of a cell after the measurement of the robot # # :param cell_prior probability stored in the cell before measurement # :param float z value sensed by the robot (i.e. 'r' or 'g') # :param float cell_measurement value in the cell (i.e. 'r' or 'g') # :param float sensor_right probability that what the robot sensed is correct # # if the value sensed and the value in the cell are equal hit = sensor_right, else miss = (1-sensor_right) # :return the probability before measurement multiplied by the probability that the measurement is correct for the # given cell def probability_cell_given_measurement(cell_prior, z, cell_measurement, sensor_right): [hit, miss] = probability_hit_miss(z, cell_measurement, sensor_right) return cell_prior * (hit + miss) # for each cell x_k compute the probability that the robot is in the cell x_k given a measurement z # # :param list world measurements in the world # :param list p current world probabilities # :param list z current measurement of the robot # :param float sensor_right probability that the robot's measurement is correct # # for each cell x_k, where k is the cell [i][j]: # compute unnormalized p(x_k|z) = p(z|x_k) * p(x_k) # where p(x_k) = p[i][k] and # p(z|x_k) is computed in probability_cell_given_measurement and equals: # - sensor_right, if measurement in x_k = z # - (1-sensor_right), if measurement in x_k != z # compute the sum over all p(x_k|z), this sum equals p(z). # obtain normalized p(x_k|z) by dividing each p(x_k|z) by p(z) # :return list q of cell probabilies after measurement update def sense(world, p, z, sensor_right): q=[] # obtain probabilities q = [[ probability_cell_given_measurement(p[i][j], z, world[i][j], sensor_right) for j in range(len(p[0]))] for i in range(len(p))] # sum all probabilities s = sum([sum(row) for row in q]) ## normalize q = [[q[i][j]/s for j in range(len(p[0]))] for i in range(len(p))] return q # obtain probabilities of each cell in the grid after the robot moves # # :param list p current world probabilities # :param list u description of the motion (i.e. [0,1] to move to the right) # :param float p_move probability of moving from one cell to another # # for each cell x_k: # p(x_k) = sum over l=1...m of p(x_k|x_l) * p(x_l) # where # - p(x_k|x_l) = p_move and p(x_l) = p[(i-y) % len(p)][(j-x) % len(p[0])] if the robot moves from cell l = [(i-y)][(j-x)] to cell k = [i][j] # - p(x_k|x_l) = (1- p_move) and p(x_l) = p[i][j] if the robot does not move from cell k = [i][j] # :return list q of cell probabilies after the robot moves def move(p, u, p_move): q = [] [y, x] = u q = [[p_move * p[(i-y) % len(p)][(j-x) % len(p[0])] + (1-p_move) * p[i][j] for j in range(len(p[0]))] for i in range(len(p))] return q # for each pair of motion-measurement, update the grid probabilities of probabilities that represents where the robot is in any given moment # # :param matrix colors grid of measurements # :param list measurements measurements sensed by the robot # :param list motions directions in which the robot moved at each moment (i.e. for [0,1] it moves to the right) # :param float sensor_right probability that the robot's measurement is correct # :param float p_move probability of moving from one cell to another # # :return list q of cell probabilies after finishing updating for every measurement-motion def localize(colors,measurements,motions,sensor_right,p_move): # initializes p to a uniform distribution over a grid of the same dimensions as colors pinit = 1.0 / float(len(colors)) / float(len(colors[0])) p = [[pinit for row in range(len(colors[0]))] for col in range(len(colors))] # update probabilities iteratively for k in range((len(measurements))): p=move(p, motions[k], p_move) p=sense(colors, p, measurements[k], sensor_right) return p so for example, for the following data: colors = [['r','g','g','r','r'], ['r','r','g','r','r'], ['r','r','g','g','r'], ['r','r','r','r','r']] measurements = ['g','g','g','g','g'] motions = [[0,0],[0,1],[1,0],[1,0],[0,1]] where: the robot does not move (\\([0,0]\\)) and senses a green cell 'g'. the robot moves down (\\([0,1]\\)) and senses a green cell 'g'. the robot moves right (\\([1,0]\\)) and senses a green cell 'g'. the robot moves right (\\([1,0]\\)) and senses a green cell 'g'. the robot moves down (\\([0,1]\\)) and senses a green cell 'g'. and the colors is the representation of the world. then, we apply the localization algorithm to obtain the probability distribution that the robot is in each cell: p = localize(colors,measurements,motions,sensor_right = 0.7, p_move = 0.8) show(p) note that the probabily that the robot sensed the measurement correctly (\\(p(z|x_i)\\)) is \\(0.7\\) and the probability that the robot moved to the cell given by the motion vector (\\(p(x_i|x_j)\\)) is \\(0.8\\). this outputs: [[0.01106,0.02464,0.06800,0.04472,0.02465], [0.00715,0.01017,0.08697,0.07988,0.00935], [0.00740,0.00894,0.11273,0.35351,0.04066], [0.00911,0.00715,0.01435,0.04313,0.03643]] where each element in the matrix is the probabily of a cell. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/KalmanFilters/Examples.html",
    "title": "Examples",
    "body": " index search search back examples contents design kalman filters for 2d 4d example motion measurement code example design kalman filters for 2d to design a kalman filter in two dimensions (position, velocity) you need two things: a state transition function, which is usually a matrix \\(f\\): \\begin{align} \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\leftarrow f \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\end{align} a measurement function, represented by the matrix \\(h\\): \\begin{align} z \\leftarrow h \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\end{align} for example, suppose we update the location and the velocity as follows: \\begin{align} x' = x + \\hat{x} \\end{align} \\begin{align} \\hat{x}' = \\hat{x} \\end{align} then the transition function is represented as the following matrix: \\begin{align} f = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ \\end{bmatrix} \\end{align} and for the measurement function, we only observe the location not the velocity, therefore: \\begin{align} h = \\begin{bmatrix} 1 & 0 \\\\ \\end{bmatrix} \\end{align} 4d example motion given a state \\((x, y, \\hat{x}, \\hat{y})\\), where \\((x, y)\\) is the position and \\((\\hat{x}, \\hat{y})\\) is the velocity.if in each iteration the motion update for the state is: \\begin{align} \\begin{matrix} x + dt\\cdot \\hat{x} \\\\ y + dt\\cdot \\hat{y} \\\\ \\hat{x} \\\\ \\hat{y} \\\\ \\end{matrix} \\end{align} so the position moves with time and the velocity does not change with time. then the state transition function is represented by the following matrix: \\begin{align} f = \\begin{bmatrix} 1 & 0 & dt & 0\\\\ 0 & 1 & 0 & dt\\\\ 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 1\\\\ \\end{bmatrix} \\end{align} measurement and, because we can only measure the position the measurement update is of the form: \\begin{align} z \\leftarrow \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} \\leftarrow h \\cdot \\begin{bmatrix} x \\\\ y \\\\ \\hat{x} \\\\ \\hat{y} \\\\ \\end{bmatrix} \\end{align} therefore the measurement function is represented as follows: \\begin{align} h = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ \\end{bmatrix} \\end{align} code example so for the following data, which deals with 4d data, that is we store a 2d location and a 2d velocity vector in the state. we will have to take this into account in the different update matrices and uncertainty matrix: # location measurements measurements = [[5., 10.], [6., 8.], [7., 6.], [8., 4.], [9., 2.], [10., 0.]] # initial location initial_xy = [4., 12.] dt = 0.1 x = matrix([[initial_xy[0]], [initial_xy[1]], [0.], [0.]]) # initial state (location and velocity) u = matrix([[0.], [0.], [0.], [0.]]) # external motion # initial uncertainty: 0 for positions x and y, 1000 for the two velocities # p = 0 0 0 0 # 0 0 0 0 # 0 0 1000 0 # 0 0 0 1000 p = matrix([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 1000., 0.], [0., 0., 0., 1000.]]) # next state function: generalize the 2d version to 4d # f = 1 0 0.1 0 # 0 1 0 0.1 # 0 0 1 0 # 0 0 0 1 # so, velocity vector (x', y') does not change, and the position vector (x, y) is updated according to the velocity and dt # f Â· x = x + 0.1x' # y + 0.1y' # x' # y' f = matrix([[1., 0., dt, 0], [0, 1., 0, dt], [0, 0, 1., 0], [0, 0, 0, 1.]]) # measurement function: reflect the fact that we observe x and y but not the two velocities # h = 1 0 0 0 # 0 1 0 0 # so, for the measurement we only take into account the position vector (x,y) and not the velocity # z = h Â· x = x # y h = matrix([[1., 0., 0., 0.], [0., 1., 0., 0.]]) # measurement uncertainty: use 2x2 matrix with 0.1 as main diagonal # r = 0.1 0 # 0 0.1 r = matrix([[.1, 0.], [0., .1]]) # 4d identity matrix # i = 1 0 0 0 # 0 1 0 0 # 0 0 1 0 # 0 0 0 1 i = matrix([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) and we execute the filter: filter(x, p) to obtain the following state \\(x\\) and uncertainty matrix \\(p\\): x= [9.999340731787717] [0.001318536424568617] [9.998901219646193] [-19.997802439292386] p= [0.03955609273706198, 0.0, 0.06592682122843721, 0.0] [0.0, 0.03955609273706198, 0.0, 0.06592682122843721] [0.06592682122843718, 0.0, 0.10987803538073201, 0.0] [0.0, 0.06592682122843718, 0.0, 0.10987803538073201] $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/KalmanFilters/Kalman Filters.html",
    "title": "Kalman Filters",
    "body": " index search search back kalman filters contents markov model gaussian distribution measurement and motion motion step motion noise measurement step updating the mean updating the variance measurement noise states predicting velocity high dimensional spaces put everything together motion measurement iterative process algorithm prediction measurement update code this is a tracking technique. it is similar to the histogram localization we talked about previously, however there are some key differences: kalman filter maintains a continuous state (therefore uses a uni-modal distribution: probability density function only has one peak) histogram localization uses discrete state to represent the world (uses a multi-modal distribution: probability density function has multiple peaks) markov model in histogram localization we assigned a probability to each cell in the world: [\\(0.2\\)][\\(0.1\\)][\\(0.5\\)][\\(0.1\\)][\\(0.2\\)] what we did is we divided the continuous space into a finite number of cells, that approximates the posterior distribution (which is continuous: red line) by a histogram (blue bars) over the original distribution. however in kalman filters this distribution is given by a gaussian distribution. gaussian distribution a gaussian distribution is a continuous function which is described in \\(\\mathbb{r}\\) by the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). the formula is the following: \\begin{align} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{-\\frac{1}{2}\\frac{(x-\\mu)}{\\sigma^2}} \\end{align} where \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) is a constant that normalizes \\(\\exp^{-\\frac{1}{2}\\frac{(x-\\mu)}{\\sigma^2}}\\) remark the bigger the covariance \\(\\sigma^2\\) the wider the distribution, which means we are less certain of the state. if it is narrow, that means we are confident about our location. measurement and motion like with localization kalman filters operate within a cycle, that is, it iterates like so: motion (or prediction): where we predict the position of the car based on data we know. to perform this prediction we sum the location distribution and the distribution that describes the motion. measurement (or measurement update/correction): where we combine the prediction with the measurement made by the sensors. to perform this update we multiply the motion prediction with the distribution that describes the measurement. this is illustrated in the following image: here the predicted state estimate corresponds to the motion step, and the measurement distribution corresponds to the measurement. the result of the product between the two eausl the optimal state estimate. motion step suppose, at moment \\(t\\), your location is represented as follows: where: the blue gaussian distribution represents your best guess (prediction) at where your are at \\(t\\), and is characterized by \\((\\mu, \\sigma^2)\\) the green gaussian distribution represents the motion of \\(\\nu\\) units, which has its own uncertainty, and is characterized by \\((\\nu, r^2)\\) the red gaussian distribution represents you location at time \\(t+1\\) after the motion where this last distribution equals the sum of the other two distributions and is characterized by: \\begin{align} \\hat{\\mu} = \\mu + \\nu \\end{align} \\begin{align} \\hat{\\sigma}^2 = \\sigma^2 + r^2 \\end{align} so, basically the mean is shifted \\(\\nu\\) units and the covariance is made larger by summing \\(\\sigma^2\\) and \\(r^2\\), as a result of summing the distributions. motion noise note that the movement may not be certain, that is why we introduce some gaussian noise. this noise is drawn from a normal distribution where the variance is given by a co-variance matrix \\(q\\) (this matrix describes the uncertainty for the initial state). we define this noise as follows: \\begin{align} u \\sim \\mathcal{n}(0, q) \\end{align} measurement step as we have already said, the update is the result of multiplying the prediction distribution (after motion step), which was characterized in the previous section, by the measurement distribution. this would mean \"creating\" a new distribution that models the robot's current state. we now show how the multiplication of two distributions is performed: updating the mean we are going to show how to the mean is computed when multiplying two distributions. suppose the prior distribution is as follows: where the covariance is very large, so we are very uncertain about a location. and we recieve a measurement of the form: which is much more certain about the location. then the mean will shift accordingly (green line): updating the variance we are going to show how to the variance is computed when multiplying two distributions. so, after multiplying the prior and the measurement shown previously, the resulting gaussian y more certain than both of the prior and the measurement gaussians. that is the covariance of this new gaussian is smaller, so the more measurements we have the more certain the are. why does this happen? well, given these two distributions: where the first distribution is characterized by \\((\\mu, \\sigma^2)\\) and the second distribution is characterized by \\((\\nu, r^2)\\). the product of the two is a distribution characterized by \\((\\hat{\\mu}, \\hat{\\sigma}^2)\\), computed as follows: \\begin{align} \\hat{\\mu} = \\frac{r^2\\mu + \\sigma^2\\nu}{r^2 + \\sigma^2} \\end{align} observe, because \\(\\sigma^2 >> r^2\\) in our example, then \\(\\hat{\\mu}\\) will be closer to the second distribution's mean \\(\\nu\\). also: \\begin{align} \\hat{\\sigma}^2 = \\frac{1}{\\frac{1}{r^2} + \\frac{1}{\\sigma^2}} = \\frac{\\sigma^2 r^2}{\\sigma^2 + r^2} \\end{align} thus, the updated covariance is not affected by the means and will always be smaller than \\(\\sigma^2\\) and \\(r^2\\). we illustrate this is the following image, where the updated distribution is the one drawn in blue: note that the wider distribution represents the prior, the measurement represents the likelihood and the updated distribution represents the posterior. measurement noise however, note that the measurement might also be noisy. so we again introduce gaussian noise \\(v\\) that is modeled after a normal distribution with known variance. that is \\(v \\sim \\mathcal{n}(0, r)\\). this indicates how much we trust the measurements provided by the sensors. this variable is called measurement noise covariance matrix states kalman filters are made up from what it's called states, and we differentiate two different kinds of states: observables (in our case the location) hidden (in our case the velocity, which i can never observe) these two types of states interact with each other in the sense that a sequence of observable variables gives us information about the hidden variables. thus we can estimate what these hidden variables are. applied to our case scenario, multiple observations of where we are, that is, our location, we can estimate how fast we are moving, that is, our velocity. predicting velocity given the following graph: where \\(\\hat{x}\\) represents the velocity and \\(x\\) represents the location. in this first instance, we represent the measurement at with an elongated gaussian because the measurement does not tell us anything about the velocity. however, if we now draw our predicition, given by our motion model which is represented by the red gaussian distribution, we obtain: suppose we take a new measurement (a second observation) represeted by the green normal distribution (remember, it tells us nothing about the velocity), it only gives us information about the location as the first observation did. then: multiply the prior (the red gaussian) and the measurement (the green gaussian) to obtain a really good estimate of an object's velocity and location (black distribution): so we were able to infer the velocity by only observing the location. high dimensional spaces up until now we have generally been operating in a one dimensional space, however if we were to work withing higher dimensional spaces we would need to make use of multivariate gaussians. so a multivariate gaussian in a d-dimensional space is characterized as follows: \\begin{align} \\mu = \\begin{bmatrix} \\mu_0 \\\\ \\vdots \\\\ \\mu_d \\\\ \\end{bmatrix}, \\sigma = \\begin{bmatrix} \\sigma_{11} & \\cdots & \\sigma_{1d}\\\\ \\vdots \\\\ \\sigma_{d1} & \\cdots & \\sigma_{dd}\\\\ \\end{bmatrix} \\end{align} also de density function is now, for \\(x \\in \\mathbb{r}^d\\): \\begin{align} f(x) = (2\\pi)^{-\\frac{d}{2}}|\\sigma|^{-\\frac{1}{2}} \\exp^{-\\frac{1}{2}(x - \\mu)^t\\sigma^{-1}(x-\\mu)} \\end{align} here are some examples of how the kalman filter works for spaces with higher dimension: put everything together motion at a given time \\(k-1\\), we have the following prediction: we use a motion model (in our case a gaussian that represents the movement) to update our prediction as follows: the motion model is described as follows: \\begin{align} x_k = f_{k-1}x_{k-1} + u_{k-1} \\end{align} where: \\(f_{k-1}\\) represents the transition function at time \\(k-1\\) \\(u_{k-1}\\) represents the noise at time \\(k-1\\) measurement then, we use the following observation model: we correct our prediction with this observation model as follows: the measurement model is described as follows: \\begin{align} y_k = h_k x_k + v_k \\end{align} where: \\(h_k\\) is the measurement function at time \\(k\\). this function maps the state into the observable state, that does not have to be the same (refer to states) \\(v_k\\) is the noise at time \\(k\\) iterative process first we make a prediction as to where the robot is at time \\(k\\): \\begin{align} \\check{x}_k = f_{k-1}x_{k-1} \\end{align} \\begin{align} \\check{p}_k = f_{k-1}\\hat{p}_{k-1}f_{k-1}^t + q_{k-1} \\end{align} then we compute the optimal gain \\(k\\) as follows: \\begin{align} k_k = \\check{p}_kh_k^t(h_k\\check{p}_kh^t+r_k)^{-1} \\end{align} this gain basically represents how much we trust our motion estimation versus our measurement estimation. finally we obtain the correction using the measurement model: shift the mean: \\begin{align} \\hat{x}_k = \\check{x}_k + k_k(y_k - h_k\\check{x}_k) \\end{align} where \\(y_k - h_k\\check{x}_k\\) represents the difference between the measurement and the prediction we made. lastly, we update the covariance of our motion model: update the variance: \\begin{align} \\hat{p}_k = (1-k_kh_k)\\check{p}_k \\end{align} algorithm so in the kalman filter cycle what we do is: first we perform the prediction and the correction or measurement update. more concretely: prediction we apply the same formulas we defined in motion model and iterative process to make a prediction: \\begin{align} x = fx + u \\end{align} \\begin{align} p = f \\cdot p \\cdot f^t \\end{align} measurement update now, for the correction: first we compute the intermediate \\(s\\) matrix, which equals the second part of the formula for \\(k\\), \\(h_k\\check{p}_kh^t+r_k\\): \\begin{align} s = h \\cdot p \\cdot h^t + r \\end{align} then we compute the kalman gain (as we defined in iterative process): \\begin{align} k = ph^ts^{-1} \\end{align} obtain difference between measurement (\\(z = y_k\\)) and our prediction \\(h \\cdot x = h_k\\check{x}_k\\) (note where \\(\\check{x}_k\\) comes from, \\(h\\) is usually a matrix that selects a concrete part of the kalman state like the position. see examples): \\begin{align} y = z - h \\cdot x \\end{align} finally obtain the correction: \\begin{align} x' = x + (k \\cdot y) \\end{align} \\begin{align} p' = (i- k\\cdot h) \\cdot p \\end{align} code the filter algorithm follows the same steps laid out in the previous section: def filter(x, p): for n in range(len(measurements)): # prediction x = (f * x) + u p = f * p * f.transpose() # measurement update z = matrix([measurements[n]]) y = z.transpose() - (h * x) s = h * p * h.transpose() + r k = p * h.transpose() * s.inverse() x = x + (k * y) p = (i - (k * h)) * p return x, p $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/AIRobotics/SLAM/SLAM.html",
    "title": "SLAM",
    "body": " index search search back slam contents graph slam constraint matrix noise how to create maps and localize a robot at the same time? for this we use a technique known as slam: simultaneous localization and mapping graph slam suppose we have a robot whose initial position is \\(x_0 = 0\\) and \\(y_0 = 0\\) at time \\(0\\), then at time \\(1\\) (because of how we mode our motion) the robot is at \\(x_1 = x_0 + 10\\) and \\(y_1 = y_0\\). however we know that our location is uncertain therefore the position at time \\(1\\) is really described by a gaussian distribution centered around \\(10\\) and with a given variance that signifies how certain we are about our position. so to express this with a gaussian, that we do is define a distribution whose pdf peaks when \\(x_1 = x_0 + 10\\) and \\(y_1 = y_0\\), therefore we would like to maximize both the following equations: \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\end{align} \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(y_1 - y_0)^2}{\\sigma^2}} \\end{align} here if \\(x_1 = x_0 + 10\\), then \\(x_1 - x_0 - 10 = 0\\) and if \\(y_1 = y_0\\) ,then \\(y_1 - y_0 = 0\\). these conditions we define are called constraints, so what graph slam does is creating our probabilities defining a sequence of these constraints. suppose we have a robot that has followed the following path: where each \\(x_i\\) is a vector (usually a three dimensional vector) then graph slam defines the following constraints: initial position constraint: \\(x_0\\) relative motion constraints: \\((x-1 - x_0)\\), \\((x-2 - x_1)\\), \\((x-3 - x_2)\\) (indicated by the red lines). ideally these are the same as the robot motion (direction vector), however in reality it tends to bend to accommodate the map. relative measurement constraints: these are the segment between each position vector (not necessarily every position vector) and each landmark defined in the map, and are also captured by gaussian distributions. in our case \\(z_0, z_1, z_2, z_3\\), the lines colored in green. after we have collected these constraints, what the algorithm does is it relaxes the position vectors \\(x_i\\) to find the most likely configuration of robot path for the given landmarks (that is measurements of distance to the landmark). constraint matrix to define our constraints, suppose we have 3 position vectors \\(x_0, x_1, x_2\\) and two landmarks \\(l_0, l_1\\), then we define the following matrix: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} we denote this structure as follows: \\begin{align} \\omega = \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\end{align} and: \\begin{align} \\xi = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix} \\end{align} suppose \\(x_0\\) moves to \\(x_1\\) by moving \\(5\\) units to the right, that is \\(x_1 = x_0 + 5\\), then we define this constrain in the matrix as follows: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 5.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} because given the initial constraint \\(x_1 = x_0 + 5\\), if we move around the \\(x_i\\) we get: \\begin{align} x_0 - x_1 = -5 \\end{align} \\begin{align} x_1 - x_0 = 5 \\end{align} now we add another constraint \\(x2 = x_1 - 4\\), therefore: \\begin{align} x_2 - x_1 = -4 \\end{align} \\begin{align} x_1 - x_2 = 4 \\end{align} so the constraint matrix is updated to: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 2.0 & -1.0 & 0.0 & 0.0 \\\\ 0.0 & -1.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 9.0 \\\\ -4.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} we add a relative measurement constraint like \\(l_0 - x_1 = 9\\), therefore: \\begin{align} l_0 - x_1 = 9 \\end{align} \\begin{align} x_1 - l_0 = -9 \\end{align} so the constraint matrix is updated to: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 3.0 & -1.0 & -1.0 & 0.0 \\\\ 0.0 & -1.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & -1.0 & 0.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 0.0 \\\\ -4.0 \\\\ 9.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} note that whenever we add a constraint to two parameters, let's say \\(x_1\\) and \\(x_2\\), we sum one to the diagonal element of the matrix corresponding to \\(x_1\\) and \\(x_2\\). noise given the following motion: we know that the localization of our robot is not an exact value, but is is modeled after a gaussian distribution, so \\(x_1 \\sim \\mathcal{n}(\\mu_{x_1}, \\sigma_{x_1})\\) and \\(x_2 \\sim \\mathcal{n}(\\mu_{x_2}, \\sigma_{x_2})\\). suppose \\(\\sigma_{x_1} = \\sigma_{x_2}\\). then we want to maximize the expected value, which is given by the expressions: \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\end{align} \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} to maximize both expressions means to maximize their product: \\begin{align} \\max_{x_0, x_1, x_2} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} we also know that constants are irrelevant during maximization: \\begin{align} \\max_{x_0, x_1, x_2} \\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} this maximization is equivalent to the maximization of its logarithm: \\begin{align} \\max_{x_0, x_1, x_2} \\log \\left(\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}}\\right) \\end{align} because the logarithm of a product equal the sum of logarithms: \\begin{align} \\max_{x_0, x_1, x_2} \\left(\\log \\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}}\\right) + \\left(\\log \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}}\\right) \\end{align} given \\(\\log \\exp (x) = x\\): \\begin{align} \\max_{x_0, x_1, x_2} \\left(-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}\\right) + \\left(-\\frac{1}{2}\\frac{(x_2 - x_1 -50)^2}{\\sigma^2}\\right) \\end{align} again, constants are irrelevant: \\begin{align} \\max_{x_0, x_1, x_2} \\left(\\frac{(x_1-x_0-10)^2}{\\sigma^2}\\right) + \\left(\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}\\right) \\end{align} so, we end up with equations of the form: \\begin{align} \\frac{1}{\\sigma} x_1 - \\frac{1}{\\sigma} x_0 = \\frac{10}{\\sigma} \\end{align} where now \\(\\sigma\\) symbolizes how confident you are in your location/measurement. usually we define a \\(\\sigma\\) for the location and another \\(\\sigma_{measurement}\\) for the measurement (distance to the landmark). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/Models/Motion Model.html",
    "title": "Motion Model",
    "body": " index search search back motion model contents introduction recursive bayes filter typical motion models reasons for motion errors of wheeled robots odometry motion model introduction the motion of the system will always contain uncertainty, because it does not move perfectly with the command it is given. for example, suppose the robot follows the path illustrated in the next image: however the internal estimate of the system yields the following result: showing that it has a tendency to drift to the right. recursive bayes filter as you may recall from bayes filter our belief at time \\(t\\) was defined as follows: \\[ bel(x_t) = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] and we said we used our motion model to estimate the next state of the system \\(\\overline{x_t}\\): \\[ bel(\\overline{x}_t) = \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] what we are doing here is marginilizing \\(x_t\\) so that we know the probability of being at state \\(x_t\\) given previous states \\(x_{t-1}\\) and control command \\(u_t\\). our motion model specifies a posterior probability, that asks what is the probability of the state being \\(x_t\\) given we were at state \\(x_{t-1}\\) and carried the command \\(u_t\\). typical motion models odometry-based models: we use the measurements (odometry) of the robot about how it moved as a command velocity-based models: we simply tell the system to move at a given velocity. reasons for motion errors of wheeled robots some errors that cause wrong movement estimations are the following: for the three different causes there are physical factors that make our robot move differently that what we expect it to for the given control command. odometry motion model suppose a motion takes place, where the initial point is described as \\((\\overline{x}, \\overline{y}, \\overline{\\theta})\\) and the final point is \\((\\overline{x}', \\overline{y}', \\overline{\\theta}')\\) our odometry information is given by \\(u = (\\delta_{rot1}, \\delta_{trans}, \\delta_{rot2})\\), where \\(\\delta_trans\\) is the distance between the two points, \\(\\delta_{rot1}\\) is the rotation on the first point and \\(\\delta_{rot2}\\) is the rotation on the second point. all of them are defined as follows: \\[ \\delta_{trans} = \\sqrt{(\\overline{x}' - \\overline{x})^2 + (\\overline{y}' - \\overline{y})^2} \\] \\[ \\delta_{rot1} = atan2(\\overline{y}' - \\overline{y}, \\overline{x}' - \\overline{x}) - \\overline{\\theta} \\] \\[ \\delta_{rot2} = \\overline{\\theta}' - \\overline{\\theta} - \\delta_{rot1} \\] each part is illustrated in the following image: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/OccpancyGridMaps/Occpancy Grid Maps.html",
    "title": "Occupancy Grid Maps",
    "body": " index search search back occupancy grid maps contents inverse sensor model for laser range finders introduction features vs volumetric maps description of the mapping task grid maps assumptions occupancy probability notation joint distribution estimating a map from data static state binary bayes filter odds ratio log odds notation algorithm inverse sensor model for laser range finders example introduction description of the mapping task grid maps assumptions occupancy probability joint distribution estimating a map from data static state binary bayes filter odds ratio log odds ration algorithm inverse sensor model for laser range finders introduction occupancy grid maps store information about the environment regarding which parts of the map are occupied and which are free. features vs volumetric maps feature map representations store where on the environment certain points or landmarks lay that the systems uses in order to estimate where it is. volumetric maps are most typically used to store free space. description of the mapping task the goal to obtain a map of a given environment is to compute the most likely map given the sensor data: \\[ m^* = \\arg \\max_m p(m|u_1,z_1, \\cdots, u_t,z_t) \\] however, we will simplify this problem by assuming we already know the poses for certain, thus we swap the control commands \\(u_t\\) for poses \\(x_t\\): \\[ m^* = \\arg \\max_m p(m|x_1,z_1, \\cdots, x_t,z_t) \\] grid maps grid maps discretize the environment by dividing it into a finite number of cells, which encode information about its occupation. that is, a cell is either free or occupied. grids are rigid structures, where cells are distributed uniformly along the grid and represent a definite space. generally we describe cells as pixels. assumptions for each cell, the area corresponding to the cell are completely free or occupied. every cell can be described with a binary random variable that models the occupancy: the world is static. the cells are independent of each other. which means: if i know the occupancy state of a given cell, it does not help me estimate the occupancy state of another. occupancy probability as we have said, each cell is a binary random variable that models the occupancy, that is: if we are certain a cell \\(m_i\\) is occupied: \\(p(m_i) = 1\\) if we are certain a cell \\(m_i\\) is free: \\(p(m_i) = 0\\) if we have no knowledge about the cell \\(m_i\\): \\(p(m_i) = 0.5\\) notation the probability of a cell \\(m_i\\) being occupied is expressed as follows: \\[ p(m_i = occ) = p_{occ}(m_i) = p(m_i) \\] the probability of it being free is given by: \\[ p(m_i = free) = p_{free}(m_i) = 1- p_{occ}(m_i) = p(\\neg m_i) \\] also, the shading in the map tells us how certain we are about \\(p(m_i)\\), that is the more intense the shade the higher the probability. joint distribution the map is described by a probability distribution defined as the joint belief of each cell in the map: \\[ p(m) = p(m_1, m_2, \\cdots, m_n) \\] to simplify this distribution we exploit one of the assumptions made before, that said cells were independent of each other, thus: \\[ p(m) = \\prod_i p(m_i) \\] estimating a map from data our goal is to estimate the map given the sensor data \\(z_{1:t}\\) and the poses \\(x_{1:t}\\), that is: \\[ p(m | z_{1:t}, x_{1:t}) = \\prod_i p(m_i|z_{1:t}, x_{1:t}) \\] in order to do this we use a variant of the bayes filter called binary bayes filter that is optimized for binary random variables (\\(m_i\\)). static state binary bayes filter so, for each cell in the environment \\(m_i\\) we compute: \\[ p(m_i|z_{1:t}, x_{1:t}) \\] we apply the bayes rule to swap \\(m_i\\) for \\(z_t\\), therefore: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, z_{1:t-1}, x_{1:t})p(m_i, z_{1:t-1}, x_{1:t})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we apply markov's assumption and assume independence between \\(z_t\\) and the previous observations \\(z_{1:t-1}\\) and poses \\(x_{1:t-1}\\), therefore: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, x_t)p(m_i, z_{1:t-1}, x_{1:t})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we also make use of the markov's assumption to discard future poses when we the most up to date observation is of \\(t-1\\): \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we apply bayes rule again over \\(p(z_t|m_i, x_t)\\) to swap \\(z_t\\) and \\(m_i\\) again: \\[ p(z_t|m_i, x_t) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t)}{p(m_i|x_t)} \\] we plug this into the previous expression: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i|x_t) p(z_t | z_{1:t-1}, x_{1:t})} \\] we assume that \\(p(mi|x_t) \\approx p(mi)\\), because knowing for certain the current pose tells us nothing about the state of the cell: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i) p(z_t | z_{1:t-1}, x_{1:t})} \\] we compute this same derivation for the complement of \\(m_i\\): \\[ p(\\neg m_i|z_{1:t}, x_{1:t}) = \\frac{p(\\neg m_i|z_t, x_t) p(z_t|x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i) p(z_t | z_{1:t-1}, x_{1:t})} \\] what we are going to do is compute the ratio between these two expressions: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{\\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i) p(z_t | z_{1:t-1}, x_{1:t})}}{\\frac{p(\\neg m_i|z_t, x_t) p(z_t|x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i) p(z_t | z_{1:t-1}, x_{1:t})}} \\] now, all of the terms that do not depend on \\(m_i\\) can be discarded: \\(p(z_t|x_t)\\) and \\(p(z_t | z_{1:t-1}, x_{1:t})\\), then \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{\\frac{p(m_i|z_t, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i)}}{\\frac{p(\\neg m_i|z_t, x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i)}} \\] we reorganize the expression: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})p(\\neg m_i)}{p(\\neg m_i|z_t, x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})p(m_i)} \\] \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{p(\\neg m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i, z_{1:t-1}, x_{1:t-1})} \\frac{p(\\neg m_i)}{ p(m_i)} \\] we express \\(\\neg m_i\\) in terms of \\(m_i\\): \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{ 1- p(m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{1-p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{1-p(m_i)}{ p(m_i)} \\] note that now we have three expressions: the first one uses the observation \\(z_t\\) the second one is a recursive term the third one describes our prior knowledge about the state of the cell without any other information about the environment. odds ratio what we do now is turn this ratio called odds ratio into the probability as follows: \\[ odds(x) = \\frac{p(x)}{1-p(x)} \\] we multiply by \\(1-p(x)\\) in both sides. \\[ odds(x)(1-p(x)) = p(x) \\] we expand the left hand side expression: \\[ odds(x)-odds(x)p(x) = p(x) \\] we add \\(odds(x)p(x)\\) to both sides: \\[ odds(x) = p(x) + odds(x)p(x) \\] we extract \\(p(x)\\) as a common factor on the right hand side: \\[ odds(x) = p(x) (1 + odds(x)) \\] we divide by \\((1 + odds(x))\\) on both sides: \\[ \\frac{odds(x)}{(1 + odds(x))} = p(x) \\] and finally: \\[ p(x) = \\frac{1}{\\left(1 + \\frac{1}{odds(x)} \\right)} \\] so by using: \\[ p(x) = [1 + odds(x)^{-1}]^{-1} \\] in our update rule: \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + (\\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})})^{-1}\\right]^{-1} \\] \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + \\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})}\\right]^{-1} \\] because: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{ 1- p(m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{1-p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{1-p(m_i)}{ p(m_i)} \\] the inverse equals: \\[ \\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})} = \\frac{ 1- p(m_i|z_t, x_t)}{p(m_i|z_t, x_t)} \\frac{1-p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{ p(m_i)}{1-p(m_i)} \\] therefore: \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + \\frac{ 1- p(m_i|z_t, x_t)}{p(m_i|z_t, x_t)} \\frac{1-p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{ p(m_i)}{1-p(m_i)}\\right]^{-1} \\] which means, we can obtain information about the state of the \\(m_i\\) grid cell given the observation and the positions. log odds notation to make the computation more efficient we are going to take the log of this expression. the notation will be the following: \\[ l(m_i | z_{1:t}, x_{1:t}) = \\log \\left(\\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})}\\right) \\] note that we can map from the log space to the probability space and vice versa as follows: \\[ l(x) = \\log \\frac{p(x)}{1-p(x)} \\] \\[ p(x) = 1- \\frac{1}{1 + \\exp(l(x))} \\] given this facts, we can turn the aforementioned product into a sum, because the log of the product of two terms equal the sum of the log of each term. \\[ l(m_i|z_{1:t}, x_{1:t}) = l(m_i|z_t,x_t) + l(m_i|z_{1:t-1}, x_{1:t-1}) - l(m_i) \\] where: \\(l(m_i|z_t,x_t)\\) is the inverse sensor model, which contains information about what we sensed. \\(l(m_i|z_{1:t-1}, x_{1:t-1})\\) is the recursive term, that is the state of cell on the previous iteration. \\(l(m_i)\\) is the prior. in short: \\[ l_{t,i} = inv\\_sensor\\_model(m_i, x_t, z_t) + l_{t-1, i} - l_0 \\] algorithm as we can see in the algorithm what we do is, given an observation \\(z_t\\) we go through each cell, and if the cell is close to the area where the observation took place then we update the state of the cell taking into account the sensor information. else we just propagate the previous state into the current state: inverse sensor model for laser range finders on the following graph we show the way we update the occupancy probability of the cells. here, the x axis represent several cells and the y axis represents the occupancy probability. at cell \\(n\\) our scanner detects an obstacle at time \\(t\\), this corresponds to the observation \\(z_{t,n}\\). therefore: the probability of cells prior to cell \\(n\\) of being occupied is low, because we were able to shot a laser through them without encountering no obstacle the probability of the n-th cell of being occupied is high, because it is the place where we found the obstacle. the probability of cells after \\(n\\) of bain occupied is unknown because we cannot see after the laser. a similar idea could be applied to sonar range sensor, which measure an area instead of a line: however the graph is a bit different, we now take into account that the sensor might not be completely reliable, therefore when the sonar detects an obstacle at a given distance we spread the probability of being occupied over adjacent cells/distances. example the idea is to add sensor information to a current \"map\" to increase the certainty of the state of each cell: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/index.html",
    "title": "Online Training: Mobile Robotics",
    "body": " index search search back online training: mobile robotics source: online training: mobile robotics by cyrill stachniss bayes filter occpancy grid maps motion model observation model kalman filter extended kalman filter particle filter markov decision processes slam graph-based slam graph-based slam with landmarks $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/KalmanFilter/Extended Kalman Filter.html",
    "title": "Extended Kalman Filter",
    "body": " index search search back extended kalman filter contents introduction local linearization jacobian error under local linearization linearized motion model linearized observation model algorithm kalman gain localization example introduction what happens if we are dealing with non-linear dynamic systems, such that we do not use our linear models anymore: \\begin{align} x_t = a_tx_{t-1} + b_tu_t + \\epsilon_t \\end{align} \\begin{align} z_t = c_t x_t + \\delta_t \\end{align} but we introduce new functions that need not be linear: \\begin{align} x_t = g(u_t, x_{t-1}) + \\epsilon_t \\end{align} \\begin{align} z_t = h(x_t) + \\delta_t \\end{align} before, when we transformed our belief (a gaussian) with a linear transformation, something like the following happened: where the distribution of the upper left is the result of transforming the distribution of the bottom by applying the linear function on the upper right. however, if we try to do this same thing with a non-linear transformation, we could end up with something like this: so, the result of the transformation is clearly no a gaussian. which means, the kalman filter is not applicable anymore. to prevent this problem we are going to use local linearization. local linearization in order to perform local linearization what we do is approximate the non-linear functions \\(g\\) and \\(h\\) by the means of the taylor expansion. thus we re-define our non-linear functions as follows: the linearization for prediction step consists of linearizing around our previous state \\(x_{t-1} = (\\mu_{t-1}, \\sigma_{t-1})\\)and is described as follows: \\begin{align} g(u_t, x_{t-1}) \\approx g(u_t, \\mu_{t-1}) + \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}(x_{t-1} - \\mu_{t-1}) \\end{align} \\(g(u_t, \\mu_{t-1})\\) is the value of our non-linear model at the linearization point \\(\\mu_{t-1}\\), which corresponds to our previous belief. \\(g_t = \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}\\) is the slope of the local linearization at \\(x_{t-1}\\). this is a first partial derivative which constitutes a jacobian. \\((x_{t-1} - \\mu_{t-1})\\) tells us how far we are away from the linearization point \\(\\mu_{t-1}\\). for the correction step we linearize around our predicted state \\(\\overline{x}_t = (\\overline{\\mu}_t, \\overline{\\sigma}_t)\\): \\begin{align} h(x_t) \\approx h(\\overline{\\mu}_t) + \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t} (x_t - \\overline{\\mu}_t) \\end{align} \\(h(\\overline{\\mu}_t)\\) is the value of our non-linear observation model at the linearization point, which now is the predicted belief, that is the best estimate that i have. \\(h_t = \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t}\\) is the jacobian that equals the slope at the linearization point. \\((x_t - \\overline{\\mu}_t)\\) signifies how far away is the variable \\(x_t\\) to our linearization point \\(\\overline{\\mu}_t\\). jacobian given a function \\(f: \\mathbb{r}^n \\rightarrow \\mathbb{r}^m\\), such that given \\(x \\in \\mathbb{r}^n\\), \\(x \\mapsto f(x) \\in \\mathbb{r}^{m}\\). then the jacobian has the following shape: \\begin{align} j = \\begin{bmatrix} \\frac{\\delta f_1}{\\delta x_1} & \\frac{\\delta f_1}{\\delta x_2} & \\cdots & \\frac{\\delta f_1}{\\delta x_n} \\\\ \\vdots & \\cdots & \\cdots & \\vdots \\\\ \\frac{\\delta f_m}{\\delta x_1} & \\frac{\\delta f_m}{\\delta x_2} & \\cdots & \\frac{\\delta f_m}{\\delta x_n} \\end{bmatrix} \\in \\mathbb{r}^{m \\times n} \\end{align} and we can illustrate it graphically: as you can see, for points close to the linearization point, it constitutes a good approximation, but the further we move away the bigger the error is. so, let's revisit the transformation of our gaussian belief. remember we had, the following non-linear transformation: what we do now, is take the mean of our belief \\(\\mu_t\\) and approximate it locally with a linear function by using the taylor expansion as we have explained before. and then we transform our gaussian belief with this linear approximation (represented by the red line) which results in the following transformation: error under local linearization when we perform local linearization the error depends on to factors: the difference between the non-linear function and its linear approximation the uncertainty of our original gaussian distribution. because the larger the uncertainty, more probability mass will fall farther from our linearization point (the mean of that same gaussian distribution), and remember that the further we are from the linearization point the worse the approximation is, and thus the bigger the error is. linearized motion model we defined our linear motion model as follows: \\begin{align} p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) \\end{align} if our world is non-linear we substitute \\begin{align} x_t = a_tx_{t-1} + b_tu_t + \\epsilon_t \\end{align} for \\begin{align} x_t = g(u_t, x_{t-1}) + \\epsilon_t \\end{align} therefore the motion model is expressed as follows: \\begin{align} p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - g(u_t, x_{t-1}))^tr^{-1}_t(x_t - g(u_t, x_{t-1}))) \\end{align} finally we find a linear approximation, such that: \\begin{align} g(u_t, x_{t-1}) \\approx g(u_t, \\mu_{t-1}) + \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}(x_{t-1} - \\mu_{t-1}) = g(u_t, \\mu_{t-1}) + g_t(x_{t-1} - \\mu_{t-1}) \\end{align} and the linearized motion model becomes: \\begin{align} p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t) ^{-\\frac{1}{2}} \\cdot \\end{align} \\begin{align} \\cdot \\exp(-\\frac{1}{2}(x_t - g(u_t, \\mu_{t-1}) - g_t(x_{t-1} - \\mu_{t-1}))^tr^{-1}_t \\cdot \\end{align} \\begin{align} \\cdot (x_t - g(u_t, \\mu_{t-1}) - g_t(x_{t-1} - \\mu_{t-1}))) \\end{align} where \\(r^{-1}_t\\) describes the motion noise. linearized observation model we defined our linear observation model as follows: \\begin{align} p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - c_tx_t)^tq^{-1}_t(z_t - c_tx_t)) \\end{align} if our world is non-linear we substitute \\begin{align} z_t = c_t x_t + \\delta_t \\end{align} for \\begin{align} z_t = h(x_t) + \\delta_t = h(\\overline{\\mu}_t) + \\delta_t \\end{align} note that \\(x_t = \\overline{\\mu}_t\\) here refers to our best estimation up until now, that comes from the prediction step. therefore the observation model is expressed as follows: \\begin{align} p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - h(\\overline{\\mu}_{t}))^tq^{-1}_t(z_t - h(\\overline{\\mu}_{t}))) \\end{align} finally we find a linear approximation, such that: \\begin{align} h(x_t) \\approx h(\\overline{\\mu}_t) + \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t} (x_t - \\overline{\\mu}_t) = h(\\overline{\\mu}_t) + h_t (x_t - \\overline{\\mu}_t) \\end{align} and the linearized observation model becomes: \\begin{align} p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\cdot \\end{align} \\begin{align} \\cdot \\exp(-\\frac{1}{2}(z_t - h(\\overline{\\mu}_t) - h_t (x_t - \\overline{\\mu}_t))^tq^{-1}_t \\cdot \\end{align} \\begin{align} \\cdot (z_t - h(\\overline{\\mu}_t) - h_t (x_t - \\overline{\\mu}_t))) \\end{align} where \\(q^{-1}_t\\) describes the measurement noise. algorithm to take into account the linearized models, we have to make a few changes to the kalman filter algorithm: the first thing that changes is that we use our linearized moition model \\(g(u_t, \\mu_{t-1})\\) to obtain our predicted estate \\(\\overline{x}_t = (\\overline{\\mu}_t, \\overline{\\sigma}_t)\\) we use the jacobian \\(g_t\\) to transform our previous uncertainty \\(\\sigma_{t-1}\\), given the jacobian is the linear transformation that approximates the non-linear transformation we defined originally for our motion model. same thing goes for the correction step. we use the jacobian \\(h_t\\) to apply a linear transformation that allows us to map \\(\\overline{\\sigma}_t\\) from the state space to the observation space, and thus calculate the kalman gain taking into account the measurement noise. then we compute the corrected mean of the estimated state \\(x_t\\) by obtaining the weighted sum of the mean of the predicted state \\(\\overline{\\mu}_t\\) and the correction factor. this correction factor equals the difference between the actual measurement \\(z_t\\) and the mapping of the predicted state to the observation space given by our linearized function \\(h(\\overline{\\mu}_t)\\). this mapping equals the expected measurement given our state is \\(\\overline{\\mu}_t\\). we do the same thing for the uncertainty \\(\\sigma_t\\). kalman gain suppose you have a perfect sensor, that is we trust completely the values given by this sensor and thus we set the measurement noise to be equal to zero (\\(q_t = 0\\)). then, the kalman gain becomes: \\begin{align} k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + q_t)^{-1} \\end{align} \\begin{align} k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + 0)^{-1} \\end{align} \\begin{align} k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t^t)^{-1} \\overline{\\sigma}_t^{-1} h_t^{-1} \\end{align} \\begin{align} k_t = \\overline{\\sigma}_t i \\overline{\\sigma}_t^{-1} h_t^{-1} \\end{align} \\begin{align} k_t = i h_t^{-1} = h_t^{-1} \\end{align} so, when we perform the correction over the mean of our belief: \\begin{align} \\mu_t = \\overline{\\mu}_t + k_t (z_t - h(\\overline{\\mu}_t)) \\end{align} \\begin{align} \\mu_t = \\overline{\\mu}_t + h_t^{-1} (z_t - h(\\overline{\\mu}_t)) \\end{align} \\begin{align} \\mu_t = \\overline{\\mu}_t + h_t^{-1} z_t - h_t^{-1}h(\\overline{\\mu}_t) \\end{align} with \\(h_t^{-1}h(\\overline{\\mu}_t)\\) what we are doing is, first computing \\(h(\\overline{\\mu}_t)\\) to map \\(\\overline{\\mu}_t\\) to the observation space, and the undoing this mapping with \\(h_t^{-1}\\), which means: \\begin{align} \\mu_t = \\overline{\\mu}_t + h_t^{-1} z_t - \\overline{\\mu}_t \\end{align} \\begin{align} \\mu_t = \\overline{\\mu}_t - \\overline{\\mu}_t + h_t^{-1} z_t \\end{align} \\begin{align} \\mu_t = h_t^{-1} z_t \\end{align} where \\(h_t^{-1}\\) maps \\(z_t\\) from the observation space to the state space, and this means in this update we trust our observation completely, and therefore our estate equals the observation. on the contrary, suppose the sensor is very unreliable, and so the noise is set to be infinity. then the correction step is executed as follows: \\begin{align} k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + q_t)^{-1} \\end{align} \\begin{align} k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + \\infty)^{-1} \\end{align} because we are dividing by infinity, \\(k_t = 0\\). so the mean of our belief is computed as follows: \\begin{align} \\mu_t = \\overline{\\mu}_t + k_t (z_t - h(\\overline{\\mu}_t)) \\end{align} \\begin{align} \\mu_t = \\overline{\\mu}_t + 0 (z_t - h(\\overline{\\mu}_t)) \\end{align} \\begin{align} \\mu_t = \\overline{\\mu}_t \\end{align} hence, if the measurement is too noisy, we only take into account our predicted state. localization example localization example using extended kalman filter (from 11') $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/KalmanFilter/Kalman Filter.html",
    "title": "Kalman Filter",
    "body": " index search search back kalman filter contents introduction properties of gaussian distributions linear model models linear motion model linear observation model gaussian world representing the belief algorithm assumptions the kalman filter requires the world to be gaussian, that is every probability distribution used must be gaussian. this filter also assumes linear models. introduction suppose your position on a 2d plane is given by the black dot: let's say you get your estimate as to where you are, inferred from a certain control command. for example if you are trying to move forward in the same direction you estimate your new position will be the cross: this is will be the prediction step. now suppose you get an observation about the distance to the nearest lighthouse: so now we can perform our new state estimate by combining by the means of a weighted sum our prediction along with the measurement: this weighted sum is performed trading off how certain you are about your prediction and how certain you are about your observation. properties of gaussian distributions in order to derive and prove some parts of the kalman filter we exploit the following properties: the product of two gaussian is a gaussian. a gaussian stays gaussian under linear transformations the marginal and conditional distribution of a gaussian is a gaussian linear model what does it mean when we say the kalman filter uses linear models? this means that both the motion model and the observation model can be expressed through a linear function, that is: \\begin{align} f(x) = ax + b \\end{align} one important property is that if a gaussian distribution is transformed through a linear function it stays gaussian. also, we introduce noise by using a zero mean gaussian distribution. models the motion model is defined as follows: \\begin{align} \\overline{x}_t = a_t x_{t-1} + b_tu_t + \\epsilon_t \\end{align} where \\(x_{t-1}\\) is the previous state estimate, \\(u_t\\) is the control command at time \\(t\\) and \\(\\epsilon_t\\) is gaussian noise. let's dive a little deeper: \\(a_t\\) is a matrix \\(n \\times n\\) (a mapping between the state space and the state space) which tells us how the state evolves from \\(t-1\\) to \\(t\\) without control commands or noise. we can use this matrix to encode information about velocity, acceleration, etc. \\(b_t\\) is a matrix \\(n \\times l\\) (a mapping between the control space and the state space) that describes how the control command \\(u_t\\) changes the state from \\(t_1\\) to \\(t\\). \\(\\epsilon_t\\) is a random variable that represents the motion noise with covariance \\(r_t\\). the observation model is defined as follows: \\begin{align} z_t = c_t \\overline{x}_t + \\delta_t \\end{align} where \\(\\overline{x}_t\\) is the estimated state and \\(\\delta_t\\) is gaussian noise. \\(c_t\\) is a matrix \\(k \\times n\\) which describes a mapping between the state \\(\\overline{x}_t\\) to an observation \\(z_t\\). \\(\\delta_t\\) is a random variable that represents the observation noise with covariance \\(q_t\\). linear motion model now that we have defined our linear models, we are going to show how to express the motion under a gaussian: \\begin{align} p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) \\end{align} which means we are obtaining the value of a probability distribution that incorporates our linear model for the prediction: \\begin{align} p(x_t|x_{t-1}, u_t) \\sim \\mathcal{n}(a_tx_{t-1} + b_tu_t, r_t) \\end{align} linear observation model we will apply the same reasoning to obtain the observation model under a gaussian: \\begin{align} p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - c_t\\overline{x}_t)^tq^{-1}_t(z_t - c_t\\overline{x}_t)) \\end{align} which means we are obtaining the difference between the observation \\(z_t\\) and what i expect to observe \\(\\overline{x}_{t}\\) (\\(z_t - c\\overline{x}_t\\)) while also taking the uncertainty into account \\(q_t^{-1}\\). we compute this incorporating our linear model for the correction step: \\begin{align} p(z_t|x_t) \\sim \\mathcal{n}(c_t\\overline{x}_t, q_t) \\end{align} gaussian world as we have said, we are assuming everything is gaussian. up until now we have described our models by using gaussian distributions, however we still have to make sure these are maintained when we are performing the prediction and the update. so, given the belief at time \\(t\\): if we suppose \\(\\overline{bel}(x_t)\\) is gaussian, then \\(bel(x_t)\\) is gaussian because the product of gaussian distribution is a gaussian distribution. therefore we need to show that \\(\\overline{bel}(x_t)\\) is also gaussian. that is: \\begin{align} \\overline{bel}(x_t) = \\int p(x_t|u_t,x_{t-1})bel(x_{t-1})dx_{t-1} \\end{align} we know, by its definition, that \\(p(x_t|u_t,x_{t-1})\\) is gaussian, and also we can prove by mathematical induction that \\(bel(xx_{t-1})\\) is gaussian. because if we start from a gaussian distributed belief and everything stays gaussian then the belief at time \\(t-1\\) will also be gaussian. noting that the convolution of two gaussian stays gaussian we conclude that \\(\\overline{bel}(x_t)\\) is gaussian and thus \\(bel(t)\\) is also gaussian. let's show however that the integral preserves the gaussian. note that we can express the predicted belief by using our linear models as follows: \\begin{align} \\overline{bel}(x_t) = \\int p(x_t|u_t,x_{t-1}) bel(x_{t-1})dx_{t-1} \\end{align} \\[ = \\int \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t))bel(x_{t-1})dx_{t-1} \\] \\[ = \\eta \\int \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t))bel(x_{t-1})dx_{t-1} \\] \\[ = \\eta \\int \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) exp(-\\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1})) \\] where \\(exp(-\\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1}))\\) is the expected value of the gaussian that describes our previous belief \\(bel(x_{t-1})\\). now, we combine both exponentials, given \\(exp(x) \\cdot exp(y) = exp(x + y)\\): \\[ \\overline{bel}(x_t) = \\eta \\int \\exp(-l_t)dx_{t-1} \\] given: \\[ l_{t} = \\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t) \\] \\[ + \\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1}) \\] we can split \\(l_t\\) up in a part that only depends on \\(x_t\\) and another part that depends on \\(x_t, x_{t-1}\\). such that: \\[ l_t = l_t(x_{t-1}, x_t) + l_t(x_t) \\] thus: \\[ \\overline{bel}(x_t) = \\eta \\int \\exp(-l_t(x_{t-1}, x_t) -l_t(x_t))dx_{t-1} \\] \\[ \\overline{bel}(x_t) = \\eta \\exp(-l_t(x_t)) \\int \\exp(-l_t(x_{t-1}, x_t) )dx_{t-1} \\] this way we have: \\(\\exp(-l_t(x_t))\\): gaussian distribution \\(\\int \\exp(-l_t(x_{t-1}, x_t) )dx_{t-1}\\): this is the marginalization of a gaussian of the variable \\(x_{t-1}\\), which happens to also be a gaussian. therefore we have shown that everything stays gaussian: representing the belief we have said that everything is gaussian, which includes our belief. this belief will be represented, like any other gaussian is, by its mean \\(\\mu\\) and variance \\(\\sigma\\). so our belief at time \\(t\\) would be represented by \\((\\mu_t, \\sigma_t)\\). algorithm the kalman filter algorithm is defined as follows: inputs: \\(\\mu_{t-1}\\): previous mean that describes our belief at time \\(t-1\\) \\(\\sigma_{t-1}\\): previous covariance that describes our uncertainty at time \\(t-1\\) \\(z_t\\): the observation at time \\(t\\). \\(u_t\\): the control command at time \\(t\\). the algorithm is, as usual, divided into a prediction step and a correction step: in the prediction step we estimate our next belief, described by a gaussian \\(\\overline{bel}(x_t) \\sim \\mathcal{n}(\\overline{\\mu_t}, \\overline{\\sigma_t})\\). first we compute our new estimated mean \\(\\overline{\\mu}_t\\) by multiplying our transformation function \\(a_t\\) by the previous mean \\(\\mu_{t-1}\\) which tells us how the state evolves generally without any motion added to it (i.e. velocity, acceleration, etc). to add the motion we add \\(b_tu_{t}\\). then we update our uncertainty. the estimate of the new covariance is derived from how a gaussian changes through a linear transformation, thus we compute \\(a_t \\sigma_{t-1} a_t^t\\). we also add additional noise that the motion adds to the new belief by adding \\(r_t\\). the we apply the correction step: what we mainly do is computing the weighted sum between two distributions first we obtain the weighting factor \\(k_t\\), also known as the kalman gain. this equals a ratio between the prediction and the observation. here we use \\(c_t^t\\) to map our uncertainty from the state space to the observation space. on the denominator we map our uncertainty onto the observation space and we also add the measurement noise \\(q_t\\) then we divide the two terms to obtain a factor that tells us if we trust more the prediction or the correction. then we modify our estimated mean \\(\\overline{\\mu}_{t}\\) with a weighted correction: we compute the error between what we observed and what we predicted \\(z_t - c_t \\overline{\\mu}_t\\) (again \\(c\\) to map to the observation space). then we change the estimated state by this error pondered by \\(k\\). we also update our uncertainty assumptions we can apply the kalman filter as long as the two following assumptions hold: everything is gaussian the motion and observation model are linear however, what if this is not the case? extended kalman filter $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/BayesFilter/Bayes Filter.html",
    "title": "Bayes Filter",
    "body": " index search search back bayes filter contents state estimation recursive state estimation recursive bayes filter intuition derivation prediction and correction step implementation popular filters model examples motion model measurement model state estimation state estimation means we want to estimate the state of the system based on sensor measurements and control commands. thus, given observations \\(z\\) and control commands \\(u\\), estimate the current state \\(x\\) at time \\(t\\): \\begin{align} p(x_t|z_{1:t}, u_{1:t}) \\end{align} recursive state estimation recursive state estimation means we want to update our belief based on the observation that comes in reusing the previous distribution that we had. therefore, using the previous definition of the current state, we would introduce recursion by computing \\(x_t\\) based on the current measurement \\(z_t\\), the current control command \\(u_t\\) and the previous state \\(x_{t-1}\\). the latter is in itself also defined recursively. recursive bayes filter intuition we start with no knowledge of the environment, so our state is described by a uniform distribution, indicating we could be located at any point in space. after receiving a measurement \\(z\\), we update our belief. in this case we have sensed a door, and we know there are three doors in our map. therefore the probability of obtaining the measurement \\(z\\) given we are in front of that door is larger than in the other possible positions. so if we combine our previous belief with this measurement's probability distribution, our belief becomes: now we move forward, so we also have to shift our belief forward. note, however, that our movement is not exact, there is also a level of uncertainty, so we describe it by using a distribution. hence, when combining our previous belief with the probability distribution for the motion our certainty about our state decreases, and our belief becomes: we receive yet another measurement \\(z\\), again we have that \\(p(z|x)\\) is larger on the locations where there is a door, because this measurement has sensed a door. so if we combine this probability distribution for this measurement with our previous state we increase our certainty about our current state. therefor, our belief becomes: derivation the belief at time \\(t\\) is given by: \\begin{align} bel(x_t) = p(x_t | z_{1:t}, u_{1:t}) \\end{align} that is, where am i at moment \\(t\\), given all previous observations \\(z_{1:t}\\) and control commands \\(u_{1:t}\\). we now apply bayes rule, to swap \\(x_t\\) and \\(z_t\\) on the conditional probability: \\begin{align} = \\eta \\cdot p(z_t | x_t, z_{1:t-1}, u_{1:t}) \\cdot p(x_t|z_{1:t-1}, u_{1:t}) \\end{align} where \\(\\eta\\) is a normalization constant. now, let's pay attention to \\(p(z_t | x_t, z_{1:t-1}, u_{1:t})\\). by the markov assumption we are going to assume that the current state \\(x_t\\) and the previous observations and control commands are conditionally independent. that is, they do not give any information about the likelihood of the observation \\(z_t\\). thus, we drop them from the equation: \\begin{align} = \\eta \\cdot p(z_t | x_t) \\cdot p(x_t|z_{1:t-1}, u_{1:t}) \\end{align} for \\(p(x_t|z_{1:t-1}, u_{1:t})\\) we are going to use the law of total probability to add a new variable, so we integrate over this new variable. more concretely to add \\(x_{t-1}\\), which will allow us to introduce recursion to our expression: \\begin{align} = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t}) dx_{t-1} \\end{align} we could interpret this rewritten expression as: for each previous state \\(x_{t-1}\\) we multiply \\(p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t})\\), the probability of being in the new state \\(x_t\\) given the previous state \\(x_{t-1}\\), observations \\(z_{1:t-1}\\) and control commands \\(u_{1:t}\\) by \\(p(x_{t-1}|z_{1:t-1},u_{1:t})\\), the probability of being in the state \\(x_{t-1}\\) given the previous observations \\(z_{1:t-1}\\) and control commands \\(u_{1:t}\\) once again we apply the markov assumption over \\(p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t})\\), because knowing where i am at moment \\(t-1\\), we assume the observations \\(z_{1:t-1}\\) do not add any information. however note the control command does indeed hold valuable information, as it tells us action last executed that moved us from \\(x_{t-1}\\) to \\(x_t\\). so we simplify the expression as follows: \\begin{align} = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t}) dx_{t-1} \\end{align} we now suppose that knowing what action or command is executed in the future does not tell us anything about the present. hence we ignore the latest control command \\(u_t\\), so the expression becomes: \\begin{align} = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t-1}) dx_{t-1} \\end{align} note that we have finally derived a recursive expression for our belief, given: \\begin{align} bel(x_{t-1}) = p(x_{t-1}|z_{1:t-1}, u_{1:t-1}) \\end{align} we substitute this expression in the belief at time \\(t\\): \\begin{align} = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\end{align} prediction and correction step usually the bayes filter is broken up into: prediction step: estimates where the future state is based on the control command at time \\(t\\) and makes use of the motion model. motion model: \\(p(x_t | x_{t-1}, u_{1:t})\\) \\begin{align} \\hat{bel}(x_t) = \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\end{align} correction step: we get an observation that we use to correct potential mistakes we make in the prediction step. this correction is made using the observation or measurement model. observation model: \\(p(z_t | x_t)\\) \\begin{align} bel(x_t) = \\eta \\cdot p(z_t | x_t) \\cdot \\hat{bel}(x_t) \\end{align} implementation in order to implement a bayes filter we need to define certain things: specify the motion model specify the observation model specify the belief how do we move from one state to the next (i.e. linear model, non-linear model) popular filters kalman filters and efk use gaussians to represent the belief, motion model and observation model they use linear or linearized models particle filter the can use arbitrary models to represent state, motion model and observation model model examples motion model given a current state \\(x_t\\), the motion model could look like: in the first case, let's suppose a point represents the next state \\(x_{t+1}\\) after the control command is applied. if we execute our system \\(n\\) times, we get \\(n\\) estimations that are illustrated by the \\(n\\) points in the graph. they represent an approximation of the distribution that describes our predicted state (illustrated by the graph in the upper left corner). this distribution is our motion model at time \\(t+1\\). for the two middle graphs, we can deduce that our system is certain about the angle of motion, however it shows more uncertainty about the distance. finally in the last two graphs we see the opposite. the system knows how much we moved, that is the distance, but is uncertain about the angle of movement. measurement model suppose we have a sensor that tell us the distance between us and the closest obstacle in front of us. we know that this sensor is noisy, so to mimic that noise we can describe the measurement model as a normal distribution. in the previous image, the star represents the closest obstacle, and we use the gaussian distribution to describe how likely it is of obtaining a given measurement. observe that the further we move away from the actual obstacle the lower is the probability of that measurement taking place. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Robotics/OnlineTrainingMobileRobotics/ParticleFilter/Particle Filter.html",
    "title": "Particle Filter",
    "body": " index search search back particle filter contents intro particle set particle generation gaussian sampling importance sampling principle characteristics algorithm summary monte carlo localization structure particle filter algorithm for localization example resampling techniques roulette wheel low variance resampling algorithm intro with particle filters we are not restricting ourselves with parametric probability distributions like we do for example with kalman filters where we use gaussian distributions. as usual, we suppose we are given a map, and instead of using one parametric from we use non-parametric samples as a hypothesis of where the system might be. so, we are going to leave behind gaussian distributions to describe the estimate: and we are going to model our estimate using an arbitrary distribution: it turns out that we can describe this kind of distribution using samples: as you can see, the amount of samples in the areas where the density of the probability distribution is higher is also higher and vice versa. basically we have samples distributed over a state space, so imagine each sample signifies a little bit of probability mass, so we only need to count how many samples fall into a certain region to asses the probability that the system is in that region. also, we can weight our samples, so the larger the weight the larger the probability mass associated with that sample (taking into account that the sum of the weight have to amount to one). the weighting of the samples allows us to use less samples to represent the same probability distribution: note that by using samples we are computing an approximation of the probability function. and we use this weighted samples to estimate our belief. some examples are: we use the particles to approximate the probability function, where the more particles fall into a region, the higher the probability of the region. particle set we represent the sample set or particle set as follows: \\[ \\mathcal{x} = \\{\\langle x^{[j]}, w^{[j]}\\rangle\\}_{j=1\\cdots j} \\] where: there are \\(j\\) samples \\(x^{[j]}\\) represents the hypothesis (i.e. the state of the system) \\(w^{[j]}\\) represents the normalized weight assigned to jth particle the sum or integration over the particles represent the posterior (i.e. the probability function): \\[ p(x) = \\sum_{j=1}^j w^{[j]} \\delta_{x^{[j]}} (x) \\] where \\(\\delta\\) is the dirac function. note that \\(\\int \\delta(x) dx = 1\\) particle generation gaussian sampling note that closed form sampling is only possible for a few distributions, for example: for a gaussian distribution to obtain an approximation from sampling, we would sample by summing \\(12\\) times a random (uniformly drawn) number \\(x \\in [-\\sigma, \\sigma]\\), where \\(\\sigma\\) represents the standard deviation, and divide the sum by \\(\\frac{1}{2}\\). then you would draw samples that are approximately close to a gaussian distribution. importance sampling principle but, how can we approximate for another probability distribution functions? it turns out we can do this by sampling from a different probability function that the actual probability function and then compensating for the mistakes that we have done by drawing from this \"mistaken\" probability function. to do this we apply the importance sampling principle which tells us: we can use a different distribution (proposal distribution) \\(\\pi\\) to generate samples from the target (real) distribution \\(f\\). we need to account for the differences between \\(\\pi\\) and \\(f\\) using a weight, given by \\(\\omega = \\frac{f(x)}{\\pi{x}}\\) we need to assert the following pre-condition: \\(f(x) > 0 \\rightarrow \\pi(x) > 0\\) you can see that the weights are larger where the difference between the proposal and the target function is bigger. observe on the right side of the graph that we have drawn a low number of samples because our proposal probability function tells us the us the density on that region is low. however the target function shows a high probability in that same region, so by computing the difference between the proposal and the target function we assign bigger weights to those few particles. characteristics it is a recursive bayes filter uses a non-parametric approach models the distribution using samples and so the model need not be linear. the prediction step consists of drawing samples from the proposal function (takes the motion into account) the correction step consists of weighting the samples by the ration between the target function and the proposal function (takes the observation into account) the more particles we use to approximate the probability function the better the estimate is. algorithm the algorithm is composed of the following three steps: (prediction step) sample the particles using the proposal distribution (this signifies: where could my system be?). because we can choose the proposal function, what we do in this step is sampling from the motion model: \\[ x_t^{[j]} \\sim proposal(x_t|\\cdots) \\] (correction step) compute the importance weights to compensate from the mistakes made by sampling from the proposal distribution. if we derive the following expression, we obtain that the weights are given by the observation model: \\[ w_t^{[j]} = \\frac{target(x_t^{j})}{proposal(x_t^{j})} \\] resampling: draw with replacement \\(j\\) samples \\(i\\) with probability \\(w_t^{[i]}\\). so now we have a resampled set of samples where we update the weights by dividing the by \\(1/j\\) so they are normalized. what we do is generate a new set of samples where we replace the weight by the frequency. we do this because we work with a finite number of samples, so it could be the case that some particles have a very low probability and thus contribute very little to approximating the probability function. so it is better to eliminate those samples and replace them with a sample that is located in an area with high probability. we start with a empty sample set for the prediction step \\(\\hat{\\mathcal{x}}_t\\) and for the correction step \\(\\mathcal{x}_t\\). (prediction step) for \\(j=1\\cdots j\\): sample particle \\(x^{[j]}_t\\) from the proposal distribution \\(\\pi(x_t)\\), this distribution can be defined by the user, and corresponds to the belief at time \\(t-1\\) and constrained to the control command at time \\(t\\), \\(u_t\\). compute the weight by obtaining the difference between the proposal distribution and the target distribution. this results in using the observation model save the pair \\(x_t^{[j]}\\), \\(w_t^{[j]}\\) to the prediction sample set \\(\\hat{\\mathcal{x}}_t\\). (correction step) for \\(j=1\\cdots j\\): draw a particle \\(x_t^{[j]}\\) with replacement from the prediction sample set with probability proportional to the weight of the sample \\(w_t^{[j]}\\). add the particle to the correction sample set \\(\\mathcal{x}_t\\) return the resampled sample set \\(\\mathcal{x}_t\\) summary what the particle filter does is: it takes each particle as a pose hypothesis that says \"this is where the system is at time \\(t\\)\". then it adds weight to each particle signifying how much that pose hypothesis conforms to the given observation, and tells us how likely the hypothesis is. if we do this with \\(n\\) particles what we obtain is a belief, that is, a set of possibilities of where we are which describe my probability distribution. monte carlo localization monte carlo localization refers to the estimation the location and orientation of the system using a particle filter. for example: with a particle filter, our belief shows where the robot is located by having a bigger density of particles right under where the robot is. another example is the following, where we start with all the particles scattered over the map that means the particles are sampled from a uniform distribution so every point in space is equally likely to be the location of the robot. once the robot drives around and obtains new measurements the probability mass concentrates on places where the robot is more likely to be in given the motion commands and the observations. eventually the system converges and you end up with a unimodal distribution that is similar to a gaussian distribution. structure each particle represents a pose hypothesis we represent the proposal probability function by drawing from the motion model. because we are sampling from the motion distribution what we do is increasing the uncertainty about the motion at time \\(t\\) and thus account for the noise present in each motion. \\[ x_t^{[j]} \\sim p(x_t|x_{t-1}, u_t) \\] we apply the correction via the observation model. so the weight of each particle is proportional to the likelihood of an observation \\(z_t\\) given i know where i am \\(x_t\\) and the map of the environment \\(m\\). this result is dependent of the choice made previously of sampling from the motion model. \\[ w_t^{[j]} = \\frac{target}{proposal} \\propto p(z_t|x_t,m) \\] particle filter algorithm for localization we modify slightly our particle filter algorithm to use it for localization: we sample from the motion model \\(p(x_t|u_t, x^{[j]}_{t-1})\\) instead from the generic proposal function \\(\\pi(x_t)\\) we compute the weights with \\(w_t^{[j]} = \\frac{target}{proposal} \\propto p(z_t|x_t,m)\\). example first we start with a uniform distribution, and we sample from that distribution, obtaining \\(j\\) particles distributed over the space with the same probability. then we obtain an observation, and in the weighting step we increase the weight of the samples with are more likely given the observation. in this case the samples in front of doors, while the rest of the particles get a lower weight. then we apply resampling to replace weight by frequencies (the probability mass of a particle is bigger if this particle has been resampled several times, which means it weight was bigger than the rest of the samples). in the following picture the resampling step is executed along with the motion step (so the probability function is offsetted): because the prediction/motion was already performed before, now we obtain another observation: when we obtain the weights, two things happen. first, and as before, the particles (pose hypothesis) more likely to be correct given the observation obtain a larger weight. second, because the is a bigger number of particles in front of the second door the density in this area is bigger than in the areas in front of the other doors. another resampling and prediction step: resampling techniques roulette wheel first we create a roulette wheel where each field represents a particle, and the bigger the weight associated with that particle the bigger the field is: the idea is that, we normalize the weights, and each time we draw a number between zero and one, which will \"point\" to a weight. however this method can lead to suboptimal solutions. suppose that for some reason each time we end up with uniform weight, so that no particle is more likely than any other. then, with the wheel roulette we will duplicate some particles and remove some others. however this does not make sense, because every particle had the same weight. thus we introduce the lower variance resampling. low variance resampling here, the idea is using \\(j\\) arrows instead of only one, where the arrows are at the same angular distance from each other. so in order to sample what we do is, we simply turn the arrows, and where all the arrows end up, that is the samples we choose. this solution is faster, with time complexity equal to \\(o(j)\\) compared to the wheel roulette's \\(o(j \\log j)\\), and resolves the suboptimal solution problem presented earlier. algorithm first we draw a random number between \\(0\\) and \\(\\frac{1}{j}\\) then we pick \\(j-1\\) particles by advancing in the array in steps of \\(\\frac{1}{j}\\) to efficiently implement this what we do is, in each element of the array we store the cummulative weight up until that point: so, we draw a random number between \\(0\\) and \\(\\frac{1}{j}\\), if that number is bigger than the weight accumulated up until weight \\(i\\), then we move to the next one, else if it is less we sample the particle \\(i\\). and then we advance \\(\\frac{1}{j}\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/SVM/Optimal Margin Classifier.html",
    "title": "Optimal Margin Classifier",
    "body": " index search search back optimal margin classifier contents train the classifier classify an example we use this classifier to categorize datasets that are perfectly separable, that is to say, we use it over data that is linearly separable. this classifier will help us find the green line we saw in the geometric margin. what the optimal margin classifier does is choose the parameters \\(w, b\\) that maximize \\(\\gamma\\) one way to solve this optimization problem is: \\begin{align} \\underset{\\gamma, w, b}{\\max} \\gamma \\end{align} subject to \\begin{align} \\frac{y^{(i)}(w^tx + b)}{||w||} \\geq \\gamma \\end{align} this will cause the maximization of the geometric margin with respect to the training set. the restriction means that we want to maximize \\(\\gamma\\) while having every example have a geometric margin of at least \\(\\gamma\\). because this is a non-convex problem, we will transform it. given \\(\\gamma = \\frac{\\hat{\\gamma}}{||w||}\\), then \\(\\gamma \\cdot ||w|| = \\hat{\\gamma}\\), and so if we multiply in the subject both sides by \\(||w||\\): \\begin{align} \\frac{y^{(i)}(w^tx + b)}{||w||} \\cdot ||w|| \\geq \\gamma \\cdot ||w|| \\leftrightarrow y^{(i)}(w^tx + b) \\geq \\hat{\\gamma} \\end{align} and the optimization problem can be re-written as: \\begin{align} \\underset{\\hat{\\gamma}, w, b}{\\max} \\frac{\\hat{\\gamma}}{||w||} \\end{align} subject to \\begin{align} y^{(i)}(w^tx + b) \\geq \\hat{\\gamma} \\end{align} however, we are still stuck with a non-convex objective \\(\\frac{\\hat{\\gamma}}{||w||}\\). because, as we've said previously scaling the functional margin (changing the magnitude of \\(w^tx + b\\)) does not change the decision boundary itself, we will add an scaling constraint that the functional margin of \\(w, b\\) with respect to the training set must be 1: \\(\\hat{\\gamma} = 1\\) observe, now, that maximizing \\(\\frac{\\hat{\\gamma}}{||w||} = \\frac{1}{||w||}\\) is like minimizing \\(||w||^2\\), we re-write the optimization problem as follows: \\begin{align} \\underset{w, b}{\\min} ||w||^2 \\end{align} subject to \\begin{align} y^{(i)}(w^tx + b) \\geq 1 \\end{align} we will revise once more the optimization problem for the optimal margin classifier. first, we have to suppose two facts: by the representer theorem we can assume that \\(w\\) can be expressed as a linear combination of \\(x\\), that is: \\begin{align} w = \\sum_{i=1}^m \\alpha_i x^{(i)} \\end{align} let's review this claim with logistic regression. we know that we apply stochastic gradient descent (we update \\(\\theta\\) for every example, instead of summing all the examples) on \\(\\theta\\) as follows: \\begin{align} \\theta = \\theta - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} \\end{align} which means that in every interation we are updating \\(\\theta\\) by adding or substracting a factor \\(\\alpha_i\\) multiplied by \\(x^{(i)}\\). therefore we can show by mathematical induction that if we start with \\(\\theta_0 = c\\), where \\(c\\) is a constant and go on adding and substracting \\(ax^{(i)}\\), where \\(a= \\alpha (h_\\theta(x^{(i)}) - y^{(i)})\\), then \\(w\\) can be expressed as a linear combination of \\(x\\). you can also derive the gradient descent expression in our optimization problem, and show that in this case \\(w\\) is also a linear combination of \\(x\\). we can rewrite \\(w\\) as follows: \\begin{align} w = \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} \\end{align} given any decision boundary, the vector \\(w\\) is always orthogonal to the decision boundary: now, the optimization problem becomes (note \\(w^2 = w^tw\\)): \\begin{align} \\underset{w, b}{min} \\frac{1}{2}||w||^2 = \\underset{w, b}{min} \\frac{1}{2} (\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)})^t(\\sum_{j=1}^m \\alpha_j y^{(j)} x^{(j)}) = \\end{align} \\begin{align} \\underset{w, b}{min} \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)}y^{(j)}(x^{(i)})^tx^{(j)} \\end{align} we now denote the inner product of \\((x^{(i)})^t x^{(j)}\\) as \\(\\langle x^{(i)}, x^{(j)} \\rangle\\), so: \\begin{align} \\underset{w, b}{min} \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)}y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle \\end{align} and the restriction of the optimization becomes: \\begin{align} y^{(i)}(w^tx^{(i)} + b) \\geq 1 \\rightarrow y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)}x^{(j)})^tx^{(i)} + b) \\geq 1 \\rightarrow \\end{align} \\begin{align} y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)}(x^{(j)})^tx^{(i)}) + b) \\geq 1 \\rightarrow y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)} \\langle x^{(j)}, x^{(i)} \\rangle) + b) \\geq 1 \\end{align} applying convex optimization theory you can simplify this optimization problem further to: \\begin{align} \\underset{\\alpha}{max} \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, x^{(j)}\\rangle \\end{align} subject to \\begin{align} \\alpha_i \\geq 0 \\end{align} \\begin{align} \\sum_{i=1} y^{(i)}\\alpha_i = 0, i=1, \\cdots,m \\end{align} train the classifier to train the svm we have to solve the optimization problem for \\(\\alpha\\) classify an example to predict an example \\(x\\): \\begin{align} h_{w,b} = g(w^tx + b) = g\\left(\\left(\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)}\\right)^tx + b\\right) = g\\left(\\left(\\sum_{i=1}^m \\alpha_i y^{(i)} \\langle x^{(i)}, x^{(j)} \\rangle\\right) + b\\right) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/SVM/Functional Margin.html",
    "title": "Functional Margin",
    "body": " index search search back functional margin contents intuition formal definition evaluation normalizing the functional margin intuition the functional margin describes how accurately do we classify an example. for example, for binary classification, given an example x: \\begin{align} h_\\theta(x) = g(\\theta x) = \\begin{cases} \\text{ predict } 1 & \\text{ if } \\theta^t x \\geq 0, \\text{ that is } h_\\theta(x)=g(\\theta x) \\geq 0.5\\\\ \\text{ predict } 0 & \\text{ otherwise } \\\\ \\end{cases} \\end{align} let's distinguish between the two cases when classifying an example \\(x^{(i)}\\): (1) if \\(y^{(i)} = 1\\), then we want \\(h_\\theta(x) = g(\\theta x) \\approx 1\\), which means we want \\(\\theta \\cdot x >> 0\\). (2) if \\(y^{(i)} = 0\\), then we want \\(h_\\theta(x) = g(\\theta x) \\approx 0\\), which means we want \\(\\theta \\cdot x << 0\\). as we can see in the following graph, the bigger \\(z = \\theta x\\) the closer \\(g(z)\\) is to one and vice versa. formal definition the functional margin of the hyperplane defined by \\((w, b)\\) with respect to the example \\((x^{(i)}, y^{(i)})\\) is defined as: \\begin{align} \\hat{\\gamma}^{(i)} = y^{(i)}(w^tx^{(i)}+b) \\end{align} so, if we modify slightly the two statements above and use the new notation for svms: if \\(y^{(i)} = 1\\), then we want \\(w^t \\cdot x + b >> 0\\). if \\(y^{(i)} = 0\\), then we want \\(w^t \\cdot x + b << 0\\). the combination of these two declarations yields the definition of the functional margin. why?, well: when \\(y^{(i)}\\) is positive, we want to have \\(w^tx^{(i)} + b >> 0\\) by (1), so \\(\\hat{\\gamma}^{(i)}\\) will be large, because both values are positive when \\(y^{(i)}\\) is negative, we want to have \\(w^tx^{(i)} + b << 0\\) by (2), so \\(\\hat{\\gamma}^{(i)}\\) will be large, because both values are negative so, given an example \\(x^{(i)}\\), if \\(\\hat{\\gamma}^{(i)} > 0\\) that means either \\(y^{(i)} = 1\\) and \\(w^tx + b > 0\\) or \\(y^{(i)} = -1\\) and \\(w^tx + b < 0\\) which shows that the classification is correct. evaluation to evaluate the functional margin with respect to the training set we make use of the worst case notion: \\begin{align} \\hat{\\gamma} = \\underset{i}{\\min} \\hat{\\gamma}^{(i)} \\end{align} that is, we evaluate how well we are doing in the worst example. normalizing the functional margin note that the functional margin is very easy to cheat (to increase its value with any meaningful change to the decision boundary). given our definition for \\(g\\): \\begin{align} g = \\begin{cases} 1, & \\text{ if } z \\geq 0 \\\\ -1, & \\text{ otherwise } \\end{cases} \\end{align} it follows that \\(h_{w,b}(x^{(i)}) = g(2w^tx^{(i)} + 2b) = g(w^tx^{(i)} + b)\\), because what matters is the sign, not the magnitude. however, if you scale \\(w\\) and \\(b\\) by a factor of \\(n\\) where \\(n\\) is a positive number then \\(\\gamma \\) increases because: \\begin{align} \\hat{\\gamma}^{(i)} = (w^tx + b) \\end{align} so, \\begin{align} n \\cdot \\hat{\\gamma}^{(i)} = n \\cdot (w^tx + b) \\end{align} where, \\begin{align} \\hat{\\gamma}^{(i)} < n \\cdot \\hat{\\gamma}^{(i)} \\end{align} one way to avoid this is to normalize the length of the parameters, that is either: add a constraint where \\(||w|| = 1\\) or set \\((w, b)\\) to be \\((\\frac{w}{||w||}, \\frac{b}{||b||})\\) in both cases we are re-scaling the parameters. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/SVM/Geometric Margin.html",
    "title": "Geometric Margin",
    "body": " index search search back geometric margin contents intuition euclidean distance to the decision boundary formal definition evaluation intuition first of all, let's assume we have a dataset that is linearly separable like: here we have two examples of two decision boundaries that do classify correctly all of the samples. however the red one looks worse than the green one. that is because for the red one there are some examples that are very close to the boundary compared to the rest. whereas for the green one there is a bigger separation. so, first we define a line by the equation \\(w^tx + b = 0\\), therefore: every example \\(x\\) that lies to the left of the line satisfies \\(w^tx + b < 0\\) and every example \\(x\\) that lies to the right of the line satisfies \\(w^tx + b > 0\\) furthermore the geometric margin with respect to a single example \\((x^{(i)}, y^{(i)})\\) is the euclidean distance between the point \\((x^{(i)}, y^{(i)})\\) and the line we have defined as \\(w^tx + b = 0\\). euclidean distance to the decision boundary the decision boundary corresponding to (w, b) is shown, along with the vector w. note that w is orthogonal (at 90Âº) to the separating hyperplane. consider the point at \\(a\\), which represents the example \\(x^{(i)}\\) with \\(y^{(i)} = 1\\). its distance to the decision boundary, denoted by \\(\\gamma^{(i)}\\), is given by the line segment \\(ab\\). how do we find \\(\\gamma^{(i)}\\): we know \\(\\frac{w}{||w||}\\) is a unit length vector pointing to the same direction as \\(w\\). also \\(a = x^{(i)}\\) we also know that the vector between points \\(a\\) and \\(b\\) is defined like \\(a - b\\), in this scenario, \\(a - b = \\gamma^{(i)}\\frac{w}{||w||}\\), where \\(\\gamma^{(i)}\\) is the length of the vector and \\(\\frac{w}{||w||}\\) is the direction of the vector. thus if we solve for \\(b\\), \\(b = x^{(i)} - \\gamma^{(i)}\\frac{w}{||w||}\\) furthermore, \\(b\\) lies on the decision boundary, therefore: \\begin{align} w^t(b) + b = 0 \\rightarrow w^t\\left(x^{(i)} - \\gamma^{(i)}\\frac{w}{||w||}\\right) + b = 0 \\end{align} solving for \\(y^{(i)}\\) yields: \\begin{align} \\gamma^{(i)} = \\frac{w^tx^{(i)} + b}{||w||} = \\left(\\frac{w}{||w||}\\right)^tx(i) + \\frac{b}{||w||} \\end{align} formal definition the geometric margin of the hyperplane \\((w, b)\\) with respect to \\((x^{(i)}, y^{(i)})\\) is defined as: \\begin{align} \\gamma^{(i)} = \\frac{w^t x^{(i)} + b}{||w||} \\end{align} this is the definition for a positive example (\\(y^{(i)} = 1\\)), and measures the euclidean distance from the decision boundary to the example \\((x^{(i)}, y^{(i)})\\). if we generalize, as to compute the geometric margin for both positive and negative examples: \\begin{align} \\gamma^{(i)} = \\frac{y^{(i)} (w^t x^{(i)} + b)}{||w||} \\end{align} evaluation to evaluate the geometric margin with respect to the training set we make use of the worst case notion: \\begin{align} \\gamma = \\underset{i}{\\min} \\gamma^{(i)} \\end{align} that is, we evaluate how well we are doing in the worst example. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/SVM/SVM.html",
    "title": "SVM",
    "body": " index search search back svm contents notation functional margin geometric margin relationship between functional margin and geometric margin optimal margin classifier svm kernels kernel trick applying kernels validity of kernels generality of the kernel trick l1-norm soft margin svm graphical representation outliers optimization kernel examples the support vector machine allows you to find potential non-linear decision boundaries: svm provides an algorithm that: maps a vector of features to a vector of features of a much higher dimension (manually picking the new features is difficult, that is why we automate it with these types of algorithms) \\begin{align} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1\\cdot x_2 \\\\ \\vdots \\end{bmatrix} \\end{align} applies a linear classifier over the high dimensional features (note: if you apply logistic regression to high dimensional vectors then it can learn non-linear decision boundaries) notation labels: \\(y^{(i)} \\in \\{-1, +1\\}\\) now the hypothesis outputs a \\(1\\) or a \\(-1\\), which means: \\begin{align} g(z) = \\begin{cases} 1, & \\text{ if } z \\geq 0 \\\\ 0, & \\text{ otherwise } \\\\ \\end{cases} \\end{align} that is, now instead of a smooth transition of probabilities from zero to one, we have a hard transition between \\(1\\) and \\(-1\\). weights: now the weights \\(\\theta \\in \\mathbb{r}^{(n+1)}\\), where \\(\\theta_0 = 1\\) are divided into: \\(w \\in \\mathbb{r}^{(n)}\\) and \\(b \\in \\mathbb{r}\\). thus we drop the convention of assigning \\(x_0 = 1\\). also now the hypothesis function is defined as: \\(h_{w,b}(x) = g(w^tx + b) = g((\\sum_{i=1}^n w_i x) + b)\\) functional margin functional margin geometric margin geometric margin relationship between functional margin and geometric margin as you may have picked up we can stablish an equality between both margins: \\begin{align} \\gamma^{(i)} = \\frac{\\hat{\\gamma}^{(i)}}{||w||} \\end{align} optimal margin classifier optimal margin classifier svm kernels kernel trick to apply kernels first we will lay out the kernel trick: write the algorithm in terms of the inner products of the training examples \\(\\langle x^{(i)}, x^{(j)} \\rangle=(\\langle x, z \\rangle)\\) let there be a mapping \\(x \\rightarrow \\phi(x)\\), where \\(\\phi(x)\\) is a high dimensional feature vector. find a way to compute \\(k(x, z) = \\phi(x)^t\\phi(z)\\), even if \\(x, z\\) are very high dimensional features vectors (which would be very computationally expensive). where \\(k(x, z)\\) is denoted as the kernel function replace \\(\\langle x, z \\rangle\\) with \\(k(x, z)\\) applying kernels given \\(x, z \\in \\mathbb{r}^n\\), where: \\begin{align} x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\end{align} we define the mapping \\(\\phi(x) \\in \\mathbb{r}^{n^2}\\) as follows: \\begin{align} \\phi(x) = \\begin{bmatrix} x_ix_i \\\\ \\end{bmatrix} \\end{align} \\(\\forall i, j\\) with \\(1 \\leq i,j \\leq n\\) so we have \\begin{align} k(x, z) = \\phi(x)^t \\phi(z) = \\sum_{i=1}^{n^2} \\phi(x)_i \\phi(z)_i = \\sum_{i=1}^n \\sum_{j=1}^n (x_ix_j) (z_iz_j) \\end{align} which would take \\(o(n^2)\\) time to compute. but, observe that: \\begin{align} (x^tz)^2 = (x^tz)^t(x^tz) = \\sum_{i=1}^n\\sum_{j=1}^n (x_iz_i)(x_jz_j) = \\sum_{i=1}^n\\sum_{j=1}^n (x_ix_j)(z_iz_j) \\end{align} whick takes \\(o(n)\\) time to compute. so we conclude that the kernel can be defined as \\(k(x, z) = (x^tz)^n\\) given \\(x, z \\in \\mathbb{r}^n\\) \\(k(x, z) = (x^tz + c)^2\\) where the mapping function \\(\\phi\\) is defined as: given \\begin{align} x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\ x_1x_2 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ \\sqrt{2c}x_1 \\\\ \\sqrt{2c}x_2 \\\\ \\end{bmatrix} \\end{align} given \\(x, z \\in \\mathbb{r}^n\\) \\(k(x, z) = (x^tz+ c)^d\\) where \\(\\phi(x)\\) contains the \\(\\binom{n+d}{d}\\) combinations of monomials of degree d. (note: a monomial of degree 3 could be \\(x_1x_2x_3\\) or \\(x_1x_2^2\\), etc) validity of kernels to test is a kernel is valid we use mercer's theorem that says: k is a valid kernel function (i.e. \\(\\exists \\phi\\) such that \\(k(x, z) = \\phi(x)^t\\phi(z)\\)) if and only if for any \\(d\\) points \\(\\{x^{(1)}, \\cdots , x^{(d)}\\}\\) the corresponding kernel matrix \\(k\\) is positive semi-definite, that is \\(k \\geq 0\\) we are going to prove the first part of this theorem: given examples \\(\\{x^{(1)}, \\cdots , x^{(d)}\\}\\), let \\(k \\in \\mathbb{r}^{d\\times d}\\), be the kernel matrix, such that \\begin{align} k_{ij} = k(x^{(i)}, x^{(j)}) \\end{align} then, if \\(k\\) is a valid kernel: \\begin{align} z^tkz = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t k_{ij} z_j = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t \\phi(x^{(i)})^t \\phi(x^{(j)}) z_j = \\end{align} we expand \\(\\phi(x^{(i)})^t \\phi(x^{(j)})\\) as follows: \\begin{align} = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t \\left[\\sum_{k=1}^d (\\phi(x^{(i)}))_k (\\phi(x^{(j)}))_k\\right] z_j = \\end{align} now, if we rearrange the sums: \\begin{align} = \\sum_{k=1}^d \\left[\\sum_{i=1}^d z_i (\\phi(x^{(i)}))_k\\right]^2 \\end{align} so, because the power of two of any real number is a positive number, and the sum of positive numbers is positive we derive: \\begin{align} \\sum_{k=1}^d \\left[\\sum_{i=1}^d z_i (\\phi(x^{(i)}))_k\\right]^2 \\geq 0 \\end{align} which means that \\(k \\geq 0\\), hence \\(k\\) is a positive, semi-definite matrix generality of the kernel trick the kernel trick can be applied to more algorithms, not only in svm. because, if you have any algorithm written in terms of \\(\\langle x^{(i)}, x^{(j)} \\rangle\\), you can apply the kernel trick to it. some of the algorithms that can be re-written like this are: lineal regression logistic regression gdm pca etc. l1-norm soft margin svm it may be the case where you map your data to a very high dimensional space, but it is still not linearly separable, or the decision boundary becomes too complex: in order to avoid this we will use a modification of the basic algorithm called l1-norm soft margin svm. with this new algorithm the optimization problem becomes \\begin{align} \\underset{w,b,\\xi_i}{min} \\frac{1}{2}||w||^2 + c \\sum_{i=1}^m \\xi_i \\end{align} subject to \\begin{align} y^{(i)}(w^tx^{(i)} + b) \\geq 1 - \\xi_i \\end{align} \\begin{align} \\xi_i \\geq 0, i = 1, \\cdots, m \\end{align} note that if \\(x^{(i)}\\) is classified correctly then \\(y^{(i)}(w^tx^{(i)} + b) \\geq 0\\) and therefore satisfies \\(y^{(i)}(w^tx^{(i)} + b) \\geq 1 - \\xi_i\\), because \\(\\xi_i \\geq 0\\) before the modification, the restriction forced the functional margin to be at least 1, however after the modification, because \\(\\xi_i\\) is positive we relax the restriction. also, we do not want \\(\\xi_i\\) to be too big, that is why it is added to the optimization objective as a cost. graphical representation with the addition of \\(\\xi_i\\) we are allowing some examples to have a functional margin less than 1, by setting \\(\\xi_i \\geq 0\\). for example look at the example \\(x^{(i)}\\) which has \\(\\xi_i = 0.5\\) outliers this relaxation on the restriction upong the geometric margin also avoids the following problem. if you have a lot of data that is linearly separable, but you have one outlier the optimal margin classifier allows for the decision boundary to be drastically changed because its optimization is based on the word performing example (which would be the outlier in this case). thus: however, the l1-norm soft margin svm allows for this example to be classified incorrectly of be close to the decision boundary without changing the boundary which makes it more robuts to outliers. optimization picking up the optimal margin classifier optimization problem, after applying the insight derived from the representer theorem, we have that the only addition needed to implement this algorithm is: \\begin{align} \\underset{\\alpha}{max} \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, x^{(j)}\\rangle \\end{align} subject to \\begin{align} \\sum_{i=1} y^{(i)}\\alpha_i = 0 \\end{align} \\begin{align} 0 \\leq \\alpha_i \\leq c, i = 1, \\cdots , m \\end{align} the parameter \\(c\\) is a parameter your choose and it determines the level of strictness you want your model to have about some examples being misclassified. kernel examples the gaussian kernel: \\(k(x, z) = \\exp\\left(\\frac{||x-z||^2}{2\\sigma}\\right)\\) linear kernel: \\(k(x, z) = \\phi(x)^t\\phi(z)\\), where \\(\\phi(x) = x\\) polynomial kernel: \\(k(x, z) = (x^tz)^d\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Grandes Datasets.html",
    "title": "Grandes Datasets",
    "body": " index search search back grandes datasets contents stochastic gradient descent mini batch gradient descent cuando los conjuntos de datos son muy grandes los algoritmos son computacionalmente mÃ¡s caros: varianza elevada: se obtiene mejor rendimiento con mÃ¡s ejemplos. sesgo/bias elevado: se obtiene mejor rendimiento con mÃ¡s caracterÃ­sticas. stochastic gradient descent el algoritmo de stochastic gradient descent es el siguiente: reordenar aleatoriamente el conjunto de datos para cada ejemplo \\(i\\) y cada caracterÃ­stica \\(j\\): \\(\\theta_j = \\theta_j - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) es decir, se ajusta \\(\\theta\\) para cada ejemplo, en lugar de hacer el cÃ¡lculo sobre todo el conjunto de datos cada iteraciÃ³n es mÃ¡s rÃ¡pida no converge como batch gradient descent, llega a una aproximaciÃ³n. mini batch gradient descent esta tÃ©cnica lo que hace el utilizar \\(b\\) ejemplos para calcular el gradiente: para cada \\(b\\) ejemplos y cada caracterÃ­stica \\(j\\): \\(\\theta_j = \\theta_j - \\alpha \\frac{1}{b}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) permite vectorizaciÃ³n $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Gradient Checking.html",
    "title": "Gradient Checking",
    "body": " index search search back gradient checking consiste en la estimaciÃ³n numÃ©rica de los gradientes, tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta} \\approx \\frac{j(\\theta - \\epsilon) - j(\\theta + \\epsilon)}{2 \\cdot \\epsilon} \\end{align} \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_j} \\approx \\frac{j(\\theta_0, ..., \\theta_j - \\epsilon, ..., \\theta_n) - j(\\theta_0, ..., \\theta_j + \\epsilon, ..., \\theta_n)}{2 \\cdot \\epsilon} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Ceiling Analysis.html",
    "title": "Ceiling Analysis",
    "body": " index search search back ceiling analysis supongamos que tenemos un pipeline que conforma todo nuestro sistema de aprendizaje automÃ¡tico y estÃ¡ formado por: obtenciÃ³n de la imagen detecciÃ³n de texto segmentaciÃ³n de caracteres reconocimiento de caracteres lo que hacemos es determinar una o varias mÃ©tricas de evaluaciÃ³n, por ejemplo nosotros utilizaremos la precisiÃ³n. entonces, ahora creamos una tabla indicando el valor de mÃ©trica para cada parte del sistema asÃ­ como para el sitema completo: componente precisiÃ³n detecciÃ³n del texto 82% segmentaciÃ³n de caracteres 90% reconocimiento de caracteres 100% total 72% a partir de esta tabla podemos comprobar que mejorar la detecciÃ³n en el texto y la segmentaciÃ³n de caracteres mejora el rendimiento del modelo. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/Partes del algoritmo en python.html",
    "title": "Algoritmo",
    "body": " index search search back algoritmo dado un conjunto de entrenamiento \\(x\\), donde \\(x\\) es una matriz \\((n + 1) \\times m\\) con \\(m\\) ejemplos: propagaciÃ³n hacia adelante def feed_forward(self, theta=none, capa=none, test=false): if theta is none: # si no se introduce theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if test: # si se indica utilizar x_test a = self.x_test n, m = self.x_test.shape # guardar dimensiones de test else: a = self.x # la primera entrada es x n, m = self.x.shape # guardar dimensiones de train if capa is not none: # si se ha indicado una capa if capa <= len(theta) and capa >= 0: # chequeamos que la capa esta dentro de los limites for i in range(capa): # recorremos las capas a = self.sigmoid(theta[i], a) # calculamos la salida de la capa a = np.concatenate((np.matrix(np.ones(m)), a)) # aÃ±adimos una fila de unos return a else: print(\"el nÃºmero de capa no es vÃ¡lido\") # mensaje de error else: for elemento in theta: a = self.sigmoid(elemento, a) # calculamos la salida de la capa actual a = np.concatenate((np.matrix(np.ones(m)), a)) # aÃ±adimos una fila de unos h = a[1:, :] # eliminamos los 1 en la Ãºltima capa return h calculo del coste en la Ãºltima capa def calculo_coste(self, theta=none, unrolled=false): if theta is none: # si no se introduce theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if unrolled: # si theta se ha flatten en un vector de una dimension theta = self._roll_theta(theta) # crear lista con matriz theta de capa capa h = self.feed_forward(theta) # obtener la salida para todos los ejemplo coste = -np.sum(np.diagonal(self.y_hot_enc.t.dot(np.log(h)) + (1 - self.y_hot_enc.t).dot(np.log(1 - h))))/self.m # calcular el error con la matriz codificada de y if self.reg: # si se ha indicado que se aplica regularizacion reg_parcial = 0 # inicializamos la variable temporal for elemento in theta: # para capa reg_parcial += np.sum(np.power(elemento[:, 1:], 2)) # no sumar el tÃ©rmino independiente en cada nodo: primera fila reg_result = self.reg_par/(2*self.m)*(reg_parcial) # calcular la regularizacion coste = coste + reg_result return coste actualizar los pesos con propagaciÃ³n hacia atrÃ¡s: def back_propagation(self, theta=none, unrolled=false, unroll=false): if theta is none: # si no se ha indicado ningun theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if unrolled: theta = self._roll_theta(theta) # creamos una lista del array delta = [] # inicializamos las lista temporal que contendra el delta de cada nodo delta_sum = [] # inicializamos la lista temporal que contendra el sumatorio delta gradientes = [] # inicializamos la lista que contendrÃ¡ los gradientes de cada capa h = self.feed_forward(theta=theta) # calculamos el valor del la salida para empezar a propagar hacia atras delta_next = h - self.y_hot_enc # calculamos el primer delta: el de la ultima capa delta.append(delta_next) # lo aÃ±adimos a la lista temporal indice = self.numero_capas - 1 # el indice indica hasta que capa calcular la salida for elemento in reversed(theta[1:]): # recorremos las capas de atras hacia adelante h = self.feed_forward(theta=theta, capa=indice) # calculamos la salida de la capa actual delta_aux = np.multiply(elemento.t.dot(delta_next), self.sigmoid_gradient(elemento, h)) # aplicamos la formula del gradiente delta_next = delta_aux[1:, :] # no cogemos el elemento independiente delta.append(delta_next) # lo aÃ±adimos a la lista de delta indice -= 1 # actualizamos el indice delta.reverse() # damos la vuelta a la lista for indice in range(len(delta)): h = self.feed_forward(theta=theta, capa=indice) # obtenemos la salida de cada capa delta_sum.append(delta[indice].dot(h.t)) # aÃ±adimos (delta * a) a la lista de delta_mayuscula -> sumatorio for indice in range(len(delta_sum)): gradiente = (1/self.m) * delta_sum[indice] # calculamos el grandiente: delta_mayuscula / m if self.reg: gradiente[1:, :] += (self.reg_par/self.m) * theta[indice][1:, :] # si se indica regularizacion aplicarla: no regularizan primer elemento gradientes.append(gradiente) # lo aÃ±adimos a la lista coste = self.calculo_coste(theta=theta) if unroll: # si se ha indicado que se quiere hacer flatten a un vector de una dimension return coste, self._unroll_theta(gradientes) else: return coste, gradientes $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/Ejemplo CÃ¡lculo FunciÃ³n de Coste.html",
    "title": "Ejemplo CÃ¡lculo FunciÃ³n de Coste",
    "body": " index search search back ejemplo cÃ¡lculo funciÃ³n de coste utilizamos como ejemplo la figura de multiclasificaciÃ³n donde tenemos que \\(c=3\\), y la hipÃ³tesis tiene los valores: \\begin{align} h_\\theta(x_1) = \\begin{bmatrix} 0.02 \\\\ 0.1 \\\\ 0.88 \\\\ \\end{bmatrix} \\end{align} y la salida real para el ejemplo \\(x_1\\) tiene los valores: \\begin{align} y_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\end{bmatrix} \\end{align} entonces la funciÃ³n de coste se calcularÃ­a como (observa que esto es sÃ³lo para un ejemplo, por lo que obviamos el primer sumatorio): \\begin{align} j(\\theta) = - \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))] \\end{align} \\begin{align} j(\\theta) = - \\{[(y_{11}\\cdot\\log(h_\\theta(x_1)_{1})) + (1-y_{11})\\cdot\\log(1-h_\\theta(x_1)_{1})] + \\end{align} \\begin{align} + [(y_{21}\\cdot\\log(h_\\theta(x_1)_{2})) + (1-y_{21})\\cdot\\log(1-h_\\theta(x_1)_{2})] + \\end{align} \\begin{align} + [(y_{31}\\cdot\\log(h_\\theta(x_1)_{3})) + (1-y_{31})\\cdot\\log(1-h_\\theta(x_1)_{3})]\\} \\end{align} sustituimos los valores de cada vector: \\begin{align} j(\\theta) = - \\{ [(0\\cdot\\log(0.02)) + (1-0)\\cdot\\log(1-0.02)] + \\end{align} \\begin{align} + [(0\\cdot\\log(0.1)) + (1-0)\\cdot\\log(1-0.1)] + \\end{align} \\begin{align} + [(1\\cdot\\log(0.88)) + (1-1)\\cdot\\log(1-0.88)] \\} = \\end{align} calculamos los valores: \\begin{align} j(\\theta) = - (\\log(0.98) + \\log(0.9) + \\log(0.88)) \\end{align} \\begin{align} j(\\theta) = - (-0.009 - 0.046 -0.056) = - (-0.111) = 0.111 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/Derivadas capas intermedias.html",
    "title": "Derivada capas intermedias",
    "body": " index search search back derivada capas intermedias donde \\(q\\) denota la capa, con \\(1 \\leq q \\leq (k-1)\\). pues lo que tenemos que hacer es, de nuevo, aplicar la regla de la cadena, entre el peso \\(\\theta_{it}^{(q)}\\) (peso \\(t\\) del nodo \\(i\\) de la capa \\(q\\)) y todo nodo \\(a_{lj}^{(q+1)}\\)(es decir para el nodo \\(l\\) en la capa \\(q+1\\) para el ejemplo \\(j\\)). \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\frac{j(\\theta)}{\\delta a^{(q+1)}_{1j}}\\frac{\\delta a_{1j}^{(q+1)}}{\\delta \\theta_{it}^{(q)}} + \\cdots + \\frac{j(\\theta)}{\\delta a^{(q+1)}_{(s_(q+1))j}}\\frac{\\delta a_{(s_(q+1))j}^{(q+1)}}{\\delta \\theta_{it}^{(q)}} \\end{align} donde \\(s_{(q+1)}\\) es el nÃºmero de nodos en la capa \\(q+1\\). para cada tÃ©rmino \\(l\\) de la suma, debemos volver a aplicar la regla de la cadena, tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\end{align} es decir: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{l=1}^{s_{(q+1)}} \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\end{align} cabe destacar que \\(\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} = \\frac{\\delta g(z_{lj}^{(q)})}{\\delta z_{lj}^{(q)}} \\frac{\\delta z_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}}\\) (explicado en derivada de la funciÃ³n del coste). entonces, si generalizamos para todos los ejemplos, \\(m\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{j=1}^m\\sum_{l=1}^{s_{(q+1)}} \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/index.html",
    "title": "Anexo",
    "body": " index search search back anexo clasificaciÃ³n mÃºltiple para crear una red neuronal que permita trabajar con \\(c\\) clases lo que hacemos es hacer que la red neuronal tenga \\(c\\) nodos en su capa de salida. esto se ilustra en la siguiente imagen: de tal manera que ahora, cada salida \\(y_j\\) serÃ¡ un vector columna \\(c\\times1\\), donde existe un valor por cada categorÃ­a, al igual que la hipÃ³tesis para el ejemplo \\(j\\), \\(h_\\theta(x_j)\\), es un vector columna \\(c\\times1\\). como podemos ver, los valores de \\(y_j\\) indican claramente a quÃ© clase pertenece el ejemplo \\(j\\) (clase 3), mientras que la hipÃ³tesis \\(h_\\theta(x_j)\\) ofrece, para cada clase (columna) la probabilidad de que el ejemplo \\(j\\) pertenezca a esa clase. funciÃ³n de coste notaciÃ³n como ya hemos visto en funciÃ³n del nÃºmero de clases la salida tendrÃ¡ distinta forma: clasificaciÃ³n binaria: para cada ejemplo \\(j\\), \\(y_j \\in \\{0, 1\\}\\), \\(h_\\theta(x_j) \\in \\mathbb{r}\\) clasificaciÃ³n mÃºltiple: para cada ejemplo \\(j\\), \\(y \\in \\mathbb{r}^c\\), \\(h_\\theta(x_j) \\in \\mathbb{r}^c\\), donde \\(c\\) es el nÃºmero de clases sea \\(k\\) el nÃºmero de capas y \\(s_i\\) el nÃºmero de nodos en la capa \\(i\\). sea \\(y=(y_{ij})\\) una matriz \\(c\\times m\\), donde \\(m\\) es el nÃºmero de ejemplos y cada \\(y_{j}\\) es el vector columna \\(c\\times1\\) de salida para el ejemplo \\(j\\). definimos la funciÃ³n de coste como sigue: \\begin{align} j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))]\\right\\} \\end{align} el primer sumatorio que va de 1 a \\(m\\) se encarga de calcular el coste para cada ejemplo \\(j\\). mientras que el segundo sumatorio, que va de 1 a \\(c\\), se encarga de calcular el coste para cada nodo de salida. esta funciÃ³n se aplica sobre los \\(k\\) nodos en la capa de salida. ejemplo cÃ¡lculo funciÃ³n de coste regularizaciÃ³n definimos la funciÃ³n de coste introduciendo regularizaciÃ³n como sigue: \\begin{align} j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))]\\right\\} + \\frac{\\lambda}{2m} \\sum_{q=1}^k \\sum_{i=1}^{s_q}\\sum_{j=1}^{s_{q+1}} (\\theta_{ji}^{(q)})^2 \\end{align} antes de nada, recordar que \\(s_q\\) denota el nÃºmero de nodos en la capa \\(q\\). entonces, el primer tÃ©rmino de la funciÃ³n es igual que cuando no se aplicaba regularizaciÃ³n. expliquemos el segundo tÃ©rmino. la regularizaciÃ³n, en este caso, consiste en sumar todos los pesos de la red neuronal, por lo tanto: por cada capa \\(q\\), con \\(1 \\leq q \\leq k\\), sumamos todos los elementos de la matriz de pesos \\(\\theta^{q}\\), que como sabemos tiene dimensiones \\(s_{q} \\times s_{q-1}\\) dada la matriz \\(\\theta^{(q)}\\) recorremos cada columna \\(i\\), con \\(1 \\leq i \\leq s_{q-1}\\) recorremos cada elemento \\(j\\) de la columna \\(i\\), con \\(1 \\leq j \\leq s_{q}\\) sumamos al total cada elemento de la matriz \\(\\theta^{(q)}_{ji}\\) una vez se han sumado todas las matrices de pesos obtenemos un escalar, que multiplicamos por \\(\\frac{\\lambda}{2m}\\) mÃºltiple ejemplos la salida de cada capa \\(q\\) es una matriz \\(s_q \\times m\\), donde \\(s_q\\) denota el nÃºmero de nodos en la capa \\(q\\) y \\(m\\) denota el nÃºmero de ejemplos. como vimos en nuestras figuras, donde se presentaban los cÃ¡lculos sÃ³lo para un ejemplo, en cada capa \\(q\\) podemos mapear la salida de los \\(s_q\\) nodos a un vector columna \\(s_q \\times 1\\). si generalizamos esto a \\(m\\) ejemplos tenemos que la salida de cada capa es una matriz \\(s_q \\times m\\). esto se ilustra en la siguiente imagen: retropropagaciÃ³n vamos, ahora a explicar cÃ³mo se aplica la retropropagaciÃ³n. lo primero que debemos tener en cuenta es que este proceso se basa en la misma idea de optimizaciÃ³n que la regresiÃ³n lineal y la regresiÃ³n logÃ­stica, es decir, lo que queremos hacer es minimizar el coste, \\(j(\\theta)\\) sea \\(c\\) el nÃºmero de nodos en la Ãºltima capa, \\(\\theta_{it}\\) el peso \\(t\\) del nodo \\(i\\) de la Ãºltima capa \\(k\\), \\(a_{ij}^{(k)}\\) la salida del nodo \\(i\\) para el ejemplo \\(j\\) en la capa \\(k\\): calculamos el gradiente de la Ãºltima capa \\(k\\) como: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta j(\\theta)}{\\delta a_{1j}^{(k)}}\\frac{\\delta a_{1j}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) calculamos el gradiente en capas intermedias utilizando la regla de la cadena como: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{i=1}^{s_{(q+1)}} \\frac{\\delta j(\\theta)}{\\delta a_{ij}^{(q+1)}}\\frac{\\delta a_{ij}^{(q+1)}}{\\delta a_{ij}^{(q)}}\\frac{\\delta a_{ij}^{(q)}}{\\delta \\theta_{it}^{(q)}}\\) normalmente en las capas intermedias, \\(q\\), nos referimos al tÃ©rmino \\(\\frac{\\delta j(\\theta)}{\\delta a_{ij}^{(q+1)}}\\) como \\(\\delta^{(q+1)}_{ij}\\). explicaciÃ³n de la retropropagaciÃ³n derivada de la funciÃ³n de coste a continuaciÃ³n explicamos cÃ³mo derivar la funciÃ³n de coste (paso 1). derivada de la funciÃ³n de coste capas intermedias veamos, ahora, cÃ³mo llevar a cabo el paso 2: Â¿cÃ³mo calculamos el gradiente (o lo que contribuye el peso \\(it\\) en el error) para los pesos de las capas intermedias?, es decir, cÃ³mo calculamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} \\end{align} derivadas capas intermedias ejemplo de retropropagaciÃ³n ejemplo de retropropagaciÃ³n algoritmo partes del algoritmo en python notaciÃ³n Ã©poca: iteraciÃ³n en el entrenamiento pesos (\\(w_j^k, b_j^k\\)): se inicializan de forma aletoria (evitar simetrÃ­a) y con valores bajos. criterios de finalizaciÃ³n \\(j\\) o gradiente de \\(j\\) inferior a un umbral nÃºmero mÃ¡ximo de Ã©pocas velocidad de apredizaje \\(\\mu\\) intermedia: evita lentitud en las oscilaciones caÃ­da en mÃ­nimos locales que pueden tener \\(j\\) elevado. es por ello que se ejecuta varias veces el entrenamiento y se selecciona aquel que obtenga mejor resultado. actualizaciÃ³n de pesos patrÃ³n a patrÃ³n en lugar de tras computar el error sobre todo el dataset. puede evitar mÃ­nimos locales y converge antes. funciÃ³n de activaciÃ³n sigmoide para clasificaciÃ³n o linear para regresiÃ³n pseudocÃ³digo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/ExplicaciÃ³n de la retropropagaciÃ³n.html",
    "title": "ExplicaciÃ³n de la retropropagaciÃ³n",
    "body": " index search search back explicaciÃ³n de la retropropagaciÃ³n contents descenso gradiente derivada del error gradiente capa de salida retropropagaci n del gradiente gradiente acumulado derivadas de las funciones de activaci n componentes: \\(w_j^k\\): peso de la neurona \\(j=1...i_k\\) en la capa \\(k=1...h\\) \\(a_{ij}^k=(w_j^k)^th_i^{k-1}+b_j^k\\): \\(i=1...n\\) (patrÃ³n), \\(k=1...h\\)(capa), \\(j=1...i_k\\) (neurona de la capa \\(k\\)). \\(h_i^{k-1}\\): salida de la capa \\(k-1\\) con \\(i_{k-1}\\) valores (uno por cada neurona \\(j=1...i_{k-1}\\) para cada patrÃ³n \\(x_i\\). \\(h_i^k\\): salida de la capa \\(k\\) para cada patrÃ³n \\(x_i\\): \\(h_{ij}=f(a_{ij}^k)\\) con \\(j=1...i_k\\) \\(y_{ij}\\): salida verdadera de la neurona de salida \\(j\\) y el patrÃ³n \\(x_i\\). descenso gradiente \\begin{align} \\delta w_j^k=-\\mu \\frac{\\delta j}{\\delta w_j^k} \\end{align} \\begin{align} \\delta b_j^k=-\\mu \\frac{\\delta j}{\\delta b_j^k} \\end{align} para \\(k=1...h\\). de tal forma que se actualizan los pesos \\(w_j^k\\) y el offset \\(b_j^k\\) de la capa \\(k\\) y de la neurona \\(j\\). tenemos que la capa de salida estÃ¡ compuesta de \\(i_h\\) neuronas que se recorren con el Ã­ndice \\(j\\). accedemos a la salida verdadera del ejemplo \\(i\\) para la neurona \\(j\\) (\\(y_{ij}\\)) y restamos la salida predicha \\(h_{ij}^h\\) que hace referencia a la salida de la funciÃ³n de activaciÃ³n de la capa \\(h\\) para la neurona \\(j\\) y el ejemplo \\(i\\). la diferencia se eleva al cuadrado para obtener mse. tambiÃ©n se puede vectorizar restando los vectores \\(y_i\\) y \\(h_i^h \\in \\mathbb{r}^j\\). de esta manera obtenemos el error para el patrÃ³n \\(x_i\\): \\(j_i\\). derivada del error aplicamos la regla de la cadena sobre \\(j_i\\), ya que este depende de \\(a_{ij}^k\\): \\begin{align} \\frac{\\delta j_i}{\\delta w_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k} \\end{align} \\begin{align} \\frac{\\delta j_i}{\\delta b_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k} \\end{align} definimos: \\begin{align} \\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k} \\end{align} este indica el gradiente de la capa siguiente, para evitar tener que calcularlo. \\begin{align} \\frac{\\delta a_{ij}^k}{\\delta w_j^k} = h_i^{k-1} \\end{align} \\begin{align} \\frac{\\delta a_{ij}^k}{\\delta b_j^k} = 1 \\end{align} debido a que el valor de \\(a_{ij}^k\\) es la combinaciÃ³n lineal de la entradas y los pesos, donde las entradas son las salidas de la capa anterior (\\(k-1\\)), es decir \\(h_i^{k-1}\\), de tal manera que: \\begin{align} a_{ij}^k = (w_j^k)^th_i^{k-1}+b_j^k \\end{align} por lo que la derivada en funciÃ³n de \\(w_j^k\\) se corresponde con \\(h_i^{k-1}\\) y la derivada en funciÃ³n de \\(b_j^k\\) es 1. gradiente si sustituimos \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\) y \\(\\frac{\\delta a_{ij}^k}{\\delta w_j^k} = h_i^{k-1}\\) en \\(\\frac{\\delta j_i}{\\delta w_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k}\\) obtenemos: \\begin{align} \\delta w_j^k=-\\mu \\frac{\\delta j}{\\delta w_j^k} \\end{align} \\begin{align} \\delta w_j^k=-\\mu \\sum_{i=1}^n \\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k} = -\\mu \\sum_{i=1}^n\\delta_{ij}^kh_i^{k-1} \\end{align} hacemos los mismo para el offset sustituyendo \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\) y \\(\\frac{\\delta a_{ij}^k}{\\delta b_j^k} = 1\\) en \\(\\frac{\\delta j_i}{\\delta b_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k}\\) obtenemos: \\begin{align} \\delta b_j^k=-\\mu \\frac{\\delta j}{\\delta b_j^k} \\end{align} \\begin{align} \\delta b_j^k= -\\mu \\sum_{i=1}^n\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k}=-\\mu \\sum_{i=1}^n\\delta_{ij}^k \\end{align} capa de salida calculamos \\(\\delta_{ij}^k\\) en la capa de salida (\\(k=h\\)), cuyo valor se va a propagar hacia las capas anteriores. lo que vamos a calcular es \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\). tenemos que la funciÃ³n de coste para el patrÃ³n \\(i\\), \\(j_i\\) viene definida por: \\begin{align} j_i=\\frac{1}{2}\\sum_{j=1}^{i_h}(y_{ij}-h_{ij}^h)^2=\\frac{|y_i-h_i^h|^2}{2} \\end{align} ademÃ¡s el valor de \\(a_{ij}^k\\), que es la combinaciÃ³n lineal de las entradas (salidas de las neuronas capa anterior, \\(k-1\\)) y los pesos junto con el offset: \\begin{align} a_{ij}^k=(w_j^k)^th_i^{k-1}+b_j^k \\end{align} por lo tanto en la capa final: \\begin{align} \\frac{\\delta j_i}{\\delta a_{ij}^h}=\\frac{1}{2}\\frac{\\delta (y_{ij}-h_{ij}^h)^2}{\\delta (y_{ij}-h_{ij}^h)}\\frac{\\delta (y_{ij}-h_{ij}^h)}{\\delta a_{ij}^h} \\end{align} donde: \\begin{align} \\frac{\\delta (y_{ij}-h_{ij}^h)^2}{\\delta (y_{ij}-h_{ij}^h)}=2(y_{ij}-h_{ij}^h) \\end{align} \\begin{align} \\frac{\\delta (y_{ij}-h_{ij}^h)}{\\delta a_{ij}^h}=\\frac{\\delta y_{ij}}{\\delta a_{ij}^h}-\\frac{\\delta h_{ij}^h}{\\delta a_{ij}^h}=0-f'(a_{ij}^h) \\end{align} ya que sabemos que \\(h_{ij}^h=f(a_{ij}^h)\\), por lo que: \\begin{align} \\frac{\\delta h_{ij}^h}{\\delta a_{ij}^h}=\\frac{\\delta f(a_{ij}^h)}{\\delta a_{ij}^h}=f'(a_{ij}^h) \\end{align} una vez desarrollado todo esto sustiuimos los resultados en \\(\\frac{\\delta j_i}{\\delta a_{ij}^h}\\): \\begin{align} \\frac{\\delta j_i}{\\delta a_{ij}^h}=\\frac{1}{2}2(y_{ij}-h_{ij}^h)(-f'(a_{ij}^h))=(y_{ij}-h_{ij}^h)f'(a_{ij}^h) \\end{align} de tal forma que: \\begin{align} \\delta_{ij}^h=\\frac{\\delta j_i}{\\delta a_{ij}^h}=(y_{ij}-h_{ij}^h)f'(a_{ij}^h)=\\epsilon_{ij}^hf'(a_{ij}^h) \\end{align} donde se define \\(\\epsilon_{ij}^h\\) como: \\begin{align} \\epsilon_{ij}^h=y_{ij}-h_{ij}^h \\end{align} finalmente obtenemos que el antigradiente en la Ãºltima capa \\(h\\) viene dado por: \\begin{align} \\delta w_j^h=-\\mu \\sum_{i=1}^n\\delta_{ij}^hh_i^{h-1}=-\\mu\\sum_{i=1}^n\\epsilon_{ij}^hf'(a_{ij}^h)h_i^{h-1} \\end{align} \\begin{align} \\delta b_j^h=-\\mu \\sum_{i=1}^n\\delta_{ij}^h=-\\mu\\sum_{i=1}^n\\epsilon_{ij}^hf'(a_{ij}^h) \\end{align} retropropagaciÃ³n del gradiente para las capas anteriores a la capa de salida (\\(k<h\\)): \\begin{align} \\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\frac{\\delta j_i}{\\delta a_{il}^{k+1}}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k} \\end{align} en este caso se utiliza la regla de la cadena para obtener \\(\\delta_{ij}^k\\) de modo que se tienen en cuenta todas las combinaciones del gradiente acumulado \\(\\delta a_{il}^{k+1}\\) con la neurona actual (\\(\\delta a_{ij}^k\\)) donde \\(l=1...i_{k+1}\\), es decir se tienen encuenta todas las neuronas de la capa siguiente. con grafos, la regla de la cadena se puede interpretar como todos los caminos posibles desde la capa de salida hasta la neurona \\(j\\) en la capa \\(k\\). cada camino une cada neurona \\(l\\) de la capa siguiente: \\(\\delta_{il}^{k+1}\\) (el cual ya tiene el gradiente acumulado de las capas siguientes) con una neurona \\(j\\) de la capa actual: \\(\\delta a_{ij}^k\\) de la siguiente forma: \\(\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\). ademÃ¡s se suman todas las combinaciÃ³n posibles: \\(\\sum_{l=1}^{i_{k+1}}\\). esto nos permite utilizar el gradiente acumulado calculado en la capa siguiente que se propaga hacia atrÃ¡s en la red neuronal, lo que evita tener que calcular \\(\\frac{\\delta j_i}{\\delta a_{ij}^k}\\). por otro lado tenemos: \\begin{align} a_{il}^{k+1}=(w_l^{k+1})^th_i^{k}+b_l^{k+1} \\end{align} que es el cÃ¡lculo de la neurona \\(l\\) de la capa siguiente, por lo que utiliza como entradas las salidas de la neurona de esta capa \\(h_i^{k}\\). esta es la versiÃ³n vectorizada del cÃ¡lculo, si lo expresamos como sumatorio: \\begin{align} a_{il}^{k+1}=\\sum_{m=1}^{i_k}w_{lm}^{k+1}h_{im}^{k}+b_{lm}^{k+1}=\\sum_{m=1}^{i_k}w_{lm}^{k+1}f(a_{im}^{k})+b_{lm}^{k+1} \\end{align} de tal forma que se multiplican los \\(i_k\\) pesos de la capa siguiente (\\(w_{lm}^{k+1}\\)) con las \\(i_k\\) salidas de la capa actual (\\(h_{im}^{k}\\)) y sumamos los offset (\\(b_{lm}^{k+1}\\)). ademÃ¡s sabemos que \\(h_{im}^{k}=f(a_{im}^{k})\\). por lo tanto: \\begin{align} \\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=\\sum_{m=1}^{i_k}(\\frac{\\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\\delta a_{ij}^k}+\\frac{\\delta b_{lm}^{k+1}}{\\delta a_{ij}^k}) \\end{align} la primera derivada tiene la siguiente forma: \\begin{align} \\frac{\\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\\delta a_{ij}^k}=w_{lm}^{k+1}\\frac{\\delta f(a_{im}^{k})}{\\delta a_{ij}^k} \\end{align} \\begin{align} \\frac{\\delta f(a_{im}^{k})}{\\delta a_{ij}^k} =\\begin{cases} f'(a^k_{im})=f'(a^k_{ij}) & m=j\\\\ 0 & m \\ne j \\end{cases} \\end{align} por lo que podemos eliminar el sumatorio sobre \\(m\\) y la derivada sobre el offset ya que su valor es nulo: \\begin{align} \\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=w_{lj}^{k+1}f'(a^k_{ij}) + 0 \\end{align} gradiente acumulado si volvemos a \\(\\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\). sustituimos \\(\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\) obteniendo: \\begin{align} \\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1}f'(a^k_{ij}) \\end{align} podemos extraer \\(f'(a^k_{ij})\\) ya que esta no depende de \\(l\\): \\begin{align} \\delta_{ij}^k=f'(a^k_{ij})\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1} \\end{align} si definimos: \\begin{align} \\epsilon_{ij}^k=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1} \\end{align} tenemos que: \\begin{align} \\delta_{ij}^k=f'(a^k_{ij})\\epsilon_{ij}^k \\end{align} derivadas de las funciones de activaciÃ³n la derivada de la funciÃ³n sigmoide: \\begin{align} f'(t)=af(t)(1-f(t)) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/Ejemplo de retropropagaciÃ³n.html",
    "title": "Ejemplo de retropropagaciÃ³n",
    "body": " index search search back ejemplo de retropropagaciÃ³n contents capa 3 capa 2 capa 1 por ejemplo, supongamos que tenemos una red con tres capas, entonces \\(k=3\\), dado un ejemplo \\(x_j\\). en este caso tenemos que capa 3 la derivada en la Ãºltima capa, para el Ãºnico vector de pesos \\(\\theta^{(3)}_1\\) que tiene \\(n\\) elementos (features o caracterÃ­sticas), es: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{1t}^{(3)}}\\), para cada \\(t\\), \\(0 \\leq t \\leq n\\) como: \\begin{align} j(\\theta) = e^{(3)}(a_1^{(3)}) = e^{(3)}(g(z_1^{(3)})) = e^{(3)}(g(\\theta^{(3)}\\cdot a^{(2)})) \\end{align} donde denotamos la funciÃ³n que calcula el error entre lo predicho y la salida real como \\(e\\), y \\(g\\) es la funciÃ³n de activaciÃ³n. entonces, aplicamos la regla de la cadena para cada elemento \\(t\\) en el vector de pesos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{1t}^{(3)}} = \\frac{\\delta j(\\theta)}{\\delta a_1^{(3)}}\\frac{\\delta a_1^{(3)}}{\\delta z_1^{(3)}}\\frac{\\delta z_1^{(3)}}{\\delta \\theta_{1t}^{(3)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{1}^{(3)}} = \\frac{\\delta j(\\theta)}{\\delta a_1^{(3)}}\\frac{\\delta a_1^{(3)}}{\\delta z_1^{(3)}}\\frac{\\delta z_1^{(3)}}{\\delta \\theta_{1}^{(3)}} \\end{align} capa 2 si ahora queremos obtener la derivada para uno de los vectores de pesos en la capa \\(2\\), volvemos a aplicar la regla de la cadena. tenemos ahora que desestructurar la funciÃ³n de coste todavÃ­a mÃ¡s, hasta obtener la expresiÃ³n que incluye las salidas de la capa \\(1\\), \\(a^{(1)}\\). \\begin{align} j(\\theta) = e^{(3)}(g(\\theta^{(3)}\\cdot a^{(2)})) = e^{(3)}(g(\\theta^{(3)}\\cdot g(z^{(2)}))) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot a^{(1)}))) \\end{align} sea \\(\\delta^{(3)}_{1j}\\): \\begin{align} \\delta^{(3)}_{1j} = \\frac{\\delta j(\\theta)}{\\delta a_{1j}^{(3)}}\\frac{\\delta a_{1j}^{(3)}}{\\delta z_{1j}^{(3)}} \\end{align} entonces, aplicamos la regla de la cadena para cada nodo \\(i\\) de la capa \\(2\\) y para cada elemento \\(t\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(2)}} = \\sum_{l=1}^{s_{(3)}} \\delta_{lj}^{(3)}\\frac{\\delta z_{lj}^{(3)}}{\\delta a_{lj}^{(2)}}\\frac{\\delta a_{lj}^{(2)}}{\\delta z_{lj}^{(2)}}\\frac{\\delta z_{lj}^{(2)}}{\\delta \\theta_{it}^{(2)}} = \\delta_{1j}^{(3)}\\frac{\\delta z_{1j}^{(3)}}{\\delta a_{1j}^{(2)}}\\frac{\\delta a_{1j}^{(2)}}{\\delta z_{1j}^{(2)}}\\frac{\\delta z_{1j}^{(2)}}{\\delta \\theta_{it}^{(2)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{i}^{(2)}} = \\delta_{j}^{(3)}\\frac{\\delta z^{(3)}}{\\delta a_{j}^{(2)}}\\frac{\\delta a_{j}^{(2)}}{\\delta z_{j}^{(2)}}\\frac{\\delta z_{j}^{(2)}}{\\delta \\theta_{i}^{(2)}} \\end{align} capa 1 para la capa \\(1\\), volvemos a expandir la funciÃ³n de coste para ver cÃ³mo aplicar la regla de la cadena: \\begin{align} j(\\theta) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot a^{(1)}))) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot g(z^{(1)})))) = \\end{align} \\begin{align} = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot g(\\theta^{(1)} x_j)))) \\end{align} para simplificar la notaciÃ³n: sea, para cada nodo \\(l\\) de la capa \\(2\\) \\begin{align} \\delta^{(2)}_{lj} = \\delta_{1j}^{(3)}\\frac{\\delta z_1^{(3)}}{\\delta a_{lj}^{(2)}}\\frac{\\delta a_{lj}^{(2)}}{\\delta z_{lj}^{(2)}} \\end{align} aplicamos la regla de la cadena, tal que para cada nodo \\(l\\) de la capa \\(2\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(1)}} = \\sum_{l=1}^{s_{(2)}} \\delta_{lj}^{(2)}\\frac{\\delta z_{lj}^{(2)}}{\\delta a_{lj}^{(1)}}\\frac{\\delta a_{lj}^{(1)}}{\\delta z_{lj}^{(1)}}\\frac{\\delta z_{lj}^{(1)}}{\\delta \\theta_{it}^{(1)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{i}^{(1)}} = \\delta_{j}^{(2)}\\frac{\\delta z_{j}^{(2)}}{\\delta a_{j}^{(1)}}\\frac{\\delta a_{j}^{(1)}}{\\delta z_{j}^{(1)}}\\frac{\\delta z_{j}^{(1)}}{\\delta \\theta_{i}^{(1)}} \\end{align} el procedimiento se ilustra en la siguiente figura: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/Derivada de la funciÃ³n de coste.html",
    "title": "Derivada de la FunciÃ³n de Coste",
    "body": " index search search back derivada de la funciÃ³n de coste contents capa de salida sabemos que la funciÃ³n de coste: \\begin{align} j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m\\sum_{i=1}^c (y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\end{align} donde \\(\\theta_{it}^{(k)}\\) es el peso que conecta el nodo \\(i\\) de la capa \\(k\\) con el nodo \\(t\\) de la capa \\((k-1)\\), es decir, es el elemento en la fila \\(i\\) columna \\(t\\) de la matriz de pesos de la capa \\(k\\), \\(\\theta^{(k)}\\). por la regla de la cadena, separamos la derivada de la funciÃ³n del coste en funciÃ³n de los pesos en dos tÃ©rminos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\sum_{j=1}^m \\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} capa de salida procedemos a calcular la derivada: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left(- \\frac{1}{m}\\right) \\left\\{ \\sum_{j=1}^m\\sum_{i=1}^c (y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\end{align} sacamos el tÃ©rmino constante de la derivada y aplicamos la propiedad: \"la derivada de una suma equivale a la suma de las derivadas\" \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left\\{(y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\end{align} sea \\(h_\\theta(x_j) = a^{(k)}_j\\), es decir la salida de la Ãºltima capa para el ejemplo \\(j\\). \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left\\{(y_{ij}\\cdot \\log(a^{(k)}_{ij})) + [(1-y_{ij})\\log(1-a^{(k)}_{ij})]\\right\\} \\end{align} sacaremos el tÃ©rmino \\(y_{ij}\\) de la derivada y juntemos todas las expresiones: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\log(a^{(k)}_{ij}) \\right) + (1-y_{ij}) \\left(\\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\log(1-a^{(k)}_{ij})\\right)\\right\\} \\end{align} aplicamos la regla de la cadena sobre el logaritmo: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\right) + (1-y_{ij}) \\left(\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\frac{\\delta (1-a^{(k)}_{ij})}{\\delta \\theta_{it}^{(k)}} \\right)\\right\\} \\end{align} como sabemos: \\(\\frac{\\delta (1)}{\\delta \\theta_{it}^{(k)}} = 0\\), entonces \\(\\frac{\\delta(1-a_{ij}^{(k)})}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta (1)}{\\delta \\theta_{it}^{(k)}} - \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = 0 + (-1) \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) entonces \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\right) + (1-y_{ij}) \\left((-1)\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\frac{\\delta a^{(k)}_{ij}}{\\delta \\theta_{it}^{(k)}} \\right)\\right\\} \\end{align} sacamos \\(\\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) como factor comÃºn y aplicamos el \\((-1)\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\left\\{y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - \\left((1-y_{ij})\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\right)\\right\\} \\end{align} sustituimos \\(\\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} = \\left\\{y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - \\left((1-y_{ij})\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\right)\\right\\}\\) \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} si resolvemos las derivadas de los logaritmos obtenemos: \\begin{align} \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} = y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - (1-y_{ij})\\frac{\\delta \\log(1-a_{ij}^{(k)})}{\\delta (1-a_{ij}^{(k)})} \\end{align} nos centraremos ahora en la derivada que nos falta \\(\\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\): sabemos que, vectorizando la operaciÃ³n, \\(a^{(k)}_j = g(z^{(k)}_j)\\), donde \\(g\\) es la funciÃ³n de activaciÃ³n (en este caso sigmoide). ademÃ¡s: \\begin{align} z^{(k)}_j = \\theta^{k} \\cdot a^{(k-1)}_j \\end{align} por lo tanto, para cada nodo \\(i\\) en la Ãºltima capa \\(k\\): \\begin{align} z^{(k)}_{ij} = \\sum_{l=1}^{s_{(k-1)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} \\end{align} donde \\(s_{(k-1)}\\) es el nÃºmero de nodos en la capa \\(k-1\\). entonces, aplicamos de nuevo la regla de la cadena: \\begin{align} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{(k)}} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} resolvemos la derivada para el segundo tÃ©rmino: \\begin{align} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\sum_{l=1}^{s_{(k-1)}} \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} \\end{align} tal que: \\begin{align} \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} = \\begin{cases} a_{lj}^{(k-1)}, & t = l \\\\ 0, & t \\neq l \\\\ \\end{cases} \\end{align} por lo tanto, como sÃ³lo hay un \\(l\\) con \\(l = t\\) donde \\(1 \\leq l \\leq s_{(k-1)}\\), entonces: \\begin{align} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = a_{lj}^{(k-1)} = a_{tj}^{(k-1)} \\end{align} juntamos ambos tÃ©rminos de la derivada inicial, con \\(\\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{k}} = \\sigma'(z_{ij}^{(k)})\\) \\begin{align} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{(k)}} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)} \\end{align} vamos a resumir lo que tenemos hasta ahora. por la regla de la cadena, separamos la derivada de la funciÃ³n del coste en funciÃ³n de los pesos en dos tÃ©rminos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\sum_{j=1}^m \\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} si sustituimos ambos tÃ©rminos, para la capa de salida \\(k\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)}\\left\\{ \\frac{y_{ij}}{a_{ij}^{(k)}} - \\left(\\frac{(1-y_{ij})}{(1-a^{(k)}_{ij})} \\right)\\right\\} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/NeuralNetworks/Neural Networks.html",
    "title": "Neural Networks",
    "body": " index search search back neural networks contents architecture algorithm output layer sigmoid softmax relu loss function forward propagation parameters outputs graphical representation optimization problem loss function optimization back-propagation output layer hidden layers vectorization output layer hidden layer graphical representation improving a neural network activation functions initialization techniques vanishing exploding gradients intuition initialization techniques optimization mini batch gradient descent momentum algorithm anexo architecture algorithm output layer forward propagation parameters outputs graphical representation optimization problem loss function optimization back-propagation vectorization improving a neural network activation functions initialization techniques anexo architecture input: given any input \\(x\\) the first thing we do is flatten it. for example if \\(x\\) is a rgb image of \\(64 \\times 64\\), then \\(x \\in \\mathbb{r}^{64 \\times 64 \\times 3}\\) (for each of the \\(64 \\times 64\\) pixels we have three color channels: red, green, blue), is flattened into a vector in \\(\\mathbb{r}^{(64*64*3) \\times 1}\\) neuron: is an operation that has two parts: linear part: we denote the linear part like \\(z^{[i]}\\), where \\(i\\) is the current layer. activation part layer: a layer is a compound of neurons that are not connected with each other. algorithm the principal steps of the algorithm are: initialize the weights \\(w\\) and biases \\(b\\) randomly find the optimal \\(w, b\\) use the optimized \\(w, b\\) to predict the output by using the formula \\(\\hat{y} = \\sigma(wx +b)\\) output layer sigmoid the output layer will be different depending on the problem we are tackling. for example if we want to discriminate between 3 classes then the output layer could be as follows: so now the output is a vector \\(\\hat{y} \\in \\mathbb{r}^{c \\times 1}\\) where \\(c\\) is the number of classes. softmax the previous classifier allows for outputting multiples classes in the result, that is we can obtain a predicted output of the form \\(\\hat{y} = \\begin{bmatrix} 1 \\\\1 \\\\ 0 \\end{bmatrix}\\). what if we want to add a constraint such that only one class can be predicted. then we use the softmax function as the activation function on the output layer: thus, instead of a probability for each class what we obtain is a probability distribution for all the classes. relu on linear regression we do not want the activation function to be linear, because then the whole network becomes a very large linear regression. instead we use as an activation function the relu function (rectified linear unit), whose output is zero if the input value is negative and linear otherwise. loss function the loss function when using the sigmoid function on the output layer is as follows: \\begin{align} \\mathcal{l} = - \\frac{1}{q} \\sum_{k=1}^q [y^{(k)} \\log(\\hat{y}^{(k)}) + (1- y^{(k)})\\log(1-\\hat{y}^{(k)})] \\end{align} where \\(\\hat{y}^{(k)}\\) are the predicted values and \\(q\\) is the total number of neurons on the output layer. however, if we use the softmax function as the activation function on the last layer we have to use a different derivative because this function does depend on the outputs of the other neurons. thus, we make use of a function called cross entropy loss: \\begin{align} \\mathcal{l}_{ce} = - \\sum_{k=1}^q y^{(k)} \\log(\\hat{y}^{(k)}) \\end{align} for linear regression we use as the loss function the l1-norm or the l2-norm. the latter is defined as follows: \\begin{align} \\mathcal{l} = || \\hat{y} - y ||_2^2 \\end{align} forward propagation the forward propagation equations are the following: \\begin{align} z^{[i]} = w^{[i]} a^{[i-1]} + b^{[i]} \\tag{1} \\end{align} where \\(i\\) is the layer with \\(i \\geq 1\\), and the first layer equals the input matrix, that is \\(a^{[0]} = x\\). by applying the activation function over \\((1)\\): \\begin{align} a^{[i]} = g(z^{[i]}) \\end{align} where \\(g\\) is the activation function (e.g \\(\\sigma(z^{[i]})\\)). now, what are the shapes of these matrices? \\(z^{[i]} \\in \\mathbb{r}^{s_i \\times m}\\) \\(a^{[i]} \\in \\mathbb{r}^{s_i \\times m}\\) where \\(s_i\\) is the number of neurons on the ith layer and \\(m\\) is the number of examples. note that the shape of the final layer changes depending on the task. so if \\(k\\) is the number of layers: in linear regression: \\(\\hat{y} = a^{[k]} \\in \\mathbb{r}^{1 \\times m}\\) in multi-class classification: \\(\\hat{y} = a^{[k]} \\in \\mathbb{r}^{c \\times m}\\), where \\(c\\) is the number of classes. also the shape of the weights are \\(w[i] \\in \\mathbb{r}^{s_i \\times s_{i-1}}\\), that is, this matrix is compatible with the outputs of the previous layer (\\(a^{[i-1]} \\in \\mathbb{r}^{s_{i-1} \\times m}\\)) and the linear part of the next layer (\\(z^{[i]} \\in \\mathbb{r}^{s^i \\times m}\\)). however, the bias are \\(b^{[i]} \\in \\mathbb{r}^{s^i \\times 1}\\), therefore we cannot perform an element wise summation because the shape of \\((w^{[i]} a^{[i-1]}) \\in \\mathbb{r}^{s_i \\times m}\\) and \\(b^{[i]}\\) are not compatible. to avoid this problem we apply a technique called broadcasting to \\(b\\), such that we replicate \\(b^{[i]}\\) \\(m\\) times: \\begin{align} \\hat{b}^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ b^{[i]} & b^{[i]} & \\cdots & b^{[i]} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} to sum up, the shapes of the data and the parameters on each layer \\(i\\) are: parameters \\begin{align} \\hat{b}^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ b^{[i]} & b^{[i]} & \\cdots & b^{[i]} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} \\begin{align} w^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ w^{[i](1)} & w^{[i](2)} & \\cdots & w^{[i](s_{i-1})} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times s_{i-1}} \\end{align} outputs note that for each example \\(j\\) on layer \\(i\\) \\(z^{[i](j)} = (w^{[i]} a^{[i-1](j)} + \\hat{b}^{[i]})\\), then: \\begin{align} z^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ z^{[i](1)} & z^{[i](2)} & \\cdots & z^{[i](m)} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} \\begin{align} a^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ g(z^{[i](1)}) & g(z^{[i](2)}) & \\cdots & g(z^{[i](m)}) \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} graphical representation now we present a small example of how forward propagation works on neural networks: optimization problem what we want to do is find the parameters \\(w^{[i]}, b^{[i]}\\) for each layer \\(i\\) that minimize the cost. loss function so first of all we define a cost function for the objective \\(\\mathcal{l}(\\hat{y}, y)\\), where \\(\\hat{y}\\) is the predicted output and \\(y\\) is the real output. the cost function will depend on the type of problem (classification, regression). optimization the we optimize the loss function we defined by using backward propagation. for each layer \\(l=1, \\cdots, k\\), where \\(k\\) is the number of layers, we apply batch gradient descent (not mandatory, but here it is convenient as we can vectorize the derivatives) as follows: \\begin{align} w^{[l]} = w^{[l]} - \\alpha \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[l]}} \\end{align} \\begin{align} b^{[l]} = b^{[l]} - \\alpha \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta b^{[l]}} \\end{align} back-propagation to compute the derivatives of the cost function with respect to \\(w^{[l]}\\) and \\(b^{[l]}\\) we use the chain rule. output layer suppose we have \\(k\\) layers, then we start by calculating \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}}\\) and \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta b^{[k]}}\\), that is, the derivatives on the last layer. thus, to update \\(w^{[k]}\\) (we apply the same logic for \\(b^{[k]}\\)): \\begin{align} \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}(\\hat{y^{(i)}}, y^{(i)})}{\\delta w^{[k]}} = \\end{align} because \\(\\hat{y^{(i)}} = (a^{[k]})^{(i)}\\): \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta w^{[k]}} \\end{align} we apply the chain rule on the derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta w^{[k]}} \\end{align} because \\((a^{[k]})^{(i)} = g((z^{[k]})^{(i)})\\), where \\(g\\) is the activation function: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta w^{[k]}} \\end{align} we apply the chain rule on the last derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k]}} \\end{align} hidden layers what about the previous layer \\(k-1\\)? \\begin{align} \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k-1]}} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}(\\hat{y^{(i)}}, y^{(i)})}{\\delta w^{[k-1]}} = \\end{align} because \\(\\hat{y^{(i)}} = (a^{[k]})^{(i)}\\): \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((a^{[k]})^{(i)} = g((z^{[k]})^{(i)})\\), where \\(g\\) is the activation function: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the last derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} as you can see the first two derivatives are the same as the derivatives on the layer \\(k\\), let's denote \\((\\delta^{[k]})^{(i)} = \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta (z^{[k]})^{(i)}}\\) the accumulated gradient on layer \\(k\\) for example \\(i\\), then: \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((z^{[k]})^{(i)} = w^{[k]} (a^{[k-1]})^{(i)} + b^{[k]}\\): \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (w^{[k]} (a^{[k-1]})^{(i)} + b^{[k]})}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta (a^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((a^{[k-1]})^{(i)} = g((z^{[k-1]})^{(i)})\\) \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta g((z^{[k-1]})^{(i)})}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the last derivative, hence: \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta g((z^{[k-1]})^{(i)})}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta (a^{[k-1]})^{(i)}}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} vectorization output layer accumulated gradient for layer \\(k\\): \\(\\delta_w^{[k]} = \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta a^{[k]}} \\frac{\\delta a^{[k]}}{\\delta z^{[k]}}\\) gradient for layer \\(k\\): \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}} = \\delta_w^{[k]} \\frac{\\delta z^{[k]}}{\\delta w^{[k]}}\\) hidden layer accumulated gradient for layer \\(k-1\\): \\(\\delta_w^{[k-1]} = \\delta_w^{[k]} \\frac{\\delta z^{[k]}}{\\delta a^{[k-1]}} \\frac{\\delta a^{[k-1]}}{\\delta z^{[k-1]}}\\) gradient for layer \\(k-1\\): \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k-1]}} = \\delta_w^{[k-1]} \\frac{\\delta z^{[k-1]}}{\\delta w^{[k-1]}}\\) graphical representation on the following image we show how to obtain the gradient of the first element of the first layer's first neuron's weights \\(w^{[1]}_{11}\\) on the first layer: improving a neural network activation functions why do we need activation functions? well, suppose you have the following network where the activation function is the identity function. that is \\(a^{[i]} = g(z^{[i]}) = z^{[i]}\\): then: \\begin{align} \\hat{y} = a^{[3]} = z^{[3]} = w^{[3]} a^{[2]} + b^{[3]} = w^{[3]} z^{[2]} + b^{[3]} = w^{[3]} (w^{[2]} a^{[1]} + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} (w^{[2]} z^{[1]} + b^{[2]}) + b^{[3]} = w^{[3]} (w^{[2]} (w^{[1]} x + b^{[1]}) + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} (w^{[2]} w^{[1]} x + w^{[2]} b^{[1]} + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} w^{[2]} w^{[1]} x + w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]} \\end{align} if \\begin{align} w = w^{[3]} w^{[2]} w^{[1]} \\end{align} \\begin{align} b = w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]} \\end{align} then: \\begin{align} \\hat{y} = wx + b \\end{align} as you can see if we do not use activation functions, it does not mater how deep your network is, it is going to be equivalent to a linear regression. depending on the task at hand we use different activation functions: sigmoid: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), it maps \\(z \\in (-\\infty, \\infty)\\) to \\((0, 1)\\) it is good for classification works well when the values are in the linear region of the function however when the values are on the extremes the gradient (slope) is very small, therefore it ends up vanishing in the network. relu: \\(relu(z) = \\begin{cases}0 & z \\leq 0 \\\\ 1 & z > 0\\end{cases}\\) tanh: \\(tanh(z) = \\frac{e^z - e^{-z}}{(e^z + e^{-z})}\\) initialization techniques usually we normalize the input to avoid having saturated activation functions. to normalize: \\begin{align} x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\end{align} for every example \\(i\\) and feature \\(j\\). where: \\(\\mu_j\\) is the mean of the \\(j\\) feature, thus: \\(\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}_j\\) \\(\\sigma_j^2\\) is the variance of the \\(j\\) feature, thus: \\(\\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}_j - \\mu_j)^2\\) vanishing/exploding gradients suppose you have the following network, where the activation function is the identity function and \\(b=0\\). then \\(\\hat{y} = w^{[l]} a^{[l-1]} = w^{[l]} w^{[l-1]} a^{[l-2]} = \\cdots = w^{[l]} w^{[l-1]} \\cdots w^{[1]} x\\) therefore, if: \\begin{align} w^{[l]} = \\begin{bmatrix} 1.5 & 0 \\\\ 0 & 1.5 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\hat{y} = \\begin{bmatrix} 1.5^l & 0 \\\\ 0 & 1.5^l \\\\ \\end{bmatrix} \\end{align} which means we end up with an exploding gradient. the inverse happens when: \\begin{align} w^{[l]} = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\hat{y} = \\begin{bmatrix} 0.5^l & 0 \\\\ 0 & 0.5^l \\\\ \\end{bmatrix} \\end{align} which results in a vanishing gradient. to avoid this somewhat, we need to initialize the weights properly. what we want is for the weights to be very close to one to avoid the exploding/diminishing problem. intuition given a single neuron: then \\(a = g(z)\\) and \\(z = w_1 x_1 + \\cdots + w_n x_n\\). we can see that \\(z\\) will increase if \\(n\\) increases, therefore we would want \\(w_i\\) to be as small as \\(n\\) is large, that is: \\begin{align} w_i = \\frac{1}{n} \\end{align} initialization techniques if we want the value of \\(w^{[l]}\\) to be proportional to the number of inputs coming from the layer \\(l\\) (\\(n^{[l-1]}\\)). it works very well for sigmoid activation: w[k] = np.random.randn(shape)*np.sqrt(1/n[l-1]) for relu: w[k] = np.random.randn(shape)*np.sqrt(2/n[l-1]) xavier initialization (used with tanh): \\(w^{[l]} \\sim \\sqrt{\\frac{1}{n^{[l-1]}}}\\) her initialization: \\(w^{[l]} \\sim \\sqrt{\\frac{2}{n^{[l]} + n^{[l-1]}}}\\) also you need to initialize the weights randomly, else you will run into the symmetry problem, where all neurons learn the same thing (that is they update very similarly). optimization mini batch gradient descent mini batch gradient descent is a trade off between batch gradient descent and stochastic gradient descent. also, because mini batch gradient descent is an approximation it introduces some noise on the loss function: however mini batch gradient descent is more used because batch gradient descent can be very computationally expensive. momentum algorithm this algorithm combines gradient descent and momentum. suppose you have the following contour plot, where the horizontal axis is much more extended that the vertical axis. by default on gradient descent the gradient of the loss will be orthogonal to the contour at the given point, as we can see: however, what we would like, so it would converge faster, is to make it move more horizontally than vertically. in order to do that we use a technique called momentum. it takes intro account past updates to find the right way to go. if you take an average of past updates, then: vertical axis: it practically cancels itself because it oscillates a lot horizontal axis: its value it's maintained because the past and present gradients go in the same direction to update the weights we apply the following equation: \\begin{align} \\upsilon = \\beta \\upsilon + (1 - \\beta) \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w} \\end{align} where: \\(\\upsilon\\): stores past updates \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w}\\): stores the current update we average with \\(\\beta\\) and \\((1 - \\beta)\\) finally we update the weights: \\begin{align} w = w - \\alpha \\upsilon \\end{align} anexo for more info about cost function and how to derive them: anexo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/EcuaciÃ³n Normal.html",
    "title": "EcuaciÃ³n Normal",
    "body": " index search search back ecuaciÃ³n normal contents descripci n de los datos hip tesis funcion de coste regularizaci n minimizaci n del coste regularizaci n anotaciones descripciÃ³n de los datos \\(x = (x_{ij})\\) una matriz \\(m \\times (n + 1)\\) donde cada \\(x_{i}\\) es un vector fila \\(1 \\times (n+1)\\), que incluye los valores de todas las caracterÃ­sticas para el ejemplo \\(i\\). \\begin{align} x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_i \\\\ \\vdots \\\\ x_m \\\\ \\end{bmatrix} \\end{align} cabe destacar que \\(x_{i0} = 1\\), es el tÃ©rmino independiente. \\(\\theta = (\\theta_i)\\) es un vector columna \\((n+1)\\times 1\\) donde cada \\(\\theta_i\\) es el peso de la caracterÃ­stica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector columna \\(m\\times 1\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 \\\\ \\cdots \\\\ y_m\\end{bmatrix} \\end{align} hipÃ³tesis dado un conjunto de \\(m\\) datos, es decir matriz \\(x\\), de dimensiones \\(m \\times (n+1)\\), la hipÃ³tesis se define como: \\begin{align} h_\\theta(x) = x \\cdot \\theta = \\begin{bmatrix} x_1 \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{1i} \\\\ \\vdots \\\\ x_j \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{ji} \\\\ \\vdots \\\\ x_m \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{mi} \\\\ \\end{bmatrix} \\end{align} observa que ahora \\(x\\) y \\(\\theta\\) estÃ¡n colocados de forma inversa a como lo hacÃ­amos en la regresiÃ³n lineal y la regresiÃ³n logÃ­stica. esto es debido a que hemos transpuesto las matrices \\(x\\) y \\(\\theta\\), con respecto a como las habÃ­amos definido en las secciones anteriores. el cÃ¡lculo es el mismo. funcion de coste se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) \\end{align} la expresiÃ³n \\((x\\theta - y)^t(x\\theta - y)\\) es equivalente a \\((h_\\theta(x) - y)^2\\), que se utilizaba en la funciÃ³n de coste de la regresiÃ³n lineal. regularizaciÃ³n con regularizaciÃ³n, se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) + \\frac{1}{2m} \\lambda \\theta^t\\theta \\end{align} minimizaciÃ³n del coste con la ecuaciÃ³n normal, en lugar de actualizar el vector de pesos \\(\\theta\\) de forma iterativa, lo que hacemos es igualar la derivada del coste en base a los pesos a cero utilizando derivaciÃ³n matricial: \\begin{align} \\delta_\\theta j(\\theta) = \\begin{bmatrix} \\frac{\\delta j(\\theta)}{\\delta \\theta_0} \\\\ \\vdots \\\\ \\frac{\\delta j(\\theta)}{\\delta \\theta_i} \\\\ \\vdots \\\\ \\frac{\\delta j(\\theta)}{\\delta \\theta_n} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ \\end{bmatrix} = \\overrightarrow{0} \\end{align} a continuaciÃ³n exponemos cÃ³mo se calcula la derivada: sustituÃ­mos la funciÃ³n de coste: \\begin{align} \\delta_\\theta j(\\theta) = \\delta_\\theta \\frac{1}{2m}(x \\theta - y)^t (x \\theta - y) \\end{align} aplicamos la propiedad \\((a + b)^t = a^t + b^t\\) \\begin{align} \\delta_\\theta j(\\theta) = \\delta_\\theta \\frac{1}{2m}((x\\theta)^t - y^t) (x \\theta - y) \\end{align} sacamos el factor constante de la derivada y realizamos la multiplicaciÃ³n: \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - (x\\theta)^ty -y^tx\\theta + y^ty \\end{align} aplicamos la propiedad \\(ab = b^ta^t\\), tal que \\(y^t(x\\theta) = (x\\theta)^t((y)^t)^t = (x\\theta)^ty\\) \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - (x\\theta)^ty - (x\\theta)^ty + y^ty \\end{align} agrupamos tÃ©rminos compatibles: \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - 2(x\\theta)^ty + y^ty \\end{align} como \\(\\delta_\\theta y^ty=0\\): \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - 2(x\\theta)^ty \\end{align} finalmente calculamos la derivada matricial: \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{2m} 2x^t(x\\theta) - 2x^ty = \\frac{1}{m} x^t(x\\theta) - x^ty \\end{align} ahora igualamos la expresiÃ³n obtenida a cero: \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{m} [x^tx\\theta - x^ty] = 0 \\end{align} multiplicamos por \\(m\\) en ambos lados de la ecuaciÃ³n: \\begin{align} x^tx\\theta - x^ty = 0 \\end{align} sumamos \\(x^ty\\) en ambos lados de la ecuaciÃ³n: \\begin{align} x^tx\\theta - x^ty + x^ty= x^ty \\end{align} \\begin{align} x^tx\\theta = x^ty \\end{align} multiplicamos por \\((x^tx)^{-1}\\) por la izquierda en ambos lados de la ecuaciÃ³n: \\begin{align} (x^tx)^{-1}x^tx\\theta = (x^tx)^{-1}x^ty \\end{align} \\begin{align} i\\theta = (x^tx)^{-1}x^ty \\end{align} \\begin{align} \\theta = (x^tx)^{-1}x^ty \\end{align} de tal manera que ahora hemos calculado el vector de pesos Ã³ptimo que minimiza el coste. regularizaciÃ³n con regularizaciÃ³n debemos derivar la funciÃ³n que coste que incluye el parÃ¡metro de regularizaciÃ³n: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) + \\frac{1}{2m} \\lambda \\theta^t\\theta \\end{align} el primer tÃ©rmino ya lo hemos derivado, por lo tanto procedemos a derivar el segundo tÃ©rmino: \\begin{align} \\delta_\\theta \\frac{1}{2m} \\lambda \\theta^t\\theta \\end{align} sacamos el factor constante \\(\\frac{\\lambda}{2m}\\) fuera de la derivada \\begin{align} \\frac{\\lambda}{2m} \\delta_\\theta [\\theta^t\\theta] \\end{align} llevamos a cabo la derivada matricial: \\begin{align} \\frac{\\lambda}{2m} 2 \\theta = \\frac{\\lambda}{m} \\theta \\end{align} juntamos las derivadas de ambos tÃ©rminos: \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{m} [x^tx\\theta - x^ty] + \\frac{\\lambda}{m} \\theta \\end{align} sacamos \\(\\frac{1}{m}\\) como factor comÃºn e igualamos a cero \\begin{align} \\delta_\\theta j(\\theta) = \\frac{1}{m} (x^tx\\theta - x^ty + \\lambda\\theta) = 0 \\end{align} multiplicamos por \\(m\\) en ambos lados de la ecuaciÃ³n: \\begin{align} x^tx\\theta - x^ty + \\lambda\\theta = 0 \\end{align} sumamos \\(x^ty\\) en ambos lados de la ecuaciÃ³n: \\begin{align} x^tx\\theta - x^ty + x^ty + \\lambda\\theta = x^ty \\end{align} \\begin{align} x^tx\\theta + \\lambda\\theta = x^ty \\end{align} sacamos \\(\\theta\\) como factor comÃºn \\begin{align} (x^tx + \\lambda i)\\theta = x^ty \\end{align} donde \\(i\\) es la matriz identidad e dimensiones \\((n+1) \\times (n+1)\\). multiplicamos \\((x^tx + \\lambda i)^{-1}\\) por la izquierda en ambos lados de la ecuaciÃ³n: \\begin{align} (x^tx + \\lambda i)^{-1}(x^tx + \\lambda i)\\theta = (x^tx + \\lambda i)^{-1}x^ty \\end{align} \\begin{align} i\\theta = (x^tx + \\lambda i)^{-1}x^ty \\end{align} \\begin{align} \\theta = (x^tx + \\lambda i)^{-1}x^ty \\end{align} de tal forma que hemos calculado el \\(\\theta\\) Ã³ptimo que minimiza el coste, utilizando regularizaciÃ³n. anotaciones no se debe utilizar la ecuaciÃ³n normal cuando el nÃºmero de ejemplos \\(m\\) es muy grande, ya que es rendimiento del algoritmo es malo hay que tener cuidado con si las matrices son inversibles si \\(m \\leq n\\), entonces las matrices no son invertibles. si \\(\\lambda > 0\\), entonces aseguramos la inversibilidad de las matrices. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/index.html",
    "title": "Machine Learning Stanford Coursera",
    "body": " index search search back machine learning stanford coursera apredizaje supervisado regresiÃ³n lineal regresiÃ³n logÃ­stica ecuaciÃ³n normal neural networks gradient checking inicializaciÃ³n aleatoria evaluaciÃ³n de modelos svm aprendizaje no supervisado en el aprendizaje no supervisado, los ejemplos de entrenamiento no tienen etiquetas (\\(y\\)). se utilizan para buscar correlaciÃ³n y patrones en los ejemplos de entrenamiento. clustering dimensionality reduction expectation-maximization algorithms sistemas de recomendaciÃ³n grandes datasets aprendizaje online map reduce datos artificiales ceiling analysis $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Aprendizaje Online.html",
    "title": "Aprendizaje Online",
    "body": " index search search back aprendizaje online contents ejemplo en estos tipos de problemas se generan datos de forma continua, tal que para cada nuevo dato: obtenemos \\((x, y)\\) actualizamos \\(\\theta\\) utilizando el nuevo ejemplo: \\(\\theta_j = \\theta_j - \\alpha (h_\\theta(x) - y)x_j\\) ejemplo aprender a buscar. supongamos que lo queremos aprender es aquellos resultados que le interesen mÃ¡s al usuario. si tenemos los siguientes datos: \\(x\\): caracterÃ­sticas del producto \\(y\\): si el usuario hace click entonces, lo que queremos aprender es \\(p(y= 1|x;\\theta)\\), tal que por ejemplo enseÃ±emos los 10 productos cuya probabilidad es mayor. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Datos Artificiales.html",
    "title": "Datos Artificiales",
    "body": " index search search back datos artificiales cÃ³mo podemos generar datos? manualmente modificando los datos de entrada (aÃ±adir ruido en sonido, distorsionar imagen, etc) no obstante, debemos evitar aÃ±adir ruido aleatorio, ya que esto no ayuda a extraer caracterÃ­sticas significativas del conjunto de datos. estos mÃ©todos se suelen utilizar si el modelos tiene un sesgo bajo y se produce underfitting, (por lo que hace falta aÃ±adir caracterÃ­sticas). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/InicializaciÃ³n aleatoria.html",
    "title": "InicializaciÃ³n aleatoria",
    "body": " index search search back inicializaciÃ³n aleatoria cuando creamos una red neuronal, si inicializamos todos los pesos \\(\\theta\\) a cero, entonces todos los nodos serÃ¡n iguales. por ello se inicializa \\(\\theta\\) con valores aleatorios dentro de un rango \\([- \\epsilon, \\epsilon]\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/DimensionalityReduction/Dimensionality Reduction.html",
    "title": "Dimensionality Reduction",
    "body": " index search search back dimensionality reduction contents standardize data pca find underlying space represent the subspace algorithm layout performing eigen-decomposition large datasets rephrasing pca ica intuition solution intuition given examples \\(\\{x^{(i)}\\}_{i=1}^n\\) where \\(x^{(i)} \\in \\mathbb{r}^d\\), we want to find out if our data lives is a low dimensional space. look at the next example: we can see that the two features are correlated, and we can project the points onto a line, reducing the space from two dimensions to one. so it might be the case that some features are highly correlated, and so de d-dimensional space can be as a k-dimensional space where \\(0 < k < d\\): \\begin{align} \\begin{bmatrix} x_{11} & \\cdots & x_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{nd} \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} x_{11} & \\cdots & x_{1k} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{nk} \\\\ \\end{bmatrix} \\end{align} standardize data a lot of the times the units of each feature in the data make the values in one column (feature) be much bigger than the values in another column. thus, the first step is to standardize your data: center data have it have variance equal to one so we transform our data as follows: \\begin{align} x_j^{(i)} = \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j} \\end{align} where: \\(u_j\\) is the mean of the feature \\(j\\) over the \\(n\\) examples, such that \\(u_j = \\frac{1}{n}\\sum_{i=1}^nx^{(i)}_j\\) \\(\\sigma_j\\) is the standard deviation of the feature \\(j\\) over the \\(n\\) examples, where \\(\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n(x^{(i)}_j - \\mu_j)^2\\) pca find underlying space to reduce the dimensionality of our data we first define a subspace and then we project each point onto the subspace. this projection is the closes point in the subspace to the point we are trying to project, this has as a consequence that the \"line\" connecting the point to its projection is always perpendicular to the subspace: the goal is to choose the subspace that maximizes the variance of the projected points, to retain the maximum possible variance of the data. as you can see if we choose the blue line as the subspace the variance is much bigger that if we choose the red line: represent the subspace let us suppose the subspace is defined by a basis vector \\(u \\in \\mathbb{r}^d\\) where \\(u\\) is a unit vector, then projection of \\(\\overrightarrow{x^{(i)}}\\) on to the space spanned by \\(u\\) will be: \\begin{align} proj(u)\\overrightarrow{x^{(i)}} \\end{align} where \\(proj(u)\\) is the projection matrix and \\(x^{(i)} \\in \\mathbb{r}^d\\). so, because \\(proj(u) = \\frac{uu^t}{u^tu}\\), then the projected point is defined as: \\begin{align} proj(u)\\overrightarrow{x^{(i)}} = \\frac{uu^t}{u^tu} \\overrightarrow{x^{(i)}} = ((x^{(i)})^tu)u \\end{align} where \\(((x^{(i)})^tu)\\) is an scalar. so, now our goal is to find a \\(u\\) that maximizes the variance across the \\(n\\) examples. that is, we want to maximize the sum of the square of the norms of the projected points: more formally: \\begin{align} u = \\underset{u}{\\arg \\max} \\frac{1}{n}\\sum_{i=1}^n ||proj(u)x^{(i)}||^2 = \\frac{1}{n}\\sum_{i=1}^n ||((x^{(i)})^tu)u||^2 \\end{align} because the norm of a unit vector multiplied by a scalar is just the square of the scalar (for \\(3 \\cdot \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\(||\\begin{pmatrix} 3 & 0 & 0 \\end{pmatrix}|| = (\\sqrt{3^2 + 0^2 + 0^2})^2 = 3^2\\)): \\begin{align} u = \\underset{u}{\\arg \\max} \\frac{1}{n}\\sum_{i=1}^n ((x^{(i)})^tu)^2 = \\frac{1}{n}\\sum_{i=1}^n ((x^{(i)})^tu)^t((x^{(i)})^tu) = \\frac{1}{n}\\sum_{i=1}^n u^t x^{(i)} (x^{(i)})^t u \\end{align} because \\(u, u^t\\) are a common factor in the sum: \\begin{align} u = \\underset{u}{\\arg \\max} \\left[u^t \\left(\\frac{1}{n}\\sum_{i=1}^n x^{(i)} (x^{(i)})^t \\right) u\\right] \\end{align} now, we know that given the optimization problem of the form \\(\\underset{u}{\\arg \\max} \\left[u^t a u\\right]\\), the solution \\(u\\) is the eigenvector corresponding to the largest eigenvalue of \\(a\\). in this scenario, \\(a = \\left(\\frac{1}{n}\\sum_{i=1}^n x^{(i)} (x^{(i)})^t \\right)\\), which equals the sample covariance matrix, which is defined as: \\begin{align} \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu)^t (x^{(i)} - \\mu) = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - 0)^t (x^{(i)} - 0) = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)})^t x^{(i)} \\end{align} note that, because our data is now centered after standardizing it, the mean \\(\\mu\\) equals zero. hence, we want to calculate the eigenvectors of the sample covariance matrix of x. mind you, we have derived this solution for a space defined by only one vector \\(u\\). however given basis vectors \\((u_1, \\cdots, u_k)\\) the optimization problem holds and the solution are the \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues of \\(a\\). algorithm layout the steps of pca are the following: calculate the sample covariance matrix as \\(x^tx\\) calculate the eigenvector and eigenvalues of \\(x^tx\\), such that: \\begin{align} \\begin{matrix} (\\lambda_1, u_1) \\\\ (\\lambda_2, u_2) \\\\ \\vdots \\\\ (\\lambda_d, u_d) \\\\ \\end{matrix} \\end{align} are the \\(d\\) eigenvectors (\\(u_i\\)) and eigenvalues (\\(\\lambda_i\\)). we assume the tuples are ordered in decreasing order with respect to the eigenvalues, such that if \\(i > j\\) then \\(\\lambda_i > \\lambda_j\\). find \\(k\\) such that we satisfy a confidence level with respect to the variance, i.e. suppose you want to preserve 95% of the variance of the original data then: \\begin{align} \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^d \\lambda_j} = 95\\% \\end{align} choose the \\(k\\) eigenvectors with the largest corresponding eigenvalues. performing eigen-decomposition first of all, let us present two properties regarding eigen-decompositions of a matrix \\(x\\): if \\(x\\) is a square matrix and symmetric then \\(x\\) has orthogonal eigenvectors and real eigenvalues. if \\(x\\) is also positive semi-definite then the eigenvalues are positive. in our case, the eigen-decomposition is done over \\(x^tx\\), therefore this matrix is guaranteed to be a square matrix, symmetric and positive semi-definite. then, performing the eigen decomposition of \\(x^tx\\) is equivalent to performing singular value decomposition (svd) over \\(x\\), such that the single values equal the square root of the eigenvalues. large datasets to perform pca on large datasets we use a technique called power iteration, which consists on: for \\(i=0\\), initialize \\(u^{(i)}\\) to random values other than zero set \\(i = i+1\\), and \\(u^{(i)} = (x^tx)u^{(i-1)}\\) re-scale \\(u^{(i)}\\) to have unit length such that: \\(u^{(i)}=\\frac{(x^tx)u^{(i-1)}}{||(x^tx)u^{(i-1)}||}\\) go to step 2. eventually it will converge to the largest eigenvector. rephrasing pca another way to describe the problem solved by pca, equivalent to the maximization variance perspective, is: find a subspace such that the projection of the points are as close to the original data as possible, that is minimize the sum of the distances between the projected points and the original points. ica this algorithm pretends to solve what is commonly known as the source separation problem, where we are given a dataset \\(x\\) that is a mixture of some source data \\(s\\). we then use these mixed sources \\(x\\) to construct a unmixing matrix \\(w\\) to recover the source \\(s\\). intuition imagine there are \\(d\\) speakers and \\(d\\) microphones randomly distributed in a room, such that: \\(s \\in \\mathbb{r}^d\\) is the representation of what a speaker says. so \\(s_j^{(i)}\\) is what the \\(j\\) speaker said in moment \\(i\\). \\(x \\in \\mathbb{r}^d\\) is the representation of what a microphone records. so \\(x_j^{(i)}\\) is what the \\(j\\) microphone recorded in moment \\(i\\). for example, given two speaker, what they say is represented as follows: meanwhile the recordings of the microphones are the following: we are only given \\(x\\), and the goal is to recover the original speech signal spoken by each speaker. we assume that \\(x\\) is a linear combination of what each speaker says, thus \\(x = as\\), where \\(a\\) is a quare matrix \\(d \\times d\\) and is called the mixing matrix. what we want to do is to compute the inverse of \\(a\\), \\(w\\) such that \\(w = a^{-1}\\), where \\(w\\) is called the unmixing matrix. then: \\begin{align} a^{-1}x = a^{-1}as \\rightarrow a^{-1}x = s \\rightarrow wx = s \\rightarrow s = wx \\end{align} solution to solve this problem we make the following assumptions: the number of sources \\(s\\) are equal to the number of \"examples\" in the mixed dataset \\(x\\) \\(x\\) is a linear combination of \\(s\\), such that \\(s = wx\\) \\(s_j\\) is independent of \\(s_k\\), whenever \\(j \\neq k\\). that is to say, each belongs to a different probability distribution, and are two independent random variables. each \\(s_j\\) is not gaussian. intuition suppose we are given a random variable \\(x\\) such that \\(x ~ unif [0,1]\\), then the probability density function is: let us define a new distribution as follows \\(y=2x\\), then the probability density function is: note, that the function is \"stretched\" as to always satisfy the condition that the integral of \\(pdf\\) must equal 1, which is the same as saying the area under the function is 1. so now, \\(p_y (y) = p_x(x) \\cdot \\frac{1}{2} = p_x(\\frac{y}{2})\\cdot\\frac{1}{2}\\), because \\(x = \\frac{y}{2}\\). but what happens in a higher dimension? that is, what happens when we multiply \\(x \\in \\mathbb{r}^d\\) by a mixing matrix \\(w \\in \\mathbb{r}^{d \\times d}\\). well, given \\(y \\in \\mathbb{r}^{d \\times d}\\), such that \\(y=wx\\), then to perform a change of random variable we use the jacobian: \\begin{align} p_y(y) = p_x(x)\\frac{1}{|w|} = p_x(w^{-1}y)\\frac{1}{|w|} \\end{align} where \\(|w|\\) is the determinant of \\(w\\). first of all we define the distribution of the mixed dataset as follows: \\begin{align} p_x(x) = \\prod_{j=1}^d p_s (s_j) |w| = \\prod_{j=1}^d p_s (w_j^tx) |w| \\end{align} note that \\(s_j=(w_j)^tx\\). we also assume that \\(p_s\\) is distributed according to a logistic distribution, thus: cumulative distribution function (cdf): \\(\\frac{1}{1+e^{-x}} = \\sigma(x)\\) probability density function (pdf): \\(\\sigma(x)\\sigma(1-x)\\) so, we obtain the likelihood of \\(w\\) as follows: \\begin{align} l(w) = \\sum_{i=1}^n \\left[\\left(\\sum_{j=1}^d \\log[\\sigma(x^{(i)})(1-\\sigma(x^{(i)}))]\\right) + \\log{|w|}\\right] \\end{align} where \\(w\\) is the parameter we are trying to obtain. so, to solve the optimization problem: we define the maximization of the likelihood as the objective we compute the derivative of \\(l(w)\\) and perform gradient descent, such that the update step is as follows: \\begin{align} w = w - \\alpha \\left\\{\\begin{bmatrix} (1- 2\\sigma(w_1^tx^{(i)})) \\\\ (1- 2\\sigma(w_2^tx^{(i)})) \\\\ \\vdots \\\\ (1- 2\\sigma(w_d^tx^{(i)})) \\\\ \\end{bmatrix} (x^{(i)})^t + (w^t)^{-1} \\right\\} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Sistemas de RecomendaciÃ³n.html",
    "title": "Sistemas de RecomendaciÃ³n",
    "body": " index search search back sistemas de recomendaciÃ³n contents content based recommendations funci n de coste descenso gradiente collaborative filtering problema de optimizaci n algoritmo buscar tems relacionados dados los parÃ¡metros: \\(n_\\mu\\): nÃºmero de usuarios \\(n_m\\): nÃºmero de Ã­tems valorables \\(r(i,j)\\): marcador de si el Ã­tem ha sido valorado, tal que: \\begin{align} r(i, j) = \\begin{cases} 1, & \\text{ si el usuario j ha valorado el Ã­tem i} \\\\ 0, & \\text{ en cualquier otro caso} \\end{cases} \\end{align} \\(y^{(i, j)}\\): valoraciÃ³n del usuario \\(j\\) al Ã­tem \\(i\\). el objetivo de un sistema de recomendaciÃ³n es predecir los valores de las valoraciones donde \\(r(i, j) \\neq 1\\) (es decir predecir las valoraciones de usuarios hacia Ã­tems que no han valorado con anteioridad) content based recommendations cada Ã­tem estÃ¡ definido por \\(n\\) caracterÃ­sticas. para cada usuario \\(j\\), debemos obtener \\(\\theta^{(j)} \\in \\mathbb{r}^{n+1}\\), de tal manera que para predecir la valoraciÃ³n de \\(x^{(i)} \\rightarrow h_\\theta(x^{(i)}) = (\\theta^{(j)})^t x^{(i)}\\) funciÃ³n de coste sea \\(m^{(j)}\\) el nÃºmero de Ã­tems valorados por el usuario \\(j\\), entonces la funciÃ³n de coste se define como: \\begin{align} j(\\theta) = \\frac{1}{2m^{(j)}}\\sum_{j=1}^{n_\\mu} \\sum_{i; r(i, j) = 1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2m^{(j)}} \\sum_{k=1}^{n_\\mu} \\theta^{(j)}_k \\end{align} donde \\(\\theta = \\{\\theta_1, \\cdots, \\theta_{n_\\mu}\\}\\) es decir queremos minimizar la \"distancia\" entre lo predicho \\((\\theta^{(j)})^t x^{(i)}\\) para el Ã­tem (que ha sido valorado, por lo tanto \\(r(i, j) = 1\\)) y el usuario \\(i\\) y la valoraciÃ³n real \\(y^{(i, j)}\\). descenso gradiente lo que queremos es minimizar el coste, por lo tanto, calculamos \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_j}\\) para obtener el vector en direcciÃ³n al mayor incremento en la funciÃ³n, seguidamente, utilizar su opuesto, obtenemos el vector que apunta a la direcciÃ³n de menor incremento. es decir, aplicamos descenso gradiente como sigue: para \\(k = 0\\): \\begin{align} \\theta_k^{(j)} = \\theta_k^{(j)} - \\alpha \\left(\\sum_{i; r(i,j)=1} (\\theta^{(j)})^tx^{(i)} - y^{(i,j)}x^{(i)}_k \\right) \\end{align} para \\(k \\neq 0\\): \\begin{align} \\theta_k^{(j)} = \\theta_k^{(j)} - \\alpha \\left(\\sum_{i; r(i,j)=1} (\\theta^{(j)})^tx^{(i)} - y^{(i,j)}x^{(i)}_k + \\lambda \\theta_k^{(j)} \\right) \\end{align} collaborative filtering collaborative filtering consiste en calcular las caracterÃ­sticas de cada usuario (ejemplo \\(x^{(i)}\\)) en funciÃ³n de los pesos \\(\\theta^{(j)}\\). una vez hecho esto se calculan los pesos Ã³ptimos que que minimizan la funciÃ³n de coste y volvemos a obtener las caracterÃ­sticas de cada usuario en funciÃ³n de estes nuevos pesos. este proceso se describe mÃ¡s formalmente a continuaciÃ³n: problema de optimizaciÃ³n el problema de optimizaciÃ³n se describe como sigue: dados \\(\\theta^{(1)}, \\cdots, \\theta^{n_\\mu}\\): para un ejemplo \\(x^{(i)}\\) \\begin{align} \\underset{x^{(i)}}{\\min{}} \\frac{1}{2} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{k=1}^n (x_k^{(i)})^2 \\end{align} para todos los ejemplos del conjunto \\(x^{0}, \\cdots, x^{(n_m)}\\): \\begin{align} \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\min{}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2 \\end{align} es decir queremos minimizar la \"distancia\" entre lo predicho \\((\\theta^{(j)})^t x^{(i)}\\) para el Ã­tem (que ha sido valorado, por lo tanto \\(r(i, j) = 1\\)) y el usuario \\(i\\) y la valoraciÃ³n real \\(y^{(i, j)}\\). ademÃ¡s como queremos obtener los valores de \\(x\\) que minimizan el coste, los aÃ±adimos como coste a problema de optimizaciÃ³n para evitar overfitting. algoritmo el algoritmo consta de los siguientes pasos: inicializar \\(x^{(1)}, \\cdots, x^{(m)}\\) y \\(\\theta^{(1)}, \\cdots, \\theta^{(n_\\mu)}\\) de forma aleatoria. calcular \\(x\\) a partir de \\(\\theta\\) calcular \\(\\theta\\) a partir de \\(x\\) volvemos al paso 2. es decir, queremos obtener \\(x\\) y \\(\\theta\\) que optimice el siguiente problema: \\begin{align} \\underset{x^{(1)}, \\cdots, x^{(n_m)}, \\theta^{(1)}, \\cdots, \\theta^{(n_\\mu)}}{\\min{}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_\\mu}\\sum_{k=1}^n (\\theta_k^{(i)})^2 \\end{align} observa que, como estamos optimizando tanto \\(\\theta\\) como \\(x\\), entonces los aÃ±adimos como coste a la funciÃ³n de optimizaciÃ³n para evitar overfitting: \\(\\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2\\) \\(\\frac{\\lambda}{2} \\sum_{i=1}^{n_\\mu}\\sum_{k=1}^n (\\theta_k^{(i)})^2\\) para aplicar la optimizaciÃ³n utilizamos descenso gradiente: primero en funciÃ³n de \\(x\\) y despuÃ©s en funciÃ³n de \\(\\theta\\): \\begin{align} x^{(i)}_k = x^{(i)}_k - \\alpha \\left( \\sum_{j:r(i, j)=1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)}) \\theta_k^{(j)} + \\lambda x^{(i)}_k\\right) \\end{align} \\begin{align} \\theta^{(j)}_k = \\theta^{(j)}_k - \\alpha \\left( \\sum_{i:r(i, j)=1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)}) x_k^{(i)} + \\lambda \\theta^{(j)}_k\\right) \\end{align} buscar Ã­tems relacionados si \\(||x^{(i)} - x^{(j)}\\)|| es un valor pequeÃ±o entonces los Ã­tems \\(i\\) y \\(j\\) son similares. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Clustering.html",
    "title": "Clustering",
    "body": " index search search back clustering contents k-means algoritmo cl sters no claramente separables inicializaci n aleatoria parametrizaci n de cl stering k-means escogemos e inicializamos \\(k\\) centroides que servirÃ¡n para hacer de clÃºsters. para cada ejemplo \\(j\\): asignamos el centroide mÃ¡s cercano una vez asignados todos los ejemplos, resituamos cada centroide en funciÃ³n de los ejemplos asignados al mismo. volvemos al paso 2. algoritmo entrada: nÃºmero de clÃºsters \\(k\\) conjunto de entrenamiento: \\(\\{x^{(0)}, \\cdots, x^{(m)}\\}\\), con \\(x^{(i)} \\in \\mathbb{r}^n\\) inicializamos los \\(k\\) centroides \\((\\mu_1, \\cdots, \\mu_k) \\in \\mathbb{r}^n\\) de forma aleatoria. repetimos: para cada ejemplo \\(x^{(j)}\\), \\(c^{(j)}\\) es el Ã­ndice del centroide mÃ¡s cercano a \\(x^{(j)}\\): \\(\\underset{i}{min}||x^{(j)} - \\mu_i||\\) para cada clÃºster: \\(\\mu_i\\) es la media de los puntos \\(x^{(j)}\\) asignados al centroide \\(i\\): \\(\\mu_i = \\frac{1}{t} \\left[\\sum_{j=1}^t x^{(j)} \\text{ donde } c^{(j)} = i\\right]\\), donde \\(t\\) es el nÃºmero de ejemplos asignados al centroide \\(i\\). si el centroide no tiene puntos, se elimina o se vuelve a inicializar de forma aleatoria. clÃºsters no claramente separables cuando los datos contienen mucho ruido lo que se hace es resolver el siguiente problema de optimizaciÃ³n: \\begin{align} \\underset{c^{(1)}, \\cdots, c^{(m)}, \\mu_1, \\cdots, \\mu_k}{min} j(c^{(1)}, \\cdots, c^{(m)}, \\mu_1, \\cdots, \\mu_k) \\end{align} donde la funciÃ³n de coste \\(j\\) se define como: \\begin{align} j(c^{(i)}, \\mu_i) = \\frac{1}{m} \\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2 \\end{align} es decir, el coste es equivalente a la suma de la distancia entre el ejemplo \\(x^{(i)}\\) y su clÃºster asignado \\(\\mu_{c^{(i)}}\\), para cada ejemplo. el algoritmo de optimizaciÃ³n lo que hace es: minimiza el coste con respecto a \\(c\\) minimiza el coste con respecto a \\(\\mu\\) inicializaciÃ³n aleatoria debemos escoger un nÃºmero de centroides \\(k\\) menor que el nÃºmero de ejemplos \\(m\\). inicializamos cada centroide equivalente a un ejemplo aleatorio del conjunto de entrenamiento: \\(\\mu_i = x^{(j)}\\) hay que tener en cuenta que, en funciÃ³n de la inicializaciÃ³n de los centroides, se pueden obtener distintos resultados en el problema de optimizaciÃ³n, por ello lo que se hace es: aplicar el algoritmo muchas veces escoger el modelo que obtuvo menor coste este proceso es viable si el nÃºmero de clÃºsters es pequeÃ±o. parametrizaciÃ³n de clÃºstering una forma de escoger el nÃºmero de clÃºsters \\(k\\) es utilizando el mÃ©todo del codo: se aplica el modelo con un nÃºmero distinto de clÃºsters se evalÃºa con alguna mÃ©trica el rendimiento (coste) del modelo y se elige el ofrece una mayor mejora con respecto a un nÃºmero de clÃºsters menor $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/RegresiÃ³n LogÃ­stica.html",
    "title": "RegresiÃ³n LogÃ­stica",
    "body": " index search search back regresiÃ³n logÃ­stica contents descripci n de los datos hip tesis funcion de coste regularizaci n descenso gradiente regularizaci n descripciÃ³n de los datos \\(x = (x_{ij})\\) una matriz \\(n \\times m\\) donde cada \\(x_{ij}\\) es la caracterÃ­stica \\(i\\) del ejemplo \\(j\\), tal que \\begin{align} x = \\begin{bmatrix} x_{11} & \\cdots & x_{1m} \\\\ \\cdots & \\ddots & \\cdots \\\\ x_{n1} & \\cdots & x_{nm} \\\\ \\end{bmatrix} \\end{align} cada columna es un ejemplo en cada fila estÃ¡n los valores de una caracterÃ­stica \\(\\theta = (\\theta_i)\\) es un vector fila \\(1\\times n\\) donde cada \\(\\theta_i\\) es el peso de la caracterÃ­stica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_1 & \\cdots & \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector fila \\(1\\times m\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 & \\cdots & y_m\\end{bmatrix} \\end{align} donde cada salida \\(y_j\\), para un clasificador de dos clases sÃ³lo puede tener los valores \\(0\\) o \\(1\\). hipÃ³tesis para un valor \\(z\\), la funciÃ³n sigmoide \\(g\\) se define como: \\begin{align} g(z) = \\frac{e^z}{(1+e^z)} = \\frac{1}{(1 + e^{-z})} \\end{align} sea \\(g\\) la funciÃ³n sigmoide. dado un ejemplo, es decir un vector columna \\(x\\), de dimensiones \\(n \\times 1\\), la hipÃ³tesis se define como: \\begin{align} h_\\theta(x) = g\\left(\\sum_{i=1}^n \\theta_i \\cdot x_i\\right) = \\begin{cases} 0, & h_\\theta(x) < 0.5 \\\\ 1, & h_\\theta(x) \\geq 0.5 \\\\ \\end{cases} \\end{align} dado un conjunto de \\(m\\) datos, es decir matrix \\(x\\), de dimensiones \\(n \\times m\\), la hipÃ³tesis se define como: \\begin{align} h_\\theta(x) = \\theta\\cdot x = \\begin{bmatrix}g(\\sum_{i=1}^n \\theta_i \\cdot x_{i1}) & \\cdots & g(\\sum_{i=1}^n \\theta_i \\cdot x_{im})\\end{bmatrix} \\end{align} el resultado es un vector fila \\(1 \\times m\\), es decir como el vector de salidas \\(y\\) funcion de coste se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{m}\\sum_{j=1}^m \\text{coste}(h_\\theta(x_j)) \\end{align} donde \\(\\text{coste}\\) es una funciÃ³n definida como sigue: \\begin{align} \\text{coste}(h_\\theta(x_j)) = [-y_j \\log(h_\\theta(x_j))] - [(1-y_j)\\log(1-h_\\theta(x_j))] \\end{align} regularizaciÃ³n con regularizaciÃ³n, se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = -\\frac{1}{m}\\sum_{j=1}^m [y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))] + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} descenso gradiente para actualizar el vector de pesos \\(\\theta\\) aplicamos el descenso gradiente. para cada peso \\(\\theta_i\\): \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) \\end{align} la derivada del coste en funciÃ³n del peso \\(\\theta_i\\) se calcula como sigue: sustituimos la funciÃ³n de coste \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\left(-\\frac{1}{m}\\sum_{j=1}^m [y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} sacamos el factor constante \\(\\frac{1}{m}\\) y aplicamos la propiedad \"la derivada de una suma equivale a la suma de las derivadas\", tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\frac{\\delta}{\\delta \\theta_i} \\left([y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{\\delta}{\\delta \\theta_i} [y_j \\log(h_\\theta(x_j))]\\right) + \\left(\\frac{\\delta}{\\delta \\theta_i}[(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} sacamos los factores constantes \\(y_j\\) y \\(1-y_j\\) y aplicamos la regla de la cadena: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(y_j\\frac{\\delta\\log(h_\\theta(x_j))}{\\delta h_\\theta(x_j)} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) + \\left((1-y_j)\\frac{\\delta\\log(1-h_\\theta(x_j))}{\\delta (1- h_\\theta(x_j))}\\frac{\\delta (1- h_\\theta(x_j))}{\\delta \\theta_i}\\right) = \\end{align} tenemos que, para el Ãºltimo tÃ©rmino: \\begin{align} \\frac{\\delta (1- h_\\theta(x_j))}{\\delta \\theta_i} = \\frac{\\delta(1)}{\\delta \\theta_i} - \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = - \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} \\end{align} por lo tanto, si sustituimos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(y_j\\frac{\\delta\\log(h_\\theta(x_j))}{\\delta h_\\theta(x_j)} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) + \\left((1-y_j)\\frac{\\delta\\log(1-h_\\theta(x_j))}{\\delta (1- h_\\theta(x_j))}(-1)\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) = \\end{align} aplicamos la regla \\(\\frac{\\delta \\log(x)}{\\delta x} = \\frac{1}{x}\\), sacamos la expresiÃ³n \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\) como factor comÃºn y hacemos negativo el segundo tÃ©rmino: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{y_j}{h_\\theta(x_j)} \\right) - \\left(\\frac{(1 - y_j)}{1-h_\\theta(x_j)}\\right) \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\end{align} aplicamos operationes aritmÃ©ticas: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{(y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j))}{h_\\theta(x_j)(1-h_\\theta(x_j))} \\right)\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\end{align} centrÃ©monos ahora en \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\). para calcular esta derivada, primero expresamos la hipÃ³tesis utilizando la funciÃ³n sigmoide: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} g(\\theta x_j) \\end{align} aplicamos la regla de la cadena \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} \\frac{\\delta \\theta x_j}{\\delta \\theta_i} \\end{align} sabemos que la derivada del segundo tÃ©rmino \\(\\frac{\\delta \\theta x_j}{\\delta \\theta_i}\\) equivale a \\(x_{ij}\\), por lo tanto, calcularemos sÃ³lo \\(\\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j}\\) sea: \\begin{align} g(\\theta x_j) = \\frac{1}{1 + e^{-\\theta x_j}} = (1 + e^{-\\theta x_j})^{-1} \\end{align} aplicamos la regla de la cadena \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = \\frac{\\delta(1 + e^{-\\theta x_j})^{-1}}{\\delta(1+e^{-\\theta x_j})}\\frac{\\delta(1+e^{-\\theta x_j})}{\\delta \\theta x_j} \\end{align} resolvemos la primera derivada aplicando las propiedades de las derivadas sobre los polinomios y volvemos a aplicar la propiedad de que la derivada de una suma equivale a la suma de las derivadas en el segundo tÃ©rmino: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2}\\left[\\frac{\\delta (1)}{\\delta \\theta x_j} + \\frac{\\delta e^{-\\theta x_j}}{\\delta \\theta x_j} \\right] \\end{align} como \\(\\frac{\\delta (1)}{\\delta \\theta x_j}\\) equivale a cero: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2} \\frac{\\delta e^{-\\theta x_j}}{\\delta \\theta x_j} \\end{align} resolvemos la Ãºltima derivada, sabiendo que \\(\\frac{\\delta e^x}{\\delta x} = e^x\\) \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2} (-1) e^{-\\theta x_j} = (1 + e^{-\\theta x_j})^{-2} e^{-\\theta x_j} = \\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} \\end{align} como \\(\\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} = \\left(\\frac{1}{1+e^{-\\theta x_j}}\\right)\\left(1 - \\frac{1}{1 + e^{-\\theta x_j}}\\right)\\), entonces: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = \\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} = \\left(\\frac{1}{1+e^{-\\theta x_j}}\\right)\\left(1 - \\frac{1}{1 + e^{-\\theta x_j}}\\right) = h_\\theta(x_j) (1- h_\\theta(x_j)) \\end{align} ya que segÃºn la definiciÃ³n de la hipÃ³tesis \\(h_\\theta(x_j) = \\frac{1}{1 + e^{-\\theta x_j}}\\) por lo tanto, juntado los resultados, tenemos que: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} \\frac{\\delta \\theta x_j}{\\delta \\theta_i} = h_\\theta(x_j) (1- h_\\theta(x_j)) x_{ij} \\end{align} volvemos, entonces, a la derivada de la funciÃ³n de coste y sustituimos \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\) \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{(y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j))}{h_\\theta(x_j)(1-h_\\theta(x_j))} \\right) h_\\theta(x_j) (1- h_\\theta(x_j)) x_{ij} = \\end{align} los tÃ©rminos \\(h_\\theta(x_j) (1- h_\\theta(x_j))\\) se cancelan tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left((y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j)) \\right) x_{ij} = \\end{align} aplicamos operaciones aritmÃ©ticas: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m (y_j - y_jh_\\theta(x_j) - h_\\theta(x_j) + y_jh_\\theta(x_j)) x_{ij} \\end{align} el tÃ©rmino \\(y_jh_\\theta(x_j)\\) se cancela, tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m (y_j - h_\\theta(x_j)) x_{ij} \\end{align} finalmente movemos el \\((-1)\\) dentro del sumatorio: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{1}{m}\\sum_{j=1}^m (h_\\theta(x_j)-y_j) x_{ij} \\end{align} por lo tanto la funciÃ³n del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\right] \\end{align} observa que tiene la misma forma que para la regresiÃ³n lineal, pero la hipÃ³tesis para la regresiÃ³n logÃ­stica estÃ¡ definida en tÃ©rminos de la funciÃ³n sigmoide regularizaciÃ³n con regularizaciÃ³n debemos derivar la funciÃ³n de coste que incluye el parÃ¡metro de regularizaciÃ³n: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) + \\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2 \\end{align} el primer tÃ©rmino ya lo hemos derivado, por lo tanto procedemos a derivar el segundo tÃ©rmino: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} \\left(\\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2\\right) \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2 = \\begin{cases} 2 \\theta_k, & k = i \\\\ 0, & k \\neq i \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sÃ³lo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} 2\\theta_i = \\frac{\\lambda}{m} \\theta_i \\end{align} por lo tanto la funciÃ³n del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) + \\frac{\\lambda}{m}\\theta_i\\right] \\end{align} al igual que antes, observa que tiene la misma forma que para la regresiÃ³n lineal, pero la hipÃ³tesis para la regresiÃ³n logÃ­stica estÃ¡ definida en tÃ©rminos de la funciÃ³n sigmoide $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/ExpectationMaximization/Expectation-Maximization Algorithms.html",
    "title": "Expectation Maximization",
    "body": " index search search back expectation maximization contents introduction gaussian mixture models em algorithm with gmm s e-step m-step optimal parameters of a gmm iterative process recap anomaly detection generalized em algorithm jensen s inequality convex function concave function some intuition motivation e-step m-step make log likelihood and lower bound equal on theta putting everything together derive em for gmm from the generalized algorithm e-step m-step introduction this technique is employed in density estimation problems and anomaly detection. such problems aim to represent data in a compact form using a statistical distribution, e.g., gaussian, beta, or gamma. you can think of those problems as a clustering task but from a probabilistic point of view. this is what makes the em algorithm a probabilistic generative model. thus, if we are given \\(n\\) samples, we model them with \\(p(x)\\), such that if \\(p(x) < \\epsilon\\), where \\(\\epsilon\\) is some threshold, then we detect an anomaly. however, you may expect that a single gaussian with its mean and variance cannot map thousands of instances in a dataset into a set of \\(k\\) clusters accurately, so we may assume that there are \\(k\\) distributions that describe the data, hence we use mixture models. for example, imagine you have the following dataset: it looks like the data comes from two different gaussian distributions: so to model this data we use a mixture of gaussian models. note that if we knew by which distribution each sample was generated, we would simply use mle, however we do not know this information, therefore we use the expectation maximization algorithm and we introduce the latent variable \\(z\\) in place of the predicted output \\(y\\) we had in supervised learning algorithms. to model the data, first of all, we suppose that there is a latent (hidden/unobserved) random variable \\(z\\), and \\(x^{(i)}, z^{(i)}\\) are distributed (by a joint distribution) like so \\begin{align} p(x^{(i)},z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)}) \\end{align} where \\(z^{(i)} \\sim multinomial(\\phi)\\), that is \\(z^{(i)}\\) is distributed according to a multinomial distribution. this distribution models for each \\(z^{(i)}\\) the probability of it being equal to \\(1, 2, ..., k\\), where \\(k\\) is the number of clusters. this will denote the probability of a point \\(x^{(i)}\\) being drawn from each of the distributions. and \\(p(x^{(i)}|z^{(i)}=j)\\) is the probability of \\(x^{(i)}\\) being generated by the cluster \\(j\\). where \\(x^{(i)}|z^{(i)} = j\\) is drawn from a normal distribution \\(\\mathcal{n}(\\mu_j, \\sigma_j)\\). gaussian mixture models to build a density estimator model, we cannot rely on a simple distribution. mixture models try to tackle this limitation by combining a set of distributions to create a convex space where we can search for the optimal parameters for such distributions using maximum likelihood estimation (mle). a mixture model is expressed by the following equations: \\begin{align} p(x^{(i)}) = \\sum_{j=1}^k \\phi^{(i)}_j p_j(x^{(i)}) \\tag{1} \\end{align} \\begin{align} 0 \\leq \\phi^{(i)}_j \\leq 1, \\sum_{j=1}^k \\phi^{(i)}_j = 1 \\end{align} where \\(k\\) is the number of mixture components (clusters), \\(\\phi^{(i)}_j\\)'s are the mixture weights, and \\(p_j(x^{(i)})\\)'s are members of a family of distributions (gaussian, poisson, bernoulli, etc). so for each example \\(x^{(i)}\\) and for each distribution \\(j\\), each weight \\(\\phi^{(i)}_j\\) is between 0 and 1, and the sum over \\(k\\) of the weights \\(\\phi_j^{(i)}\\) for every example \\(x^{(i)}\\) equals one. consequently, a gmm is a mixture model where the \\(p_j(x^{(i)})\\) is a finite combination of gaussian distributions. therefore, a gmm can be precisely defined by the following set of equations: \\begin{align} p(x^{(i)};\\theta) = \\sum_{j=1}^k \\phi^{(i)}_j \\mathcal{n}(x^{(i)};\\mu_j,\\,\\sigma_j) \\end{align} \\begin{align} 0 \\leq \\phi^{(i)}_j \\leq 1, \\sum_{j=1}^k \\phi^{(i)}_j = 1 \\end{align} where \\(\\theta\\) is the collection of all the parameters of the model (mixture weights, means, and covariance matrices): \\begin{align} \\theta = \\{\\phi_1, \\cdots, \\phi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k\\} \\end{align} for example, the following plot shows what a gmm derived from 3 mixture components looks like: as a consequence, for each data point, \\(x^{(i)}\\) (in red), we can compute the probability that it belongs to each component (\\(p(x^{(i)}|z^{(i)} = j)\\), where \\(j = 1, 2, 3\\))(make a âsoftâ assignment). this quantity is called âresponsibilityâ. em algorithm with gmm's the expectation maximization algorithm is comprised of two steps: guess the value of the responsibilities \\(w^{(i)}_j\\), that represent the \"amount\" of each \\(x^{(i)}\\) that was generated from the distribution \\(j\\) (or the probability that the \\(j\\)th distribution generated the point \\(x^{(i)}\\)). compute the values of the parameters of the distributions: \\(\\theta = \\{\\phi, \\mu, \\sigma\\}\\) according to the \\(mle\\) (maximum likelihood estimation) with respect to the parameters. thus, we want to maximize \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\). e-step in this step, as we have said, we will compute the value of the responsibilities with the given parameters \\(\\phi, \\mu, \\sigma\\). so for each example \\(i\\) and each component (distribution) \\(j\\), the amount of \\(x^{(i)}\\) that is generated by the component \\(j\\) is given by: \\begin{align} w^{(i)}_j = p(z^{(i)} = j | x^{(i)}; \\phi_j, \\mu_j, \\sigma_j) \\end{align} by bayes' rule, we can rewrite the equation as follows: \\begin{align} w^{(i)}_j = \\frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\\sum_{l=1}^k \\left[p(x^{(i)}|z^{(i)} = l)p(z^{(i)} = l)\\right]} \\end{align} note that the likelihood \\(p(x^{(i)}|z^{(i)} = j)\\) and each likelihood \\(p(x^{(i)}|z^{(i)} = l)\\) come from a gaussian distribution, therefore: \\begin{align} p(x^{(i)}|z^{(i)} = j) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\sigma_j|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(x^{(i)} - \\mu_j)^t \\sigma_j^{-1} (x^{(i)} - \\mu_j)\\right) \\tag{2} \\end{align} to simplify notation we will denote \\(p(x^{(i)}|z^{(i)} = j)\\) as \\(\\mathcal{n}(\\mu_j, \\sigma_j)\\). on the other hand, the prior \\(p(z^{(i)} = j)\\) comes from a multinomial distribution, hence: \\begin{align} p(z^{(i)} = j) = \\phi_j \\tag{3} \\end{align} combining all the expressions: \\begin{align} w^{(i)}_j = \\frac{\\phi_j\\mathcal{n}(\\mu_j, \\sigma_j)}{\\sum_{l=1}^k \\left[\\phi_l\\mathcal{n}(\\mu_l, \\sigma_l)\\right]} \\tag{4} \\end{align} all that is left to do is plug all of the values into each equation \\((2)\\) and \\((3)\\) (this values are known, given the equations are written in terms of the distributions' parameters) and compute each \\(w^{(i)}_j\\) given \\((4)\\). m-step in this step what we do is maximize the log likelihood of the distributions' parameters \\(\\theta\\), that is we maximize \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\). but first, let us see how do we maximize the parameters in gmm. optimal parameters of a gmm we are going to show how to maximize the log likelihood of the parameters of a gaussian mixture model. the goal of the gmm is to represent the distribution of the data as accurately as possible using a linear combination of gaussian distributions. given a dataset \\(x\\) of \\(m\\) data points, we assume they are i.i.d (independent and identically distributed), therefore the maximum likelihood estimator over \\(x\\) can be expressed as the product of the individual likelihoods. to simplify the equations, we are going to directly apply the logarithm to the mle function: \\begin{align} \\log \\mathcal{l}(x|\\theta) = \\log p(x|\\theta) = \\log \\prod_{i=1}^m p(x^{(i)}|\\theta) = \\sum_{i=1}^m \\log p(x^{(i)}|\\theta) \\end{align} by \\((1)\\) we know that \\(p(x^{(i)}|\\theta)\\) is a linear combination of gaussian distributions, therefore: \\begin{align} \\log \\mathcal{l}(x|\\theta) = \\sum_{i=1}^n \\log \\sum_{j=1}^k \\phi_j^{(i)}\\mathcal{n}(x^{(i)}|\\mu_j, \\sigma_j) \\end{align} this equation is not tractable, so we won't get an analytical solution by just taking the its derivative with respect to \\(\\theta\\) and setting it to 0. the following set of equations outline how we would evaluate it: \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\mu_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\mu_j} = 0^t \\end{align} \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\sigma_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\sigma_j} = 0 \\end{align} \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\phi_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\phi_j} = 0 \\end{align} observe that the computation of each parameter from \\(\\theta (\\mu, \\sigma, \\phi)\\) depends on the other parameters in a complex way. to solve those equations, we can use the strategy of optimizing some parameters while keeping the others fixed. going back to the expectation maximization algorithm, there is a way of updating the individual parameters of a gmm given prior (initialized at random) parameters \\(\\mu, \\sigma, \\phi\\). this approach works by updating some parameters while keeping the others fixed. so, by solving the derivatives presented above we derive the three following updating rules: \\begin{align} \\hat{\\mu}_j = \\frac{\\sum_{i=1}^m w^{(i)}_jx^{(i)}}{\\sum_{l=1}^m w^{(l)}_j} \\end{align} \\begin{align} \\hat{\\sigma}_j = \\frac{\\sum_{i=1}^m w^{(i)}_j (x^{(i)} - \\hat{\\mu}_j)(x^{(i)} - \\hat{\\mu}_j)^t}{\\sum_{l=1}^m w^{(l)}_j} \\end{align} \\begin{align} \\hat{\\phi}_j = \\frac{1}{m} \\sum_{i=1}^m w^{(i)}_j \\end{align} to simplify a bit the notation, if \\(n_j = \\sum_{l=1}^m w^{(i)}_l\\): \\begin{align} \\hat{\\mu}_j = \\frac{1}{n_j} \\sum_{i=1}^m w^{(i)}_jx^{(i)} \\end{align} \\begin{align} \\hat{\\sigma}_j = \\frac{1}{n_j}\\sum_{i=1}^m w^{(i)}_j (x^{(i)} - \\hat{\\mu}_j)(x^{(i)} - \\hat{\\mu}_j)^t \\end{align} \\begin{align} \\hat{\\phi}_j = \\frac{n_j}{m} \\end{align} note that the update of \\(\\mu, \\sigma, \\phi\\), all depend on the responsibilities (\\(w^{(i)}_j\\)), which by its turn, depends on \\(\\mu, \\sigma, \\phi\\). thatâs why there's not a closed-form solution to equations. furthermore these equations do not aim to precisely maximize over \\(\\theta\\) the actual log likelihood. instead they maximize a proxy function of the log-likelihood over \\(\\theta\\), namely, the expected log-likelihood, which can be derived from the log-likelihood using jensen's inequality as follows: \\begin{align} \\hat{\\mathcal{l}}(x|\\theta) = \\sum_{i=1}^m\\sum_{j=1}^k w^{(i)}_j \\log \\left( \\frac{\\phi_j \\mathcal{n}(x^{(i)} | \\mu_j, \\sigma_j)}{w^{(i)}_j} \\right) \\tag{5} \\end{align} iterative process the process consists of an iterative process that alternates between two steps. the first step is to compute the responsibilities (e step) of each mixture component for each data point using the current parameters (\\(\\mu, \\sigma, \\phi\\)). the second step consists of updating the parameters (m step) in order to maximize the expected log-likelihood given by \\((5)\\) the e and m steps are repeated until there is no significant progress in the proxy function of the log-likelihood computed after the m step. recap: anomaly detection thus, when the parameters \\(\\theta\\) are optimized, we can compute \\(p(x) = \\sum_{j=1}^k p(x|z = j)\\) and if \\(p(x) < \\epsilon\\) you can flag \\(x\\) as an anomaly. generalized em algorithm jensen's inequality convex function we are going to show what jensen's inequality is about. so: let \\(f\\) be a convex function (e.g. \\(f''(x) > 0\\)) and let \\(x\\) be a random variable, then \\begin{align} f(e[x]) \\leq e[f(x)] \\end{align} where \\(e\\) is the expected value. further, if \\(f''(x) > 0\\) (we say f is strictly convex, that is f is not a straight line), then: \\begin{align} e[f(x)] = f(e[x]) \\leftrightarrow \\text{ x is a constant, more formally } x = e[x] \\text{ with probability 1} \\end{align} concave function we are going to apply the same arguments with a concave function. note that a concave function equals the negative of a convex function, thus: let \\(f\\) be a concave function (e.g. \\(f''(x) < 0\\)) and let \\(x\\) be a random variable, then \\begin{align} f(e[x]) \\geq e[f(x)] \\end{align} where \\(e\\) is the expected value. further, if \\(f''(x) < 0\\) (we say f is strictly concave), then: \\begin{align} e[f(x)] = f(e[x]) \\leftrightarrow \\text{ x is a constant, more formally } x = e[x] \\text{ with probability 1} \\end{align} some intuition given any convex function (the inverse also applies to concave functions), if we draw a chord between any two points, its middle point (that is the expected value of the function or \\(e[f(x)]\\)) is always above that the value of the expected value under the function (that is \\(f(e[x])\\)). graphically: motivation given a model for \\(p(x, z , \\theta)\\) where \\(\\theta\\) are the parameters of the model. we only observe \\(x = \\{x^{(1)}, \\cdots, x^{(m)}\\}\\). the goal is to obtain by the maximum likelihood estimation the value of \\(\\theta\\) that maximizes the log likelihood, defined as: \\begin{align} \\theta = \\underset{\\theta}{\\arg \\max{l(\\theta)} } = \\sum_{i=1}^m \\log (p(x^{(i)}; \\theta)) \\end{align} if we marginalize \\(z^{(i)}\\): \\begin{align} \\theta = \\underset{\\theta}{\\arg \\max{l(\\theta)} } = \\sum_{i=1}^m \\log \\sum_{z^{(i)}} (p(x^{(i)}, z^{(i)}; \\theta)) \\end{align} e-step in the e-step we construct a lower bound from a given theta: so, let's say \\(l(\\theta)\\) is the log likelihood. on the first iteration, the graph would be as follows: and on the second iteration: we iterate until there are no significant changes in the lower bound, that is the algorithm converges to a local optimum (it should be noted the optimum is local not absolute, and it depends on the initialization of the distributions' parameters). m-step now, in the m-step we maximize the log likelihood as follows: \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log p(x^{(i)}; \\theta) \\end{align} by marginalizing \\(z^{(i)}\\): \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\theta) \\end{align} we now introduce a probability distribution over \\(z^{(i)}\\) (thus \\(\\sum_{z^{(i)}}q(z^{(i)}) = 1\\)), and we multiply by \\(\\frac{q(z^{(i)})}{q(z^{(i)})}\\): \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log \\sum_{z^{(i)}} q(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\end{align} now, by the definition of expected value (given a sequence of real values \\(a_1, \\cdots, a_n\\) with probabilities \\(p_1, \\cdots, p_n\\), the expected value is defined as: \\(e = \\sum_{i=1}^n p_i a_i\\)), if \\(p_i = q(z^{(i)})\\) and \\(a_i = \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\) \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\end{align} if we apply the concave version of jensen's inequality we obtain a lower bound of the form: \\begin{align} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\geq \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\right] \\end{align} if \\(\\log (x) = f(x)\\), then this equation can be mapped to the inequality: \\begin{align} f(e[x]) \\geq e[f(x)] \\end{align} note that \\(log\\) is a concave function. if we \"unpack\" the expected value: \\begin{align} \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\right] = \\sum_{i=1}^m \\sum_{z^{(i)}} \\log q(z^{(i)}) \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\end{align} make log likelihood and lower bound equal on theta for each \\(\\theta\\) on the e-step you wan the value of \\(\\theta\\) under the lower bound function to be equal to \\(l(\\theta)\\), which is what guarantees that when you optimize the lower bound you optimize \\(l(\\theta)\\). so, for a given iteration the current value of \\(\\theta\\), denoted by \\(\\hat{\\theta}\\), we want: \\begin{align} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})}\\right] = \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})} \\right] \\end{align} remember, by the extension on jensen's inequality we know that \\(e[f(x)] = f(e[x])\\) if and only if \\(x\\) is a constant. in this case \\begin{align} x = \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})} = constant \\end{align} for this to hold, we need \\(q(z^{(i)})\\) to be directly proportional to \\(p(x^{(i)}, z^{(i)}; \\hat{\\theta})\\) (so when one is bigger the other is bigger and vice versa, so the ratio between the two remains constant). so: \\begin{align} q(z^{(i)}) \\propto p(x^{(i)}, z^{(i)}; \\hat{\\theta}) \\end{align} because \\(\\sum_{z^{(i)}}q(z^{(i)}) = 1\\), a way to ensure this is to set each \\(q^{(i)} = p(x^{(i)}, z^{(i)}; \\hat{\\theta})\\) and then normalize it to make sure the sum of \\(q\\) over \\(z^{(i)}\\) equals one. hence: \\begin{align} q(z^{(i)}) = \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{\\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\hat{\\theta})} \\end{align} it turns out you can further derive this equation to be: \\begin{align} q(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\hat{\\theta}) \\end{align} putting everything together so, after everything we have seen, we can summarize the em generalized algorithm as follows: if \\(\\theta\\) is the value of the parameters in the current iteration: e-step: set \\begin{align} q_i(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\theta) \\end{align} m-step: set \\begin{align} \\theta := \\underset{\\theta}{\\arg \\max} \\sum_{i=1}^m \\sum_{z^{(i)}} q_i(z^{(i)}) \\log \\left[\\frac{p(x^{(i)}, z^{(i)};\\theta)}{q_i(z^{(i)})}\\right] \\end{align} derive em for gmm from the generalized algorithm given a model described by: \\(p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)})\\) where \\(z^{(i)} \\sim multinomial(\\phi)\\) (which means \\(p(z^{(i)} = j) = \\phi_j\\)) and \\(x^{(i)} | z^{(i)} \\sim \\mathcal{n}(\\mu_j, \\sigma_j)\\) e-step on the e-step we compute: \\(q_i(z^{(i)}) p(z^{(i)} = j | x^{(i)}; \\phi, \\mu, \\sigma)\\) if we look at e-step from gmm's we can see that the expression above equals \\(w^{(i)}_j\\). m-step now on the m-step what we do is maximize the lower bound we have constructed in the e-step. for that we need to compute the value of the parameters \\(\\phi, \\mu, \\sigma\\) that maximize this function, that is: \\begin{align} \\underset{\\phi, \\mu, \\sigma}{\\max} \\sum_{i=1}^m \\sum_{z^{(i)}} q_i(z^{(i)}) \\log \\left( \\frac{p(x^{(i)}, z^{(i)}; \\phi, \\mu, \\sigma)}{q_i(z^{(i)})} \\right) = \\end{align} as we know \\(q_i(z^{(i)}) = w^{(i)}_j\\) and \\(p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)})\\), thus: \\begin{align} = \\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{p(x^{(i)}|z^{(i)} = j, \\mu_j, \\sigma_j) p(z^{(i)} = j)}{w^{(i)}_j} \\right) \\end{align} we also know that \\(p(z^{(i)} = j) = \\phi_j\\) and \\(x^{(i)} | z^{(i)} \\sim \\mathcal{n}(\\mu_j, \\sigma_j)\\), therefore: \\begin{align} = \\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{\\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) \\phi_j}{w^{(i)}_j} \\right) \\end{align} where: \\begin{align} \\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) = \\frac{1}{(2\\pi)^{1/2}|\\sigma_j|^{1/2}} \\exp \\left( -\\frac{1}{2}(x^{(i)} - \\mu_j)^t \\sigma_j^{-1}(x^{(i)} - \\mu_j)\\right) \\end{align} from now on we denote \\(\\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{\\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) \\phi_j}{w^{(i)}_j} \\right)\\) as \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\): to maximize this formula over \\(\\phi, \\mu\\) and \\(\\sigma\\) you have to compute the derivatie of the function with respect to each parameter, such that: \\(\\delta_{\\mu_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\), then: \\(\\mu_j = \\sum_{i}^m \\frac{w^{(i)}_j x^{(i)}_j}{w^{(i)}_j}\\) (same as in m-step in gmm's) \\(\\delta_{\\sigma_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\) and \\(\\delta_{\\phi_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/RegresiÃ³n Lineal.html",
    "title": "RegresiÃ³n Lineal",
    "body": " index search search back regresiÃ³n lineal contents descripci n de los datos hip tesis funcion de coste regularizaci n descenso gradiente regularizaci n descripciÃ³n de los datos \\(x = (x_{ij})\\) una matriz \\(n \\times m\\) donde cada \\(x_{ij}\\) es la caracterÃ­stica \\(i\\) del ejemplo \\(j\\), tal que \\begin{align} x = \\begin{bmatrix} x_{11} & \\cdots & x_{1m} \\\\ \\cdots & \\ddots & \\cdots \\\\ x_{n1} & \\cdots & x_{nm} \\\\ \\end{bmatrix} \\end{align} cada columna es un ejemplo en cada fila estÃ¡n los valores de una caracterÃ­stica \\(\\theta = (\\theta_i)\\) es un vector fila \\(1\\times n\\) donde cada \\(\\theta_i\\) es el peso de la caracterÃ­stica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_1 & \\cdots & \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector fila \\(1\\times m\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 & \\cdots & y_m\\end{bmatrix} \\end{align} hipÃ³tesis dado un ejemplo, es decir un vector columna \\(x\\), de dimensiones \\(n \\times 1\\), la hipÃ³tesis se define como: \\begin{align} h_\\theta(x) = \\sum_{i=1}^n \\theta_i \\cdot x_i \\end{align} dado un conjunto de \\(m\\) datos, es decir matriz \\(x\\), de dimensiones \\(n \\times m\\), la hipÃ³tesis se define como: \\begin{align} h_\\theta(x) = \\theta\\cdot x = \\begin{bmatrix}\\sum_{i=1}^n \\theta_i \\cdot x_{i1} & \\cdots & \\sum_{i=1}^n \\theta_i \\cdot x_{im}\\end{bmatrix} \\end{align} el resultado es un vector fila \\(1 \\times m\\), es decir como el vector de salidas \\(y\\) funcion de coste se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 \\end{align} esta funciÃ³n de coste se denomina mÃ­nimos cuadrados. regularizaciÃ³n con regularizaciÃ³n, se define la funciÃ³n de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} descenso gradiente para actualizar el vector de pesos \\(\\theta\\) aplicamos el descenso gradiente. para cada peso \\(\\theta_i\\): \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) \\end{align} la derivada del coste en funciÃ³n del peso \\(\\theta_i\\) se calcula como sigue: sustituimos la funciÃ³n de coste \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2\\right) = \\end{align} sacamos el factor constante de la derivada \\begin{align} = \\frac{1}{2m} \\frac{\\delta}{\\delta \\theta_i} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2\\right) \\end{align} aplicamos la propiedad que dice que la derivada de una suma equivale a la suma de las derivadas \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m \\frac{\\delta}{\\delta \\theta_i}(h_\\theta(x_j) - y_j)^2\\right) \\end{align} aplicamos la regla de la cadena \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m \\frac{\\delta(h_\\theta(x_j) - y_j)^2}{\\delta (h_\\theta(x_j) - y_j)} \\frac{\\delta (h_\\theta(x_j) - y_j)}{\\delta \\theta_i}\\right) \\end{align} aplicamos aritmÃ©tica \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\left[\\frac{\\delta (h_\\theta(x_j))}{\\delta \\theta_i} - \\frac{\\delta (y_j)}{\\delta \\theta_i}\\right]\\right) \\end{align} como la derivada de \\(y_i\\) es funciÃ³n de \\(\\theta_i\\) es cero, procedemos a calcular la derivada de \\(h_\\theta(x_j)\\): \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\sum_{k=1}^n \\theta_k x_{kj} = \\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k x_{kj} \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k x_{kj} = \\begin{cases} x_{kj}, & k = i \\\\ 0, & k \\neq i \\\\ \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sÃ³lo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = x_{kj} \\end{align} volemos a la derivada del peso, con \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = x_{kj}\\) y \\(\\frac{\\delta y_j}{\\delta \\theta_i} = 0\\): \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\left[x_{ij} - 0\\right]\\right) \\end{align} \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\end{align} sacamos el factor constante 2 como factor comÃºn que se elimina con 1/2 \\begin{align} = \\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\end{align} finalmente, sustituimos todo en la funciÃ³n del gradiente: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right)\\right] \\end{align} regularizaciÃ³n con regularizaciÃ³n debemos derivar la funciÃ³n que coste que incluye el parÃ¡metro de regularizaciÃ³n: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) + \\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2 \\end{align} el primer tÃ©rmino ya lo hemos derivado, por lo tanto procedemos a derivar el segundo tÃ©rmino: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} \\left(\\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2\\right) \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2 = \\begin{cases} 2 \\theta_k, & k = i \\\\ 0, & k \\neq i \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sÃ³lo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} 2\\theta_i = \\frac{\\lambda}{m} \\theta_i \\end{align} por lo tanto la funciÃ³n del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) + \\frac{\\lambda}{m}\\theta_i\\right] \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/EvaluaciÃ³n de modelos.html",
    "title": "EvaluaciÃ³n de modelos",
    "body": " index search search back evaluaciÃ³n de modelos contents separaci n de datos entrenamiento en regresi n lineal entrenamiento en regresi n log stica selecci n de modelos cross validation proceso de selecci n diagn stico sesgo vs varianza regresi n lineal con regularizaci n escoger el par metro de regularizaci n curva de aprendizaje debugging un algoritmo de aprendizaje medidas de evaluaci n balance entre precisi n y recall separaciÃ³n de datos a la hora de entrenar un modelo, separamos los datos en dos conjuntos: conjunto de entrenamiento: \\(70\\%\\) - \\(80\\%\\) conjunto de test: \\(30\\%\\) - \\(20\\%\\) entrenamiento en regresiÃ³n lineal el proceso de entrenamiento en la regresiÃ³n lineal consiste en: entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \\(\\theta\\) minimizando el coste \\(j(\\theta)\\) calcular el coste sobre el conjunto de test \\(j_{test}(\\theta)\\) \\[%align j_{test}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m_{test}} (h_\\theta(x^{(i)}_{test}) - y^{(i)}_{test})^2 \\] entrenamiento en regresiÃ³n logÃ­stica el proceso de entrenamiento en la regresiÃ³n logÃ­stica consiste en: entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \\(\\theta\\) minimizando el coste \\(j(\\theta)\\) calcular el coste sobre el conjunto de test \\(j_{test}(\\theta)\\) \\[%align j_{test}(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m_{test}} \\left[y^{(i)}_{test} \\log(h_\\theta(x^{(i)}_{test})) + (1-y^{(i)}_{test})\\log(1-h_\\theta(x^{(i)}_{test})) \\right] \\] el error de clasificaciÃ³n en la regresiÃ³n logÃ­stica se define como sigue: \\begin{align} error(h_\\theta(x), y) = \\begin{cases} 1, & \\text{ si } h_\\theta(x) \\geq 0.5 \\rightarrow \\log(h_\\theta(x)) = 1 \\text{ e } y = 0 \\\\ 1, & \\text{ si } h_\\theta(x) < 0.5 \\rightarrow \\log(h_\\theta(x)) = 0 \\text{ e } y = 1 \\\\ 0, \\text{ en cualquier otro caso } \\\\ \\end{cases} \\end{align} selecciÃ³n de modelos supongamos que tenemos \\(n\\) modelos, tal que cada modelo es equivalente al anterior pero con una caracterÃ­stica mÃ¡s en sus datos: modelo 1: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1\\) modelo 2: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2\\) modelo n: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + \\cdots + \\theta_n \\cdot x_n\\) para evaluar los modelos lo que hacemos es escoger el que menor coste obtenga sobre el conjunto de test, tras ser entrenado sobre el conjunto de entrenamiento. \\begin{align} \\begin{bmatrix} \\theta^{(1)} \\\\ \\theta^{(2)} \\\\ \\vdots \\\\ \\theta^{(n)} \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} j_{test}(\\theta^{(1)}) \\\\ j_{test}(\\theta^{(2)}) \\\\ \\vdots \\\\ j_{test}(\\theta^{(n)}) \\\\ \\end{bmatrix} \\end{align} sin embargo, se puede dar el problema de que el mejor simplemente produzca overfitting sobre el conjunto de test (lo cual es probable cuando el vector de pesos tiene dimensiones grandes). para solventar este problema: cross validation separaremos el conjunto de datos en tres conjuntos: conjunto de entrenamiento: \\(60\\%\\) conjunto de validaciÃ³n cruzada (cross validation): \\(20\\%\\) conunto de test: \\(20\\%\\) por lo tanto ahora la funciÃ³n de coste para cada conjunto tiene la forma: funciÃ³n de coste para el conjunto de entrenamiento: \\begin{align} j_{train}(\\theta) = \\frac{1}{2m_{train}} \\sum_{i=1}^{m_{train}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} funciÃ³n de coste para el conjunto de test: \\begin{align} j_{test}(\\theta) = \\frac{1}{2m_{test}} \\sum_{i=1}^{m_{test}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} funciÃ³n de coste para el conjunto de validaciÃ³n cruzada: \\begin{align} j_{cv}(\\theta) = \\frac{1}{2m_{cv}} \\sum_{i=1}^{m_{cv}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} proceso de selecciÃ³n entonces ahora para seleccionar un modelo lo que hacemos que para cada modelo \\(q\\): minimizamos \\(j_{train}(\\theta^{(q)})\\) para obtener los pesos \\(\\theta^{(q)}\\) Ã³ptimos. calculamos el coste sobre el conjunto de validaciÃ³n cruzada \\(j_{cv}(\\theta^{(q)})\\) una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validaciÃ³n cruzada y calculamos \\(j_{test}(\\theta^{(q)})\\) para evaluar la capacidad de generalizaciÃ³n del modelo. diagnÃ³stico: sesgo vs varianza underfitting: cuando se produce underfitting el coste de entrenamiento y el coste de validaciÃ³n tienen valores similares y ambos tiene valores bastante altos overfitting: cuando se produce overfitting el coste de entrenamiento es mucho menor que el coste de validaciÃ³n cruzada. regresiÃ³n lineal con regularizaciÃ³n tambiÃ©n es importante observar cÃ³mo afecta el parÃ¡metro de regularizaciÃ³n a nuestros modelos. por ejemplo, en la regresiÃ³n linear, la funciÃ³n de coste tiene la forma: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} por lo tanto, el aumentar o reducir \\(\\lambda\\) es directamente proporcional al coste. si el parÃ¡metro de regularizaciÃ³n \\(\\lambda\\) es muy grande entonces los pesos van a tender a ser muy pequeÃ±os (ya que el coste aumenta al aumentar el valor de \\(\\lambda\\)) si el parÃ¡metro de regularizaciÃ³n \\(\\lambda\\) es muy pequeÃ±o entonces los pesos van a poder ser grandes (ya que el coste se reduce al reducir el valor de \\(\\lambda\\)) escoger el parÃ¡metro de regularizaciÃ³n para escoger el parÃ¡metro de regularizaciÃ³n seguimos el mismo proceso que para escoger el mejor modelo, para cada modelo \\(q\\): minimizamos \\(j_{train}(\\theta^{(q)})\\) para obtener los pesos \\(\\theta^{(q)}\\) Ã³ptimos. calculamos el coste sobre el conjunto de validaciÃ³n cruzada \\(j_{cv}(\\theta^{(q)})\\) una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validaciÃ³n cruzada y calculamos \\(j_{test}(\\theta^{(q)})\\) para evaluar la capacidad de generalizaciÃ³n del modelo. curva de aprendizaje a continuaciÃ³n vamos a estudiar cÃ³mo afecta el tamaÃ±o del conjunto de datos \\(m\\), el sesgo y la varianza a nuestro modelo: cuanto mayor es el tamaÃ±o, mÃ¡s difÃ­cil es encontrar una hipÃ³tesis que se adapte (\\(j_{train}(\\theta)\\) es mayor), pero el modelo generaliza mejor (\\(j_{cv}(\\theta)\\) es menor) cuando el sesgo (bias) es grande, entonces se produce underfitting y las predicciones de nuestro modelo son malas: el error del modelo es elevado, tanto sobre el conjunto de entrenamiento como sobre el conjunto de validaciÃ³n cruzada tener mÃ¡s ejemplos ayuda al modelo cuando la varianza (variance) es grande, entonces se produce overfitting, tal que el error en el conjunto de validaciÃ³n cruzada es muy alto: el modelo se adapta al conjunto de datos, por lo que el error de entrenamiento es menor tener mÃ¡s muestras ayuda al modelo debugging un algoritmo de aprendizaje para arreglar el overfitting que se produce cuando la varianza es elevada: obtener mÃ¡s datos de entrenamiento utilizar menos caracterÃ­sticas (reducir el grado del vector de pesos), pero tras un proceso de selecciÃ³n de aquellas mÃ¡s relevantes intentar aumentar el parÃ¡metro de regularizaciÃ³n para arregar el underfitting que se produce cuando el sesgo es elevado: aÃ±adir mÃ¡s caracterÃ­sticas aÃ±adir caracterÃ­sticas polinÃ³micas intentar reducir el parÃ¡metro de regularizaciÃ³n en las redes neuronales: las redes pequeÃ±as tienden a producir underfitting pero son menos costosas computacionalmente las redes grandes tienen mÃ¡s caracterÃ­sticas, por lo tanto hay una mayor probabilidad de overfitting gestionar datos sesgados: hay que ser consciente que a veces, por ejemplo en problemas de clasificaciÃ³n, hay categorÃ­as que con mÃ¡s comunes que el resto medidas de evaluaciÃ³n la precisiÃ³n y el recall son medidas de evaluaciÃ³n que se complementan: Â  Â  resultado resultado Â  Â  1 0 predicciÃ³n 1 verdadero positivo (tp) falso positivo (fp) predicciÃ³n 0 falso negativo (fn) verdadero negativo (tn) precisiÃ³n = \\(\\frac{tp}{\\text{ # positivos predichos }} = \\frac{tp}{tp + fp}\\) recall = \\(\\frac{tp}{\\text{ # positivos reales }} = \\frac{tp}{tp + fn}\\) balance entre precisiÃ³n y recall cuÃ¡nto mayor es la precisiÃ³n menor es el recall y viceversa. entonces si queremos un modelo mÃ¡s preciso: \\begin{align} \\begin{cases} \\text{predecir } 1, \\text{ si } h_\\theta(x) \\geq 0.7 \\\\ \\text{predecir } 0, \\text{ si } h_\\theta(x) < 0.7 \\\\ \\end{cases} \\end{align} entonces, la precisiÃ³n es mayor ya que el nÃºmero de \\(fp\\) es menor, pero el recall es menor, ya que el nÃºmero de \\(fn\\) es mayor. lo mismo pasa si queremos evitar falsos negativos, entonces hacemos: \\begin{align} \\begin{cases} \\text{predecir } 1, \\text{ si } h_\\theta(x) \\geq 0.5 \\\\ \\text{predecir } 0, \\text{ si } h_\\theta(x) < 0.5 \\\\ \\end{cases} \\end{align} tal que se reduce el nÃºmero de \\(fn\\), y se aumenta el recall, pero el nÃºmero de \\(fp\\) es mayor, por lo que se reduce la precisiÃ³n. entonces, para encontrar un punto de balance entre las dos medidas tenemos que seleccionar un valor lÃ­mite, tal que hacemos la predicciÃ³n en base a \\(h_\\theta(x) \\geq \\text{ limite }\\). para calibrar ese lÃ­mite podemos utilizar dos mÃ©tricas de evaluaciÃ³n: la media de ambas mÃ©tricas: \\(\\frac{p + r}{2}\\), funciona mal cuando \\(p >> r\\) o \\(r >> p\\), ya que el valor va a ser alto, pero no se ha encontrado un equilibrio. la puntuaciÃ³n \\(f_1 = 2 \\cdot \\frac{p\\cdot r}{(p + r)}\\), tal que cuanto mayor sea esta puntuaciÃ³n mejor ahora \\begin{align} \\begin{cases} f_1 \\approx 0, && p >> r \\\\ f_1 \\approx 0, && r >> p \\\\ \\end{cases} \\end{align} para escoger el lÃ­mite lo que se hace es calcular la puntuaciÃ³n \\(f_1\\) sobre el conjunto de validaciÃ³n cruzada, y se escoge aquel lÃ­mite que ofrezca la mayor puntuaciÃ³n. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/ML/StanfordCoursera/Map Reduce.html",
    "title": "Map Reduce",
    "body": " index search search back map reduce map reduce nos permite paralelizar los algoritmos. por ejemplo, supongamos que: tenemos \\(m = 400\\) datos utilizamos batch gradient descent para resolver el problema de optimizaciÃ³n tenemos un nÃºmero de pc equivalente a 4 sea \\(i\\) el Ã­ndice de un pc este entrena sobre \\(x^{(i)}, \\cdots, x^{(i+100)}\\) calculamos el coste parcial para este conjunto de datos como: \\(temp_j^{(k)} = \\sum_{i}^{i+100} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) ahora, combinamos todos los pesos y aplicamos descenso gradiente: \\(\\theta_j = \\theta_j - \\alpha \\frac{1}{400} \\left( \\sum_{i}^k temp_j^{(i)}\\right)\\) este tipo de tÃ©cnicas se utilizan si los algoritmos de entrenamiento pueden ser utilizamos como la suma de funciones, tanto el coste como el gradiente. tambiÃ©n es aplicable a pcs con mÃºltiples cores. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T3/02_information_criteria_and_crossvalidation.html",
    "title": "Information criteria and cross-validation",
    "body": " index search search back information criteria and cross-validation contents estimating out-of-sample predictive accuracy using available data akaike information criterion aic deviance information criterion dic and effective number of parameters watanabe-akaike or widely applicable information criterion waic effective number of parameters as a random variable bayesian information criterion bic leave-one-out cross-validation summary for historical reasons, measures of predictive accuracy are referred to as information criteria. these criteria are usually based on something called deviance that it's calculated by taking the negative two times the logarithm of the likelihood of the data given a point estimate of the model, \\(-2 \\log p(y|\\hat{\\theta}))\\). when we make predictions about new data that the model hasn't seen before, these predictions are usually less accurate than what we'd expect based on how well the model fits the data it was trained on. even if the family of models being fit happens to include the true data-generating process, and even if the parameters in the model happen to be sampled exactly from the specified prior distribution. we are interested in prediction accuracy for two reasons: to measure the performance of a model. to compare models. when comparing models with the same number of parameters, we can directly compare their best-fit log predictive densities. but when comparing models of different sizes or complexities we need to adjust for the fact that larger models can sometimes fit data better just by chance. estimating out-of-sample predictive accuracy using available data several methods are available to estimate the expected predictive accuracy without waiting for out-of-sample data. within-sample predictive accuracy: a naive estimate of the expected log predictive density for new data is the log predictive density for existing data using the computed lppd. this summary is in general an overestimate of elppd because it is evaluated on the data from which the model was fit. adjusted within-sample predictive accuracy: given that lppd is a biased estimate of elppd, the next logical step is to correct that bias. formulas such as aic, dic, and waic (all discussed below) give approximately unbiased estimates of elppd. cross-validation: one can attempt to capture out-of-sample prediction error by fitting the model to training data and then evaluating this predictive accuracy on a holdout set. cross-validation can be computationally expensive. akaike information criterion (aic) in much of the statistical literature on predictive accuracy, inference for \\(\\theta\\) is summarized not by a posterior distribution \\(p_{\\text{post}}\\) but by a point estimate \\(\\hat{\\theta}\\), typically the maximum likelihood estimate. out-of-sample predictive accuracy is then defined not by the expected log posterior predictive density (elppd) but by \\(\\text{elpd}_{\\hat{\\theta}} = \\mathbb{e}_f[\\log p(\\tilde{y}|\\tilde{\\theta}(y))]\\). let \\(k\\) be the number of parameters estimated in the model. aic is defined as follows: \\begin{align} \\hat{\\text{elpd}}_{\\text{aic}} = -2 \\log p(y|\\hat{\\theta}_{\\text{mle}}) + 2k \\end{align} subtracting \\(k\\) from the log predictive density given the maximum likelihood estimate \\(\\theta_{\\text{mle}}\\) is a correction to account for how much the fitting of \\(k\\) parameters will increase predictive accuracy, purely by chance. when we move beyond linear models with simple priors, just adding the number of fitted parameters \\(k\\) to adjust the deviance isn't accurate. informative priors and hierarchical structures typically decrease overfitting compared to simple estimation methods like least squares or maximum likelihood. in models with informative priors or hierarchical setups, the actual number of parameters depends heavily on the variance of the group-level parameter. deviance information criterion (dic) and effective number of parameters dic is a somewhat bayesian version of aic making two changes, replacing the maximum likelihood estimate with the posterior mean \\(\\hat{\\theta}_{\\text{bayes}} = \\mathbb{e}[\\theta|y]\\) and replacing \\(k\\) with a data-based bias correction. the new measure of predictive accuracy is: \\begin{align} \\hat{\\text{elpd}}_{dic} = \\log p(y|\\hat{\\theta}_{\\text{bayes}}) - p_{\\text{dic}} \\end{align} where \\(p_{\\text{dic}}\\) is the effective number of parameters, defined as: \\begin{align} p_{\\text{dic}} = 2 \\left(\\log p(y|\\hat{\\theta}_{\\text{bayes}}) - \\mathbb{e}_{post}[\\log p(y|\\theta)]\\right) \\end{align} where \\(\\mathbb{e}_{post}[\\log p(y|\\theta)]\\) is an average of \\(\\theta\\) over its posterior distribution. this is computed using simulation \\(\\theta^s, s= 1, \\cdots, s\\) as: \\begin{align} \\text{computed } p_{\\text{dic}} = 2 \\left(\\log p(y|\\hat{\\theta}_{\\text{bayes}}) - \\frac{1}{s} \\sum_{s=1}^s \\log p(y|\\theta^s)\\right) \\end{align} when the average value of \\(\\theta\\) in the posterior distribution matches the highest point (mode), it leads to the maximum log predictive density. however, if the average value is significantly different from the mode, it can result in a negative value for \\(p_{\\text{dic}}\\). an alternative version of dic uses a slightly different effective number of parameters: \\begin{align} p_{\\text{dic}_{\\text{alt}}} = 2 \\mathbb{v}_{\\text{post}}[\\log p(y|\\theta)] \\end{align} of these two measures, \\(p_{\\text{dic}}\\) is more numerically stable but \\(p_{\\text{dic}_{\\text{alt}}}\\) has the advantage of always being positive. the actual quantity called dic is defined in terms of the deviance rather than the log predictive density; thus: \\begin{align} \\text{dic} = -2 \\log p(y|\\hat{\\theta}_{\\text{bayes}}) + 2p_{dic} \\end{align} watanabe-akaike or widely applicable information criterion (waic) waic is a more fully bayesian approach for estimating the out-of-sample expectation. starting with the computed lppd and then adding a correction for effective number of parameters to adjust for overfitting. two adjustments have been proposed: \\begin{align} p_{\\text{waic}1} = 2 \\sum_{n=1}^n \\left(\\log(\\mathbb{e}_{\\text{post}}[p(y_i|\\theta)]) - \\mathbb{e}_{\\text{post}}[\\log p(y_i|\\theta)] \\right) \\end{align} computed by replacing the expectations by averages over the \\(s\\) posterior draws \\(\\theta^s\\): \\begin{align} \\text{computed } p_{\\text{waic}1} = 2 \\sum_{n=1}^n \\left(\\log\\left(\\frac{1}{s}\\sum_{s=1}^s p(y_i|\\theta^s)\\right) - \\frac{1}{s}\\sum_{s=1}^s \\log p(y_i|\\theta^s) \\right) \\end{align} the other measure uses the variance of individual terms: \\begin{align} p_{\\text{waic}2} = \\sum_{i=1}^n \\mathbb{v}_{\\text{post}}[\\log p(y_i|\\theta)] \\end{align} to compute it we compute the posterior sample variance (\\(\\mathbb{v}_{s=1}^s\\)) of the log predictive density for each data point \\(y_i\\) and we sum over all the data points: \\begin{align} \\text{computed } p_{\\text{waic}2} = \\sum_{i=1}^n \\mathbb{v}_{s=1}^s[\\log p(y_i|\\theta^s)] \\end{align} we can then use either \\(p_{\\text{waic}1}\\) or \\(p_{\\text{waic}2}\\) as a bias correction: \\begin{align} \\hat{\\text{elppd}}_{\\text{waic}} = \\text{lppd} - p_{\\text{waic}} \\end{align} as with \\(\\text{aic}\\) and \\(\\text{dic}\\), we define \\(\\text{waic}\\) so as to be on the deviance scale: \\begin{align} \\text{waic} = -2\\text{lppd} + 2p_{\\text{waic}2} \\end{align} for a normal linear model with a large sample size, known variance, and a uniform prior distribution on the coefficients, \\(p_{\\text{waic}1}\\) and \\(p_{\\text{waic}2}\\) are roughly equal to the number of parameters in the model. in general, this adjustment approximates the number of \"unconstrained\" parameters in the model. a parameter is counted as \\(1\\) if it's estimated without constraints or prior information, \\(0\\) if it's fully constrained, or if all the information comes from the prior distribution, or a value in between if both the data and prior distributions provide information. waic stands out because it averages over the whole posterior distribution rather than relying on a single point estimate, which is what aic and dic do. this makes waic more relevant when it comes to predicting new data in a bayesian framework. however, using waic requires dividing the data into \\(n\\) parts, which can be challenging in certain data setups like time series or spatial data. aic and dic don't require this explicit partition, but they assume that residuals are independent given a point estimate \\(\\hat{\\theta}\\), which may not fully capture posterior uncertainty. effective number of parameters as a random variable the number of parameters estimated in a model, as measured by \\(p_{\\text{dic}}\\) and \\(p_{\\text{waic}}\\), can vary depending on the observed data. let's take a simple example: imagine a model where the data \\(y_1, \\cdots, y_n\\) follow a normal distribution with a mean parameter \\(\\theta\\) and a known standard deviation of \\(1\\). the parameter \\(\\theta\\) is drawn from a uniform distribution between \\(0\\) and infinity, meaning it's positive but otherwise not informative. now, consider two scenarios: imagine you have a bunch of data points, but they are all very close to zero. in this case, the model has to consider that the parameter \\(\\theta\\) could be anywhere from very small positive values up to infinity. however, since the data are all close to zero, they don't provide much information about where \\(\\theta\\) might lie. the only constraint is that \\(\\theta\\) has to be positive. because the data don't give a strong indication of where \\(\\theta\\) might be, we say the effective number of parameters is roughly half. this is because half of the information about \\(\\theta\\) comes from the data, and the other half comes from the prior constraint that \\(\\theta\\) must be positive. now, imagine your data points are all large and positive. in this case, the constraint that \\(\\theta\\) must be positive doesn't really affect things much because the data already tell us that \\(\\theta\\) needs to be positive to explain those large positive values. since the data provide most of the information about where \\(\\theta\\) might lie, we say the effective number of parameters is approximately \\(1\\). this means that the data have a stronger influence on determining \\(\\theta\\) in this scenario. this example shows that even with the same model and true parameters, the effective number of parameters can change depending on the observed data. bayesian information criterion (bic) bic is a way to decide between different models by considering both how well the model fits the data and how complex the model is. the formula for bic is: \\begin{align} -2 \\log (p(y|\\hat{\\theta})) + k \\log(n) \\end{align} where \\(p(y|\\hat{\\theta})\\) is the likelihood of the data given the estimated parameters, \\(k\\) is the number of parameters in the model and \\(n\\) is the sample size. bic aims to approximate the marginal probability density of the data under the model, which can be used for comparing models and estimating relative posterior probabilities. bic tends to favor simpler models for large datasets because it penalizes complexity more, so a complicated model may perform well in predicting data but still have a high bic due to the penalty for complexity. unlike aic, which doesn't take the sample size into account, bic penalizes complex models more as the sample size increases. unlike aic, dic, and waic, bic doesn't focus on predicting future data but rather on estimating the probability of the observed data under the model. leave-one-out cross-validation in bayesian cross-validation, we split the data into two parts: a training set (\\(y_{\\text{ytrain}}\\)) and a holdout set (\\(y_{\\text{holdout}}\\)). we repeat this process multiple times, such that for each split: we train the model using the training set (\\(\\text{ytrain}\\)). this gives us a distribution of possible parameter values called \\(p_{\\text{train}}(\\theta) = p(\\theta|y_{\\text{train}})\\). then, we use this trained model to make predictions on the holdout set (\\(y_\\text{holdout}\\)). we evaluate the performance of our predictions using the log predictive density: \\begin{align} \\log p_{\\text{train}}(y_{\\text{holdout}}) = \\log \\mathbb{e}_{\\text{post}}[p_{\\text{train}}(y_{\\text{holdout}})] = \\log \\int p_{\\text{pred}}(y_{\\text{holdout}}|\\theta)p_{\\text{train}}(\\theta)d\\theta \\end{align} assuming the posterior distribution \\(p(\\theta|y_{\\text{train}})\\) is summarized by \\(s\\) simulation draws \\(\\theta^s\\), we calculate the log predictive density as: \\begin{align} \\log \\left(\\frac{1}{s} \\sum_{s=1}^s p(y_{\\text{holdout}}|\\theta^s)\\right) \\end{align} in loocv, we split the data into \\(n\\) partitions, where each partition represents a single data point. performing the analysis for each of the \\(n\\) data points yields n different inferences \\(p_{\\text{post}(-i)}\\), each summarized by \\(s\\) posterior simulations, \\(\\theta^{is}\\). the bayesian loo-cv estimate of out-of-sample predictive fit is: \\begin{align} \\text{lppd}_{\\text{loo-cv}} = \\sum_{i=1}^n \\log (p_{\\text{post}(-i)}(y_i)) \\end{align} computed as: \\begin{align} \\sum_{i=1}^n \\log \\left(\\frac{1}{s} \\sum_{s=1}^s p(y_i|\\theta^{is})\\right) \\end{align} where \\(\\theta^s\\) represents the \\(s\\) simulations under the posterior distribution \\(p(\\theta|y_{-1})\\). each prediction is conditioned on \\(n â 1\\) data points, which causes underestimation of the predictive fit. for large \\(n\\) the difference is negligible, but for small \\(n\\) (or when using \\(k\\)-fold cross-validation) we can use a first order bias correction b by estimating how much better predictions would be obtained if conditioning on \\(n\\) data points: \\begin{align} b = \\text{lppd} - \\overline{\\text{lppd}}_{-i} \\end{align} where: \\begin{align} \\overline{\\text{lppd}}_{-i} = \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n \\log p_{\\text{post}(-i)}(y_j) \\end{align} computed as: \\begin{align} \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n \\log \\left(\\frac{1}{s} \\sum_{s=1}^s p(y_j|\\theta^{is})\\right) \\end{align} the bias-corrected bayesian loo-cv is then: \\begin{align} \\text{lppd}_{\\text{cloo-cv}} = \\text{lppd}_{\\text{loo-cv}} + b \\end{align} the bias correction \\(b\\) is rarely used as it is usually small, but we include it for completeness. we compute an estimate of the effective number of parameters as: \\begin{align} p_{\\text{loo-cv}} = \\text{lppd} - \\text{lppd}_{\\text{loo-cv}} \\end{align} or, using bias-corrected loo-cv: \\begin{align} p_{\\text{cloo-cv}} = \\text{lppd} - \\text{lppd}_{\\text{cloo-cv}} \\end{align} \\begin{align} = \\overline{\\text{lppd}}_{-i} - \\text{lppd}_{\\text{loo-cv}} \\end{align} cv, like waic, requires the data to be split into distinct and ideally independent pieces. this can be challenging for structured models where the data isn't easily divided. additionally, cv can be computationally expensive, especially if the model needs to be re-fit for each fold. however, there are some shortcuts available, such as leave-one-out cross-validation (loo-cv), which can efficiently approximate predictions using the full posterior distribution. under certain conditions, different information criteria (like aic, dic, and waic) have been shown to be equivalent to loo-cv as the size of the dataset becomes very large. aic is equivalent to loo-cv when using maximum likelihood estimates. dic is a variation of regularized information criteria that approximates loo-cv using plug-in predictive densities. waic has been shown to be equivalent to bayesian loo-cv. loo-cv predicts the outcome for one data point using all other data points except that one. waic predicts the outcome for a data point using all observed data points. this difference becomes noticeable when dealing with small datasets or complex models, like hierarchical models. in regression or hierarchical models, loo-cv focuses on predicting specific data points, while waic predicts outcomes based on all observed data. this distinction can be important in models where predictions at one point are only weakly influenced by other data points. summary all the different measures discussed above are based on adjusting the log predictive density of the observed data by subtracting an approximate bias correction. the measures differ both in their baseline measures of fit and in their adjustments. aic starts with the log predictive density of the data conditional on the maximum likelihood estimate \\(\\hat{\\theta}\\), dic conditions on the posterior mean \\(\\mathbb{e}[\\theta|y]\\), and waic starts with the log predictive density, averaging over \\(p_{\\text{post}}(\\theta) = p(\\theta|y)\\). of these three approaches, only waic is fully bayesian and so it is our preference when using a bias correction formula. cross-validation can be applied to any measure of fit; we use the log pointwise posterior predictive density as with waic. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T3/03_model_comparison_based_on_predictive_performance.html",
    "title": "Model Comparison Based on Predictive Performance",
    "body": " index search search back model comparison based on predictive performance contents example aic dic waic cross validation comparing the three models evaluating predictive error comparisons bias induced by model selection challenges there are two common scenarios where we compare models. first, when we expand a model, we naturally want to compare the simpler version to the more complex one. we want to see if adding complexity improves the model's performance. conversely, if we simplify a model, we want to understand what information we might be losing. in essence, we're trying to figure out how much complexity is necessary to accurately represent the data. when comparing nested models, the bigger model usually fits the data better but can be more complex and harder to understand. so, we need to ask two main questions: does the better fit of the larger model justify its added complexity? and are the extra parameters in the larger model reasonable based on our prior knowledge? in the second scenario, we're comparing models that aren't nested. for example, we might compare two regression models that use different predictors to explain the same data. in these cases, we're not necessarily trying to pick one model over the other. instead, we might want to build a larger model that includes both sets of predictors, along with any interactions between them. this way, we get a more comprehensive picture. however, it's still useful to compare the performance of each model on its own to see how well they do individually. example on the eight schools example we defined three separate models: no pooling: separate estimates for each of the eight schools, reflecting that the experiments were performed independently. this model has eight parameters: an estimate for each school. complete pooling: a combined estimate averaging the data from all schools into a single number, reflecting that the eight schools come from the same population. this model has only one, shared, parameter. hierarchical model: a bayesian meta-analysis, partially pooling the eight estimates toward a common mean. this model has eight parameters but they are constrained through their hierarchical distribution and are not estimated independently; thus the effective number of parameters should be some number less than 8. in the following table we show the performance metrics for each of the models using predivtive log densities and information criteria. aic the log predictive density is higherâthat is, a better fitâfor the no pooling model. this makes sense: with no pooling, the maximum likelihood estimate is right at the data, whereas with complete pooling there is only one number to fit all \\(8\\) schools. however, the ranking of the models changes after adjusting for the fitted parameters (\\(8\\) for no pooling, \\(1\\) for complete pooling), and the expected log predictive density is estimated to be the best (that is, aic is lowest) for complete pooling. the last column of the table is blank for aic, as this procedure is defined based on maximum likelihood estimation which is meaningless for the hierarchical model. dic for both the no-pooling and complete-pooling models with their flat priors, dic provides results similar to aic. however, for the hierarchical model, dic falls in between the two extremes: it fits the data better than complete pooling but not as well as no pooling, and it suggests an effective number of parameters closer to \\(1\\) than to \\(8\\). this indicates that the estimated school effects are mostly pooled back to their common mean. when considering the correction for fitting, complete pooling emerges as the winner, which aligns with the idea that the data support very little variation between groups. waic this bayesian measure, similar to dic, indicates slightly worse fit to observed data for each model. this is because the posterior predictive density has a wider distribution, resulting in lower density values at the mode compared to the predictive density conditional on the point estimate. however, the correction for the effective number of parameters is lower with waic compared to dic. for models with no pooling and hierarchical models, the effective number of parameters (\\(p_{\\text{waic}}\\)) is about half of what's estimated by dic, suggesting that waic behaves as expected when there's only one data point per parameter. conversely, for complete pooling, \\(p_{\\text{waic}}\\) is only slightly less than \\(1\\), which aligns with expectations given the sample size of \\(8\\). overall, \\(p_{\\text{waic}}\\) is much less than pdic for all three models, mainly because the waic already considers much of the uncertainty stemming from parameter estimation. cross validation for this example, it's impossible to cross-validate the no-pooling model because it would mean predicting the performance of one school using data from the other seven, which isn't feasible. this highlights a key difference from information criteria, which assume predictions for the same schools and can work even in the absence of pooling. however, for the complete pooling and hierarchical models, we can directly perform leave-one-out cross-validation. in this setup, cross-validation predicts based only on information from other schools, while waic considers both the local observation and information from other schools. although both methods predict unknown future data, they differ in the amount of information used. as the hierarchical prior becomes less informative (or more vague), the predictive performance estimates diverge further, with the difference approaching infinity when the hierarchical prior becomes uninformative, effectively yielding the no-pooling model. comparing the three models in this dataset, the complete pooling model performs best in predicting new data. surprisingly, setting the hierarchical variance \\(\\tau\\) to zero results in a better fit to the data compared to both no pooling and complete pooling models. however, despite this result, we still prefer the hierarchical model because we don't believe \\(\\tau\\) is truly zero. for instance, the estimated effects in schools a and c show some differences, although they are not statistically significant. the data suggest that there might be no variation in effects between schools, but we are not entirely confident in this conclusion. therefore, while the model with \\(\\tau = 0\\) performs well, we might consider using a more informative prior distribution for \\(\\tau\\) to better capture the uncertainty and avoid implausible scenarios. in general, predictive accuracy measures are useful in parallel with posterior predictive checks to see if there are important patterns in the data that are not captured by each model. evaluating predictive error comparisons when comparing models for their predictive accuracy, we face two main challenges: statistical significance and practical significance. statistical significance arises from the uncertainty in estimating how well a model predicts new data. this uncertainty is due to variation in individual prediction errors, which can affect the averages we calculate from any finite dataset. a practical estimate of related sampling uncertainty can be obtained by analyzing the variation in the expected log predictive densities \\(\\hat{\\text{elppd}}\\) using parametric or nonparametric approaches. in some cases, we can use scoring functions that are familiar to experts in a particular field to understand the significance of differences in predictive accuracy. however, in situations where there are no established measures like auc, it can be challenging to interpret the significance of differences in log predictive probability between two models. one way to gauge the importance of such differences is by comparing them to simpler models. consider a scenario where we have two models for a survey of voters in an election: one model predicts a \\(50\\)/\\(50\\) chance for each voter to support either party, while the other model correctly assigns probabilities of \\(0.4\\) and \\(0.6\\) to the voters. in this case, the improvement in log predictive probability from using the better model can be calculated. for instance, if we have \\(1000\\) voters, the improvement would be \\(20\\), but for only \\(10\\) voters, the improvement would be just \\(2\\). this aligns with our intuition: a clear improvement in prediction is more noticeable in a larger dataset than in a smaller one where noise might overshadow the improvement. bias induced by model selection cross-validation and information criteria are methods that adjust for using the data twiceâonce for building the model and again for evaluating its performance. they aim to provide unbiased estimates of how well a model predicts new data. however, when these methods are used to select a model from multiple options, the estimate of predictive performance for the chosen model can be biased because of the selection process. when there are only a few models to compare, any bias introduced by the selection process is usually small. however, if there are many models to choose from, especially as the number of observations or predictors increases, the selection process can lead to significant overfitting. while it's possible to estimate and correct for this bias using additional cross-validation, it doesn't guarantee that the selected model will have the best predictive performance. therefore, cross-validation and information criteria are better suited for understanding models rather than selecting the best one among many options. challenges the methods we have for measuring how well predictive models fit still have their flaws. aic, dic, and waic don't always work perfectly: aic struggles with strong prior information, dic gives odd results when the average of the posterior distribution isn't reliable, and waic can be tricky to use with structured data like spatial or network data. cross-validation seems like a good alternative, but it can be slow to compute and doesn't always work well with dependent data. bayesian statisticians often don't rely solely on predictive error comparisons in their work because of various limitations. however, there are situations where comparing very different models is necessary, and in those cases, predictive comparisons can be valuable. additionally, measures of effective numbers of parameters are useful for understanding statistical methods. currently, we prefer cross-validation because it's similar to waic in large samples. however, in finite cases with weak priors or strong outliers, pareto-smoothed importance sampling loo-cv is both computationally efficient and robust. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T3/04_model_comparison_using_bayes_factors.html",
    "title": "Model Comparison Using Bayes Factors",
    "body": " index search search back model comparison using bayes factors contents example a discrete example in which bayes factors are helpful example a continuous example where bayes factors are a distraction in this chapter, we've been talking about how to evaluate and compare models by looking at how well they predict outcomes. another way to compare models is using bayesian analysis, where we use something called the bayes factor. so, if we have two models, let's call them \\(h_1\\) and \\(h_2\\), the bayes factor is just the ratio of how probable they are after we've looked at the data: \\begin{align} \\frac{p(h_2|y)}{p(h_1|y)} = \\frac{p(h_2)}{p(h_1)} \\times \\text{ bayes factor }(h_2;h_1) \\end{align} where \\begin{align} \\text{ bayes factor }(h_1; h_2) = \\frac{p(y|h_2)}{p(y|h_2)} = \\frac{\\int p(\\theta_2|h_2) p(y|\\theta_2, h_2) d\\theta_2}{\\int p(\\theta_1|h_1) p(y|\\theta_1, h_1) d\\theta_1} \\end{align} the bayes factor is only defined when the marginal density of \\(y\\) under each model is proper. this bayesian approach might sound good, but we usually don't recommend it. that's because the likelihood of the data given the model can be strongly influenced by parts of the model that are chosen somewhat arbitrarily and can't be tested with data. bayes factors can work well when the underlying model is truly discrete and for which it makes sense to consider one or the other model as being a good description of the data. we illustrate with an example from genetics. example: a discrete example in which bayes factors are helpful in the genetics example we talked about earlier, we can use bayes factors to help us make sense of things. imagine we have two possible scenarios: \\(h_1\\), where the woman is affected, and \\(h_2\\), where she's not affected. we can represent these scenarios using some numbers. for example, let's say \\(\\theta = 1\\) means she's affected, and \\(\\theta = 0\\) means she's not. now, let's say before we look at any data, we're equally likely to believe either scenario. so, the odds of h2 compared to h1 are 1 to 1, that is \\(\\frac{p(h_2)}{p(h_1)} = 1\\). then, when we look at the data and find out the woman has two unaffected sons, the data is \\(4\\) times more likely under \\(h_2\\) than under \\(h_1\\). that is \\(\\frac{p(y|h_2)}{p(y|h_1)} = \\frac{1.0}{0.25}\\). the posterior odds are thus \\(\\frac{p(h_2|y)}{p(h_1|y)} = 4\\) this example is helpful for bayes factors because the scenarios we're comparing make sense scientifically, and there are no other possible scenarios in between. also, the way the data fits with each scenario makes sense and gives us clear results. bayes factors don't work as well for models that are continuous. for instance, if we're looking at something like the effectiveness of a treatment, which can vary along a scale, it doesn't make sense to assign a probability to it being exactly zero. similarly, if we're comparing different models in regression, like deciding which variables to include, it's better to have all the possible variables in our consideration. we can then use a prior distribution to decide how much to trust each variable, even if we think some might not have much impact. to show why bayes factors struggle with continuous models, let's consider the example of the 8 schools problem, comparing the no-pooling and complete-pooling models. example. a continuous example where bayes factors are a distraction suppose we had analyzed the data from the 8 schools using bayes factors for the discrete collection of previously proposed standard models, no pooling (\\(h_1\\)) and complete pooling (\\(h_2\\)): \\begin{align} h_1: p(y|\\theta_1, \\cdots, \\theta_j) = \\prod_{j=1}^j text{n}(y_j|\\theta_j, \\sigma_j^2), p(\\theta_1, \\cdots, \\theta_j) \\propto 1 \\end{align} \\begin{align} h_2: p(y|\\theta_1, \\cdots, \\theta_j) = \\prod_{j=1}^j text{n}(y_j|\\theta_j, \\sigma_j^2), \\theta_1 = \\cdots = \\theta_j = \\theta \\propto 1 \\end{align} if we try to use bayes factors to pick or combine these models, we run into a problem. the bayes factor, which is the ratio of how likely the data is under one model compared to another, isn't defined here. that's because the prior distributions we're using are improper, which means they don't behave properly in the calculations. specifically, when we try to divide one function by another, we end up with \\(\\frac{0}{0}\\), which doesn't give us a clear answer. so, if we want to stick with the idea of assigning probabilities to these two specific models, we have two options: either use proper prior distributions or carefully construct improper ones in a way that makes sense. however, no matter which route we take, the results won't be very satisfying. more explicitly, suppose we replace the flat prior distributions in \\(h_1\\) and \\(h_2\\) by independent normal prior distributions, \\(\\text{n}(0, a^2)\\), for some large \\(a\\). the resulting posterior distribution for the effect in school \\(j\\) is: \\begin{align} p(\\theta_j|y) = (1 - \\lambda)p(\\theta_j|y, h_1) + \\lambda p(\\theta_j|y, h_2) \\end{align} the bayes factor, which compares how likely the data is under different models, is very sensitive to the prior variance, which is represented by \\(a^2\\). as we increase \\(a\\) (while keeping the data and prior odds fixed), the results tend to favor one model over the other more strongly. this means that bayes factors can't be reliably used with non-informative prior densities, even if we carefully define them in certain ways. another problem with bayes factors in this example is that they behave differently as we change the number of schools in the model. the results can vary significantly depending on how many schools are included, which doesn't make much sense from a scientific perspective. so, if we were to use bayes factors here, we'd likely run into issues during the model-checking stage, where we compare the model's predictions to what we know from real-world knowledge. instead, it might be better to use a smoother, continuous family of models that bridges the gap between the extreme models. this continuous model doesn't assign discrete probabilities to extreme values that don't make scientific sense. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T3/01_measure_precditive_accuracy.html",
    "title": "Measures of Predictive Accuracy",
    "body": " index search search back measures of predictive accuracy contents point prediction probabilistic prediction predictive accuracy for a single data point averaging over the distribution of future data evaluating predictive accuracy for a fitted model we begin by considering different ways of defining the accuracy or error of a modelâs predictions then discuss methods for estimating predictive accuracy or error from data. preferably, the measure of predictive accuracy is specifically tailored for the application at hand, and it measures as correctly as possible the benefit (or cost) of predicting future data with the model. point prediction in point prediction (predictive point estimation or point forecasting) a single value is reported as a prediction of the unknown future observation. measures of predictive accuracy for point prediction are called scoring functions. for example, the mean squared error: \\begin{align} \\frac{1}{n} \\sum_{i=1}^n (y_i - \\mathbb{e}[y_i|\\theta])^2 \\end{align} or its weighted version: \\begin{align} \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i - \\mathbb{e}[y_i|\\theta])^2}{\\mathbb{v}[y_i|\\theta]} \\end{align} these are easy to compute but they are less appropiated for models that are far from the normal distribution. probabilistic prediction in probabilistic prediction (probabilistic forecasting) the aim is to report inferences about \\(\\hat{y}\\) in such a way that the full uncertainty over \\(\\hat{y}\\) is taken into account. these are called scoring rules. examples include the quadratic, logarithmic, and zero-one scores good scoring rules for prediction are: proper: the scoring rule encourages the decision maker to be honest when reporting their beliefs. local: the scoring rule takes into account the fact that some predictions may be worse than others, and it adjusts accordingly. for example the log predictive density or log-likelihood, \\(p(y|\\theta)\\), which is proportional to the mean squared error if the model is normal with constant variance. why not use the log posterior? the answer is that we are interested here in summarizing the fit of model to data, and for this purpose the prior is relevant in estimating the parameters but not inassessing a model's accuracy. we are not saying that the prior cannot be used in assessing a model's fit to data; rather we say that the prior density is not relevant in computing predictive accuracy. predictive accuracy for a single data point the best way to measure how well a model fits is by seeing how accurately it predicts outcomes in new data that it hasn't seen before (out-of-sample predictive performance), but that comes from the same process as the original data. we label \\(f\\) as the true model, \\(y\\) as the observed data and \\(\\tilde{y}\\) as future data. the out-of-sample predictive fit for a new data point \\(\\tilde{y}_i\\) using logarithmic score is: \\begin{align} \\log p_{\\text{post}}(\\tilde{y}_i) = \\log \\mathbb{e}_{\\text{post}}[p(\\tilde{y}_i|\\theta)] = \\end{align} by the definition of the expected value for a random variable: \\begin{align} = \\log \\int p(\\tilde{y}_i|\\theta) p_{\\text{post}}(\\theta)d\\theta \\end{align} where \\(p_{\\text{post}}(\\tilde{y}_i)\\) is the predictive density for \\(\\tilde{y}_i\\) induced by the posterior distribution \\(p_{\\text{post}}(\\theta)\\). note that we use \\(p_{\\text{post}}\\) and \\(\\mathbb{e}_{\\text{post}}\\) to denote any probability or expectation that averages over the posterior distribution of \\(\\theta\\). averaging over the distribution of future data the future data \\(\\tilde{y}_i\\) are themselves unknown and thus we define the expected out-of-sample log predictive density. by the definition of expected value of the function \\(\\log (x)\\) over \\(\\tilde{y}\\) with respect to a function \\(f\\) that describes the distribution of the data, we compute the expected log predictive density or elpd for a new data point as follows: \\begin{align} \\mathbb{e}_f[\\log p_{\\text{post}}(\\tilde{y}_i)] = \\int \\log (p_{\\text{post}}(\\tilde{y}_i)) f(\\tilde{y}_i) d\\tilde{y} \\end{align} in general we do not know the data distribution \\(f\\). a natural way to estimate the expected out-of-sample log predictive density would be to plug in an estimate for \\(f\\), but this will tend to imply too good a fit. for now we consider the estimation of predictive accuracy in a bayesian context. one can define a measure of predictive accuracy for the \\(n\\) data points taken one at a time: \\begin{align} \\sum_{i=1}^n \\mathbb{e}_f[\\log(p_{\\text{post}}(\\tilde{y}_i))] \\end{align} this gives us the expected log pointwise predictive density for a new dataset. using a single-point measure instead of dealing with the entire set of predictions (the joint distribution \\(p_{\\text{post}}(\\tilde{y})\\)) allows us to connect it to cross-validation, which helps us approximate how well our model performs on new data based on the data we already have. it is sometimes useful to consider predictive accuracy given a point estimate \\(\\theta(\\tilde{y})\\) (sampled data point given the parameter \\(\\theta\\)?). this gives us the expected log predictive density given \\(\\hat{\\theta}\\): \\begin{align} \\mathbb{e}_f[\\log(p(\\tilde{y}|\\theta))] \\end{align} evaluating predictive accuracy for a fitted model in practice the parameter \\(\\theta\\) is not known, so we cannot know the log predictive density \\(\\log p(y|\\theta)\\), which tells us how well our model predicts new data based on \\(\\theta\\). so, instead of using \\(\\theta\\) directly, we use something called the posterior distribution, denoted as \\(p_{\\text{post}}(\\theta) = p(\\theta|y)\\). this distribution gives us a range of possible values for \\(\\theta\\) based on the data we have. from this distribution, we can summarize how accurately our model predicts new data. so we define the log pointwise predictive density or lppd as: \\begin{align} \\log \\prod_{i=1}^n p_{\\text{post}}(y_i) = \\sum_{i=1}^n \\log \\int p(y_i|\\theta)p_{\\text{post}}(\\theta)d\\theta \\end{align} to calculate this predictive density, we can use samples drawn from the posterior distribution \\(p_{\\text{post}}(\\theta)\\) using simulation. these samples are labeled as \\(\\theta_s\\), where \\(s\\) ranges from \\(1\\) to \\(s\\). so we define the computed log pointwise predictive density or computed lppd as: \\begin{align} \\sum_{i=1}^n \\log \\left(\\frac{1}{s}\\sum_{s=1}^s p(y_i|\\theta^s)\\right) \\end{align} we basically compute the sample mean of the likelihood \\(p(y_i|\\theta)\\) for over all the \\(\\{\\theta^s\\}_{s=1}^s\\) we typically assume that the number of simulation draws \\(s\\) is large enough to fully capture the posterior distribution. the lppd of observed data y is an overestimate of the elppd for future data. hence the plan is to start with lppd and then apply some sort of bias correction to get a reasonable estimate of elppd. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/01_intro.html",
    "title": "T1. Introduccion a la Inferencia Bayesiana",
    "body": " index search search back t1. introduccion a la inferencia bayesiana contents the three steps of bayesian data analysis notation exchangeability explanatory variables hierarchical modeling bayesian inference prediction likelikhood likelihood and odds ratios probability theory means and variances of conditional distributions transformation of variables the three steps of bayesian data analysis the process of bayesian data analysis can be idealized by dividing it into the following three steps: setting up a full probability model: a joint probability distribution for all observable and unobservable quantities in a problem. conditioning on observed data: calculating and interpreting the appropriate posterior distribution. evaluating the fit of the model and the implications of the resulting posterior distribution we distinguish between two kinds of estimands quantities that are not directly observable: for example, parameters that govern the hypothetical process leading to the observed data, for which statistical inferences are made. potentially observable quantities (such as future observations of a process, or the outcome under the treatment not received) notation \\(\\theta\\): unobservable vector quantities or population parameters. \\(y\\): observed data \\(\\tilde{y}\\): unknown, but potentially observable, quantities. exchangeability we assume assume that the \\(n\\) values \\(y_i\\) may be regarded as exchangeable. we express uncertainty as a joint probability density \\(p(y_1, \\cdots, y_n)\\) that is invariant to permutations of the indexes. we commonly model data from an exchangeable distribution as independently and identically distributed (iid). explanatory variables observations on each unit that we do not model as random. hierarchical modeling hierarchical models (also called multilevel models), which are used when information is available on several different levels of observational units. bayesian inference we define a prior distribution \\(p(\\theta)\\), and a sampling distribution (or data distribution) is given by \\(p(y|\\theta)\\), such that the joint probability distribution for \\(\\theta\\) and \\(y\\) is obtained as follows: \\begin{align} p(\\theta,y) = p(\\theta|y)p(y) \\end{align} by baye's rule: \\begin{align} p(\\theta|y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(y|\\theta)p(\\theta)}{p(y)} \\end{align} where \\(p(y) = \\sum_y p(y, \\theta) = \\sum_y p(y|\\theta) p(y) = \\int_y p(y|\\theta) p(y) dy\\) an equivalent form is omitting the factor \\(p(y)\\), yielding the unnormalized posterior density: \\begin{align} p(\\theta|y) \\propto p(y|\\theta)p(\\theta) \\end{align} prediction the marginal distribution of \\(y\\) or prior predictive distribution is given by: \\begin{align} p(y) = \\int p(y, \\theta) dy = \\int p(y|\\theta) p(\\theta) d\\theta \\end{align} the distribution of \\(\\tilde{y}\\) is called the posterior predictive distribution, posterior because it is conditional on the observed \\(y\\) and predictive because it is a prediction for an observable \\(\\tilde{y}\\). it is defined as the marginalization of \\(\\tilde{y}\\) over \\(y\\). \\begin{align} p(\\tilde{y}|y) = \\int p(\\tilde{y}, \\theta|y)d\\theta \\end{align} we note that the statistical process is also conditioned on the unobservable data \\(\\theta\\). por la propiedad \\(p(x, y|z) = p(x|y, z)p(z)\\): \\begin{align} = \\int p(\\tilde{y}|y, \\theta)p(\\theta|y) d\\theta \\end{align} asumimos independencia condicional entre \\(y\\) y \\(\\tilde{y}\\): \\begin{align} = \\int p(\\tilde{y}|\\theta)p(\\theta|y) d\\theta \\end{align} likelikhood when regarded as a function of \\(\\theta\\), for fixed y \\(p(y|\\theta)\\) is the likelihood function. likelihood and odds ratios odds a posteriori: \\begin{align} \\frac{p(\\theta_1|y)}{p(\\theta_2|y)} = \\frac{\\frac{p(y|\\theta_1)p(\\theta_1)}{p(y)}}{\\frac{p(y|\\theta_2)p(\\theta_2)}{p(y)}} = \\frac{p(y|\\theta_1)p(\\theta_1)}{p(y|\\theta_2)p(\\theta_2)} \\end{align} odds a priori: \\begin{align} \\frac{p(\\theta_1)}{p(\\theta_2)} \\end{align} likelihood ratio: \\begin{align} \\frac{p(\\theta_1|y)}{p(\\theta_2|y)} \\end{align} probability theory the expected value of a continuous random variable \\(u\\) is given by: \\begin{align} \\mathbb{e}[u] = \\int u p(u)du \\end{align} the variance for a continuous random variable \\(u\\) is given by: \\begin{align} \\mathbb{e}[u] = \\int (u - \\mathbb{e}[u])^2 p(u)du \\end{align} the expected value of a discrete random variable \\(u\\) is given by: \\begin{align} \\mathbb{e}[u] = \\sum u p(u) \\end{align} the variance for a discrete random variable \\(u\\) is given by: \\begin{align} \\mathbb{e}[u] = \\sum (u - \\mathbb{e}[u])^2 p(u) \\end{align} means and variances of conditional distributions given two continuous random variables \\(u\\) and \\(y\\), the mean of \\(u\\) can be obtained by averaging the conditional mean over the marginal distribution of \\(v\\): \\begin{align} \\mathbb{e}(u) = \\int u p(u) du = \\int u \\left(\\int p(u, v)\\right) dv du \\end{align} \\begin{align} = \\int \\left(\\int u p(u|v) du \\right) p(v) dv = \\int \\mathbb{e}_u[u|v] p(v) dv = \\mathbb{e}_v[\\mathbb{e}_u[u|v]] \\end{align} the corresponding result for the variance: \\begin{align} \\mathbb{v}[u] = \\mathbb{e}[\\mathbb{v}[u|v]] - \\mathbb{v}[\\mathbb{e}[u|v]] \\end{align} transformation of variables suppose \\(p_u(u)\\) is the density of the vector \\(u\\), and we transform to \\(v = f(u)\\), where \\(v\\) has the same number of components as \\(u\\). if \\(p_u\\) is a discrete distribution, and \\(f\\) is a one-to-one function, then the density of \\(v\\) is given by: \\begin{align} p_v(v) = p_u(f^{-1}(v)) \\end{align} if \\(p_u\\) is a continuous distribution, and \\(v = f(u)\\) is a one-to-one transformation, then the joint density of the transformed vector is: \\begin{align} p_v(v) = |j| p_u(f^{-1}(v)) \\end{align} where \\(|j|\\) is the absolute value of the determinant of the jacobian of the transformation \\(u = f^{â1}(v)\\) as a function of \\(v\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T4/01_gibbs_sampler.html",
    "title": "Gibbs Sampler",
    "body": " index search search back gibbs sampler contents introduction gibbs sampler example bivariate normal distribution introduction many smart techniques have been developed to create and sample from different types of posterior distributions. one common method is called markov chain simulation, also known as markov chain monte carlo (mcmc). it works by first drawing values of a parameter (usually denoted as \\(\\theta\\)) from rough estimates of the distribution, and then adjusting those draws to better match the actual distribution we're interested in, called the posterior distribution, denoted as \\(p(\\theta|y)\\). the drawing process is done step by step, with each draw depending on the previous one, forming what's called a markov chain. figure 11.1 shows a simple example of markov chain simulation, using what's called a metropolis algorithm. imagine we have a parameter called \\(\\theta\\) that is a vector with two components, such that \\(\\theta \\sim \\text{n}(0, i)\\). now, let's look at figure 11.1a, which shows the early steps of this simulation. the picture represents all the possible values that \\(\\theta\\) can take, and each of the five squiggly lines shows the path taken by a random walk. these random walks start either near the center or at the edges of the distribution and move around based on a series of random steps. in figure 11.1b, we see the later stages of the same simulation. each of the random walks has now traced a path throughout the entire space of possible \\(\\theta\\) values. they've settled into a common pattern, which matches the target distribution we're interested in. finally, in figure 11.1c, we can use the information gathered from the second halves of these simulated random walks to make inferences about \\(\\theta\\). in our use of markov chain simulation, we create multiple separate sequences. each sequence starts from a particular point, like \\(\\theta_0\\), and then we move step by step, drawing a new value \\(\\theta_t\\) from a transition distribution \\(t_t(\\theta_t|\\theta_{tâ1})\\), which depends on the previous draw. markov chain simulation is used when it is not possible to sample \\(\\theta\\) directly from \\(p(\\theta|y)\\) instead we sample iteratively in such a way that at each step of the process we expect to draw from a distribution that becomes closer to \\(p(\\theta|y)\\). once the simulation algorithm has been implemented and the simulations drawn, it is absolutely necessary to check the convergence of the simulated sequences; for example, the simulations of figure 11.1a are far from convergence and are not close to the target distribution. gibbs sampler imagine you have a \\(d\\)-dimensional parameter vector \\(\\theta\\) that's been split into smaller parts, such that \\(\\theta = (\\theta_1, \\cdots, \\theta_d)\\). each time the gibbs sampler runs, it goes through each of the dimensions, one at a time, while keeping the rest fixed. so, if there are \\(d\\) dimensions, there are \\(d\\) steps in each iteration \\(t\\). at each iteration, we pick an order for the \\(d\\) parts of \\(\\theta\\). then, we go through each part one by one and we sample a new value for each part based on the current values of all the other parts. this continues for each part until we've updated all of them once, and then we start the process over again for the next iteration. \\begin{align} p(\\theta_j|\\theta_{-j}^{t-1}, y) \\end{align} where \\(\\theta_{-j}^{t-1}\\) represents all the components of \\(\\theta\\) except for \\(\\theta_j\\) at their current values: \\begin{align} \\theta_{-j}^{t - 1} = (\\theta_1^t, \\cdots, \\theta^t_{j - 1}, \\theta_{j + 1}^{t - 1}, \\theta_{d}^{t - 1}) \\end{align} thus, each subvector \\(\\theta_j\\) is updated conditional on the latest values of the other components of \\(\\theta\\), which are the iteration \\(t\\) values for the components already updated and the iteration \\(t â 1\\) values for the others. here, we illustrate the workings of the gibbs sampler with a simple example. example: bivariate normal distribution consider a single observation \\((y_1, y_2)\\) from a bivariate normally distributed population with unknown mean \\(\\theta = (\\theta_1, \\theta_2)\\) and known covariance matrix \\(\\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}\\). with a uniform prior distribution on \\(\\theta\\), the posterior distribution is: \\begin{align} \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\end{bmatrix} | y \\sim \\text{n}(\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\end{bmatrix}, \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\\\ \\end{bmatrix}) \\end{align} we need the conditional posterior distributions, which, from the properties of the multivariate normal distribution, are: \\begin{align} \\theta_1 |\\theta_2, y \\sim \\text{n}(y_1 + \\rho(\\theta_2 - y_2), 1 - \\rho^2) \\end{align} \\begin{align} \\theta_2 |\\theta_1, y \\sim \\text{n}(y_2 + \\rho(\\theta_1 - y_1), 1 - \\rho^2) \\end{align} the gibbs sampler proceeds by alternately sampling from these two normal distributions. figure 11.2 illustrates for the case \\(\\rho = 0.8\\), data \\((y_1, y_2) = (0, 0)\\), and four independent sequences started at \\((\\pm 2.5, \\pm 2.5)\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T4/05_effective_number_of_simulations.html",
    "title": "Effective Number of Simulation Draws",
    "body": " index search search back effective number of simulation draws contents bounded or long-tailed distributions stopping the simulations one way to define effective sample size for correlated simulation draws is to consider the statistical efficiency of the average of the simulations \\(\\overline{\\psi}_{..}\\), as an estimate of the posterior mean, \\(\\mathbb{e}[\\psi|y]\\). this might be inappropriate, for example, if there is particular interest in accurate representation of low-probability events in the tails of the distribution. continuing with this definition, it is usual to compute effective sample size using the following asymptotic formula for the variance of the average of a correlated sequence: \\begin{align} \\lim_{n \\rightarrow \\infty} mn \\mathbb{v}[\\overline{\\psi}_{..}] = \\left(1 + 2 \\sum_{t = 1}^{\\infty} \\rho_t\\right) \\mathbb{v}[\\psi|y] \\end{align} where \\(\\rho_t\\) is the autocorrelation of the sequence \\(\\psi\\) at \\(t\\). in the presence of correlation we then define the effective sample size as: \\begin{align} n_{eff} = \\frac{mn}{1 + 2 \\sum_{t = 1}^{\\infty} \\rho_t} \\end{align} the asymptotic nature of the previous equations might seem disturbing given that in reality we will only have a finite simulation, but this should not be a problem given that we already want to run the simulations long enough for approximate convergence to the (asymptotic) target distribution. we then estimate the correlations as: \\begin{align} \\hat{\\rho}_t = 1 - \\frac{v_t}{2 \\overline{\\mathbb{v}}^+} \\end{align} where: \\begin{align} v_t = \\frac{1}{m(n - t)} \\sum_{j=1}^m \\sum_{i = t+1}^n (\\psi_{i,j} - \\psi_{i - t, j})^2 \\end{align} unfortunately we cannot simply sum all of these to estimate \\(n_{eff}\\) given that for large values of \\(t\\) the sample correlation \\(\\psi_t\\) is too noisy. instead we compute a partial sum, starting from \\(t = 0\\) and continuing until the sum of autocorrelation estimates for two successive iterationsis negative: \\begin{align} \\hat{n}_{eff} = \\frac{mn}{1 + 2 \\sum_{t = 1}^t \\hat{\\rho}_t} \\end{align} all these calculations should be performed using only the saved iterations, after discarding the warm-up period. bounded or long-tailed distributions the above convergence diagnostics are based on means and variances, and they will not work so well for parameters or scalar summaries for which the posterior distribution, \\(p(\\phi|y)\\), is far from gaussian. for summaries \\(\\phi\\) whose distributions are constrained or otherwise far from normal, we can preprocess simulations using transformations before computing the potential scale reduction factor \\(\\hat{r}\\) and the effective sample size \\(\\hat{n}_{eff}\\). stopping the simulations we monitor convergence for the entire multivariate distribution, \\(p(\\theta|y)\\), by computing the potential scale reduction factor \\(\\hat{r}\\) and the effective sample size \\(\\hat{n}_{eff}\\) for each scalar summary of interest. we recommend computing the potential scale reduction for all scalar estimands of interest; if \\(\\hat{r}\\) is not near \\(1\\) for all of them, continue the simulation runs. we can use effective sample size \\(\\hat{n}_{eff}\\) to give us a sense of the precision obtained from our simulations. as a default rule, we suggest running the simulation until \\(\\hat{n}_{eff}\\) is at least \\(5m\\), that is, until there are the equivalent of at least \\(10\\) independent draws per sequence. for some purposes, more precision will be desired, and then a higher effective sample size threshold can be used. once \\(\\hat{r}\\) is near \\(1\\) and \\(\\hat{n}_{eff}\\) is more than \\(10\\) per chain for all scalar estimands of interest, just collect the \\(mn\\) simulations (with warm-up iterations already excluded) and treat them as a sample from the target distribution. even if an iterative simulation appears to converge and has passed all tests of convergence, it still may actually be far from convergence if important areas of the target distribution were not captured by the starting distribution and are not easily reachable by the simulation algorithm. when we declare approximate convergence, we are actually concluding that each individual sequence appears stationary and that the observed sequences have mixed well with each other. these checks are not hypothesis tests. there is no p-value and no statistical significance. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T4/03_using_gibbs_and_metropolis.html",
    "title": "Using Gibbs and Metropolis as Building Blocks",
    "body": " index search search back using gibbs and metropolis as building blocks contents interpretation of the gibbs sampler as a special case of the metropolis-hastings algorithm gibbs sampler with approximations the gibbs sampler is the simplest of the markov chain simulation algorithms, and it is our first choice for conditionally conjugate models. the metropolis algorithm can be used for models that are not conditionally conjugate. a general problem with conditional sampling algorithms is that they can be slow when parameters are highly correlated in the target distribution. interpretation of the gibbs sampler as a special case of the metropolis-hastings algorithm we first define iteration \\(t\\) to consist of a series of \\(d\\) steps, with step \\(j\\) of iteration \\(t\\) corresponding to an update of the subvector \\(\\theta_j\\) conditional on all the other elements of \\(\\theta\\). then the jumping distribution, \\(j_{j,t}(\\cdot|\\cdot)\\) is defined as follows: \\begin{align} j_{j, t}^{\\text{gibbs}}(\\theta^*|\\theta^{t-1}) = \\begin{cases} p(\\theta_j^*|\\theta_{-j}^{t-1}, y) & \\text{ if } \\theta_{-j}^* = \\theta_{-j}^{t-1} \\\\ 0 & \\text{ otherwise} \\end{cases} \\end{align} such that at step \\(j\\) of iteration \\(t\\) it only jumps along the \\(j\\)th subvector, and does so with the conditional posterior density of \\(\\theta_j\\) given \\(\\theta_{-j}^{t - 1}\\). the only possible jumps are to parameter vectors \\(\\theta^*\\) that match \\(\\theta^{t-1}\\) on all components other than the \\(j\\)th. under this jumping distribution, the ratio at the \\(j\\)th step of iteration t is: \\begin{align} r = \\frac{\\frac{p(\\theta^*|y)}{j_{j, t}^{\\text{gibbs}}(\\theta^*|\\theta^{t-1})}}{\\frac{p(\\theta^{t-1}|y)}{j_{j,t}^{\\text{gibbs}}(\\theta^{t-1}|\\theta^*)}} \\end{align} \\begin{align} = \\frac{\\frac{p(\\theta^*|y)}{p(\\theta_j^*|\\theta^{t-1}_{-j}, y)}}{\\frac{p(\\theta^{t-1}|y)}{p(\\theta^{t-1}_j|\\theta^{t-1}_{-j}, y)}} \\end{align} \\begin{align} = \\frac{p(\\theta^{t-1}_{-j}, y)}{p(\\theta^{t-1}_{-j}, y)} \\end{align} \\begin{align} = 1 \\end{align} and thus every jump is accepted. the second line above follows from the first because, under this jumping rule, \\(\\theta^*\\) differs from \\(\\theta^{tâ1}\\) only in the \\(j\\)th component. the third line follows from the second by applying the rules of conditional probability to \\(\\theta = (\\theta_j, \\theta_{âj})\\) and noting that \\(\\theta^*_{-j} = \\theta^{t-1}_{-j}\\). it is possible to define gibbs sampling without the restriction that each component be updated in each iteration, as long as each component is updated periodically. gibbs sampler with approximations for some problems, sampling from some, or all, of the conditional distributions \\(p(\\theta_j|\\theta_{âj}, y)\\) is impossible, but one can construct approximations, which we label \\(g(\\theta_j|\\theta_{âj})\\), from which sampling is possible. the jumping function at the jth metropolis step at iteration \\(t\\) is then: \\begin{align} j_{j, t}(\\theta^*|\\theta^{t-1}) = \\begin{cases} g(\\theta^*_j|\\theta_{-j}^{t-1}) & \\text{ if } \\theta^*_{-j} = \\theta^{t-1}_{-j} \\\\ 0 & \\text{ otherwise } \\end{cases} \\end{align} and the ratio \\(r\\) must be computed and the acceptance or rejection of \\(\\theta^*\\) decided. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T4/04_inference_and_assesing_convergence.html",
    "title": "Inference and Assessing Convergence",
    "body": " index search search back inference and assessing convergence contents difficulties of inference from iterative simulation discarding early iterations of the simulation runs dependence of the iterations in each sequence multiple sequences with overdispersed starting points monitoring scalar estimands challenges of monitoring convergence mixing and stationarity splitting each saved sequence into two parts assessing mixing using between- and within-sequence variances example bivariate unit normal density with bivariate normal jumping kernel continued the basic method of inference from iterative simulation is the same as for bayesian simulation in general: use the collection of all the simulated draws from \\(p(\\theta|y)\\) to summarize the posterior density and to compute quantiles, moments, and other summaries of interest as needed. difficulties of inference from iterative simulation iterative simulation adds two challenges to simulation inference: if the iterations have not proceeded long enough the simulations may be unrepresentative of the target distribution (figure 11.1a) the iterative simulation draws present within-sequence correlation. serial correlation in the simulations is not necessarily a problem because, at convergence, the draws are identically distributed as \\(p(\\theta|y)\\). but such correlation can cause inefficiencies in simulations. we handle these problems as follows: we design the simulation runs to allow effective monitoring of convergence by simulating multiple sequences with starting points dispersed throughout parameter space. we monitor the convergence of all quantities of interest by comparing variation between and within simulated sequences until \"within\" variation roughly equals \"between\" variation. only when the distribution of each simulated sequence is close to the distribution of all the sequences mixed together can they all be approximating the target distribution. if the simulation efficiency is low, the algorithm may be altered. discarding early iterations of the simulation runs to diminish the influence of the starting values, we discard the first half of each sequence and focus attention on the second half. so our inferences will be based on the assumption that the distributions of the simulated values \\(\\theta_t\\), for large enough \\(t\\), are close to the target distribution, \\(p(\\theta|y)\\). we refer to the practice of discarding early iterations in markov chain simulation as warm-up. depending on the context, different warm-up fractions (number of elements on the sequence to discard) can be appropriate. dependence of the iterations in each sequence once approximate convergence has been reached, is whether to thin the sequences by keeping every \\(k\\)th simulation draw from each sequence and discarding the rest. whether or not the sequences are thinned, if the sequences have reached approximate convergence, they can be directly used for inferences about the parameters \\(\\theta\\) and any other quantities of interest. multiple sequences with overdispersed starting points our recommended approach to assessing convergence of iterative simulation is based on comparing different simulated sequences, as illustrated in figure 11.1. in figure 11.1a, the multiple sequences clearly have not converged; the variance within each sequence is much less than the variance between sequences. later, in figure 11.1b, the sequences have mixed, and the two variance components are essentially equal. monitoring scalar estimands we monitor each scalar estimand or other scalar quantities of interest separately. estimands include all the parameters of interest in the model and any other quantities of interest (for example, the ratio of two parameters or the value of a predicted future observation). it is often useful also to monitor the value of the logarithm of the posterior density, which has probably already been computed if we are using a version of the metropolis algorithm. challenges of monitoring convergence: mixing and stationarity figure 11.3a illustrates that, to achieve convergence, the sequences must together have mixed. the second graph in figure 11.3 shows two chains that have mixed, in the sense that they have traced out a common distribution, but they do not appear to have converged. figure 11.3b illustrates that, to achieve convergence, each individual sequence must reach stationarity. so to check convergence we have to simultaneously tests mixing (if all the chains have mixed well, the separate parts of the different chains should also mix) and stationarity (at stationarity, the first and second half of each sequence should be traversing the same distribution). splitting each saved sequence into two parts we diagnose convergence (as noted above, separately for each scalar quantity of interest) by checking mixing and stationarity. our approach consists on splitting each chain in half and check that all the resulting halfsequences have mixed. we start with some number of simulated sequences in which the warm-up period has already been discarded. we then take each of these chains and split into the first and second half. let \\(m\\) be the number of chains (after splitting) and \\(n\\) be the length of each chain. for example, suppose we simulate \\(5\\) chains, each of length \\(1000\\), and then discard the first half of each as warm-up. we are then left with \\(5\\) chains, each of length \\(500\\), and we split each into two parts: iterations \\(1â250\\) (originally iterations \\(501â750\\)) and iterations \\(251â500\\) (originally iterations \\(751â1000\\)). we now have \\(m = 10\\) chains, each of length \\(n = 250\\). assessing mixing using between- and within-sequence variances for each scalar estimand \\(\\psi\\), we label the simulations as \\(\\psi_{ij}, (i = 1, \\cdots, n; j = 1, \\cdots, m)\\), and we compute \\(b\\) and \\(w\\), the between- and within-sequence variances: \\begin{align} b = \\frac{n}{m - 1} \\sum_{j=1}^m (\\overline{\\psi}_{.j} - \\overline{\\psi}_{..})^2 \\end{align} where: \\begin{align} \\overline{\\psi}_{.j} = \\frac{1}{n} \\sum_{i=1}^n \\psi_{ij} \\end{align} \\begin{align} \\overline{\\psi}_{..} = \\frac{1}{m} \\sum_{j=1}^m \\overline{\\psi}_{.j} \\end{align} and \\begin{align} w = \\frac{1}{m} \\sum_{j=1}^m s_{j}^2 \\end{align} where \\begin{align} s^2_j = \\frac{1}{n - 1} \\sum_{i = 1}^n (\\psi_{ij} - \\overline{\\psi}_{.j})^2 \\end{align} we can estimate \\(\\mathbb{v}[\\psi|y]\\), the marginal posterior variance of the estimand, by a weighted average of \\(w\\) and \\(b\\), namely: \\begin{align} \\hat{\\mathbb{v}}^+[\\psi|y] = \\frac{n - 1}{n}w + \\frac{1}{n} b \\end{align} this quantity overestimates the marginal posterior variance assuming the starting distribution is appropriately overdispersed, but is unbiased under stationarity. meanwhile, for any finite \\(n\\), the \"within\" variance \\(w\\) should be an underestimate of \\(\\mathbb{v}[\\psi|y]\\) because the individual sequences have not had time to range over all of the target distribution and, as a result, will have less variability; in the limit as \\(n \\rightarrow \\infty\\), the expectation of \\(w\\) approaches \\(\\mathbb{v}[\\psi|y]\\). we monitor convergence of the iterative simulation by estimating the factor by which the scale of the current distribution for \\(\\psi\\) might be reduced if the simulations were continued in the limit \\(n \\rightarrow \\infty\\). this potential scale reduction is estimated by: \\begin{align} \\hat{r} = \\sqrt{\\frac{\\hat{\\mathbb{v}}[\\psi|y]}{w}} \\end{align} which declines to \\(1\\) as \\(n \\rightarrow 1\\). if the potential scale reduction is high, then we have reason to believe that proceeding with further simulations may improve our inference about the target distribution of the associated scalar estimand. example. bivariate unit normal density with bivariate normal jumping kernel (continued) table 11.1 displays posterior inference for the two parameters of the distribution as well as the log posterior density. after \\(50\\) iterations, the variance between the five sequences is much greater than the variance within, for all three univariate summaries considered. however, the five simulated sequences have converged adequately after \\(2000\\) or certainly \\(5000\\) iterations for the quantities of interest. the comparison with the true target distribution shows how some variability remains in the posterior inferences even after the markov chains have converged. the method of monitoring convergence presented here has the key advantage of not requiring the user to examine time series graphs of simulated sequences. inspection of such plots is a notoriously unreliable method. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T4/02_metropolist_algorithm.html",
    "title": "Metropolis and Metropolis-Hastings Algorithms",
    "body": " index search search back metropolis and metropolis-hastings algorithms contents the metropolis algorithm example bivariate unit normal density with normal jumping kernel why does the metropolis algorithm work the metropolis-hastings algorithm relation between the jumping rule and efficiency of simulations the metropolis-hastings algorithm is a general term for a family of markov chain simulation methods that are useful for sampling from bayesian posterior distributions. we have already seen the gibbs sampler in the previous section; it can be viewed as a special case of metropolis-hastings. the metropolis algorithm the metropolis algorithm is an adaptation of a random walk with an acceptance/rejection rule to converge to the specified target distribution. the algorithm proceeds as follows. draw a starting point \\(\\theta_0\\), for which \\(p(\\theta_0|y) > 0\\), from a starting distribution \\(p_0(\\theta)\\). the starting distribution might be based on an approximation or we may simply choose starting values dispersed around a crude approximate estimate. for \\(t = 1, 2, \\cdots\\): sample a proposal \\(\\theta^*\\) from a jumping distribution (or proposal distribution) at time \\(t\\), \\(j_t(\\theta^*|\\theta^{t-1})\\). for the metropolis algorithm (but not the metropolis-hastings algorithm, as discussed later in this section), the jumping distribution must be symmetric. calculate the ratio of the densities: \\begin{align} r = \\frac{p(\\theta^*|y)}{p(\\theta^{t- 1}|y)} \\end{align} set: \\begin{align} \\theta^t = \\begin{cases} \\theta^* & \\text{ with probability } \\min(r, 1) \\\\ \\theta^{t-1} \\text{ otherwise } \\end{cases} \\end{align} the acceptance/rejection rule of the metropolis algorithm can be stated as follows: if the jump increases the posterior density, set \\(\\theta^t = \\theta^*\\); if the jump decreases the posterior density, set \\(\\theta^t = \\theta^*\\) with probability equal to the density ratio, \\(r\\), otherwise set \\(\\theta_t = \\theta^{t - 1}\\) (with probability \\(1 - r\\)). the metropolis algorithm can thus be viewed as a stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density but only sometimes accepting downward steps. to use the algorithm, we need to calculate the ratio \\(r\\) for every pair of \\((\\theta, \\theta^*)\\), and we also need to choose \\(\\theta\\) from the jumping distribution \\(j_t(\\theta^*|\\theta)\\) for all \\(\\theta\\) and \\(t\\). additionally, we need to generate a random number for step (\\(c\\)) in the process. even if the jump isn't accepted and \\(\\theta_t\\) equals \\(\\theta_{t-1}\\), it still counts as a step in the algorithm. example: bivariate unit normal density with normal jumping kernel for simplicity, we illustrate the metropolis algorithm with the simple example of the bivariate unit normal distribution. the target density is the bivariate unit normal, \\(p(\\theta|y) = \\text{n}(\\theta|0, i)\\). the jumping distribution is also bivariate normal, centered at the current iteration and scaled to \\(\\frac{1}{5}\\) the size: \\(j_t(\\theta^*|\\theta^{tâ1}) = \\text{n}(\\theta^*|\\theta^{tâ1}, 0.22\\cdot i)\\). at each step, it is easy to calculate the density ratio: \\begin{align} r = \\frac{\\text{n}(\\theta^*|0, i)}{\\text{n}(\\theta^{t-1}|0, i)} \\end{align} it is clear from the form of the normal distribution that the jumping rule is symmetric. figure 11.1 displays five simulation runs starting from different points. we have purposely set the scale of this jumping algorithm to be too small, relative to the target distribution, so that the algorithm will run inefficiently and its random-walk aspect will be obvious in the figure. why does the metropolis algorithm work? the proof that the sequence of iterations \\(\\theta_1, \\theta_2, \\cdots\\) converges to the target distribution has two steps: it is shown that the simulated sequence is a markov chain with a unique stationary distribution. it is shown that the stationary distribution equals the target distribution. except for trivial exceptions, the latter two conditions hold for a random walk on any proper distribution, and irreducibility holds as long as the jumping distributions jt is eventually be able to jump to all states with positive probability. to show (1) consider starting the algorithm at time \\(t â 1\\) with a draw \\(\\theta^{tâ1}\\) from the target distribution \\(p(\\theta|y)\\). now consider any two such points \\(\\theta_a\\) and \\(\\theta_b\\), drawn from \\(p(\\theta|y)\\) and labeled so that \\(p(\\theta_b|y) \\geq p(\\theta_a|y)\\). the unconditional probability density of a transition from \\(\\theta_a\\) to \\(\\theta_b\\) is: \\begin{align} p(\\theta^{t - 1} = \\theta_a, \\theta^t = \\theta_b) = p(\\theta_a|y)j_t(\\theta_b|\\theta_a) \\end{align} where the acceptance probability is \\(1\\) because of our labeling of \\(a\\) and \\(b\\), and the unconditional probability density of a transition from \\(\\theta_b\\) to \\(\\theta_a\\) is: \\begin{align} p(\\theta^t = \\theta_a, \\theta^{t-1} = \\theta_b) = p(\\theta_b|y)j_t(\\theta_a|\\theta_b) \\left(\\frac{p(\\theta_a|y)}{p(\\theta_b|y)}\\right) \\end{align} \\begin{align} = p(\\theta_b|y)j_t(\\theta_a|\\theta_b) \\end{align} which is the same as the probability of a transition from \\(\\theta_a\\) to \\(\\theta_b\\), since we have required that \\(j_t(\\cdot|\\cdot)\\) be symmetric. since their joint distribution is symmetric, \\(\\theta^t\\) and \\(\\theta^{tâ1}\\) have the same marginal distributions, and so \\(p(\\theta|y)\\) is the stationary distribution of the markov chain of \\(\\theta\\). the metropolis-hastings algorithm the metropolis-hastings algorithm generalizes the basic metropolis algorithm presented above in two ways. the jumping rules \\(j_t\\) need no longer be symmetric. to correct for the asymmetry in the jumping rule the ratio \\(r\\) is replaced by a ratio of ratios: \\begin{align} r = \\frac{\\frac{p(\\theta^*|y)}{j_t(\\theta^*|\\theta^{t-1})}}{\\frac{p(\\theta^{t-1}|y)}{j_t(\\theta^{t-1}|\\theta^*)}} \\end{align} allowing asymmetric jumping rules can be useful in increasing the speed of the random walk. relation between the jumping rule and efficiency of simulations the ideal metropolis-hastings jumping rule is simply to sample the proposal, \\(\\theta^*\\), from the target distribution; such that our jumping distribution is equal to the target distribution, \\(j(\\theta^*|\\theta) â¡ p(\\theta^*|y)\\). then the ratio \\(r\\) is always exactly \\(1\\), so we always choose the new sampled \\(\\theta^*\\) to update \\(\\theta^t\\) instead of remaining with \\(\\theta^{t-1}\\). a good jumping distribution has the following properties: for any \\(\\theta\\), it is easy to sample from \\(j(\\theta^*|\\theta)\\) it is easy to compute the ratio \\(r\\) each jump goes a reasonable distance in the parameter space (otherwise the random walk moves too slowly). the jumps are not rejected too frequently (otherwise the random walk wastes too much time standing still). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/index.html",
    "title": "Modelos Bayesianos JerÃ¡rquicos",
    "body": " index search search back modelos bayesianos jerÃ¡rquicos introduccion a la inferencia bayesiana modelos jerarquicos constructing a parametrized prior distribution exchangeability and hierarchical models bayesian analysis of conjugate hierarchical models normal model with exchangeable parameters example: parallel experiments in eight schools hierarchical modeling applied to meta-analysis weakly informative priors evaluaciÃ³n y comparaciÃ³n de modelos measures of predictive accuracy information criteria and cross-validation model comparison based on predictive performance model comparison using bayes factors aspectos computacionales de la inferencia bayesiana gibbs sampler metropolis and metropolis-hastings algorithms using gibbs and metropolis as buliding blocks inference and assessing convergence effective number of simulation draws appendix t2 thanks to how little i understand this book i will use other sources in order to properly understand bayesian hierarchical modeling, the contents on the following chapter explains how hierarchical bayesian models came to be and its appeal. then on section 2 it lays out hierarchical nolmal modeling followed by an explanation on hierarchical beta-binomial modeling on section 3. bayesian hierarchical modeling introduction hierarchical normal modeling hierarchical beta-binomial modeling $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/02.html",
    "title": "Exchangeability and hierarchical models",
    "body": " index search search back exchangeability and hierarchical models contents exchangeability example exchangeability when additional information is available on the units example assumptions model formulation bayesian inference objections to exchangeable models the full bayesian treatment of the hierarchical model posterior predictive distributions let \\(e\\) be a set of experiments, such that \\(e_j = {y_i, \\theta_j}, j = 1, \\cdots, j\\) where \\(y_i\\) are the vector data, \\(\\theta_j\\) are the vector parameters and \\(p(y_j|\\theta_j)\\) is the likelihood function. exchangeability when we have no additional data on the parameters, we assume exchangeability between them, such that \\(p(\\theta_1, \\cdots, \\theta_j)\\) is invariant to permutation of the indexes. the simplest form of an exchangeable distribution has each of the parameters \\(\\theta_j\\) as an independent sample from a prior distribution governed by some unknown parameter vector \\(\\phi\\); thus: \\begin{align} p(\\theta|\\phi) = \\prod_{j = 1}^j p(\\theta_j|\\phi) \\end{align} in general, \\(\\phi\\) is unknown, so our distribution for \\(\\theta\\) must average over our uncertainty in \\(\\phi\\): \\begin{align} p(\\theta) = \\int_{\\phi} p(\\theta|\\phi)p(\\phi) d \\phi \\end{align} \\begin{align} = \\int_{\\phi} \\left(\\prod_{j = 1}^j p(\\theta_j|\\phi)\\right) p(\\phi) d \\phi \\end{align} this form, the mixture of independent identical distributions, is usually all that we need to capture exchangeability in practice. example we use a nonhierarchical example with exchangeability at the level of \\(y\\) rather than \\(\\theta\\). in this example, eight states in the united states were selected, and the divorce rate per \\(1000\\) population in each state in \\(1981\\) was recorded. since you have no information to distinguish any of the eight states from the others, you must model them exchangeably. however, you can't assign an exchangeable prior to the set of eight diverse states when there's specific information about one of them. for example, if we know that nevada differentiates itself from the others because it divorce rate is known to be unusually high, that lets us know before even seeing the data (observed values), that there's a strong reason to believe that nevada's divorce rate is higher than the other states. this means that in a bayesian analysis, the prior distribution should reflect this belief, assigning more probability mass to nevada having a higher divorce rate in comparison to the other states. exchangeability when additional information is available on the units sometimes obervations are partially or conditionally exchangeable. for example, when: in the case where observations can be grouped, a hierarchical model can be created. in this context, each group has unknown properties. the assumption of exchangeability allows for the use of a common prior distribution for these group properties, meaning that any group can be considered as a random sample of the same underlying population. if \\(y_i\\) has additional information \\(x_i\\) so that \\(y_i\\) are not exchangeable but \\((y_i, x_i)\\) still are exchangeable, then we can make a joint model for \\((y_i, x_i)\\) or a conditional model for \\(y_i|x_i\\). in general, the usual way to model exchangeability with covariates is through conditional independence: \\begin{align} p(\\theta_1, \\cdots, \\theta_j) = \\int \\left[ p(\\theta_j|\\phi,x_j)\\right]p(\\phi, x) d\\phi \\end{align} whith \\(x = [x_1, \\cdots, x_j]\\) example let's consider an example in the field of education where we want to analyze the test scores of students from different schools. we can view the test scores as observations that can be grouped by schools. let \\(y_{ij}\\) be the test score of the student \\(i\\) in school \\(j\\), where \\(i = 1, 2, \\cdots, n_j\\) and \\(j = 1, 2, \\cdots, j\\) and \\(n_j\\) are the number of students at school \\(j\\). assumptions each school \\(j\\) has an unknown mean test score \\(\\mu_j\\). the mean test scores \\(\\mu_j\\) are assumed to follow a common distribution. *exchangeability: the test scores within each school are exchangeable, implying that any school could be considered a random sample from the overall population of schools. common prior distribution: we assume a common prior distribution for the group mean test scores \\(\\mu_j\\) across schools. model formulation likelihood: the likelihood of the test scores given the group mean and variance \\begin{align} p(y_{ij}|\\mu_j, \\sigma^2) \\end{align} prior: common prior distribution for the group mean test scores \\begin{align} p(\\mu_j|\\theta) \\sim \\mathcal{n}(\\theta, \\tau^2) \\end{align} where \\(\\theta\\) represents the overall mean test score and \\(\\tau\\) is the variance parameter. hyperprior: prior distribution for the overall test score \\begin{align} p(\\theta) \\sim \\mathcal{n}(\\mu_0, \\sigma_0^2) \\end{align} where \\(\\mu_0\\) is the prior mean and \\(\\sigma_0^2\\) is the prior variance. bayesian inference the posterior distribution of the group mean test scores and the overall mean test score can be obtained using bayesian inference techniques, such as markov chain monte carlo (mcmc) sampling. objections to exchangeable models in statistical applications, it is common to raise objections to the assumption that different data or experiments are exchangeable. for example experiments may which may have been conducted at different times, with different subjects, and likely in different places. despite these differences, the text suggests that it might be acceptable to consider the data as if they were from the same distribution due to model ignorance. the full bayesian treatment of the hierarchical model the true 'hierarchical' part of the models is that some parameters are not known and thus have their own prior distributions, denoted as \\(p(\\phi)\\). the bayesian posterior distribution is of the vector \\((\\phi, \\theta)\\). the joint prior distribution is: \\begin{align} p(\\phi, \\theta) = p(\\phi)p(\\theta|\\phi) \\end{align} and the joint posterior distribution (after seeing the data \\(y\\)) is: \\begin{align} p(\\phi, \\theta|y) \\propto p(\\phi, \\theta)p(y|\\phi, \\theta) \\end{align} given that \\(p(y|\\phi, \\theta)\\) depends only on \\(\\theta\\): \\begin{align} = p(\\phi, \\theta)p(y|\\theta) \\end{align} in order to create a joint probability distribution for \\((\\phi, \\theta)\\), we must assign a prior distribution to \\(\\phi\\). it is often practical to start with a simple, relatively noninformative, prior distribution on \\(\\phi\\) and seek to add more prior information if there remains too much variation in the posterior distribution. posterior predictive distributions hierarchical models are characterized both by parameters \\(\\theta\\) and hyperparameters, \\(\\phi\\), that parametrize the prior distribution over \\(\\theta\\). there are two posterior predictive distributions that might be of interest: the distribution of future observations \\(\\tilde{y}\\) corresponding to an existing \\(j\\) \"group\" described by \\(\\theta_j\\). the distribution of future observations \\(\\tilde{y}\\) corresponding to future \\(\\theta_j\\) (a \"new group\"), denoted by \\(\\tilde{\\theta}\\), drawn from the superpopulation \\(p(\\theta|\\phi)\\). in the rat tumor example, future observations can be (1) additional rats from an existing experiment, or (2) results from a future experiment (explained by a different set of parameters \\(\\theta\\)). for (1) the posterior predictive draws \\(\\tilde{y}\\) are based on the posterior draws of \\(\\theta_j\\) (\\(p(\\theta_j|y)\\)) for the existing experiment. for (2) one must first draw \\(\\tilde{\\theta}\\) for the new experiment from the population distribution, given the posterior draws of \\(\\phi\\), and then draw \\(\\tilde{y}\\) given the simulated \\(\\tilde{\\theta}\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/05_example_normal.html",
    "title": "Example: parallel experiments in eight schools",
    "body": " index search search back example: parallel experiments in eight schools contents inferences based on nonhierarchical models and their problems posterior simulation under the hierarchical model discussion a study was performed for the educational testing service to analyze the effects of special coaching programs on test scores in each of eigth schools. the outcome variable in each study was the score on a special administration of the sat-v. the scores can vary between \\(200\\) and \\(800\\), with mean about \\(500\\) and standard deviation about \\(100\\). also, there was no prior reason to believe that any of the eight programs was more effective than any other or that some were more similar in effect to each other than to any other. the results of the experiments are summarized in: inferences based on nonhierarchical models and their problems before fitting the hierarchical bayesian model, we first consider two simpler nonhierarchical methodsâestimating the effects from the eight experiments independently (separate estimates), and complete poolingâand discuss why neither of these approaches is adequate for this example. consider \\(\\theta_1\\), the effect in school \\(a\\). the effect in school \\(a\\) is estimated as \\(28.4\\) with a standard error of \\(14.9\\) under the separate analysis, versus a pooled estimate of \\(7.7\\) with a standard error of \\(4.1\\) under the common-effect model. note: given a normal distribution (symmetrical with respect to it mean) the probability that an estimate takes a value under the mean is \\(\\frac{1}{2}\\) (cumulative density function), as the \\(\\mu\\) serves as the midpoint of a normal distribution such that half the area for the normal curve is contained under \\([0, \\mu]\\). the separate analyses of the eight schools imply the following posterior statement: 'the probability is \\(\\frac{1}{2}\\) that the true effect in \\(a\\) is more than \\(28.4\\)' a doubtful statement, considering the results for the other seven schools. on the other hand, the pooled model implies the following statement: 'the probability is \\(\\frac{1}{2}\\) that the true effect in a is less than \\(7.7\\),' which seems an inaccurate summary of our knowledge. as in the theoretical discussion of the previous section, neither estimate is fully satisfactory, and we would like a compromise that combines information from all eight experiments without assuming all the \\(\\theta_j\\)'s to be equal. the bayesian analysis under the hierarchical model provides exactly that. posterior simulation under the hierarchical model consequently, we compute the posterior distribution of \\(\\theta_1, \\cdots, \\theta_8\\), based on the normal model presented in section 4. we draw from the posterior distribution for the bayesian model by simulating the random variables \\(\\tau\\), \\(\\mu\\), and \\(\\theta\\), in that order, from their posterior distribution, as discussed at the end of the previous section. the sampling standard deviations, \\(\\sigma_j\\), are assumed known and equal to the values in table 5.2, and we assume independent uniform prior densities on \\(\\mu\\) and \\(\\tau\\). the marginal posterior density function, \\(p(\\tau|y)\\) from, is plotted in the next figure: values of \\(\\tau\\) near zero are mos plausible. in the normal hierarchical model, however, we learn a great deal by considering the conditional posterior distributions given \\(\\tau\\) (and averaged over \\(\\mu\\)), that is \\(\\mathbb{e}[\\theta_j|\\tau, y]\\), averaging over \\(\\mu\\). this is displayed on the following image: comparing with the previous figure, which has the same scale on the horizontal axis, we see that for most of the likely values of \\(\\tau\\), that is for \\(\\tau \\approx 0\\) the estimated effects for all the groups are relatively close together (when \\(\\tau = 0\\) you would guess they are clustered on the same point). however, as \\(\\tau\\) becomes larger, corresponding to more variability among schools, the estimates become more like the raw values shown on the first figure of this section. the lines in the following figure show the conditional standard deviations, \\(sd(\\theta_j|\\tau, y)\\), as a function of \\(\\tau\\). as \\(\\tau\\) increases, the population distribution allows the eight effects to be more different from each other, and hence the posterior uncertainty in each individual \\(\\tau_j\\) increases, approaching the standard deviations shown in the raw data in the limit of \\(\\tau \\rightarrow \\infty\\). contrary to what we saw with separate estimates and pooled estimates, for the likely values of \\(\\tau\\) (see figure for \\(p(\\tau|y)\\)), the estimates in all schools are substantially less than \\(28\\) points. for example, even at \\(\\tau = 0\\), the probability that the effect in school a is less than \\(28\\) points is \\(\\phi[(28 â 14.5)/9.1] = 93\\%\\), where \\(\\phi\\) is the standard normal cumulative distribution function. of substantial importance, we do not obtain an accurate summary of the data if we condition on the posterior mode of \\(\\tau\\) as it ignores the uncertainty associated with \\(\\tau\\) as conveyed by the full posterior distribution. in bayesian statistics, the posterior distribution encapsulates both the most likely values of parameters as well as the uncertainty or variability in those estimates. by only considering the mode (the peak or maximum) of the posterior distribution and neglecting its shape and spread, we may miss out on valuable information about the range of plausible values for Ï and the associated uncertainty. discussion table 5.3 summarizes the \\(200\\) simulated effect estimates for all eight schools. the bayesian probability that the effect in school a is as large as \\(28\\) points is less than \\(10\\%\\), which is substantially less than the \\(50\\%\\) probability based on the separate estimate for school a. as an illustration of the simulation-based posterior results, \\(200\\) simulations of school a's effect are shown in figure 5.8a. having simulated the parameter \\(\\theta\\), it is easy to ask more complicated questions of this model. for example, what is the posterior distribution of \\(\\max(\\theta_j)\\), the effect of the most successful of the eight coaching programs? figure 5.8b displays a histogram of \\(200\\) values from this posterior distribution and shows that only \\(22\\) draws are larger than \\(28.4\\). for another example, we can estimate \\(pr(\\theta_1 > \\theta_3|y)\\), the posterior probability that the coaching program is more effective in school a than in school c, by the proportion of simulated draws of \\(\\theta\\) for which \\(\\theta_1 > \\theta_3\\); the result is \\(\\frac{141}{200} = 0.705\\). to sum up, the bayesian analysis of this example not only allows straightforward inferences about many parameters that may be of interest, but the hierarchical model is flexible enough to adapt to the data, thereby providing posterior inferences that account for the partial pooling as well as the uncertainty in the hyperparameters. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/06_hierarchical_meta_analysis.html",
    "title": "Hierarchical modeling applied to a meta-analysis",
    "body": " index search search back hierarchical modeling applied to a meta-analysis contents a normal approximation to the likelihood goals of inference in meta-analysis what if exchangeability is inappropriate a hierarchical model meta-analysis is an increasingly popular and important process of summarizing and integrating the findings of research studies in a particular area. the data in our medical example are displayed in the first three columns of the previous image, which summarize mortality after myocardial infarction in 22 clinical trials. the aim of a metaanalysis is to provide a combined analysis of the studies that indicates the overall strength of the evidence for a beneficial effect of the treatment under study. a normal approximation to the likelihood if clinical trial \\(j\\) (in the series to be considered for meta-analysis) involves the use of \\(n_{0j}\\) subjects in the control group and \\(n_{1j}\\) in the treatment group, giving rise to \\(y_{0j}\\) and \\(y_{1j}\\) deaths in control and treatment groups, respectively, then the usual sampling model involves two independent binomial distributions with probabilities of death \\(p_{0j}\\) and \\(p_{1j}\\), respectively. for each study \\(j\\), one can estimate \\(\\theta_j\\) by: \\begin{align} y_j = \\log\\left(\\frac{y_{1j}}{n_{1j} - y_{1j}}\\right) - \\log\\left(\\frac{y_{0j}}{n_{0j} - y_{0j}}\\right) \\end{align} with approximate sampling variance \\begin{align} \\sigma_j^2 = \\frac{1}{y_{1j}} + \\frac{1}{n_{1j} - y_{1j}} + \\frac{1}{y_{0j}} + \\frac{1}{n_{0j} - y_{0j}} \\end{align} based on empirical logits. the estimated log-odds ratios \\(y_j\\) and their estimated standard errors \\(\\sigma_j^2\\) are displayed as the fourth and fifth columns of table 5.4. goals of inference in meta-analysis our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities. complete pooling: we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. separate estimates: the studies are so different that the results of any one study provide no information about the results of any of the others. bayesian analysis: we regard the studies as exchangeable but not necessarily either identical or completely unrelated. the first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall âaverageâ effect across all studies that could be regarded as exchangeable with the observed studies. other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. what if exchangeability is inappropriate? what if other information (in addition to the data \\((n, y)\\)) is available to distinguish among the \\(j\\) studies in a meta-analysis, so that an exchangeable model is inappropriate? in this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates. a hierarchical model let \\(y_j\\) represent generically the point estimate of the effect \\(\\theta_j\\) in the \\(j\\)th study, the sampling distribution is defined as: \\begin{align} y_j|\\theta_j, \\sigma_j \\sim \\text{n}(\\theta_j, \\sigma_j^2) \\end{align} where \\(\\sigma_j\\) represents the corresponding estimated standard error, which is assumed known without error. at the second stage of the hierarchy, we again use an exchangeable normal prior distribution, with mean \\(\\mu\\) and standard deviation \\(\\tau\\), which are unknown hyperparameters. \\begin{align} \\theta|\\mu, \\tau \\sim \\text{n}(\\mu, \\tau) \\end{align} finally, a hyperprior distribution is required for \\(\\mu\\) and \\(\\tau\\). for this problem, it is reasonable to assume a noninformative or locally uniform prior density for \\(\\mu\\). we also assume a locally uniform prior density for \\(\\tau\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/01.html",
    "title": "Constructing a parametrized prior distribution",
    "body": " index search search back constructing a parametrized prior distribution contents example analysis with a fixed prior distribution approximate estimate of the population distribution using the historical data appendix to begin our description of hierarchical models, we consider the problem of estimating a parameter \\(\\theta\\) using data from a small experiment and a prior distribution constructed from similar previous experiments. example suppose the aim is to estimate \\(\\theta\\), the probability of tumor in a population of female laboratory rats of type \"f344\" that receive a zero dose of the drug (control group). it is natural to assume a binomial model for the number of tumors, given \\(\\theta\\). for convenience, we select a prior distribution for \\(\\theta\\) from the conjugate family, \\(\\theta \\sim beta(\\alpha, \\beta)\\) analysis with a fixed prior distribution from historical data, suppose we knew that the tumor probabilities \\(\\theta\\) among groups of female lab rats of type \"f344\" follow an approximate beta distribution, with known mean and standard deviation. the use of a fixed prior distribution from historical data allows for the construction of a parameterized prior distribution, which in turn influences the posterior distribution for the current experiment. then, assuming a \\(beta(\\alpha, \\beta)\\) prior distribution for \\(\\theta\\) yields a \\(beta(\\alpha + 4, \\beta + 10)\\) posterior distribution for \\(\\theta\\). approximate estimate of the population distribution using the historical data contrary to last section, typically, the mean and standard deviation of underlying tumor risks are not available, rather historical data is available from previous experiments on similar conditions. in the \\(j\\)th historical experiment, let the number of rats with tumors be \\(y_j\\) and the total number of rats be \\(n_j\\). we model the \\(y_j\\)'s (probability that \\(p\\) rats have tumor given a total of \\(n\\) rats) as independent binomial data, given sample sizes \\(n_j\\) and study-specific means \\(\\theta_j\\). we can display the hierarchical model schematically as follows: the observed sample mean and standard deviation of the 70 values \\(y_j\\), \\(n_j\\) are \\(0.136\\) and \\(0.103\\). if we set the mean and standard deviation of the population distribution to these values we can solve for \\(\\theta\\) and \\(\\beta\\). the resulting estimate for \\((\\alpha, \\beta)\\) is \\((1.4, 8.6)\\). this is not a bayesian calculation because it is not based on any specified full probability model. appendix binomial model: in probability theory and statistics, the binomial distribution with parameters \\(n\\) and \\(p\\) is the discrete probability distribution of the number of successes in a sequence of \\(n\\) independent experiments (binomial distribution) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/12_hierarchical_normal_modeling.html",
    "title": "Hierarchical Normal Modeling",
    "body": " index search search back hierarchical normal modeling contents example ratings of animation movies [[#a hierarchical normal model with random \\(\\sigma\\)|a hierarchical normal model with random \\(\\sigma\\)]] graphical representation of the hierarchical model second-stage prior inference through mcmc describe the model by a script define the data and prior parameters mcmc diagnostics and summarization shrinkage sources of variability example: ratings of animation movies movielens is a website which provides personalized movie recommendations from users who create accounts and rate movies that they have seen. based on such information, movielens works to build a custom preference profile for each user and provide movie recommendations. a hierarchical normal model with random \\(\\sigma\\) in this situation it is reasonable to develop a model for the movie ratings where the grouping variable is the movie title. we index a rating by two subscripts, where \\(y_{ij}\\) denotes the \\(i\\)th rating for the \\(j\\)th movie title, with \\(j = 1, \\cdots, 8\\). since the ratings are continuous, it is reasonable to use the normal data model. for simplicity and ease of illustration, a common and shared unknown standard deviation \\(\\sigma\\) is assumed for all normal models (however it could also be modeled). therefore we define the sampling distribution as: \\begin{align} y_{ij} | \\mu_j, \\sigma \\sim \\text{normal}(\\mu_j, \\sigma) \\end{align} since these movies are all animations, it is reasonable to believe that the mean ratings are similar across movies. so one assigns each mean rating the same normal prior distribution at the first stage: \\begin{align} \\mu_j | \\mu, \\tau \\sim \\text{normal}(\\mu, \\tau) \\end{align} the hyperparameters \\(\\mu\\) and \\(\\tau\\) are treated as random since we are unsure about the degree of pooling of the eight sets of ratings. after observing data, inference is performed about \\(\\mu\\) and \\(\\tau\\) based on their posterior distributions. treating \\(\\mu\\) and \\(\\tau\\) as random, one arrives at the following hierarchical model: sampling for \\(j = 1, \\cdots, 8\\) and \\(i = 1, \\cdots, n_j\\): \\begin{align} y_{ij} | \\mu_j, \\sigma \\sim \\text{normal}(\\mu_j, \\sigma) \\end{align} prior for \\(\\mu_j\\), stage 1, \\(j = 1, \\cdots, 8\\): \\begin{align} \\mu_j | \\mu, \\tau \\sim \\text{normal}(\\mu, \\tau) \\end{align} prior for \\(\\mu_j\\), stage 2, the hyperprior: \\begin{align} \\mu, \\tau \\sim \\pi(\\mu, \\tau) \\end{align} to complete the model, one needs to specify a prior distribution for the standard deviation parameter, \\(\\sigma\\): \\begin{align} \\frac{1}{\\sigma^2} | a_{\\sigma}, b_{\\sigma} \\sim \\text{gamma}(a_{\\sigma}, b_{\\sigma}) \\end{align} one assigns a known gamma prior distribution for \\(\\frac{1}{\\sigma^2}\\), with fixed hyperparameter values \\(a_{\\sigma}\\) and \\(b_{\\sigma}\\). in some situations, one may consider the situation where \\(a_{\\sigma}\\) and \\(b_{\\sigma}\\) are random and assign hyperprior distributions for these unknown hyperparameters. it is helpful to contrast the two-stage prior distribution for \\(\\{\\mu_j\\}\\) and the one-stage prior distribution for \\(\\sigma\\). for the means \\(\\{\\mu_j\\}\\), we have discussed that specifying a common prior distribution for different \\(j\\) pools information across the movies. one is simultaneously estimating both a mean for each movie (the \\(\\mu_j\\)'s) and the variation among the movies (\\(\\mu\\) and \\(\\tau\\)). for the standard deviation, the hierarchical model also pools information across movies. however, all of the observations are combined in the estimation of \\(\\sigma\\). since separate values of \\(\\sigma_j\\), one cannot learn about the differences and similarities among the \\(\\sigma_j\\)'s. graphical representation of the hierarchical model an alternative way of expressing this hierarchical model uses the following graphical representation. in the middle section of the graph, \\(y_{ij}\\) represents the collection of random variables for all ratings of movie \\(j\\). the upper section of the graph focuses on the \\(\\mu_j\\)'s. all means follow the same prior, a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). since \\(\\mu\\) and \\(\\tau\\) are random, these second-stage parameters are associated with the prior label \\(\\pi(\\mu, \\tau)\\). second-stage prior the hierarchical normal model presented in equations (10.6) through (10.9) has not specified the hyperprior distribution \\(\\pi(\\mu, \\tau)\\). how does one construct a prior on these second-stage hyperparameters? a typical approach for normal models is to assign two independent prior distributions â a normal distribution for the mean \\(\\mu\\) and a gamma distribution for the precision \\(\\frac{1}{\\tau^2}\\). such a specification facilitates the use of the gibbs sampling. using this approach, the density \\(\\pi(\\mu, \\tau)\\) is replaced by the two hyperprior distributions below: \\begin{align} \\mu | \\mu_0, \\gamma_0 \\sim \\text{normal}(\\mu_0, \\gamma_0) \\end{align} \\begin{align} \\frac{1}{\\tau^2} | a, b \\sim \\text{gamma}(a_{\\tau}, b_{\\tau}) \\end{align} the task of choosing a prior for \\((\\mu, \\tau)\\) reduces to the problem of choosing values for the four hyperparameters \\(\\mu_0, \\gamma_0, a_{\\tau}\\) and \\(b_{\\tau}\\). if one believes that \\(mu\\) is located around the value of \\(3\\) and she is not very confident of this choice, the set of values \\(\\mu_0 = 3\\) and \\(\\gamma_0 = 1\\) could be chosen. as for \\(\\tau\\), one chooses a weakly informative prior with \\(a_{\\tau} = b_{\\tau} = 1\\) as \\(\\text{gamma}(1, 1)\\). moreover, to choose a prior for \\(\\sigma\\), let \\(a_{\\sigma} = b_{\\sigma} = 1\\) to have the weakly informative \\(\\text{gamma}(1, 1)\\) prior. inference through mcmc with the specification of the prior, the complete hierarchical model is described as follows: sampling for \\(j = 1, \\cdots, 8\\) and \\(i = 1, \\cdots, n_j\\): \\begin{align} y_{ij} | \\mu_j, \\sigma \\sim \\text{normal}(\\mu_j, \\sigma) \\end{align} prior for \\(\\mu_j\\), stage 1, \\(j = 1, \\cdots, 8\\): \\begin{align} \\mu_j | \\mu, \\tau \\sim \\text{normal}(\\mu, \\tau) \\end{align} prior for \\(\\mu_j\\), stage 2: the hyperpriors: \\begin{align} \\mu \\sim \\text{normal}(3, 1) \\end{align} \\begin{align} \\frac{1}{\\tau^2} \\sim \\text{gamma}(1, 1) \\end{align} prior for \\(\\sigma\\) \\begin{align} \\frac{1}{\\sigma^2} \\sim \\text{gamma}(1, 1) \\end{align} describe the model by a script the first step in using the jags software is to write the following script defining the hierarchical model. the model is saved in the character string modelstring. modelstring <-\" model { ## sampling for (i in 1:n){ y[i] ~ dnorm(mu_j[movieindex[i]], invsigma2) } ## priors for (j in 1:j){ mu_j[j] ~ dnorm(mu, invtau2) } invsigma2 ~ dgamma(a_s, b_s) sigma <- sqrt(pow(invsigma2, -1)) ## hyperpriors mu ~ dnorm(mu0, g0) invtau2 ~ dgamma(a_t, b_t) tau <- sqrt(pow(invtau2, -1)) } \" in the sampling part of the script, note that the loop goes from 1 to n, where n is the number of observations with index i. however, because now n observations are grouped according to movies, indicated by j, one needs to create one vector, mu_j of length eight, and use movieindex[i] to grab the corresponding mu_j based on the movie index. in the priors part of the script, the loop goes from 1 to j, and j = 8 in the current example. inside the loop, the first line corresponds to the prior distribution for mu_j. due to a commonly shared sigma, invsigma2 follows dgamma(a_g, b_g) outside of the loop. in addition, sigma <- sqrt(pow(invsigma2, -1)) is added to help tracksigma directly. finally in the hyperpriors section of the script, one specifies the normal hyperprior for mu, a gamma hyperprior for invtau2. keep in mind that the arguments in the dnorm in jags are the mean and the precision (std). if one is interested instead in the standard deviation parameter tau, one could return it in the script by using tau <- sqrt(pow(invtau2, -1)), enabling the tracking of its mcmc chain in the posterior inferences. define the data and prior parameters after one has defined the model script, the next step is to provide the data and values for parameters of the prior. in the r script below, a list the_data contains the vector of observations, the vector of movie indices, the number of observations, and the number of movies. it also contains the normal hyperparameters mu0 and g0, and two sets of gamma hyperparameters (a_t and b_t) for invtau2, and (a_s and b_s) for invsigma2. y <- movieratings$rating movieindex <- movieratings$group_number n <- length(y) j <- length(unique(movieindex)) the_data <- list(\"y\" = y, \"movieindex\" = movieindex, \"n\" = n, \"j\" = j, \"mu0\" = 3, \"g0\" = 1, \"a_t\" = 1, \"b_t\" = 1, \"a_s\" = 1, \"b_s\" = 1) one uses the run.jags() function in the runjags r package to generate posterior samples by using the mcmc algorithms in jags. the script below runs one mcmc chain with \\(1000\\) iterations in the adapt period (preparing for mcmc), \\(5000\\) iterations of burn-in and an additional set of \\(5000\\) iterations to be run and collected for inference. by using monitor = c(\"mu\", \"tau\", \"mu_j\", \"sigma\"), one collects the values of all parameters in the model. in the end, the output variable posterior contains a matrix of simulated draws. posterior <- run.jags(modelstring, n.chains = 1, data = the_data, monitor = c(\"mu\", \"tau\", \"mu_j\", \"sigma\"), adapt = 1000, burnin = 5000, sample = 5000) mcmc diagnostics and summarization to perform some mcmc diagnostics in our example, one uses the plot() function, specifying the variable to be checked by the vars argument. for example, the script below returns four diagnostic plots (trace plot, empirical pdf, histogram, and autocorrelation plot) for the hyperparameter \\(\\tau\\). plot(posterior, vars = \"tau\") in practice mcmc diagnostics should be performed for all parameters to justify the overall mcmc convergence. in our example, the above diagnostics should be implemented for each of the eleven parameters in the model: \\(\\mu, \\tau, \\mu_1, \\cdots, \\mu_8\\) and \\(\\sigma\\). once diagnostics are done, one reports posterior summaries of the parameters using print(). note that these summaries are based on the 5000 iterations from the sample period, excluding the adapt and burn-in iterations. print(posterior, digits = 3) lower95 median upper95 mean sd mode mcerr mu 3.19 3.78 4.34 3.77 0.286 -- 0.00542 tau 0.357 0.638 1.08 0.677 0.2 -- 0.00365 mu_j[1] 2.96 3.47 3.99 3.47 0.262 -- 0.00376 mu_j[2] 3.38 3.81 4.25 3.82 0.221 -- 0.00313 mu_j[3] 3.07 3.91 4.75 3.91 0.425 -- 0.00677 mu_j[4] 3.21 3.74 4.31 3.74 0.285 -- 0.00428 mu_j[5] 3.09 4.15 5.43 4.18 0.588 -- 0.0115 mu_j[6] 2.7 3.84 4.99 3.85 0.576 -- 0.00915 mu_j[7] 2.74 3.53 4.27 3.51 0.388 -- 0.00595 mu_j[8] 3.58 4.12 4.66 4.12 0.276 -- 0.00423 sigma 0.763 0.92 1.12 0.93 0.0923 -- 0.00142 for example, the movies \"how to train your dragon\" (corresponding to \\(\\mu_1\\)) and \"megamind\" (corresponding to \\(\\mu_7\\)) have the lowest average ratings with short \\(90\\%\\) credible intervals, \\((2.96, 3.99)\\) and \\((2.74, 4.27)\\) respectively, whereas \"legend of the guardians: the owls of gaâhoole\" (corresponding to \\(Î¼_6\\)) also has a low average rating but with a wider \\(90\\%\\) credible interval \\((2.70, 4.99)\\). the differences in the width of the credible intervals stem from the sample sizes: there are eleven ratings for \"how to train your dragon\", four ratings for \"megamind\", and only a single rating for \"legend of the guardians: the owls of gaâhoole\". the smaller the sample size, the larger the variability in the inference, even if one pools information across groups. shrinkage recall that the two-stage prior specifies a shared prior normal \\((\\mu, \\tau)\\) for all \\(\\mu_j\\)'s which facilitates simultaneous estimation of the movie mean ratings (the \\(\\mu_j\\)'s), and estimation of the variation among the movie mean ratings through the parameters \\(\\mu\\) and \\(\\tau\\). the posterior mean of the rating for a particular movie \\(\\mu_j\\) shrinks the observed mean rating towards an average rating. the following figure displays a shrinkage plot which illustrates the movement of the observed sample mean ratings towards an average rating. the left side plots the sample movie rating means and lines connect the sample means to the corresponding posterior means (i.e. means of the posterior draws of \\(\\mu_j\\)). the shrinkage effect is obvious for the movie \"batman: under the red hood\" which corresponds to the dot at the value \\(5.0\\) on the left. this movie only received one rating of \\(5.0\\) and its mean rating \\(\\mu_5\\) shrinks to the value \\(4.178\\) on the right, which is still the highest posterior mean among the nine movie posterior means. a large shrinkage is desirable for a movie with a small number of ratings such as \"batman: under the red hood\". for a movie with a small sample size, information about other ratings of similar movies helps to produce a more reasonable estimate at the true average movie rating. the amount of shrinkage is more modest for movies with larger sample sizes. sources of variability we know that the prior distribution \\(\\text{normal}(\\mu, \\tau)\\) is shared among the means \\(\\mu_j\\)'s of all groups in a hierarchical normal model, and the hyperparameters \\(\\mu\\) and \\(\\tau\\) provide information about the population of \\(\\mu_j\\)'s. specifically, the standard deviation \\(\\tau\\) measures the variability among the \\(\\mu_j\\)'s. when the hierarchical model is estimated through mcmc, summaries from the simulation draws from the posterior of \\(\\tau\\) provide information about this source of variation after analyzing the data. there are actually two sources for the variability among the observed \\(y_{ij}\\)'s: sampling level: within-group variability: \\begin{align} y_{ij} \\sim \\text{normal}(\\mu_j, \\sigma) \\end{align} group level between-group variability: \\begin{align} \\mu_{j} | \\mu, \\tau \\sim \\text{normal}(\\mu, \\tau) \\end{align} when the hierarchical model is fit through mcmc, summaries from the marginal posterior distributions of \\(\\sigma\\) and \\(\\tau\\) provide information about the two sources of variability. the bayesian posterior inference in the hierarchical model is able to compare these two sources of variability, taking into account the prior belief and the information from the data. one initially provides prior beliefs about the values of the standard deviations \\(\\sigma\\) and \\(\\tau\\) through gamma distributions. what can be said about these two sources of variability after the estimation of the hierarchical model? as seen in the output of print(posterior, digits = 3), the \\(90\\%\\) credible interval for \\(\\sigma\\) is \\((0.763, 1.12)\\) and the \\(90\\%\\) credible interval for \\(\\tau\\) is \\((0.357, 1.08)\\). after observing the data, the within-group variability in the measurements is estimated to be larger than the between-group variability. to compare both variability sources we compute: \\begin{align} r = \\frac{\\tau^2}{\\tau^2 + \\sigma^2} \\end{align} it represents the fraction of the total variability in the movie ratings due to the differences between groups. if the value of \\(r\\) is close to \\(1\\), most of the total variability is attributed to the between-group variability. on the other side, if \\(r\\) is close to \\(0\\), most of the variation is within groups and there is little significant differences between groups. a \\(95\\%\\) credible interval for \\(r\\) is \\((0.149, 0.630)\\). since much of the posterior probability of \\(r\\) is located below the value \\(0.5\\), this confirms that the variation between the mean movie rating titles is smaller than the variation of the ratings within the movie titles in this example. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/11_introduction.html",
    "title": "Introduction",
    "body": " index search search back introduction contents separate estimates combined estimates a two-stage prior as a new example, consider a study in which students' scores of a standardized test such as the sat are collected from five different senior high schools in a given year. it is inappropriate to use \\(y_{ij}\\) as the random variable for the sat score of student \\(i = 1, 2, \\cdots, n_j\\) in school \\(j = 1, \\cdots, 5\\). within school \\(j\\), one assumes that sat scores are i.i.d. from a normal data model with a mean and standard deviation depending on the school: \\begin{align} y_{ij} \\sim \\text{normal}(\\mu_j, \\sigma_j) \\end{align} separate estimates? one approach for handling this group estimation problem is find separate estimates for each school. one focuses on the observations in school \\(j\\), \\(\\{y_{1j}, \\cdots, y_{n_jj}\\}\\), choose a prior distribution \\(\\pi(\\mu_j, \\sigma_j)\\) for the mean and the standard deviation parameters. this \"separate estimates\" approach may be reasonable, especially if the researcher thinks the means and the standard deviations from the five normal models are completely unrelated to each other. that is, oneâs prior beliefs about the parameters of the sat score distribution in one school are unrelated to the prior beliefs about the distribution parameters in another school. combined estimates? another way to handle this group estimation problem is to ignore the fact that there is a grouping variable and estimate the parameters in the combined sample. in our school example, one ignores the school variable and simply assumes that the sat scores \\(y_i\\)'s are distributed from a single normal population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) where \\(i = 1, \\cdots, n\\) is the total number of students from all five schools. using this approach, one is effectively ignoring any differences between the five schools. a two-stage prior is there an alternative approach that compromises between the separate and combined estimate methods? for simplicity of discussion it is assumed the standard deviation \\(\\sigma_j\\) of the \\(j\\)th school is known. consider the collection of five mean parameters, \\(\\{\\mu_1, \\mu_2, \\mu_3, \\mu_4, \\mu_5\\}\\) representing the means of the five schools' sat scores. one believes that the \\(\\mu_j\\)'s are distinct, because each \\(\\mu_j\\) depends on the characteristics of school \\(j\\). one wishes to construct a prior distribution for the five mean parameters that reflects the belief that \\(\\{\\mu_1, \\mu_2, \\mu_3, \\mu_4, \\mu_5\\}\\) are related or similar in size. the prior belief in similarity of the means is constructed in two stages: [stage 1] the prior distribution for the \\(j\\)th mean \\(\\mu_j\\) is normal, where the mean and standard deviation parameters are shared among all \\(\\mu_j\\): \\begin{align} \\mu_j | \\mu, \\tau \\sim \\text{normal}(\\mu, \\tau), j = 1, \\cdots, 5 \\end{align} [stage 2] in stage 1, the parameters \\(\\mu\\) and \\(\\tau\\) are unknown. so this stage assigns the parameters a prior density \\(\\pi\\) (hyperprior): \\begin{align} \\mu, \\tau \\sim \\pi(\\mu, \\tau) \\end{align} stage 1 indicates that the \\(\\mu_j\\)'s a priori are related and thus come from the same distribution. if one considers the limit of the stage 1 prior as the standard deviation \\(\\tau\\) approaches zero, the group means \\(\\mu_j\\) will be identical. then one is in the combined groups' situation where one is pooling the sat data to learn about a single population. at the other extreme, if one allows the standard deviation \\(\\tau\\) of the stage 1 prior to approach infinity, then one is saying that the group means are unrelated and that leads to the separate estimates situation. since \\(\\mu\\) and \\(\\tau\\) are parameters in the prior distribution, they are called hyperparameters. learning about \\(\\mu\\) and \\(\\tau\\) provides information about the population of \\(\\mu_j\\). in bayesian inference, one learns about \\(\\mu_j\\) and \\(\\tau\\) by specifying a hyperprior distribution and performing inference based on the posterior distribution. it will be seen that the hierarchical model posterior estimates for one school borrows information from other schools. this process is often called partial pooling information among groups. from the structural point of view, due to the two stages of the model, this approach is called hierarchical or multilevel modeling. in essence, hierarchical modeling takes into account information from multiple levels, acknowledging differences and similarities among groups. in the posterior analysis, one learns simultaneously about each group and learns about the population of groups by pooling information across groups. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/07_weakly_informative_priors.html",
    "title": "Weakly Informative Priors",
    "body": " index search search back weakly informative priors contents concepts relating to the choice of prior distribution improper limit of a prior distribution calibration classes of noninformative and weakly informative prior distributions for hierarchical variance parameters general considerations uniform prior distributions [[#inverse-gamma\\((\u001bpsilon, \u001bpsilon)\\) prior distributions|inverse-gamma\\((\u001bpsilon, \u001bpsilon)\\) prior distributions]] half-cauchy prior distributions application to the 8-schools example application for the 3-schools problem concepts relating to the choice of prior distribution improper limit of a prior distribution improper prior densities can, but do not necessarily, lead to proper posterior distributions. calibration posterior inferences can be evaluated using the concept of calibration of the posterior mean. for any parameter \\(\\theta\\), if we label the posterior mean as \\(\\hat{\\theta} = \\mathbb{e}[\\theta|y]\\), we can define the miscalibration of the posterior mean as \\(\\mathbb{e}[\\theta|\\hat{\\theta}] - \\hat{\\theta}\\). we can judge the accuracy of our conclusions from bayesian analysis by checking how close the average value we predict (the posterior mean) is to the true value. here's how it works: if we call our predicted average value \\(\\hat{\\theta}\\), and we calculate how far off it is from the true value \\(\\theta\\), that's what we call the miscalibration of the prediction. if our initial guesses (prior distribution) are accurate and our data matches those guesses, then our predictions will be right on target, meaning the miscalibration will be zero. these are models where the probabilities don't add up to \\(1\\), which makes it impossible to draw a parameter \\(\\theta\\) from them. so, we need to expand our theory to deal with this. to see if our predictions are accurate in these cases, we need to imagine a \"true\" prior distribution where \\(\\theta\\) comes from, and compare it to the \"inferential\" prior distribution we actually use for our bayesian analysis. let's take the example of the 8 schools model. here, we consider an improper uniform distribution on \\(\\tau\\) (a parameter in the model) as a limit of uniform distributions on a range (from \\(0\\) to a really large number \\(a\\), which is getting bigger and bigger). when we use this improper uniform distribution, our inferences tend to overestimate \\(\\tau\\). let's see why: if both the \"true\" and \"inferential\" prior distributions are uniform on \\((0, a)\\), our miscalibration is zero. this means our predictions are accurate. now, if we keep the \"true\" prior distribution as \\(u(0, a)\\) but let the \"inferential\" prior distribution go to \\(u(0, \\infty)\\), our predictions tend to increase (because now we're including more and more extreme values of \\(\\theta\\)), which leads to a positive miscalibration. classes of noninformative and weakly informative prior distributions for hierarchical variance parameters general considerations we view any noninformative or weakly informative prior distribution as inherently provisionalâafter the model has been fit, one should look at the posterior distribution and see if it makes sense. if the posterior distribution does not make sense, this implies that additional prior knowledge is available that has not been included in the model, and that contradicts the assumptions of the prior distribution that has been used uniform prior distributions when we're setting up our model, we often start with uniform priors. but we have to be careful about how we define the scale of this uniform distribution. one common situation is when we're dealing with parameters that must be positive, like variance parameters. using a uniform prior on the logarithm of these parameters (\\(\\log \\tau\\)) might seem like a good idea, but it can lead to problems because the resulting posterior distribution becomes improper (doesn't add up to 1). an alternative is trying to set up a range for our prior distribution, like \\([-a, a]\\) where \\(a\\) is a really large number. this seems like a good idea to keep things in check, but there's a catch: the posterior distribution (our updated belief after looking at the data) can end up heavily influenced by the lower bound, \\(-a\\), of our range. when we calculate the marginal likelihood \\(p(y|\\tau)\\) of our data given a certain parameter (\\(\\tau\\)), it ends up approaching a fixed, non-zero value as \\(\\tau\\) gets really close to \\(0\\). because when we calculate the likelihood of our data given a parameter (\\(\\tau\\)), it's like asking, \"how likely is it that we'd see this data if our parameter Ï were true?\" now, imagine \\(\\tau\\) is getting really close to \\(0\\). in many situations, this means we're saying there's almost no variability in our data. but even if \\(\\tau\\) is very close to \\(0\\), the likelihood of observing our data isn't exactly \\(0\\). there's still some chance, even if it's tiny, that we'd see our data just by random chance, even with very little variability. so, as \\(\\tau\\) approaches \\(0\\), the likelihood doesn't drop to \\(0\\) as well. instead, it approaches a fixed, non-zero value. another option we can consider is using a uniform prior distribution directly on the parameter \\(\\tau\\) itself. this helps avoid some of the problems we discussed earlier because it keeps the total probability finite, especially near \\(\\tau = 0\\). however, there's a drawback to this approach. it tends to lean slightly towards positive values, because it allows for the possibility of very large values of \\(\\tau\\) as well. when we're dealing with just one or two groups (\\(j = 1\\) or \\(2\\)), using this uniform prior actually results in an improper posterior density. this means that our analysis essentially concludes that \\(\\tau\\) is infinite, and it doesn't do any pooling of data from different groups. in a way, this makes sense because it's hard to decide from just a few groups how much we should pool their data together. but from a bayesian perspective, it's a bit awkward because we're making this decision before even looking at the data. when we're dealing with these improper uniform prior distributions, we can think of them as being like the limit of certain types of weakly informative priors. for example, the uniform prior distribution on the logarithm of \\(\\tau\\) is basically like saying that \\(\\tau\\) follows a distribution where the probability decreases as \\(\\tau\\) gets bigger. sometimes, in bayesian statistics, people suggest using a uniform prior distribution directly on \\(\\tau^2\\). this means that every possible value for \\(\\tau^2\\) is considered equally likely. however, we don't recommend this approach. it tends to have a bigger issue with miscalibration towards higher values compared to the other approaches we discussed earlier. plus, using this uniform prior on Ï squared requires us to have at least 4 groups for the analysis to work properly and give us a reasonable posterior distribution. inverse-gamma\\((\\epsilon, \\epsilon)\\) prior distributions in the schools model, the parameter \\(\\tau\\) doesn't have any simple family of prior distributions that work well because its likelihood depends on all the data from all the groups in a complex way. however, there's a kind of distribution called the inverse-gamma family that works well in this situation. this means that if we use an inverse-gamma distribution as a prior for \\(\\tau^2\\), then after we collect our data and update our beliefs, the conditional distribution for \\(\\tau^2\\), \\(p(\\tau^2|\\mu, \\theta, y)\\) that we get is still an inverse-gamma distribution. the inverse-gamma prior distribution is a way to set up our beliefs in a noninformative (or weakly informative) manner when we're dealing with certain types of data. we choose a parameter called alpha (\\(\\alpha\\)) to control how informative the prior is. now, here's the thing: if we set alpha to a very low value, like \\(1\\) or \\(0.01\\) or \\(0.001\\), it's supposed to mean we're not putting much prior information into our model. but there's a problem: when we make alpha too small, the posterior distribution (our updated beliefs after looking at the data) can end up being improper, which means it doesn't add up to \\(1\\). to avoid this, we need to set alpha to a reasonable value, not too small. half-cauchy prior distributions we're going to look at another type of distribution called the \\(t\\) family, specifically the \\(\\text{half}-t\\) because our scale parameter (\\(\\tau\\)) has to be positive. now, we're interested in the \\(t\\) family for this problem because it's pretty flexible and can cover a wide range of situations. plus, we can use a neat trick called reparameterization to express it as a prior distribution for our scale parameter (\\(\\tau\\)) in a way that works well with our model. here's why it's helpful: the half-cauchy distribution has a wide peak around zero and just one parameter that we can adjust, which we'll call \\(a\\). we can set \\(a\\) to be a large value, and as it gets bigger and bigger (approaching infinity), the half-cauchy distribution starts to look more like a uniform distribution on our parameter \\(\\tau\\). when we set \\(a\\) to a large but finite value, it means we're using a slightly informative prior distribution. even though it's not completely flat, it's still pretty gentle, especially in the tails. this means that even if we have some prior beliefs, the data we collect can still have a big influence on our final results, especially if the data is strong. so, we're going to use the half-cauchy distribution for situations where we're estimating variance parameters from just a few groups. in these cases, our choices about our prior beliefs can really affect our results, so we want to use a prior distribution that's flexible and doesn't have a strong influence unless the data really supports it. application to the 8-schools example we demonstrate the properties of some proposed noninformative prior densities on the eight-schools example of section 5. figure 5.9 displays the posterior distributions for the 8-schools model resulting from three different choices of prior distributions that are intended to be noninformative. the first histogram (on the left) shows what we think about the parameter \\(\\tau\\) when we use a uniform prior distribution. the data suggest that \\(\\tau\\) could be anywhere below \\(20\\), but there's a small chance it could be even larger. this makes sense because we only have data from \\(8\\) groups, and it's hard to be sure about large values of \\(\\tau\\) with that little data. now, look at the second histogram (in the middle). here, we've changed our prior to something called an \\(\\text{inverse-gamma}(1, 1)\\) distribution. this changes our conclusions. now, our estimate for \\(\\tau\\) is lower, and we're more confident in our estimates for the individual group parameters (\\(\\theta_j\\)'s). to understand why this happens, let's think about the shape of our prior distribution. with the inverse-gamma prior, it's concentrated in a narrow range, from \\(0.5\\) to \\(5\\). this means it's not giving much weight to really large or really small values of \\(\\tau\\). in comparison, the uniform prior seemed less informative, meaning it didn't strongly influence our conclusions. the last histogram (on the right) in figure 5.9 shows what happens when we use a different kind of prior distribution called \\(\\text{inverse-gamma}(0.001, 0.001)\\) for \\(\\tau\\) squared. this prior is very sharply peaked near zero, meaning it puts a lot of emphasis on very small values of \\(\\tau\\). because of this, our conclusions from the data get distorted. even though the data might suggest that \\(\\tau\\) could be larger, the prior is pulling our estimates towards smaller values. the reason this happens is because the likelihood for \\(\\tau\\), stays high near zero. so even though our data might suggest that larger values of \\(\\tau\\) are possible, the strong influence of the prior near zero pulls our estimates towards smaller values. in this example, we're not considering two other options: using a uniform prior distribution on the logarithm of \\(\\tau\\), which would result in an improper posterior density with a spike at \\(\\tau = 0\\), similar to the last histogram but even more pronounced. using a uniform prior distribution directly on \\(\\tau^2\\), which would result in a posterior distribution similar to the first histogram, but with a slightly higher tail on the right side. application for the 3-schools problem the uniform prior distribution seems fine for the 8-school analysis, but problems arise if the number of groups \\(j\\) is much smaller, in which case the data supply little information about the group-level variance, and a noninformative prior distribution can lead to a posterior distribution that is improper or is proper but unrealistically broad. figure 5.10 displays the inferences for \\(\\tau\\) based on two different priors. we start with a default uniform distribution, which means we're not favoring any particular values for our parameter \\(\\tau\\). this worked well when we had data from \\(8\\) groups (as seen in figure 5.9). but now, we're looking at a new dataset with only \\(3\\) groups. unfortunately, the resulting histogram (the left one in figure 5.10) shows that the posterior distribution for \\(\\tau\\) has a really long tail on the right side. this means it's suggesting values of \\(\\tau\\) that are way too high to be reasonable. this long tail is expected because we have such a small number of groups (if we had even fewer groups, the tail would be even longer, going on forever). using this kind of posterior distribution can be a problem because it means we're not pooling the estimates of the school effects (\\(\\theta_j\\)) as much as we should be. the last histogram (on the right) in figure 5.10 shows what happens when we use a different kind of prior distribution called a half-cauchy. we set the scale parameter (\\(a\\)) of this prior to \\(25\\). we chose this value to be a bit higher than what we expect for the standard deviation of the underlying \\(\\theta_j\\)'s in our educational testing example. this way, our model only weakly constrains the parameter \\(\\tau\\). on the graph, you'll see a line that represents this prior distribution. it's highest for values of \\(\\tau\\) less than \\(50\\) and gradually falls off beyond that. this means the prior puts more weight on smaller values of \\(\\tau\\) but still allows for larger values. this half-cauchy prior distribution would also perform well in the 8-schools problem; however it was unnecessary because the default uniform prior gave reasonable results. with only 3 schools, we went to the trouble of using a weakly informative prior, a distribution that was not intended to represent our actual prior state of knowledge about \\(\\tau\\) but rather to constrain the posterior distribution, to an extent allowed by the data. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/03.html",
    "title": "Bayesian analysis of conjugate hierarchical models",
    "body": " index search search back bayesian analysis of conjugate hierarchical models contents analytic derivation of conditional and marginal distributions drawing simulations from the posterior distribution application to the model for rat tumors joint conditional and marginal posterior distributions choosing a standard parameterization and setting up a noninformative hyperprior distribution computing the marginal posterior density of the hyperparameters sampling from the joint posterior distribution of parameters and hyperparameters displaying the results analytic derivation of conditional and marginal distributions hierarchical models involve multiple levels of parameters and dependencies between them, making the analysis more intricate. the following steps are necessary to disentangle the relationships between parameters at different levels of the hierarchy and to estimate their distributions accurately. joint posterior density: combines the prior information (hyperprior distribution \\(p(\\phi)\\)), the population distribution (\\(p(\\theta|\\phi)\\)), and the likelihood function \\(p(y|\\theta)\\) to form the joint posterior distribution. \\begin{align} p(\\theta, \\phi|y) \\propto p(y|\\theta)p(\\theta|\\phi)p(\\phi) \\end{align} conditional posterior density of the parameters: calculates the posterior distribution of \\(\\theta\\) given the hyperparameters \\(\\phi\\), allows us to understan how parameters interact and influence each other. this is usually done using a priori conjugate distributions. \\begin{align} p(\\theta|\\phi, y) \\end{align} hyperparameter estimation: estimating \\(\\phi\\) through the bayesian paradigm helps in updating our knowledge about the higher-level parameters based on the observed data. this step can be perfomed by integrating the joint posterior distribution over \\(\\theta\\) in to be able to marginalize \\(\\phi\\) conditionally on \\(y\\). \\begin{align} p(\\phi|y) = \\int p(\\theta, \\phi|y)d\\theta \\end{align} for many standard models the marginal posterior distribution of \\(\\phi\\) can be computed algebraically using the conditional probability formula: \\begin{align} p(\\phi|y) = \\frac{p(\\theta, \\phi|y)}{p(\\theta|\\phi, y)} \\end{align} drawing simulations from the posterior distribution the following strategy is useful for simulating a draw from the joint posterior distribution \\(p(\\theta, \\phi|y)\\) draw the vector of hyperparameters, \\(\\phi\\), from its marginal posterior distribution, \\(p(\\phi|y)\\). draw the parameter vector \\(\\theta\\) from its conditional posterior distribution, \\(p(\\theta|\\phi, y)\\). for the examples we consider in this chapter, the factorization \\(p(\\theta|\\phi, y) = \\prod_j p(\\theta_j|\\phi, y)\\) holds. if desired, draw predictive values \\(\\tilde{y}\\) from the posterior predictive distribution. the above steps are performed \\(l\\) times in order to obtain a set of \\(l\\) draws. from the joint posterior simulations of \\(\\theta\\) and \\(\\tilde{y}\\), we can compute the posterior distribution of any estimand or predictive quantity of interest. application to the model for rat tumors the data from experiments \\(j = 1, \\cdots, j\\), \\(j = 71\\), are assumed to follow independent binomial distributions: \\begin{align} y_j \\sim bin(n_j, \\theta_j) \\end{align} this models the probability of getting exactly \\(\\theta_j\\) successes in \\(n_j\\) independent bernoulli trials. with the number of rats \\(n_j\\) unknown. the parameters \\(\\theta_j\\) are assumed to be independent samples from a beta distribution: \\begin{align} \\theta_j \\sim beta(\\alpha, \\beta) \\end{align} and we shall assign a noninformative hyperprior distribution to reflect our ignorance about the unknown hyperparameters \\(\\alpha, \\beta\\). we defer the choice of noninformative hyperprior distribution, a relatively arbitrary and unimportant part of this particular analysis, until we inspect the integrability of the posterior density. joint, conditional, and marginal posterior distributions the joint posterior distribution of all the parameters is: \\begin{align} p(\\theta, \\alpha, \\beta|y) \\propto p(\\alpha, \\beta)p(\\theta|\\alpha,\\beta)p(y|\\theta, \\alpha, \\beta) \\end{align} where \\(p(\\alpha, \\beta)\\) is the hyperprior distribution (\\(p(\\phi)\\)). then \\(p(\\theta|\\alpha,\\beta)\\) is the population distribution (\\(p(\\theta|\\phi)\\)). the pdf of \\(x \\sim beta(\\alpha, \\beta)\\), ignoring the normalization constant, is given by: \\begin{align} p(\\theta|\\alpha, \\beta) \\propto \\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)} x^{\\alpha - 1} (1- x)^{\\beta - 1} \\end{align} for \\(j = 1, \\cdots, j\\) i.i.d \\(\\theta_j \\sim beta(\\alpha, \\beta)\\): \\begin{align} p(\\theta|\\alpha, \\beta) \\propto \\prod_{j=1}^j \\theta_j^{\\alpha - 1} (1- \\theta_j)^{\\beta - 1} \\end{align} and \\(p(y|\\theta, \\alpha, \\beta)\\) is the likelihood (\\(p(y|\\theta)\\)). the pdf of \\(x \\sim bin(n, p)\\) is given by: \\begin{align} p(x = k|n, p) \\propto p^k (1 - p)^{n - k} \\end{align} for \\(j = 1, \\cdots, j\\) i.i.d \\(y_j \\sim bin(n_j, \\theta_j)\\): \\begin{align} p(y_j|n_j, \\theta_j) \\propto \\theta_j^{y_j} (1 - \\theta_j)^{n_j - y_j} \\end{align} therefore we obtain: \\begin{align} \\propto p(\\alpha, \\beta) \\left(\\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)}\\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1}\\right) \\left(\\prod_{j=1}^j \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}\\right) \\end{align} the conditional posterior density of \\(\\theta\\) given the hyperparameters is defined using a beta-binomial conjugate prior (page 7), therefore if: \\begin{align} y_i|n_j, \\theta_j \\sim bin(n_j, \\theta_j) \\end{align} \\begin{align} \\theta_j|\\alpha,\\beta \\sim beta(\\alpha, \\beta) \\end{align} then \\begin{align} \\theta_j|\\alpha, \\beta, y_j, n_j \\sim beta(\\alpha + y_j, \\beta + n_j - y_j) \\end{align} which gives us the following pdf for a beta distribution of i.i.d \\(\\theta\\): \\begin{align} p(\\theta|\\alpha, \\beta, y) = \\prod_{j=1}^j \\frac{\\gamma(\\alpha + y_j + \\beta + n_j - y_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1} \\end{align} \\begin{align} = \\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta + n_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1} \\end{align} we can determine the marginal posterior distribution of the hyperparameters \\((\\alpha, \\beta)\\) by substituting on the previous equations on the following formula: \\begin{align} p(\\phi|y) = \\frac{p(\\theta, \\phi|y)}{p(\\theta|\\phi, y)} \\end{align} where \\(\\phi = (\\alpha, \\beta)\\), so: \\begin{align} p(\\alpha, \\beta|y) = \\frac{p(\\theta, \\alpha, \\beta|y)}{p(\\theta|\\alpha, \\beta, y)} \\end{align} \\begin{align} \\propto \\frac{p(\\alpha, \\beta) \\left(\\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)}\\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1}\\right) \\left(\\prod_{j=1}^j \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}\\right)}{\\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta + n_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1}} \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\frac{\\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)}\\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1} \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}}{\\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta + n_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1}} \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\prod_{j=1}^j \\frac{\\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)}\\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1} \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}}{\\frac{\\gamma(\\alpha + \\beta + n_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1}} \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\prod_{j=1}^j \\left(\\frac{\\frac{\\gamma(\\alpha + \\beta)}{\\gamma(\\alpha)\\gamma(\\beta)}}{\\frac{\\gamma(\\alpha + \\beta + n_j)}{\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}}\\frac{\\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1} \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}}{\\theta_j^{\\alpha + y_j - 1}(1-\\theta_j)^{\\beta + n_j - y_j - 1}}\\right) \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\prod_{j=1}^j \\left(\\frac{\\gamma(\\alpha + \\beta)\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}{\\gamma(\\alpha)\\gamma(\\beta)\\gamma(\\alpha + \\beta + n_j)}\\theta_j^{\\alpha - 1 + y_j - \\alpha - y_j + 1}(1-\\theta_j)^{\\beta - 1 + n_j - y_j - \\beta - n_j + y_j + 1}\\right) \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\prod_{j=1}^j \\left(\\frac{\\gamma(\\alpha + \\beta)\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}{\\gamma(\\alpha)\\gamma(\\beta)\\gamma(\\alpha + \\beta + n_j)}\\theta_j^{0}(1-\\theta_j)^{0}\\right) \\end{align} \\begin{align} \\propto p(\\alpha, \\beta) \\prod_{j=1}^j \\frac{\\gamma(\\alpha + \\beta)\\gamma(\\alpha + y_j)\\gamma(\\beta + n_j - y_j)}{\\gamma(\\alpha)\\gamma(\\beta)\\gamma(\\alpha + \\beta + n_j)} \\end{align} choosing a standard parameterization and setting up a 'noninformative' hyperprior distribution because we have no immediately available information about the distribution of tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribution for \\((\\alpha, \\beta)\\). by reparameterizing the hyperparameters, we transform them into a space that may have more intuitive or meaningful interpretations. in this case, \\(logit(\\frac{\\alpha}{\\alpha + \\beta}) = \\log(\\frac{\\alpha}{\\beta})\\) represents the log-odds of \\(\\alpha\\) relative to the total of \\(\\alpha\\) and \\(\\beta\\), providing a clear interpretation of the prior mean in the beta distribution for \\(\\theta\\). similarly, \\(\\log(\\alpha + \\beta)\\) captures the logarithm of the \"sample size,\" influencing the precision or spread of the distribution. also the logit transformation helps stabilize the numerical computations, especially when dealing with probabilities or proportions that are bounded between 0 and 1. by working in the logit space, we avoid issues related to extreme values or boundaries that can arise in the original parameter space. and transforming the hyperparameters can facilitate the specification of appropriate prior distributions. one reasonable choice of diffuse hyperprior density is uniform on \\((\\frac{\\alpha}{\\alpha + \\beta}, (\\alpha + \\beta)^{â1/2})\\), which when multiplied by the appropriate jacobian yields the following densities on the original scale, \\begin{align} p(\\alpha, \\beta) \\propto (\\alpha + \\beta)^{-5/2} \\end{align} and on the natural transformed scale: \\begin{align} p(\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta)) \\propto \\alpha\\beta(\\alpha + \\beta)^{-5/2} \\end{align} computing the marginal posterior density of the hyperparameters now that we have established a full probability model for data and parameters, we compute the marginal posterior distribution of the hyperparameters. the next figure shows a contour plot of the unnormalized marginal posterior density on a grid of values of \\((\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta))\\) to create the plot, we first compute the logarithm of the density function of \\(p(\\alpha, \\beta|y)\\) with prior density \\(p(\\alpha, \\beta) \\propto (\\alpha + \\beta)^{-5/2}\\), multiplying by the jacobian to obtain the density \\(p(\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta)|y)\\) the most obvious features of the contour plot are (1) the mode is not far from the point estimate (as we would expect), and (2) important parts of the marginal posterior distribution lie outside the range of the graph. we recompute the previous pdf in a different range \\((\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta)) \\in [-2.3, -1.3] \\times [1, 5]\\). figure \\(5.3b\\) displays \\(1000\\) random draws from the numerically computed posterior distribution. the graphs show that the marginal posterior distribution of the hyperparameters, under this transformation, is approximately symmetric about the mode, roughly \\((â1.75, 2.8)\\). this corresponds to approximate values of \\((\\alpha, \\beta) = (2.4, 14.0)\\), which differs somewhat from the crude estimate obtained earlier. having computed the relative posterior density at a grid that covers the effective range of \\((\\alpha, \\beta)\\), we normalize by approximating the distribution as a step function over the grid and setting the total probability in the grid to \\(1\\). sampling from the joint posterior distribution of parameters and hyperparameters we draw \\(1000\\) random samples from the joint posterior distribution of \\((\\alpha, \\beta, \\theta_1, \\cdots, \\theta_j)\\), as follows. simulate \\(1000\\) draws of \\((\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta))\\) from their posterior distribution using the same discrete-grid sampling procedure used to draw \\((\\alpha, \\beta)\\) for figure \\(3.3b\\). for \\(l = 1, \\cdots, 1000\\): transform the \\(l\\)th draw of \\((\\log(\\frac{\\alpha}{\\beta}), \\log(\\alpha + \\beta))\\) to the scale \\((\\alpha, \\beta)\\) to yield a draw of the hyperparameters from their marginal posterior distribution. for each \\(j = 1, \\cdots, j\\), sample \\(\\theta_j\\) from its conditional posterior distribution, \\(\\theta_j|\\alpha, \\beta, y \\sim beta(\\alpha + y_j, \\beta + n_j â y_j)\\). displaying the results figure \\(5.4\\) shows posterior medians and \\(95\\%\\) intervals for the \\(\\theta_j\\)âs, computed by simulation. the results are superficially similar to what would be obtained based on a point estimate of the hyperparameters, which makes sense in this example, because of the fairly large number of experiments. but key differences remain, notably that posterior variability is higher in the full bayesian analysis, reflecting posterior uncertainty in the hyperparameters. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/04_normal_model.html",
    "title": "Normal model with exchangeable parameters",
    "body": " index search search back normal model with exchangeable parameters contents model definition inference joint posterior distribution the conditional posterior distribution the marginal posterior distribution [[#posterior distribution of \\(\\mu\\) given \\(\tau\\)|posterior distribution of \\(\\mu\\) given \\(\tau\\)]] [[#posterior distribution \\(\tau\\)|posterior distribution \\(\tau\\)]] simulation posterior predictive distributions model definition we now present a full treatment of a simple hierarchical model based on the normal distribution, with different means for each \"group\" but with known observation variance and a normal population distribution for the group means. consider \\(j\\) independent experiments. the likelihood (sampling distribution) is defined as: \\begin{align} y_{ij} | \\theta_j \\sim \\text{n}(\\theta_j, \\sigma^2), \\text{ for } i = 1, \\cdots, n_j; j = 1, \\cdots, j. \\end{align} where we label the sample mean of each group \\(j\\) as: \\begin{align} \\overline{y}_j = \\frac{1}{n_j} \\sum_{i = 1}^{n_j} y_{ij} \\end{align} and the sampling variance as: \\begin{align} \\sigma^2_j = \\frac{\\sigma^2}{n_j} \\end{align} here we assume that \\(\\sigma\\) is a know value. we can then write the likelihood for each \\(\\theta_j\\) using the sufficient statistics, \\(\\overline{y}_j\\): \\begin{align} \\overline{y}_j | \\theta_j \\sim \\text{n}(\\theta_j, \\sigma_j^2) \\end{align} sufficient statistics are summary statistics of the data that capture all the information about the parameter of interest. in this case, the sufficient statistic \\(\\overline{y}_j\\) represents the data summary for experiment \\(j\\) that is used to estimate the parameter \\(\\theta_j\\). by using the sufficient statistic \\(\\overline{y}_j\\), the likelihood function for each \\(\\theta_j\\) is constructed based on the observed data in experiment \\(j\\). the prior distribution over \\(\\theta_j\\), assuming the prior to be normal for the sake of conjugacy is defined as: \\begin{align} \\theta_j|\\mu, \\tau \\sim \\text{n}(\\theta_j|\\mu, \\tau^2) \\end{align} assuming each \\(\\theta_j\\) to be independent we obtain the following joint distribution: \\begin{align} p(\\theta_1, \\cdots, \\theta_j|\\mu, \\tau) = \\prod_{j=1}^j \\text{n}(\\theta_j|\\mu, \\tau^2) \\end{align} and by process of marginalization: \\begin{align} p(\\theta_1, \\cdots, \\theta_j) = \\int \\left[\\prod_{j=1}^j \\text{n}(\\theta_j|\\mu, \\tau^2)\\right]p(\\mu, \\tau) d(\\mu, \\tau) \\end{align} the hyperprior over the parameters \\(\\mu\\) and \\(\\tau\\) is defined as a non-informative distribution (i.e. uniform density), such that: \\begin{align} p(\\mu, \\tau) = p(\\mu|\\tau)p(\\tau) \\propto p(\\tau) \\end{align} we define a prior distribution over \\(\\tau\\). for our illustrative analysis, we use the uniform prior distribution \\(p(\\tau) \\propto 1\\). once an initial analysis is performed using the noninformative 'uniform' prior density, a sensitivity analysis with a more realistic prior distribution is often desirable. inference joint posterior distribution this distribution combines prior information (hyperprior distribution \\(p(\\mu, \\tau)\\)) the population distribution \\(p(\\theta_j|\\mu, \\tau)\\) and the likelihood function \\(p(y_{ij}|\\theta_j)\\). we define it as follows: \\begin{align} p(\\theta, \\mu, \\tau|y) \\end{align} by bayes theorem (ignoring the normalization term): \\begin{align} \\propto p(y|\\theta) p(\\theta|\\mu, \\tau) p(\\mu, \\tau) \\end{align} here \\(p(y|\\theta)\\) is the likelihood function previously defined in terms of the sufficient statistics \\(\\overline{y}_j\\) \\begin{align} \\propto \\left[\\prod_{j=1}^j \\text{n}(\\overline{y}_j|\\theta_j, \\sigma_j^2)\\right] p(\\theta|\\mu, \\tau) p(\\mu, \\tau) \\end{align} and \\(p(\\theta|\\mu, \\tau)\\) is the prior, also previouly defined, such that: \\begin{align} \\propto \\left[\\prod_{j=1}^j \\text{n}(\\overline{y}_j|\\theta_j, \\sigma_j^2)\\right] \\left[\\prod_{j=1}^j \\text{n}(\\theta_j|\\mu, \\tau^2) p(y|\\theta)\\right] p(\\mu, \\tau) \\end{align} where we can ignore factors that depend only on \\(y\\) and the parameters \\(\\sigma_j\\), which are assumed known for this analysis. the conditional posterior distribution the conditional posterior distribution calculates the posterior distribution of \\(\\theta_j\\) given the hyperparameters \\(\\mu, \\tau\\). it allows us to understand how parameters intereact and influence each other. we define them for each \\(\\theta_j\\) as follows: \\begin{align} \\theta_j | \\mu, \\tau, y \\sim \\text{n}(\\hat{\\theta}_j, v_j) \\end{align} where: \\begin{align} \\hat{\\theta}_j = \\frac{\\frac{1}{\\sigma_j^2}\\overline{y}_j + \\frac{1}{\\tau^2}\\mu}{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}} \\end{align} and: \\begin{align} v_j = \\frac{1}{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}} \\end{align} the marginal posterior distribution this distribution allows us to estimate the hyperparameters \\(\\mu\\) and \\(\\tau\\) through the bayesian paradigm. by the conditional rule we obtain: \\begin{align} p(\\mu, \\tau|y) \\propto p(\\mu, \\tau)p(y|\\mu, \\tau) \\end{align} where: \\begin{align} \\overline{y}_j | \\mu, \\tau \\sim \\text{n}(\\mu, \\sigma_j^2 + \\tau^2) \\end{align} such that: \\begin{align} p(\\mu, \\tau|y) \\propto p(\\mu, \\tau) \\prod_{j=1}^j \\text{n}(\\overline{y}_j|\\mu, \\sigma^2_j + \\tau^2) \\end{align} posterior distribution of \\(\\mu\\) given \\(\\tau\\) we can further simplify by integrating over \\(\\mu\\), leaving a simple univariate numerical computation of \\(p(\\tau|y)\\), by the conditional rule: \\begin{align} p(\\mu, \\tau|y) = p(\\mu | \\tau, y) p(\\tau|y) \\end{align} where: \\begin{align} \\mu | \\tau, y \\sim \\text{n}(\\hat{\\mu}, v_\\mu) \\end{align} with: \\begin{align} \\hat{\\mu} = \\frac{\\sum_{j=1}^j \\frac{1}{\\sigma_j^2 + \\tau^2}\\overline{y}_j}{\\sum_{j=1}^j \\frac{1}{\\sigma_j^2 + \\tau^2}} \\end{align} and \\begin{align} v_{\\mu}^{-1} = \\sum_{j=1}^j \\frac{1}{\\sigma_j^2 + \\tau^2} \\end{align} posterior distribution \\(\\tau\\) we know from the previous section that: \\begin{align} p(\\mu, \\tau|y) = p(\\mu | \\tau, y) p(\\tau|y) \\end{align} such that: \\begin{align} p(\\tau|y) = \\frac{p(\\mu, \\tau|y)}{p(\\mu | \\tau, y)} \\end{align} we previously defined \\(\\mu | \\tau, y \\sim \\text{n}(\\hat{\\mu}, v_\\mu)\\), therefore: \\begin{align} p(\\tau|y) = \\frac{p(\\mu, \\tau|y)}{\\text{n}(\\hat{\\mu}, v_\\mu)} \\end{align} we also defined \\(p(\\mu, \\tau|y) \\propto p(\\mu, \\tau) \\prod_{j=1}^j \\text{n}(\\overline{y}_j|\\mu, \\sigma^2_j + \\tau^2)\\): \\begin{align} \\propto \\frac{p(\\tau) \\prod_{j=1}^j \\text{n}(\\overline{y}_j|\\mu, \\sigma^2_j + \\tau^2)}{\\text{n}(\\hat{\\mu}, v_\\mu)} \\end{align} simulation for this model, computation of the posterior distribution of Î¸ is most conveniently performed via simulation, following the factorization: \\begin{align} p(\\theta, \\mu, \\tau|y) = p(\\theta|\\mu, \\tau, y) p(\\mu|\\tau, y) p(\\tau, y) \\end{align} posterior predictive distributions to obtain a draw from the posterior predictive distribution of new data \\(\\tilde{y}\\) from the current batch of parameters, \\(\\theta\\), first obtain a draw from \\(p(\\theta, \\mu, \\tau|y)\\) and then draw the predictive data \\(\\tilde{y}\\) from the sampling distribution: \\begin{align} y_{ij} | \\theta_j \\sim \\text{n}(\\theta_j, \\sigma^2), \\text{ for } i = 1, \\cdots, n_j; j = 1, \\cdots, j. \\end{align} to obtain posterior predictive simulations of new data \\(\\tilde{y}\\) for \\(\\tilde{j}\\) new groups, perform the following three steps: draw \\((\\mu, \\tau)\\) from their posterior distribution \\(p(\\mu, \\tau|y)\\) draw \\(\\tilde{j}\\) new parameters \\(\\tilde{\\theta} = \\tilde{\\theta}_1, \\cdots, \\tilde{\\theta}_{\\tilde{j}}\\) from the population distribution \\(p(\\tilde{\\theta}|\\mu, \\tau)\\). draw \\(\\tilde{y}\\) given \\(\\tilde{\\theta}\\) from the sampling distribution. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/MBJ/T2/13_hierarchical_beta_binomial_modeling.html",
    "title": "Hierarchical Beta-Binomial Modeling",
    "body": " index search search back hierarchical beta-binomial modeling contents example deaths after heart attack a hierarchical beta-binomial model graphical representation inference through mcmc second-stage prior mcmc diagnostics and summarization example: deaths after heart attack the new york state (nys) department of health collects and releases data on mortality after a heart attack. we focus on 13 hospitals in manhattan, new york city, with the goal of learning about the percentages of resulted deaths from heart attack for hospitals in this sample. a hierarchical beta-binomial model treating âcasesâ as trials and âdeathsâ as successes, the binomial sampling model is a natural choice for this data, and the objective is to learn about the death probability \\(p\\) of the hospitals. if one creates thirteen separate binomial sampling models, one for each hospital, and conducts separate inferences, one loses the ability to use potential information about the death rate from hospital \\(i\\) when making inference about that of a different hospital \\(j\\). since these are all hospitals in manhattan, new york city, they may share attributes in common related to death rates from heart attack. therefore, one builds a hierarchical model based on a common beta distribution that generalizes the beta-binomial conjugate model described in chapter 7. let \\(y_i\\) denote the number of resulted deaths from heart attack, \\(n_i\\) the number of heart attack cases, and \\(p_i\\) the death rate for hospital \\(i\\). so the sampling and first stage of the prior of our model is written as follows: sampling for \\(i = 1, \\cdots, 13\\): \\begin{align} y_i \\sim \\text{binomial}(n_i, p_i) \\end{align} prior for \\(p_i\\), \\(i = 1, \\cdots, 13\\): \\begin{align} p_i \\sim \\text{beta}(a, b) \\end{align} note that the hyperparameters \\(a\\) and \\(b\\) are shared among all hospitals. if \\(a\\) and \\(b\\) are known values, then the posterior inference for \\(p_i\\) of hospital \\(i\\) is simply another beta distribution by conjugacy (refer to beta-binomial conjugate prior (page 7)): \\begin{align} p_i | y_i \\sim \\text{beta}(a + y_i, b + n_i - y_i) \\end{align} in the general situation where the hyperparameters \\(a\\) and \\(b\\) are unknown, a second stage of the prior \\(\\pi(a, b)\\) needs to specified for these hyperparameters, such that the model is now defined as: sampling for \\(i = 1, \\cdots, 13\\): \\begin{align} y_i \\sim \\text{binomial}(n_i, p_i) \\end{align} prior for \\(p_i\\), stage 1: \\(i = 1, \\cdots, 13\\): \\begin{align} p_i \\sim \\text{beta}(a, b) \\end{align} prior for \\(p_i\\), stage 2: the hyperprior: \\begin{align} a, b \\sim \\pi(a, b) \\end{align} when we start analyzing the new york state heart attack death rate dataset, the specification of this hyperprior distribution \\(\\pi(a, b)\\) will be described. graphical representation one sees that the upper section of the graph represents the sampling density, with the arrow directing from \\(p_i\\) to \\(y_i\\). here the start of the arrow is the parameter and the end of the arrow is the random variable. the lower section of the graph represents the prior, with arrows directing from \\(a\\) and \\(b\\) to \\(p_i\\). inference through mcmc second-stage prior for a \\(\\text{beta}(a, b)\\) prior distribution for a proportion \\(p\\), one considers the parameter \\(a\\) as the prior count of âsuccessesâ, the parameter \\(b\\) as the prior count of \"failures\", and the sum \\(a + b\\) represents the prior sample size. also the expectation is given by \\(\\frac{a}{a + b}\\). from these facts a more natural parametrization of the hyperprior distribution \\(\\pi(a, b)\\) is \\(\\pi(\\mu, \\eta)\\) where \\(\\mu = \\frac{a}{a + b}\\) is the hyperprior mean and \\(\\eta = a + b\\) is the hyperprior sample size. therefore: \\begin{align} \\mu, \\eta \\sim \\pi(\\mu, \\eta) \\end{align} where \\(a = \\mu\\eta\\) and \\(b = (1 - \\mu)\\eta\\). assume \\(\\mu\\) and \\(\\eta\\) are independent which means that one's beliefs about the prior mean are independent of the beliefs about the prior sample size. the hyperprior expectation \\(\\mu\\) is the mean measure for \\(p_i\\), the average death rate across \\(13\\) hospitals. if one has little prior knowledge about the expectation \\(\\mu\\), one assigns this parameter a uniform prior which is equivalent to a \\(\\text{beta}(1, 1)\\) prior. to motivate the prior choice for the hyperparameter sample size \\(\\eta\\), consider the case where the hyperparameter values are known. if \\(y^*\\) and \\(n^*\\) are respectively the number of deaths and number of cases for one hospital, then the posterior mean of death rate parameter \\(p^*\\) is given by (refer to the beta-binomial conjugate definition): \\begin{align} \\mathbb{e}[p^*|y^*] = \\frac{y^* + \\mu\\eta}{n^* + \\eta} \\end{align} with a little algebra, the posterior mean is rewritten as \\begin{align} \\mathbb{e}[p^*|y^*] = (1 - \\lambda)\\frac{y^*}{n^*} + \\lambda\\mu \\end{align} where \\(\\lambda\\) is the shrinkage fraction: \\begin{align} \\lambda = \\frac{\\eta}{n^* + \\eta} \\end{align} the parameter \\(\\lambda\\) falls in the interval \\((0, 1)\\) and represents the degree of shrinkage of the posterior mean away from the sample proportion \\(\\frac{y^*}{n^*}\\) towards the prior mean \\(\\mu\\). suppose one believes a priori that, for a representative sample size \\(n^*\\), the shrinkage \\(\\lambda\\) is uniformly distributed on \\((0, 1)\\). by performing a transformation, this implies that the prior density for the prior sample size \\(\\eta\\) has the form: \\begin{align} \\pi(\\eta) = \\frac{n^*}{(n^* + \\eta)^2}, \\eta > 0 \\end{align} equivalently, the logarithm of \\(\\eta\\), \\(\\theta = \\log(\\eta)\\), has a logistic distribution with location \\(\\log(n^*)\\) and scale \\(1\\). we represent this distribution as \\(logistic(\\log(n^*), 1)\\), with pdf: \\begin{align} \\pi(\\theta) = \\frac{e^{-(\\theta - \\log(n^*))}}{(1 + e^{-(\\theta - \\log(n^*))})^2} \\end{align} with this specification of the hyperparameter distribution, one writes down the complete hierarchical model as follows: sampling for \\(i = 1, \\cdots, 13\\): \\begin{align} y_i \\sim \\text{binomial}(n_i, p_i) \\end{align} prior for \\(p_i\\), stage 1: \\(i = 1, \\cdots, 13\\): \\begin{align} p_i \\sim \\text{beta}(a, b) \\end{align} prior for \\(p_i\\), stage 2: the hyperpriors: \\begin{align} \\mu, \\eta \\sim \\pi(\\mu, \\eta) \\end{align} \\begin{align} \\log \\eta \\sim \\text{logistic}(\\log n^*, 1) \\end{align} where \\(a = \\mu\\eta\\) and \\(b = (1 - \\mu)\\eta\\) mcmc diagnostics and summarization after the diagnostics are performed, one reports posterior summaries of the parameters: from the posterior output, one evaluates the effect of information pooling in the hierarchical model. see figure 10.6 displays a shrinkage plot showing how the sample proportions are shrunk towards the overall death rate. to compare the posterior densities of the different \\(p_i\\), one displays the density estimates in a single graph as in the following figure: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T3/01_introduccion.html",
    "title": "Introduccion",
    "body": " index search search back introduccion contents bibliography while the idea of combining classifiers is not new, there is still a lot of scope for developing new combination approaches, types of features and classifiers used, and novel applications. fumera et al. presented a theoretical and experimental analysis of multiple classifiers with a focus on linear combiners. the authors particularly focused on simple and weighted averaging. their analysis showed that the performance of linear combiners depends on the accuracy of the individual classifiers and the correlation between their outputs. they also showed that the weighted averaging rule outperforms simple averaging. in weighted averaging, finding the optimal weights for each classifier is still an open problem. further research areas can involve the use of meta-heuristic optimization approaches such as the bat algorithm and other related techniques. in [8], polikar discussed the idea of combining classifiers' output labels in comparison with combination using classifiers' continuous outputs as well as their application to ensemble based systems. in combining continuous outputs, the degrees of support given to each class, by the different classifiers, are used by the combination technique. this support is referred to as score in some literatures. the classifiers output labels are mainly combined using majority voting or its variants. the author primarily reviewed conditions for which ensemble based systems outperform individual classifiers. different combination techniques were analyzed in details. in conclusion, the author stated that no single ensemble generation algorithm or combination rule is universally better than others. while there are several review papers in the literature, this paper presents a more updated survey including more recent approaches introduced during the past few years. since the last major review paper [10], several novel techniques have been introduced. these include a signal strength based combining approach [11], a novel bayes voting strategy [12], a modified weighted averaging technique using graph-theoretical clustering [13], a neural network based approach for training the combination rules [14], weighted features combination [15], and hierarchical fuzzy stack generalization [16], among others. bibliography classifiers combination techniques: a comprehensive review [10] s. chitroub, \"classifier combination and score level fusion: concepts and practical aspects,\" int. j. image data fusion, vol. 1, no. 2, pp. 113â135, jun. 2010. [11] h. he and y. cao, \"ssc: a classifier combination method based on signal strength.,\" ieee trans. neural networks learn. syst., vol. 23, no. 7, pp. 1100â17, jul. 2012. [12] c. de stefano, f. fontanella, and a. s. di freca, \"a novel naive bayes voting strategy for combining classifiers.,\" in icfhr, 2012, pp. 467â472. [13] j. hou, z.-s. feng, and b.-p. zhang, \"a graph-theoretic approach to classifier combination,\" in acoustics, speech and signal processing (icassp), 2012 ieee international conference on, 2012, pp. 1017â1020. [14] y.-d. lan and l. gao, \"a new model of combining multiple classifiers based on neural network,\" 2013 fourth int. conf. emerg. intell. data web technol., no. 2, pp. 154â159, sep. 2013. [15] h. kuang, x. zhang, y.-j. li, l. l. h. chan, and h. yan, \"nighttime vehicle detection based on bio-inspired image enhancement and weighted score-level feature fusion,\" ieee trans. intell. transp. syst., vol. 18, no. 4, pp. 927â936, apr. 2017. [16] c. senaras, m. ozay, and f. t. yarman vural, \"building detection with decision fusion,\" 2013. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T3/04_conclusion.html",
    "title": "Conclusiones",
    "body": " index search search back conclusiones while a large number of combination techniques have been proposed, the literature still lacks a comprehensive performance analysis of such techniques for a given application. the review showed that while one strategy (e.g. fusion at decision level) may outperform others for a given application, the results from such a strategy may not be the best for another application. however overall, it was shown that classifiers combinations in general improve performance significantly over individual classifiers for most problems. an important research direction relies on adding an enhancement stage (post processing) to the classifiers output before applying combination rules. this would improve the performance of individual classifiers before the combination stage. many classifiers combination techniques have performed well under certain restrictions which include independence assumption, gaussian distribution, linear process, limited class problem (mostly 2-class problem) and low dimensional feature space. thus, future work can reconsider relaxing some of these constraints another issue that needs to be further investigated is to explore the advantages of using different strategies for the fusion including probabilistic, learning, decision based, or evidence based techniques. the discussion on voting based approaches has shown that there is a scope for improving classification accuracy. this issue also offers numerous opportunities for developing optimization techniques to determine the weights. some of the approaches including ga, pso, and ant optimization techniques, among others, can be investigated. neural networks as well as similar models such as fuzzy networks, deep neural networks; svm, etc. also offer an excellent opportunity for developing adaptive techniques to combine individual classifiers outputs. for example, can individual classifiers be considered as layers of more general architectures. computational complexity is an important issue that needs further research especially when the different algorithms are deployed over mobile or low power platform an important issue which is still open is that of finding the optimal number of classifiers to be combined for a given application. additionally, for a given number of features, is there a way to distribute these among the different classifiers to be combined. finally, the use of hybrid approaches to integrate results from different combination techniques, offers further opportunities for solving more involved applications. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T3/02_marco_general.html",
    "title": "Marco general para la combinacion de clasificadores",
    "body": " index search search back marco general para la combinacion de clasificadores the task is seen as a problem of finding a combination function which accepts \\(n\\)-dimensional score vectors from each of the \\(m\\) classifiers, then producing a single final classification score representing the selected class. given the \\(n\\) possible classes \\(\\{\\omega_1, \\omega_2, \\cdots, \\omega_n\\}\\) and a pattern \\(z\\) (that is a data sample), let \\(x_k\\) be the measurement vector (the numerical attributes of \\(z\\)) used by the \\(k\\)th classifier (different classifiers may use different attributes to discriminate). the probability density function of the measurement vector is represented by: \\begin{align} p(x_k|\\omega_n) \\end{align} while the prior probability of the occurrence of the class is denoted by \\(p(\\omega_n)\\). the bayesian framework aims to determine the class label for the pattern \\(z\\) by considering the information provided by all \\(m\\) classifiers. the final decision is based on the aposteriori probability, which is the probability of the pattern belonging to a specific class \\(j\\) given the measurement vectors from all classifiers \\(x_1, x_2, \\cdots, x_m\\). \\begin{align} p(\\theta = \\omega_j|x_1, x_2, \\cdots, x_m) = \\max_{k} p(\\theta = \\omega_k | x_1, x_2, \\cdots, x_m) \\end{align} so the pattern \\(z\\) is assigned to class \\(\\omega_j\\) which produces the maximum a posterior probability. where the aposteriori distribution is computed as follows: \\begin{align} p(\\theta = \\omega_j|x_1, x_2, \\cdots, x_m) = \\frac{p(x_1, x_2, \\cdots, x_m|\\theta = \\omega_j)p(\\theta=\\omega_j)}{p(x_1, x_2, \\cdots, x_m)} \\end{align} provided that each classifier provides independently a decision support obtained from \\(x_k\\), where \\(p(x_1, x_2, \\cdots, x_m)\\) is the joint pdf of the observations independently of class label. the important issue is that the individual classifiers should not make identical erroneous decisions on the same observation instances, they should provide complementary information. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T3/03_estrategias.html",
    "title": "Estrategias para la combinaciÃ³n de clasificadores",
    "body": " index search search back estrategias para la combinaciÃ³n de clasificadores contents levels of classifiers combination hard and soft level classifier combination weighted majority voting dynamic weighted consult-and-vote divide and conquer quality based combination techniques feature combination critic classifier adaptative voting technique different model combination dynamic entropy based combination technique assuming linear dependency between predictors runtime weighted opinion pool using hidden markov models assigning weights to classes combination rules assesment adaptative and non-adaptative combiners adaptative combiners ad and cm combination ann vs svm combination combination through ann incorporating a priori knowledge combining combination strategies dynamically modifying majority voting based on genetic theory fuzzy based non-adaptative combiners conclusion classification based on the number of classifiers other combination techniques references this section provides a detailed review of classifiers combination techniques first, we discuss the different levels at which combination is performed: sensors, features and decisions. we then expand on the concept of soft vs. hard combination techniques. finally, we discuss how the combination techniques can be grouped as either adaptive or non-adaptive. levels of classifiers combination classifiers combination can be carried out at three different levels: early combination at sensor data level: combination of data collected from two or more sensors before feature selection technique is applied. combination at feature level: it may simply involve basic concatenation of feature vectors with equal or different weights (might result in high dimensonal vectors, whose dimension has to be reduced). late combination at the decision level: they are based on one of three approaches: abstract, rank, and score: abstract-based: a single output label from each individual classifier is used as input to the combination scheme. rank-based: each classifier yields several labels ranked from the most likely to the least likely. this information is then used by the combination scheme to reach the final decision. score-based: each classifier outputs the \\(n\\) best labels together with their confidence scores. the combination can be density-based, transformation-based or classifier-based score fusion. hard and soft level classifier combination another way to categorize combination algorithms is whether hard thresholding or soft scoring is used with each of the classifiers. hard-level combination: uses the output of the classifier after it is hard thresholded. soft-level combination: uses estimates of the aposteriori probability of the class. the sum, product, max, min rules, etc., fall under the soft level combiners as they use the output aposteriori probability of the classifier or a score. sum rule: the class with the highest sum of probabilities is chosen as the final prediction. product rule: the class with the highest product of probabilities is chosen as the final prediction. max rule: the class with the highest posterior probability among all classifications made by individual classifiers is selected as the final prediction. min rule: the class with the lowest posterior probability among all classifications made by individual classifiers is selected as the final prediction. majority voting is a typical example of hard-level combiners and has found widespread use in the literature. there are three different versions of voting: unanimous voting. more than half voting. highest number of votes. considering the output label vector of the \\(i\\)th classifier as: \\begin{align} [d_{i, 1}, \\cdots, d_{i, n}]^t \\in [0, 1]^n \\end{align} where \\(i = 1, 2, \\cdots, m\\) and \\(d_{i, j} = 1\\) if the classifier \\(d_i\\) labels the \\(i\\)th instance as class \\(\\omega_j\\) and \\(0\\) otherwise. the majority vote results in a decision for class \\(\\omega_k\\) if: \\begin{align} \\sum_{i = 1}^m d_{i, k} = \\max_{j = 1}^n \\sum_{i = 1}^m d_{i, j} \\end{align} where \\(m\\) is the total number of classifiers and \\(n\\) is total number of classes. such that class \\(\\omega_k\\) is the most \"selected\" on all the classifier. the accuracy of the combination scheme is given as: \\begin{align} p_{maj} = \\sum_{m = \\frac{m}{2} + 1}^m \\binom{m}{m} p^m (1-p)^{m - m} \\end{align} where \\(p\\) is the probability of correct classification. majority voting provides an accurate class label when at least \\(\\frac{m}{2} + 1\\) classifiers give correct classifications, it also requires participating classifiers to have comparable accuracies. weighted majority voting weighted majority voting is used when the classifiers' accuracies are not similar, so it is reasonable to assign more weight to the most accurate classifier. now the decision rule becomes: \\begin{align} \\sum_{i = 1}^m b_i d_{i, k} = \\max_{j = 1}^n \\sum_{i = 1}^m b_i d_{i, j} \\end{align} where \\(b_i\\) is the weight associated with classifier \\(d_i\\). for the sake of convenience, it is a good practice to normalize the weights such that the sum is one. the weight selection is very important in determining the overall accuracy of the classifier combinations. therefore, to minimize the classification error of the combination, the weights are assigned as follows: \\begin{align} b_i \\propto \\log\\left(\\frac{p_i}{1 - p_i}\\right) \\end{align} where \\(p_i, \\cdots, p_m\\) are the individual accuracies for each independent classifier. dynamic weighted consult-and-vote in [30], muhlbaier et al. introduced a method for combining ensembles of classifiers using a dynamic weighted consult-and-vote approach for incremental learning of new classes. the proposed technique focuses on incremental learning, specifically for incorporating new classes into the classification system. the consult-and-vote strategy involves a dynamic process where individual classifiers within the ensemble consult with each other to determine their respective voting weights for classifying test instances. the voting weights assigned to each classifier are determined based on their relative performance on the training data. the method proposed by muhlbaier et al. represents an enhancement over a previous approach developed by the authors. the previous approach may have encountered the \"out-voting\" problem, where certain classifiers dominate the decision-making process, potentially leading to biased or inaccurate results. divide and conquer another modification of majority voting, involving a divide and conquer strategy, is described in [31]. it aims to enhance the majority voting approach by breaking down the classification task into smaller, more manageable sub-problems. each sub-problem is then addressed independently, with dedicated classifiers or classification algorithms focusing on solving the specific challenges within that subset of data. after solving each of the smaller sub-problems individually, the results or decisions from these segments are combined using a majority voting scheme. majority voting involves aggregating the outputs of the classifiers or algorithms involved in solving the sub-problems and selecting the class label that receives the most votes as the final decision. quality based combination techniques the quality-based combination approach, as referenced in [32], focuses on assigning higher weights to more reliable classifiers based on specific quality measures. quality measures used to assess the performance of classifiers may vary depending on the application domain and specific requirements. feature combination the challenge of increasing dimensionality resulting from the simple combination of features from different datasets is addressed through innovative approaches to decision-level combination, as discussed in [33]. combining features from diverse datasets can lead to a significant increase in the dimensionality of the data. the study proposed an effective approach for decision-level combination by leveraging spectral reflectance and its higher-order derivatives to classify hyperspectral land images. spectral reflectance and its derivatives provide valuable information about the characteristics of the land surface, which can aid in classification tasks. the study conducted experiments under two scenarios to address the curse of dimensionality: scenario 1 - lda-based dimensionality reduction: linear discriminant analysis (lda) was employed for dimensionality reduction. scenario 2 - multiple classifiers decision fusion (mcdf): multiple classifiers were utilized for decision fusion to enhance classification performance. critic classifier in the study referenced as [34], the authors introduced an innovative approach that focuses specifically on addressing two-class classification problems. the key feature of this approach is the incorporation of a classifier critic associated with each individual classifier, aimed at predicting the error rate of the classifier. the role of the classifier critic is to forecast the potential error or misclassification rate of its associated classifier. the approach relies on classical standard voting techniques for combining the outputs of multiple classifiers. adaptative voting technique in the study referenced as [35], the authors introduced an adaptive approach to voting techniques. the key innovation in this approach is the dynamic weighting of classifiers based on their estimated recognition performance. different model combination in [36], the authors explored the combination of three distinct classifiers - naive bayes, j48 decision tree, and decision table - using various voting techniques to enhance classification accuracy. the classifiers were combined using different voting strategies, including simple voting (where each classifier has equal weight), weighted voting (assigning different weights to classifiers based on their performance), and probability-based voting (considering the confidence or probability estimates of classifiers). the study reported that the ensemble of classifiers using weighted and probability-based voting techniques outperformed simple majority voting. in addition to classifier combination and voting strategies, the study applied a supervised dimensionality reduction algorithm to further enhance performance. dimensionality reduction techniques aim to reduce the complexity of the feature space while preserving relevant information, thereby improving classification accuracy and efficiency. in [38], a voting strategy was employed for land cover classification of remotely sensed images by utilizing an ensemble of six different classifiers. common voting techniques include simple majority voting, weighted voting, or probability-based voting, where the combined decision is based on the collective predictions of the ensemble members. in the study referenced as [39], researchers proposed an approach that combines artificial neural network (ann) and k-nearest neighbors (knn) based classifiers using a majority voting scheme. this method was specifically designed to enhance accuracy in scenarios where sensor data is prone to drift. the researchers employed a majority voting scheme to combine the predictions of the ann and knn classifiers. in this approach, each classifier in the ensemble provides a prediction, and the final decision is made based on the majority vote of the individual classifier outputs. the primary goal of the study was to leverage the complementary strengths of ann and knn classifiers to address the challenge of sensor data drift. ann models are known for their ability to learn complex patterns from data, while knn is a non-parametric method based on instance-based learning that can be effective in classification tasks. by combining multiple knn classifiers using majority voting and employing median voting for the ann classifiers, the researchers observed a substantial improvement in classification performance. the ensemble of ann and knn classifiers demonstrated enhanced accuracy in classifying sensor data, showcasing the effectiveness of the majority voting scheme in mitigating the impact of data drift on classification outcomes. dynamic entropy based combination technique in [37], the authors introduced a novel dynamic entropy-based technique for combining classifiers. the key idea behind the combination scheme is to assign weights to individual classifiers based on their confidence levels in making decisions. classifiers that exhibit high confidence in their predictions are assigned larger weights, while those with lower confidence receive smaller weights. assuming linear dependency between predictors in [40], the authors considered linear dependency of both classifiers and features. to address this, they proposed a new approach that models the dependencies between features without making any assumptions about the distribution of features (independency) or classifiers. the researchers introduced two key models as part of their framework: linear classifier dependency modelling (lcdm): this model focuses on capturing dependencies between the classifiers themselves, exploring how the outputs of different classifiers may be linearly related. linear feature dependency modelling (lfdm) : this model is designed to identify and model dependencies between the features used by the classifiers, allowing for a more comprehensive understanding of the relationships within the feature space. the results of the study demonstrated that the proposed approach outperformed existing methods in scenarios where linear dependencies between features and classifiers play a significant role. by explicitly modeling these dependencies, the framework was able to capture more complex relationships within the data and improve classification accuracy. runtime weighted opinion pool in [45], introduces a novel classifiers combination approach known as the runtime weighted opinion pool (rwop). this approach dynamically assigns weights to the classifiers during runtime based on their local performance, leading to an adaptive and context-aware combination strategy. unlike traditional weighted sum-based approaches, rwop utilizes an intuitive runtime strategy to determine the weights for combining classifier outputs. the dynamic weight assignment in rwop allows the system to adapt to changing conditions and varying input patterns, leading to more robust and accurate classification outcomes. using hidden markov models in the study referenced as [46], the authors introduce a novel approach for combining classifiers specifically designed for hidden markov model (hmm) based classifiers. unlike traditional methods where the combination typically occurs at the decision level, this new approach operates at a more elementary level within the hmm framework. assigning weights to classes in the study referenced as [12], a novel weighted majority voting approach was proposed for classifiers combination. this approach differs from traditional methods by assigning weights to different classes rather than individual base classifiers. the weights are determined by estimating the joint probability distribution of each class using the scores provided by all classifiers in the combination pool. the joint probability distribution is computed using the naÃ¯ve bayes probabilistic model. combination rules in the context of multiple classifiers combination, the way individual classifiers handle input patterns can vary. some classifiers may use the same representation of the input pattern, while others may employ their own unique representations. the effectiveness of combining these classifiers using different strategies has been explored in various studies, including those referenced as [41], [42], and [43]. simple sum rule: despite being developed under restrictive assumptions, the simple sum rule was found to outperform other combination rules in certain scenarios. majority voting: the experimental results indicated that the majority voting approach was the most effective combination rule for the specific dataset used in the study. assesment fixed rule-based combination techniques in the context of multiple classifiers do not require a training stage and rely on class labels, distances, or confidences provided by individual classifiers. the efficiency of fixed rule techniques is influenced by various factors, as discussed in reference [44]. according to reference [44], fixed rule techniques are most efficient under specific conditions: availability of large training sets: adequate training data is essential for reliable performance. generation of reliable confidences: individual classifiers should provide accurate and trustworthy confidence values. training on different feature spaces: base classifiers should be trained on diverse feature representations to capture varied aspects of the data. in scenarios where the strict conditions for fixed rule techniques are not met, a trained combination rule may yield better results. adaptative and non-adaptative combiners adaptative combiners adaptive techniques for classifiers combination are mainly based on evolution or artificial intelligence algorithms. they include neural networks combination strategies and genetic algorithms as well as fuzzy set theory. techniques under these categories are summarized on the following figure: artificial neural networks are usually used as a base classifier [29], however, it has also found wide use in combination of classifiers. ad and cm combination in [48], the author introduces two innovative approaches for combining classifiers to enhance robustness and fault tolerance: the attractor dynamics (ad) algorithm and the classifier masking (cm) algorithm. the cm algorithm is described as a non-neural version of the ad algorithm, inspired by modeling properties of sensory integration in the central nervous system. both approaches are designed to promote consensus among individual classifiers and improve the overall performance of the combined system by discarding corrupted classifier outputs. ann vs svm combination in [49], the authors conducted a comparative analysis of combining the outputs of an ensemble of artificial neural networks (anns) with support vector machine (svm) classifiers for processing remotely sensed data. they employed an multi-layer perceptron (mlp) module to facilitate the non-linear combination of the outputs generated by the networks. the researchers explored two distinct approaches for optimizing coefficient selection: the bayesian method and the error correlation matrix. through experimental evaluation, the authors found that the mlp-based combination scheme yielded the most favorable results compared to the svm classifiers. combination through ann in [50], the authors proposed a novel approach where they utilized an artificial neural network (ann) as a model for combining classifiers. instead of combining the outputs of different classifiers, they integrated various training sets with distinct classifiers to train a unified combination rule within a three-layer ann architecture. in this setup, each classifier represented a unit in the hidden layer of the ann. incorporating a priori knowledge in [51], the authors delved into the significance of leveraging a priori knowledge within existing classifiers combination techniques, specifically exploring the application of the behavior knowledge space and the dempster-shafer (d-s) theory. this investigation aimed to elucidate how incorporating prior knowledge can enhance the performance of classifiers combination, particularly when dealing with strongly correlated classifiers. the study also highlighted the utilization of adaptive combiners, encompassing strategies such as adaptive weighting, associative switching, mixture of local experts (mle), and hierarchical mle combining combination strategies dynamically in [57] an adaptive approach to combining classifiers was introduced. the proposed approach dynamically selects between two different combination strategies based on the belief values obtained from each strategy. specifically, the study compared the performance of a bayesian classifiers combination approach and product and max rule combination strategy. in the bayesian method, the combination of classifiers is based on probabilistic principles, where the posterior probabilities of class labels are calculated using bayes' theorem. the product rule combines the outputs of individual classifiers by multiplying their probabilities or scores for each class. the max rule operates by selecting the class label that receives the highest score or confidence level among all the individual classifiers in the ensemble. modifying majority voting in [58] the authors introduced various modifications to the traditional majority voting rule by incorporating a bayesian framework and a genetic algorithm (ga) to determine the weights assigned to different classifiers in the ensemble. the bayesian framework allowed for the probabilistic modeling of the weights, while the ga provided an optimization technique to search for the best combination of weights that maximized the ensemble performance. the results of the study indicated that the modified majority voting rule, when combined with the bayesian framework and ga for weight optimization, achieved significant improvements in accuracy. specifically, the optimal accuracies obtained were \\(94.3\\%\\) for the majority vote, \\(95.4\\%\\) for the genetic algorithm, and 95.95% for the bayesian approach. based on genetic theory in the study referenced as [22], the authors introduced an innovative approach that involved the simultaneous extraction and selection of features and classifiers to improve the performance of gender and age classification using speech signals collected from a typical korean home environment. the authors employed a genetic algorithm to simultaneously select features and classifiers. ga is a metaheuristic optimization technique inspired by the process of natural selection and genetics, used to search for the optimal combination of features and classifiers for the classification task. the outputs of the selected classifiers were combined using the dempster-shafer theory, a mathematical theory for combining evidence from different sources to make decisions under uncertainty. in [59] a novel approach based on genetic algorithm (ga) with self-configuration capabilities was developed for classifier combination. the researchers employed a pool of twelve expert classifiers that were already trained on the task of character recognition, including both printed and handwritten characters. these expert classifiers likely had different strengths and weaknesses, making them suitable candidates for ensemble learning. the ga was integrated into the system to optimize the combination of outputs from the expert classifiers. by using the evolutionary principles of genetic algorithms, the system could iteratively adjust the weights assigned to each classifier in the ensemble to maximize the overall accuracy of the system. fuzzy based in the study referenced as [16], the authors introduced a novel approach known as fuzzy stacked generalization (fsg) to combine the outputs of multiple classifiers. fsg operates within a hierarchical framework where multiple base-layer classifiers are utilized to make individual predictions on the input data. in fsg, the decisions made by the base-layer classifiers are aggregated to form a decision vector. this decision vector is then fed into a meta-layer classifier, which combines the outputs of the base-layer classifiers to make the final decision. in [53], the authors proposed a classifiers combination technique using fuzzy templates (ft). an object is labeled with the class whose fuzzy template is closest to the objects' decision profile. the authors obtained an improved performance over majority, min, max and product rules, and unweighted average combination techniques in [54], an adaptive fuzzy integral was used to combine multiple classifiers. the parameter \\(\\lambda\\)-fuzzy, which measures performance, is adaptively adjusted depending upon the interaction among the classifiers. the essence of the parameter is to search for the maximum degree of agreement between the conflicting and complementary sources of evidence. in [55], a fuzzy decision rule was employed to combine the outputs of multiple classifiers without the need for a training stage. each classifier was independently applied to the input data, but no final decision was made based on their outputs at this stage. these classifiers are pre-existing models that have been trained on labeled data to make predictions or classifications. the results from the classifiers were aggregated using a fuzzy decision rule. this rule considered the membership degrees of the classes assigned by each classifier and selected the class with the highest membership degree (the confidence or certainty with which each classifier assigns a data point to a specific class) as the correct class. two measures of accuracy, namely information reliability and global accuracy, were utilized in the combination rule to assess the performance of the combined classifiers. in [56] the authors introduced a first-order takagi-sugeno-kang (tsk) fuzzy model for combining multiple classifiers. unlike conventional linear combination methods that assign different weights to pairs of classifiers and classes, the proposed tsk fuzzy model assigns weights to each individual classifier, class, and region of the classifier output space (decision boundary). this finer granularity in weight assignment allows for a more nuanced and adaptive combination of classifier outputs. the tsk fuzzy model is utilized to integrate the outputs of multiple classifiers. this model leverages fuzzy logic to combine the predictions of individual classifiers in a way that considers the uncertainty and variability in the classifier outputs. the study demonstrated improved accuracy compared to using individual classifiers alone. while the tsk fuzzy model showed promising results in enhancing classification accuracy, the authors did not explicitly address the potential bias and variance reduction that could arise from using a linear model for combining classifiers. non-adaptative combiners the highest confidence approach is an example of nonadaptive combination techniques. it involves ranking the individual classifiers based on their confidence then selecting the decision of the top ranked one. the borda count technique is also an example of nonadaptive methods. it is based on the principle of single winner classifier in which the individual classifiers provide a ranked list of the classes. it is a more sophisticated alternative to majority voting [60] based on ranking level [9]. it does not require training, just like averaging, sum, and voting rules [52]. conclusion in summary, adaptive combiners tend to do better than the non-adaptive types. this is due to the fact that adaptive combiners update the weights given to the individual classifier dynamically before making the final decision. given the fact that the performance of the individual classifiers can vary over input patterns, such a dynamic combination provides an edge over its non-adaptive counterpart especially when the data space is wide and diverse. classification based on the number of classifiers the most commonly used techniques for ensemble based combinations are displayed in the following figure: bagging is one of the most intuitive and simple techniques used for ensemble based combination. however, unlike bagging, in boosting, the individual classifiers are trained hierarchically to discriminate more complex regions in the feature space. adaboost is a variation of the boosting technique. it is an adaptive boosting meta-algorithm that combines outputs of weak classifiers into a weighted sum that represents the final decision. however, the technique is sensitive to noisy data and outliers. in [66], the authors used adaboost to enhance the performance of a hybrid hidden markov model (hmm) and neural network (nn) speech recognition system. hmms are commonly used for modeling sequential data like speech signals, while nns are effective in capturing complex patterns in data. the researchers evaluated the performance of the hybrid hmm/nn system with and without the adaboost algorithm under noisy environments. noise in speech signals can introduce distortions and affect the accuracy of the recognition system. by applying adaboost, the system was expected to adapt better to noisy conditions and enhance its robustness. the results of the study demonstrated that incorporating adaboost into the hybrid hmm/nn speech recognition system led to improved performance, even in the presence of noise. adaboost's ability to focus on difficult instances and adjust the weights of the classifiers based on their performance contributed to the system's ability to handle noisy environments and enhance overall recognition accuracy. in [67] the researchers explored a combination approach at the feature level using support vector machine (svm) classifiers and a global adaboost classifier. the study focused on combining features extracted from different datasets at the feature level. by utilizing svm classifiers and a global adaboost classifier, the researchers aimed to leverage the strengths of both classifiers in integrating information from multiple datasets to improve classification performance. one significant drawback identified in the study regarding feature-level combination is the issue of high dimensionality. combining features from multiple datasets can result in a large number of features, which can lead to challenges such as increased computational complexity, overfitting, and reduced interpretability of the model. other combination techniques in [68] the researchers introduced a novel classifiers combination technique based on an svm active learning algorithm. the study proposed a method that leverages support vector machine (svm) classifiers in conjunction with an active learning algorithm. active learning refers to a machine learning approach where the algorithm can select the most informative data points for labeling, thereby improving the learning process iteratively. the researchers developed a strategy where an initial classifier, likely an svm model, is used to generate class aposteriori probabilities. these probabilities serve as inputs to the classifiers-combiner, which is based on the svm active learning algorithm. the approach outperforms traditional classifiers combination rules when considering class labeling cost and classification accuracy. in [70] and [71] the researchers introduced a novel approach using eigenclassifiers for combining correlated classifiers. the proposed method involves utilizing principal component analysis (pca) projection to create eigenclassifiers from a set of initially correlated classifiers. by applying pca, the goal is to transform the correlated classifiers into uncorrelated eigen-classifiers. this transformation process aims to enhance the diversity and independence of the classifiers, enabling them to complement each other effectively during the combination stage. the results of the study indicated that the pca-based eigenclassifiers technique provided better or comparable accuracy with a reduced number of classifiers compared to bagging and adaboost. this suggests that the uncorrelation process facilitated by pca enhanced the performance of the combined classifiers, leading to improved classification results with fewer individual classifiers. similarly, in [72] the researchers explored methods to address linear and non-linear correlations among the outputs of individual classifiers by leveraging principal component analysis (pca) and a generalized kernel-based pca approach. initially, the authors identified linear correlations among the outputs of individual classifiers. to mitigate these linear correlations, the researchers applied a simple pca approach. building on the success of addressing linear correlations, the authors extended their approach to consider non-linear dependencies among the outputs of individual classifiers. o handle these non-linear dependencies, the researchers proposed a generalized kernel-based pca approach. the results of the experiments demonstrated that the generalized kernel-based pca approach outperformed alternative methods in improving classification accuracy. in [73] the researchers introduced a novel classifier combination technique that focused on extracting class boundaries and utilizing a set of local linear combination rules. the proposed technique involved extracting class boundaries, which are the decision boundaries that separate different classes in the dataset. the researchers employed a set of local linear combination rules to combine the outputs of individual classifiers. these rules likely involved linear combinations of classifier outputs within specific regions of the feature space, allowing for adaptive and context-aware decision-making. the experimental results demonstrated that the classifier combination technique based on class boundaries and local linear combination rules achieved better accuracy compared to other methods such as linear combination, voting, and decision templates. in [13], the researchers proposed a weighted averaging approach that incorporated graph-theoretical clustering and a support vector machine (svm) classifier for classifier combination. the researchers utilized graph-theoretical clustering techniques as part of the weighted averaging approach. graph theory provides a framework for analyzing relationships between data points, and clustering algorithms can group similar data points together based on certain criteria. by incorporating graph-theoretical clustering, the approach likely aimed to identify clusters of data points with similar characteristics for more effective combination of classifier outputs. in addition to clustering, the approach involved the use of an svm classifier. svms are powerful machine learning models commonly used for classification tasks. by integrating an svm classifier into the weighted averaging process, the researchers likely leveraged its ability to create optimal decision boundaries between classes in the feature space. the results obtained from the experiments indicated that the proposed approach, despite its simplicity and intuitive nature, performed comparably to more sophisticated methods. in [74], the researchers employed three different techniques - highest rank (hr), borda count (bc), and logistic regression (lr) - for combining decisions in a multi-classifier system. the decisions produced by each individual classifier were ranked based on their confidence or accuracy. the hr, bc, and lr techniques were then applied to either reduce the set of possible classes or re-rank them during the combination process. the results obtained from the experiments demonstrated a substantial improvement in the performance of the multi-classifier system. similarly, in [75] a new combination technique called mixed group rank (mgr) was introduced as a novel approach to balancing between preference and confidence in a multi-classifier system. this technique aimed to generalize the principles of highest rank (hr), borda count (bc), and logistic regression (lr) by incorporating elements of both preference-based ranking and confidence-based decision-making. in [76], the authors introduced an innovative approach that involves dynamically switching between classifier combination and classifier selection based on the characteristics of different regions in the feature space. the authors further introduced a hybrid combination scheme that integrates clustering-and-selection (cs) techniques with decision template (dt) methods. this hybrid approach likely combines the benefits of clustering for identifying regions of dominance and selection of the most appropriate classifier, along with decision templates for combining classifier outputs in a structured manner. the authors discussed the tradeoff between selecting the best classifier and combining classifiers. this tradeoff likely involves considerations of the strengths and weaknesses of individual classifiers versus the potential benefits of combining multiple classifiers. in [77], the authors introduced a method based on classifier selection that focused on identifying the most suitable candidate through confidence evaluation of distance-based classifiers. the method aimed to select the most precise candidate from a set of distance-based classifiers by evaluating their confidence levels. this process likely involved assessing the certainty or reliability of each classifier's decision-making based on the distances between data points in the feature space. the authors likely defined specific rules or criteria for selecting the precise candidate based on the confidence evaluations of the distance-based classifiers. these rules may have considered factors such as the proximity of data points to decision boundaries, the consistency of classifier outputs, or the overall confidence levels of individual classifiers. the experiments conducted in the study likely utilized distance metrics such as euclidean distance and city block distance for recognizing handwritten characters. in [78], the author used information from the confusion matrix to merge multiple classifiers using a class ranking borda type reconciliation method. the class ranking borda type reconciliation method is a technique that combines the outputs of multiple classifiers by ranking the classes based on their performance and then using a borda count approach to reconcile the rankings. the results obtained from this method were compared with three other classifier combination techniques: majority voting, sum rule, and median rule. the comparison was done using three types of confusion matrices: deterministic, uniform, and stochastic. the apborda (aposteriori borda count) and sum rule gave the overall best improvement, except in the case of a stochastic confusion matrix and disparate combination (where classifiers had a \\(10\\%\\) accuracy difference from each other). this means that in most cases, the apborda and sum rule performed better in combining the classifiers, but there were specific scenarios where they did not perform as well. in [79] a combination technique based on the f-measure was proposed for recognizing human emotions using an svm (support vector machine) classifier. in this technique, the f-measure was used to form a decision matrix to determine the final emotion. in [80], the authors proposed an approach for detecting vacant parking spaces by combining two different systems. the first system was based on analyzing image data, while the second system relied on sensor data. the experiments conducted by the authors demonstrated that combining the outputs of these two different systems resulted in a reduced error in detecting vacant parking spaces. in summary, several classifiers combination techniques have been proposed in the literature with each technique having its own strengths and weaknesses. recent techniques mostly involve hybridization or modification of previous techniques to achieve better accuracy or to remove an associated constraint on which a particular technique was built on. some of these constraints include the issue of correlated classifiers, gaussian distribution, and iid. there is still a need to develop classifiers combination strategies which are not constrained to specific distributions. references [12] c. de stefano, f. fontanella, and a. s. di freca, \"a novel naive bayes voting strategy for combining classifiers.,\" in icfhr, 2012, pp. 467â472. [16] c. senaras, m. ozay, and f. t. yarman vural, âbuilding detection with decision fusion,â 2013. [22] y. zhan, h. leung, k.-c. kwak, and h. yoon, âautomated speaker recognition for home service robots using genetic algorithm and dempster--shafer fusion technique,â instrum. meas. ieee trans., vol. 58, no. 9, pp. 3058â3068, 2009. [29] l. i. kuncheva, combining pattern classifiers: methods and algorithms. john wiley & sons, 2004. [32] n. poh and j. kittler, \"a unified framework for biometric expert fusion incorporating quality measures,\" pattern anal. mach. intell. ieee trans., vol. 34, no. 1, pp. 3â18, 2012. [33] h. r. kalluri, s. prasad, and l. m. bruce, \"decision-level fusion of spectral reflectance and derivative information for robust hyperspectral land cover classification,\" geosci. remote sensing, ieee trans., vol. 48, no. 11, pp. 4047â4058, 2010. [34] d. j. miller and l. yan, \"ensemble classification by critic-driven combining,\" in acoustics, speech, and signal processing, 1999. proceedings., 1999 ieee international conference on, 1999, vol. 2, pp. 1029â1032 [35] f. mattern, t. rohlfing, and j. denzler, \"adaptive performancebased classifier combination for generic object recognition,\" in proc. of international fall workshop vision, modeling and visualization (vmv), 2005, pp. 139â146 [36] g. jain, a. ginwala, and y. a. aslandogan, \"an approach to text classification using dimensionality reduction and combination of classifiers,\" in information reuse and integration, 2004. iri 2004. proceedings of the 2004 ieee international conference on, 2004, pp. 564â569. [37] m. magimai-doss, d. hakkani-tur, o. cetin, e. shriberg, j. fung, and n. mirghafori, \"entropy based classifier combination for sentence segmentation,\" in acoustics, speech and signal processing, 2007. icassp 2007. ieee international conference on, 2007, vol. 4, p. iv--189 [39] s. adhikari and s. saha, \"multiple classifier combination technique for sensor drift compensation using ann & knn,\" in advance computing conference (iacc), 2014 ieee international, 2014, pp. 1184â1189. [40] a. j. ma, p. c. yuen, and j.-h. lai, \"linear dependency modeling for classifier fusion and feature combination,\" pattern anal. mach. intell. ieee trans., vol. 35, no. 5, pp. 1135â1148, 2013. [43] z. wu, c.-h. li, and v. cheng, \"large margin maximum entropy machines for classifier combination,\" in wavelet analysis and pattern recognition, 2008. icwaprâ08. international conference on, 2008, vol. 1, pp. 378â383 [44] r. p. w. duin, \"the combining classifier: to train or not to train?,\" in pattern recognition, 2002. proceedings. 16th international conference on, 2002, vol. 2, pp. 765â770. [45] w. wang, a. brakensiek, and g. rigoll, \"combination of multiple classifiers for handwritten word recognition,\" in frontiers in handwriting recognition, 2002. proceedings. eighth international workshop on, 2002, pp. 117â122. [48] a. v bogdanov, \"neuroinspired architecture for robust classifier fusion of multisensor imagery,\" geosci. remote sensing, ieee trans., vol. 46, no. 5, pp. 1467â1487, 2008. [49] g. pasquariello, n. ancona, p. blonda, c. tarantino, g. satalino, and a. dâaddabbo, \"neural network ensemble and support vector machine classifiers for the analysis of remotely sensed data: a comparison,\" in geoscience and remote sensing symposium, 2002. igarssâ02. 2002 ieee international, 2002, vol. 1, pp. 509â511. [50] y.-d. lan and l. gao, \"a new model of combining multiple classifiers based on neural network,\" in emerging intelligent data and web technologies (eidwt), 2013 fourth international conference on, 2013, pp. 154â159. [51] v. di lecce, g. dimauro, a. guerriero, s. impedovo, g. pirlo, and a. salzo, \"knowledge-based methods for classifier combination: an experimental investigation,\" in image analysis and processing, 1999. proceedings. international conference on, 1999, pp. 562â565. [52] a. k. jain, r. p. w. duin, and j. mao, âstatistical pattern recognition: a review,â pattern anal. mach. intell. ieee trans., vol. 22, no. 1, pp. 4â37, 2000. [53] l. i. kuncheva, j. c. bezdek, and m. a. sutton, âon combining multiple classifiers by fuzzy templates,â in fuzzy information processing society-nafips, 1998 conference of the north american, 1998, pp. 193â197 [54] t. d. pham, âcombination of multiple classifiers using adaptive fuzzy integral,â in artificial intelligence systems, 2002.(icais 2002). 2002 ieee international conference on, 2002, pp. 50â55. [55] m. fauvel, j. chanussot, and j. a. benediktsson, âdecision fusion for the classification of urban remote sensing images,â geosci. remote sensing, ieee trans., vol. 44, no. 10, pp. 2828â2838, 2006. [56] m. cococcioni, b. lazzerini, and f. marcelloni, âa tsk fuzzy model for combining outputs of multiple classifiers,â in fuzzy information, 2004. processing nafipsâ04. ieee annual meeting of the, 2004, vol. 2, pp. 871â876. [57] y. yaslan and z. cataltepe, âco-training with adaptive bayesian classifier combination,â in computer and information sciences, 2008. iscisâ08. 23rd international symposium on, 2008, pp. 1â4. [58] l. lam and c. y. suen, âoptimal combinations of pattern classifiers,â pattern recognit. lett., vol. 16, no. 9, pp. 945â954, 1995. [59] k. sirlantzis and m. c. fairhurst, âoptimisation of multiple classifier systems using genetic algorithms,â in image processing, 2001. proceedings. 2001 international conference on, 2001, vol. 1, pp. 1094â1097 [66] h. schwenk, âusing boosting to improve a hybrid hmm/neural network speech recognizer,â in acoustics, speech, and signal processing, 1999. proceedings., 1999 ieee international conference on, 1999, vol. 2, pp. 1009â1012. [67] j. hu and y. chen, âoffline signature verification using real adaboost classifier combination of pseudo-dynamic features,â in document analysis and recognition (icdar), 2013 12th international conference on, 2013, pp. 1345â1349 [68] x. yi, z. kou, and c. zhang, âclassifier combination based on active learning,â in pattern recognition, 2004. icpr 2004. proceedings of the 17th international conference on, 2004, vol. 1, pp. 184â187. [69] j. kremer, k. steenstrup pedersen, and c. igel, âactive learning with support vector machines,â wiley interdiscip. rev. data min. knowl. discov., vol. 4, no. 4, pp. 313â326, jul. 2014. [70] a. ulaÅ, o. t. yÄ±ldÄ±z, and e. alpaydÄ±n, âeigenclassifiers for combining correlated classifiers,â inf. sci. (ny)., vol. 187, pp. 109â 120, 2012. [71] e. ulaÅ, a., semerci, m., yÄ±ldÄ±z, o. t., & alpaydÄ±n, âincremental construction of classifier and discriminant ensembles,â inf. sci. (ny)., vol. 179, no. 9, pp. 1298â1318, 2009 [72] u. ekmekci and z. cataltepe, âclassifier combination with kernelized eigenclassifiers,â in information fusion (fusion), 2013 16th international conference on, 2013, pp. 743â749. [73] m. liu, k. li, and r. zhao, âa boundary based classifier combination method,â in control and decision conference, 2009. ccdcâ09. chinese, 2009, pp. 3777â3782 [74] t. k. ho, j. j. hull, and s. n. srihari, âdecision combination in multiple classifier systems,â pattern anal. mach. intell. ieee trans., vol. 16, no. 1, pp. 66â75, 1994. [75] o. melnik, y. vardi, and c.-h. zhang, âmixed group ranks: preference and confidence in classifier combination,â pattern anal. mach. intell. ieee trans., vol. 26, no. 8, pp. 973â981, 2004. [76] l. i. kuncheva, âswitching between selection and fusion in combining classifiers: an experiment,â syst. man, cybern. part b cybern. ieee trans., vol. 32, no. 2, pp. 146â156, 2002. [77] c.-l. liu and m. nakagawa, âprecise candidate selection for large character set recognition by confidence evaluation,â pattern anal. mach. intell. ieee trans., vol. 22, no. 6, pp. 636â641, 2000. [78] j. r. parker, âcombining multiple non-homogeneous classifiers: an empirical approach,â in cognitive informatics, ieee international conference on, 2002, p. 288. [79] a. agrawal and n. k. mishra, âfusion based emotion recognition system,â in 2016 international conference on computational science and computational intelligence (csci), 2016, pp. 727â732 [80] junzhao, l., mohandes, m., deriche, m., âa multi-classifier image based vacant parking detection systemâ, ieee international conference on electronics, circuits, and systems icesc, pp. 933-936, abu dhabi, uae, dec 8-11, 2013 $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T4/01_introduction.html",
    "title": "Introduction",
    "body": " index search search back introduction in this chapter we address unsupervised learning. in this case one has a set of \\(n\\) observations \\((x_1,x_2, \\cdots ,x_n)\\) of a random vector \\(x\\) having joint density \\(\\text{pr}(x)\\). the goal is to directly infer the properties of this probability density without the help of a supervisor or teacher providing correct answers or degree-of-error for each observation. principal components, multidimensional scaling, self-organizing maps, and principal curves try to find simpler patterns in complex data. they look for lower-dimensional structures in the data that capture where most of the data points lie. by doing this, they help us understand how variables are related to each other and if they can be thought of as being controlled by a smaller group of underlying factors. cluster analysis looks for groups or clusters in the data that are like little \"bumps\" or \"peaks\" where the data is most concentrated, that is convex regios of the \\(x\\)-space that contain modes of \\(\\text{pr}(x)\\). it helps us see if the data can be divided into different types or categories. mixture modeling aims for the same thing. association rules try to find simple rules or patterns that describe where the data is most concentrated, especially when dealing with data that has many features and is either present or absent (binary-valued). in unsupervised learning, where we don't have clear outcomes to compare against. we often have to rely on guesswork and intuition to decide if the results make sense or not. this uncertainty has led to many different methods being proposed, but ultimately, it's hard to know for sure which one is the best since there's no straightforward way to check their effectiveness. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T4/03_evaluation_metrics.html",
    "title": "Evaluation Metrics",
    "body": " index search search back evaluation metrics contents formal limitations of clustering methods for clustering evaluation null hypothesis testing internal validation partitional methods hierarchical methods cophenetic correlation coefficient hubert statistic external validation matching sets peer-to-peer correlation measures based on information theory hyperparameter tuning formal limitations of clustering jon kleinberg proposes three axioms that highlight the characteristics that a grouping problem should exhibit and can be considered \"good\". scale invariance: indicates that a clustering algorithm should not modify its results when all distances between points are scaled by the factor determined by a constant \\(\\alpha\\). richness: the clustering function must be flexible enough to produce any arbitrary partition/clustering of the input data set. consistency: the clustering results do not change if the distances within clusters decrease and/or the distances between clusters increase. given the above three axioms, kleinberg proves the following theorem: for every \\(n \\geq 2\\), there is no clustering function \\(f\\) that satisfies scale invariance, richness, and consistency. since the three axioms cannot hold simultaneously, clustering algorithms can be designed to violate one of the axioms while sarisfying the other two. \\(k\\)-cluster stopping condition: stop merging clusters when we have \\(k\\) clusters (violates the richness axiom). distance \\(r\\) stopping condition: stop merging clusters when the nearest pair of clusters are farther than \\(r \\) (violates scale invariance). scale-\\(\\epsilon\\) stopping condition: stop merging clusters when the nearest pair of clusters are farther than a fraction \\(\\epsilon\\) the maximum pairwise distance \\(\\delta\\). (consistency is violated). methods for clustering evaluation when analyzing clustering results, several aspects must be taken into account for the validation of the algorithm results: determining the clustering tendency in the data (i.e. whether non-random structure really exists). determining the correct number of clusters. assessing the quality of the clustering results without external information. comparing the results obtained with external information. comparing two sets of clusters to determine which one is better. the first three issues are addressed by internal or unsupervised validation, because there is no use of external information. the fourth issue is resolved by external or supervised validation. finally, the last issue can be addressed by both supervised and unsupervised validation techniques. null hypothesis testing one of the desirable characteristics of a clustering process is to show whether data exhibits some tendency to form actual clusters. in this case, the null hypothesis \\(h_0\\) is the randomness of data and, when the null hypothesis is rejected, we assume that the data is significantly unlikely to be random. one of the difficulties of null hypothesis testing in this context is determining the statistical distribution under which the randomness hypothesis can be rejected. jain and dubes propose three alternatives: random plot hypothesis \\(h_0\\): all proximity matrices of order \\(n \\times n\\) are equally likely. random label hypothesis \\(h_0\\): all permutations of labels of \\(n\\) objects are equally likely. randon position hypothesis \\(h_0\\): all sets of \\(n\\) locations is some region of a \\(d\\)-dimensional space are equally likely. internal validation internal validation methods (or internal indices) make it possible to establish the quality of the clustering structure without having access to external information. in general, two types of internal validation metrics can be combined: cohesion measures: evaluates how closely the elements of the same cluster are to each other. separation measures: quantify the level of separation between clusters. internal indices are usually employed in conjunction with two clustering algorithm families: hierarchical clustering algorithms and partitional algorithms. for partitional algorithms, metrics based on the proximity matrix, as well as metrics of cohesion and separation, such as the silhouette coefficient, are often used. for hierarchical algorithms, the cophenetic coefficient is the most common. partitional methods in general, the internal validation value of a set of \\(k\\) clusters can be decomposed as the sum of the validation values for each cluster: \\begin{align} \\text{general validity} = \\sum_{i=1}^k w_i \\text{validity}(c_i) \\end{align} this measure of validity can be cohesion, separation, or some combination of both. quite often, the weights that appear in the previous expression correspond to cluster size. the individual measures of cohesion and separation are defined as follows: \\begin{align} \\text{cohesion}(c_i) = \\sum_{x \\in c_i, y \\in c_i} \\text{proximity}(x, y) \\end{align} \\begin{align} \\text{separation}(c_i, c_j) = \\sum_{x \\in c_i, y \\in c_j} \\text{proximity}(x, y) \\end{align} it should be noted that the cohesion metric defined above is equivalent to the cluster sse [sum of squared errors]: \\begin{align} sse(c_i) = \\sum_{x \\in c_i} d(c_i, x)^2 = \\frac{1}{2m_i} \\sum_{x \\in c_i} \\sum_{y \\in c_i} d(x, y)^2 \\end{align} likewise, we can maximize the distance between clusters using a separation metric. this approach leads to the between group sum of squares, or ssb: \\begin{align} ssb = \\sum_{i = 1}^k m_i d(c_i, c)^2 = \\frac{1}{2k} \\sum_{i=1}^k \\sum_{j = 1}^k \\frac{m}{k} d(c_i, c_j)^2 \\end{align} where \\(c_i\\) is the mean of the \\(i\\)th cluster and \\(c\\) is the overall mean. instead of dealing with separate metrics for cohesion and separation, there are several metrics that try to quantify the level of separation and cohesion in a single measure: the calisnki-harabasz coefficient: it is a measure based on the internal dispersion of clusters and the dispersion between clusters. we would choose the number of clusters that maximizes the ch. \\begin{align} ch = \\frac{\\frac{ssb_m}{m - 1}}{\\frac{sse_m}{m}} \\end{align} the dunn index is the ratio of the smallest distance between data from different clusters and the largest distance between clusters. again, this ratio should be maximized: \\begin{align} d = \\min_{1 < i < k} \\left\\{\\min_{1 < j < k, i\\neq j} \\left\\{\\frac{\\delta (c_i, c_j)}{\\max_{1 < l < k} \\{\\delta (c_l)\\}}\\right\\}\\right\\} \\end{align} the xie-beni score was designed for fuzzy clustering, but it can applied to hard clustering. it is a ratio whose numerator estimates the level of compaction of the data within the same cluster and whose denominator estimates the level of separation of the data from different clusters: \\begin{align} xb = \\frac{\\sum_{i=1}^n \\sum_{k=1}^m u^2_{ik} ||x_i - c_k||^2}{n_{t \\neq s} \\min (||c_t - c_s||^2)} \\end{align} the ball-hall index is a dispersion measure based on the quadratic distances of the cluster points with respect to their centroid \\begin{align} bh = \\frac{sse_m}{m} \\end{align} the hartigan index is based on the logarithmic relationship between the sum of squares within the cluster and the sum of squares between clusters: \\begin{align} h = \\log \\left(\\frac{ssb_m}{sse_m}\\right) \\end{align} the xu coefficient takes into account the dimensionality \\(d\\) of the data, the number \\(n\\) of data examples, and the sum of squared errors \\(sse_m\\) form \\(m\\) clusters: \\begin{align} x_u = d \\log_2 \\left(\\sqrt{\\frac{sse_m}{dn^2}}\\right) + \\log m \\end{align} the silhouette coefficient is the most common way to combine the metrics of cohesion and separation in a single measure. its computation is divided into four steps: compute the average intracluster distance for each example \\(i\\): \\(a(i) = \\frac{1}{|c_a|} \\sum_{j \\in c_a, i \\neq j} d(i, j)\\) compute the minimum intercluster distance for each example \\(i\\): \\(b(i) = \\min_{c_b \\neq c_a} \\frac{1}{|c_b|} \\sum_{j \\in c_b} d(i, j)\\) compute the silhouette coefficient for each example \\(i\\): \\(s(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))}\\) compute the silhouette puntuation as the average of the silhouette coefficients: \\(s = \\frac{1}{n} \\sum_{i = 1}^n s(i)\\) the silhouette is defined in the interval \\([-1, 1]\\). positive values indicate a high separation between clusters, negative values are an indication that the clusters are mixed with each other. when the silhouette coefficient is zero, it is an an indication that the data are uniformly distributed throughout the euclidean space. unfortunately, one of the main drawbacks of the silhouette coefficient is its high computational complexity. cohesion and separation metrics are not the only validation method available for partitional clustering techniques. in fact, cohesion and separation metrics do not perform well when it comes to analyzing results obtained by algorithms based on density analysis. one way to validate clustering is by comparing the actual proximity matrix with an ideal version based on the provided clustering by the algorithm. if we reorder rows and columns so that all examples of the same cluster appear together, the ideal proximity matrix has a block diagonal structure. high correlation between the actual and ideal proximity matrices indicates that examples in the same cluster are close to each other, although it may not be a good measure for density-based clustering. imagine you have a table where each row and column represents a data point, and the cells contain numbers indicating how similar or close those data points are to each other. now, if you group similar data points together into clusters, you can rearrange the rows and columns of the table so that all the data points within each cluster are together. when you do this, the table will have a diagonal pattern where each cluster forms a block of closely related data points. this diagonal pattern is what we mean by a \"block diagonal structure\" in the context of a proximity matrix. unfortunately, the mere construction of the whole proximity matrix is computationally expensive. hierarchical methods cophenetic correlation coefficient the cophenetic distance between two examples is the proximity at which an agglomerative hierarchical clustering algorithm puts the examples in the same cluster for the first time. the cophenetic correlation coefficient (cpcc) is defined as he correlation between the entries of the cophenetic matrix \\(p_c\\) containing cophenetic distances, and the proximity matrix \\(p\\), containing similarities. the cophenetic correlation coefficient is then defined as: \\begin{align} \\text{cpcc} = \\frac{\\sum_{i < j} (d_{ij} - \\overline{d})(d_{ij}^* - \\overline{d}^*)}{\\sqrt{\\sum_{i < j} (d_{ij} - \\overline{d})^2 \\sum_{i < j}(d_{ij}^* - \\overline{d}^*)}} \\end{align} where \\(d_{ij}\\) is the distance between the example pair \\((i, j)\\), \\(d_{ij}^*\\) is their cophenetic distance, \\(\\overline{d}\\) is the average of the distances in the proximity matrix and \\(d_{ij}^*\\) is the average of the cophenetic distances in the cophenetic matrix. \\begin{align} \\overline{d} = \\frac{\\sum_{i < j} d_{ij}}{2(n^2 - n)} \\end{align} \\begin{align} \\overline{d}^* = \\sqrt{\\frac{\\sum_{i < j} (d_{ij} - d_{ij}^*)^2}{\\sum_{i < j} (d_{ij}^*)^2}} \\end{align} the cophenetic correlation coefficient is a value in the interval \\([â1, 1]\\). high cpcc values indicate a high level of similarity between the two matrices, an indication that the clustering algorithm has been able to identify the underlying structure of its input data. hubert statistic first, concordance are discordance are defined for pairs of examples. a pair \\((i, j)\\) is concordant when \\(((v_{p_i} < v_{c_i}) \\& (v_{p_j} < v_{c_j}))\\) or \\(((v_{p_i} > v_{c_i}) \\& (v_{p_j} > v_{c_j}))\\). and it is said to be discordant when \\(((v_{p_i} < v_{c_i}) \\& (v_{p_j} > v_{c_j}))\\) or \\(((v_{p_i} > v_{c_i}) \\& (v_{p_j} < v_{c_j}))\\). therefore, a pair is neither concordant nor discordant if \\(v_{p_i} = v_{c_i}\\) or \\(v_{p_j} = v_{c_j}\\). let \\(s_+\\) and \\(s_-\\) be the number of concordant and discordant pairs, respectively. then, the hubert coefficient is defined as: \\begin{align} \\gamma = \\frac{s_+ - s_-}{s_+ + s_-} \\end{align} the hubert statistic is between \\(-1\\) and \\(1\\). it has been mainly used to compare the results of two hierarchical clustering algorithms. a higher hubert \\(\\gamma\\) value corresponds to a better clustering of data. external validation external validation proceeds by incorporating additional information in the clustering validation process, i.e. external class labels for the training examples. we want to compare the result of a clustering algorithm \\(c = \\{c_1, c_2, \\cdots, c_m\\}\\) to a potentially different partition of data \\(p = \\{p_1, p_2, \\cdots, p_s\\}\\) which might represent the expert knowledge of the analyst (his experience or intuition), prior knowledge of the data in the form of class labels, the results obtained by another clustering algorithm, or simply a grouping considered to be \"correct\". in order to carry out this analysis, a contingency matrix must be built to evaluate the clusters detected by the algorithm that encompasses the following data: \\(tp\\): the number of data pairs found in the same cluster, both in \\(c\\) and in \\(p\\). \\(fp\\): the number of data pairs found in the same cluster in \\(c\\) but in different clusters in \\(p\\). \\(fn\\): the number of data pairs found in different clusters in \\(c\\) but in the same cluster in \\(p\\). \\(tn\\): the number of data pairs found in different clusters, both in \\(c\\) and in \\(p\\). matching sets several measures can be defined to measure the similarity between the clusters in \\(c\\), obtained by the clustering algorithm, and the clusters if \\(p\\), corresponding to our prior (external) knowledge: precision: \\(pr = \\frac{tp}{tp + fp}\\) recall: \\(r = \\frac{tp}{tp + fn}\\) f-measure: \\(f_{\\alpha} = \\frac{1 + \\alpha}{\\frac{1}{pr} + \\frac{\\alpha}{r}}\\) quite often, precision and recall are evenly combined with an unweighted harmonic mean (\\(\\alpha = 1\\)): \\begin{align} f = \\frac{2 \\cdot pr \\cdot r}{pr + r} \\end{align} purity: evaluates whether each cluster contains only examples from the same class: \\begin{align} u = \\sum_{i} p_i (\\max_j \\frac{p_{ij}}{p_i}) \\end{align} where \\(p_i = \\frac{n_i}{n}\\), \\(p_j = \\frac{n_j}{n}\\) and \\(p_{ij} = \\frac{n_{ij}}{n}\\). where \\(n_{ij}\\) are the number of examples belonging to the class \\(i\\) found in the cluster \\(j\\) and \\(n_i\\) is the number of examples in the cluster \\(i\\). peer-to-peer correlation a second family of measures for external validation are based on the correlation between pairs, i.e. they seek to measure the similarity between two partitions under equal conditions, such as the result of a grouping process for the same set, but by means of two different methods \\(c\\) and \\(p\\). the jaccard coefficient: \\(j = \\frac{tp}{tp + fp + fn}\\) the rand coefficient: \\(rand = \\frac{tp + tn}{m}\\) the folkes and mallows coefficient: \\(fm = \\sqrt{\\frac{tp}{tp + fp} \\cdot \\frac{tp}{tp + fn}}\\) the hubert statistical coefficient: \\(\\gamma = \\frac{1}{m} \\sum_{i=1}^{n - 1} \\sum_{j = i + 1}^{n} x_{ij} y_{ij}\\) where \\(n_{ij}\\) are the number of examples belonging to the class \\(i\\) found in the cluster \\(j\\) and \\(n_i\\) is the number of examples in the cluster \\(i\\). measures based on information theory this family includes basic measures such as entropy and mutual information, as well as their respective normalized variants. entropy: \\(h = - \\sum_{i} p_i \\left(\\sum_{j} \\frac{p_{ij}}{p_i} \\log \\frac{p_{ij}}{p_i}\\right)\\) mutual information: \\(mi = \\sum_{i} \\sum_{j} p_{ij} \\log \\frac{p_{ij}}{p_i p_j}\\) where \\(p_{ij} = \\frac{n_{ij}}{n}\\) and \\(p_i = \\frac{n_i}{n}\\). hyperparameter tuning even though external validation metrics can help us evaluate whether the obtained clusters closely match the underlying categories in the training data, which the clustering algorithm tries to identify without externally-provided class labels, those metrics cannot address other issues such as the right number of clusters for our current data set. hyperparameter tuning tries to determine, for the different possible values of the parameters in \\(p_{alg}\\) , which set of parameter values is the most suitable for our particular clustering problem. we could proceed in the following way: when the algorithm does not include the number of clusters \\(n_c\\) among its parameters, we run the algorithm with different values for its parameters so that we can determine their largest range for which \\(n_c\\) remains constant. later, we choose as parameter values the values in the middle of this range. when the algorithm parameters palg include the desired number of clusters \\(n_c\\), we run the algorithm for a range of values for \\(n_c\\). for each value of \\(n_c\\), we run the algorithm multiple times using different sets of values (i.e. starting from different initial conditions) and choose the value that optimizes our desired validation metric. when we just want to determine the ârightâ number of clusters, \\(n_c\\), plotting the validation results for different values of \\(n_c\\) can sometimes show a relevant change in the validation metric, commonly referred to as a \"knee\" or \"elbow\". hyperparameter tuning can then be seen as a combinatorial optimization problem using different strategies: grid search: is based on a systematic exploration of the hyperparameter space. random search: chooses parameter configurations at random. smart search techniques try to optimize the problem of searching for hyperparameter values. different strategies can be implemented, such as bayesian optimization using gaussian processes and evolutionary optimization using genetic algorithms or evolution strategies. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T4/02_cluster_analysis.html",
    "title": "Cluster Analysis",
    "body": " index search search back cluster analysis contents proximity matrices dissimilarities based on attributes quantitative variables ordinal variables categorical variables object dissimilarity clustering algorithms combinatorial algorithms k-means gaussian mixtures as soft k-means clustering vector quantization k-medoids practical issues hierarchical clustering agglomerative clustering divisive clustering cluster analysis, also called data segmentation, has a variety of goals. all relate to grouping or segmenting a collection of objects into subsets or \"clusters\", such that those within each cluster are more closely related to one another than objects assigned to different clusters. the method we use to group these objects depends on what we consider to be similar, and that decision usually comes from what we know about the subject we're studying. an object can be described by a set of measurements, or by its relation to other objects. proximity matrices sometimes the data is represented directly in terms of the proximity (alikeness or affinity) between pairs of objects. this type of data can be represented by an \\(n \\times n\\) matrix \\(d\\), where \\(n\\) is the number of objects, and each element \\(d_{ij}\\) records the proximity between the \\(j\\)th and \\(j\\)th objects. this matrix is then provided as input to the clustering algorithm. most algorithms assume symmetric dissimilarity matrices, so if the original matrix \\(d\\) is not symmetric it must be replaced by \\(\\frac{(d + d^t)}{2}\\). dissimilarities based on attributes since most of the popular clustering algorithms take a dissimilarity matrix as their input, we must first construct pairwise dissimilarities between the observations. by far the most common choice is squared distance: \\begin{align} d_j(x_{ij}, x_{i'j}) = (x_{ij} - x_{i'j})^2 \\end{align} where \\(j\\) denotes the attribute and \\(i, i'\\) denotes the instance. we first discuss alternatives in terms of the attribute type: quantitative variables measurements of this type of variable or attribute are represented by continuous real-valued numbers. one way to do this is by looking at the absolute difference between them: \\begin{align} d(x_i, x_{i'}) = l(|x_i - x_{i'}|) \\end{align} alternatively, clustering can be based on the correlation \\begin{align} \\rho (x_i, x_{i'}) = \\frac{\\sum_{j} (x_{ij} - \\overline{x}_i)(x_{ij} - \\overline{x}_i)}{\\sqrt{\\sum_{j} (x_{ij} - \\overline{x}_i)^2 \\sum_j(x_{ij} - \\overline{x}_i)^2}} \\end{align} ordinal variables error measures for ordinal variables are generally defined by replacing their \\(m\\) original values with: \\begin{align} \\frac{i - \\frac{1}{2}}{m}, i = 1, \\cdots, m \\end{align} in the prescribed order of their original values. they are then treated as quantitative variables on this scale. categorical variables if the variable assumes \\(m\\) distinct values, these can be arranged in a symmetric \\(m \\times m\\) matrix with elements: \\begin{align} m_{ij} = \\begin{cases} 0 & x_{i} = x_{j} \\\\ 1 & x_{i} \\neq x_{j} \\\\ \\end{cases} \\end{align} object dissimilarity next we define a procedure for combining the \\(p\\)-individual attribute dissimilarities \\(d_j(x_{ij},x_{i'j}), j = 1,2, \\cdots, p\\) into a single overall measure of dissimilarity \\(d(x_i, x_i')\\). this is nearly always done by means of a weighted average: \\begin{align} d(x_i, x_{i'}) = \\sum_{j=1}^p w_j d_j(x_{ij}, x_{i'j}) \\end{align} where: \\begin{align} \\sum_{j=1}^p w_j = 1 \\end{align} here \\(w_j\\) is a weight assigned to the \\(j\\)th attribute regulating the relative influence of that variable in determining the overall dissimilarity between objects. if the goal is to discover natural groupings in the data, some attributes may exhibit more of a grouping tendency than others. variables that are more relevant in separating the groups should be assigned a higher influence in defining object dissimilarity. giving all attributes equal influence in this case will tend to obscure the groups to the point where a clustering algorithm cannot uncover them. specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering algorithm. clustering algorithms clustering algorithms fall into three distinct types: combinatorial algorithms: work directly on the observed data with no direct reference to an underlying probability model. mixture modeling: supposes that the data is an i.i.d sample from some population described by a probability density function. mode seeking: take a nonparametric perspective, attempting to directly estimate distinct modes of the probability density function combinatorial algorithms each observation is uniquely labeled by an integer \\(i \\in {1, \\cdots, n}\\). one seeks the particular encoder \\(c^*(i)\\) that assigns the \\(i\\)th observation to the \\(k\\)th cluster that satisfies the required goal based on the dissimilarities \\(d(x_i, x_{i'})\\). the \"parameters\" of the procedure are the individual cluster assignments for each of the \\(n\\) observations. these are adjusted so as to minimize a \"loss\" function. since the goal is to assign close points to the same cluster, a natural loss function would be: \\begin{align} w(c) = \\frac{1}{2} \\sum_{k=1}^k \\sum_{c(i) = k}\\sum_{c(i') = k} d(x_i, d_{i'}) \\end{align} this measure tells us how close together the things in the same group are. it is sometimes referred to as the \"within cluster\" point scatter. the total point scatter is given by: \\begin{align} t = \\frac{1}{2} \\sum_{i=1}^n\\sum_{i'=1}^n d_{ii'} = \\frac{1}{2}\\sum_{k=1}^k\\sum_{c(i) = k} \\left(\\sum_{c(i') = k} d_{ii'} + \\sum_{c(i') \\neq k} d_{ii'}\\right) \\end{align} this basically divides, for each \\(i\\)th instance on cluster \\(k\\), so \\(c(i) = k\\), the distances into two categories, distances to instances on the same cluster \\(\\sum_{c(i') = k} d_{ii'}\\), and distances to instances on a different cluster \\(\\sum_{c(i') \\neq k} d_{ii'}\\). that is: \\begin{align} t = w(c) + b(c) \\end{align} where \\(b(c)\\) is the between-cluster point scatter: \\begin{align} b(c) = \\frac{1}{2} \\sum_{k=1}^k \\sum_{c(i) = k} \\sum_{c(i')\\neq k} d_{ii'} \\end{align} so, minimizing \\(w(c)\\) is equivalent to maximizing \\(b(c)\\) given \\(w(c) = t - b(c)\\). cluster analysis by combinatorial optimization is straightforward in principle. one simply minimizes \\(w\\) or equivalently maximizes \\(b\\) over all possible assignments of the \\(n\\) data points to \\(k\\) clusters. unfortunately, such optimization by complete enumeration is feasible only for very small data sets. for this reason, practical clustering algorithms are able to examine only a very small fraction of all possible encoders \\(k = c(i)\\). the goal is to identify a small subset that is likely to contain the optimal one, or at least a good suboptimal partition. such feasible strategies are based on iterative greedy descent. an initial partition is specified. at each iterative step, the cluster assignments are changed in such a way that the value of the criterion is improved from its previous value. clustering algorithms of this type differ in their prescriptions for modifying the cluster assignments at each iteration. when the prescription is unable to provide an improvement, the algorithm terminates with the current assignments as its solution. however, these algorithms converge to local optima which may be highly suboptimal when compared to the global optimum. k-means the k-means algorithm is one of the most popular iterative descent clustering methods, which uses the squared euclidean distance. so the within point-scatter can be written as: \\begin{align} w(c) = \\frac{1}{2} \\sum_{k=1}^k\\sum_{c(i) = k}\\sum_{c(i')=k} ||x_i - x_{i'}||^2 \\end{align} \\begin{align} = \\sum_{k=1}^k n_k \\sum_{c(i) = k} ||x_i - \\overline{x}_k||^2 \\end{align} where \\(\\overline{x}_k\\) is the mean vector associated with the \\(k\\)th cluster and \\(n_k = \\sum_{i=1}^n i(c(i) = k)\\) is the number of instances on the \\(k\\)th cluster. therefore the goal is to group the \\(n\\) observations into \\(k\\) clusters in a way that minimizes the average difference between each observation and the mean of its cluster. \\begin{align} c^* \\min_{c} \\sum_{k=1}^k n_k \\sum_{c(i) = k} ||x_i - \\overline{x}_k||^2 \\end{align} can be obtained by noting that for any set of observations \\(s\\): \\begin{align} \\overline{x}_s = \\arg \\min_{m} \\sum_{i \\in s} ||x_i - m||^2 \\end{align} that is we defined the centroid for \\(s\\) as the point \\(m\\) that minimizes the sum of distances for each instance on \\(s\\). hence we can obtain \\(c^*\\) by solving the enlarged optimization problem: \\begin{align} \\min_{\\{c, m_k\\}^k_1} \\sum_{k=1}^k n_k \\sum_{c(i) = k} ||x_i - m_k||^2 \\end{align} where for each cluster \\(k\\), we search for the encoder \\(c\\) and the optimal centroid \\(m_k\\). thus the optmimization process can be performed in two steps as seen in algorithm 14.1. on (1) we obtain the optimal centroids \\(m_i, i = 1, \\cdots, k\\), and on (2) we try to find the best encoder \\(c\\) given the set of centroids \\(\\{m_1, \\cdots, m_k\\}\\). gaussian mixtures as soft k-means clustering the k-means clustering procedure is closely related to the em algorithm for estimating a certain gaussian mixture model. the e-step of the em algorithm assigns \"responsibilities\" for each data point based in its relative density under each mixture component (cluster). while the m-step recomputes the component density parameters based on the current responsibilities. where the relative density is a monotone function of the euclidean distance between the data point and the mixture center. hence in this setup em is a \"soft\" version of k-means clustering, making probabilistic (rather than deterministic) assignments of points to cluster centers. as \\(\\sigma^2\\) tends to \\(1\\), these probabilities become \\(0\\) and \\(1\\), and the two methods coincide: vector quantization vector quantization (vq) is a technique used in data compression, particularly in the compression of digital signals or images. it involves representing a large set of data points (vectors) by a smaller set of representative values, called codewords or centroids. these centroids are selected from the original data set and are used to approximate the original data. by replacing groups of similar data points with these representative values, vector quantization can significantly reduce the amount of data needed to represent the information while minimizing the loss of quality. in this example of vector quantization, given an image of \\(1024\\times 1024\\) pixels we start by dividing the image into small blocks of \\(2\\times 2\\). each block, which contains four pixels, is treated like a tiny picture of its own. each of the \\(512 \\times 512\\) blocks of four numbers is regarded as vector in \\(\\mathbb{r}^4\\). then, using k-means clustering, we group these blocks together based on their similarity. the clustering process is called the encoding step, and the collection of centroids is called the codebook. why do we expect vq to work at all? the reason is that for typical everyday images like photographs, many of the blocks look the same. what we have described is known as lossy compression, since our images are degraded versions of the original. k-medoids the k-means algorithm can be generalized for use with arbitrarily defined dissimilarities \\(d(x_i, x_{i'})\\). the process of finding the centers of the clusters stays similar, but the way we measure similarity can change. this makes the algorithm versatile and applicable to various types of data. this gives way to the k-medoids algorithm: practical issues in order to apply k-means or k-medoids one must select the number of clusters \\(k^*\\) and an initialization. the latter can be defined by specifying an initial set of centers \\(\\{m_1,\\cdots,m_k\\}\\) or \\(\\{i_1, \\cdots,i_k\\}\\) or an initial encoder \\(c(i)\\). a choice for the number of clusters \\(k\\) depends on the goal. for data segmentation \\(k\\) is usually defined as part of the problem. data-based methods for estimating \\(k^*\\) typically examine the withincluster dissimilarity \\(w_k\\) as a function of the number of clusters \\(k\\). the corresponding values generally decrease with increasing \\(k\\). thus cross-validation techniques, so useful for model selection in supervised learning, cannot be utilized in this context. as we increase the number of clusters, the solution quality will improve because more natural groups will be captured separately. so, if we keep adding more clusters beyond the true number (\\(k > k^*\\)), some estimated clusters will start to split the real groups. however, splitting a group that's already close together won't improve the solution as much as properly separating two distinct groups. to the extent this scenario is realized, there will be a sharp decrease in successive differences in criterion value, \\(w_k â w_{k+1}\\), at \\(k = k^*\\). that is, \\(\\{w_k â w_{k+1} |k < k^*\\} \\geq \\{w_k â w_{k+1} |k \\geq k^*\\}\\). an estimate \\(\\hat{k}*\\) for \\(k^*\\) is then obtained by identifying a âkinkâ in the plot of \\(w_k\\) as a function of \\(k\\). the recently proposed gap statistic compares the shape of a curve based on our data to a curve we'd get if the data were spread out evenly. we're looking for a point where there's a big gap between these two curves. this point tells us the best number of clusters for our data. if \\(g(k)\\) is the gap curve at \\(k\\) clusters, the formal rule for estimating \\(k^*\\) is: \\begin{align} k^* = \\text{argmin}_{k} \\{k | g(k) \\geq g(k + 1 - s'_{k + 1}) \\} \\end{align} where \\(s_k\\) is the standard deviation. the following figure shows and example on how to choose the optimal number of clusters: hierarchical clustering as the name suggests, they produce hierarchical representations in which the clusters at each level of the hierarchy are created by merging clusters at the next lower level. at the lowest level, each cluster contains a single observation. at the highest level there is only one cluster containing all of the data. strategies for hierarchical clustering divide into two basic paradigms: agglomerative (bottom-up) and divisive (top-down). each level of the hierarchy represents a particular grouping of the data into disjoint clusters of observations. the entire hierarchy represents an ordered sequence of such groupings. it is up to the user to decide which level (if any) actually represents a \"natural\" clustering a dendrogram provides a highly interpretable complete description of the hierarchical clustering in a graphical format. cutting the dendrogram horizontally at a particular height partitions the data into disjoint clusters represented by the vertical lines that intersect it. the height at which we cut represents the level of similarity required to form a cluster. generally, groups that merge at higher levels in the dendrogram are considered more significant clusters. however, it's essential to be cautious when interpreting dendrograms because different clustering methods or slight changes in the data can lead to different dendrogram structures. also, dendrogram interpretations are only valid if the data truly exhibits the hierarchical structure imposed by the clustering algorithm. this can be assesed using the cophenetic correlation coefficient, it measures the correlation between the cophenetic dissimilarities \\(c_{ii'}\\) and the distances between observations in the original data \\(d_{ii'}\\). the cophenetic dissimilarity \\(c_{ii'}\\) between two observations \\((i, i')\\) is the intergroup dissimilarity at which observations \\(i\\) and \\(i'\\) are first joined together in the same cluster. agglomerative clustering agglomerative clustering starts with each observation as its own cluster. then, at each step, it merges the two closest clusters together until there's only one big cluster left. to measure dissimilarity between two clusters, let's call them \\(g\\) and \\(h\\). we look at all the pairs of observations, one from \\(g\\) and one from \\(h\\), and find the dissimilarity between them. single linkage (sl) or nearest neighbour agglomerative clustering chooses the closest pair of observations between the two clusters as the measure of dissimilarity between the clusters: \\begin{align} d_{sl}(g, h) = \\min_{i \\in g, i' \\in h} d_{ii'} \\end{align} complete linkage (cl) or furthest-neighbor technique agglomerative clustering measures the dissimilarity between two clusters as the distance the pair of observations that are the farthest apart. \\begin{align} d_{cl}(g, h) = \\max_{i \\in g, i' \\in h} d_{ii'} \\end{align} group average (ga) clustering uses the average dissimilarity between the groups: \\begin{align} d_{ga}(g, h) = \\frac{1}{n_g n_h} \\sum_{i \\in g} \\sum_{i \\in h} d_{ii'} \\end{align} where \\(n_g\\) and \\(n_h\\) are the respective number of observations in each group. if the data shows clear clusters that are close together and distinct from each other, all three methodsâsingle linkage, complete linkage, and average linkageâwill give similar results. however, if the data doesn't exhibit this pattern, the results of the three methods will differ. single linkage tends to join clusters even if just one pair of observations is close together, which can lead to long chains of connections between clusters. this phenomenon is known as chaining. complete linkage will tend to produce compact clusters with small diameters because it considers two groups close only if all the observations in their combined set are similar. sometimes it may violate the rule that observations within a cluster should be closer to each other than to observations in other clusters. group average clustering strikes a balance between single and complete linkage. it tries to make clusters compact while keeping them relatively far apart. however, its outcome can be affected by how the dissimilarities between observations are measured. changing the measurement scale can change the clustering result. in contrast, single and complete linkage methods are not affected by such changes in scale. divisive clustering divisive clustering starts with the entire dataset as one cluster and then splits it into smaller clusters step by step. this method isn't as widely studied as agglomerative clustering, but it has been explored, especially in engineering contexts like compression. one potential advantage of divisive clustering is when you want to divide the data into only a few clusters. divisive methods can be used recursively with techniques like k-means or k-medoids to split clusters into smaller ones. however the way you begin the splitting process at each step can influence the final outcome. a method has been developed to overcome this limitations. the divisive algorithm, proposed by macnaughton smith et al. (1965), is defined as: puts all observations in one big cluster called \\(g\\). it picks the observation that's farthest on average from all the others and makes it the first member of a new cluster called \\(h\\). at each successive step that observation in \\(g\\) whose average distance from those in \\(h\\), minus that for the remaining observations in \\(g\\) is largest, is transferred to \\(h\\). this continues until there are no more observations in \\(g\\) that are closer to those in \\(h\\). this splitting procedure is repeated for each new cluster formed at the previous level, creating a hierarchical structure. kaufman and rousseeuw (1990) suggest choosing the cluster with the largest diameter for splitting at each level, but another option is to pick the one with the largest average dissimilarity among its members. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/index.html",
    "title": "Aprendizaje AutomÃ¡tico II",
    "body": " index search search back aprendizaje automÃ¡tico ii tema 1. random forests bias/variance tradeoff bootstrapping bagging random forests tema 2. intensificaciÃ³n (boosting) boosting ada boost boosting trees gradient boosting interpretability examples practice tema 3. otras combinaciones de modelos introducciÃ³n marco general para la combinaciÃ³n de clasificadores estrategias para la combinaciÃ³n de clasificadores conclusiÃ³n tema 4. aprendizaje no supervisado introducciÃ³n cluster analysis evaluation metrics $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T1/01_bias_variance_tradeoff.html",
    "title": "Bias/Variance Tradeoff",
    "body": " index search search back bias/variance tradeoff test error, also referred to as generalization error, is the prediction error over an independent test sample: \\begin{align} err_{\\mathcal{t}} = \\mathbb{e}[l(y | \\hat{f}(x)) | \\mathcal{t}] \\end{align} where \\(l\\) is the loss function. a related quantity is the expected prediction error (or expected test error): \\begin{align} err = \\mathbb{e}[err_{\\mathcal{t}}] \\end{align} where \\(err_{\\mathcal{t}}\\) is the test error. the error can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\) the squared bias of \\(\\hat{f}(x_0)\\) the variance of the error terms \\(\\epsilon\\). that is, \\begin{align} \\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2] = \\mathbb{v}[\\hat{f}(x_0)] + [bias(\\hat{f}(x_0))]^2 + \\mathbb{v}[\\epsilon] \\end{align} this amount is derived from: \\begin{align} err(x_0) = \\mathbb{e}[(y - \\hat{f}(x_0))^2] = \\mathbb{e}[y^2 + \\hat{f}(x_0)^2 - 2y\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[y^2] + \\mathbb{e}[\\hat{f}(x_0)^2] -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} we know that \\(\\mathbb{v}[x] = \\mathbb{e}[(x - \\mathbb{e}[x])^2] = \\mathbb{e}[x^2] - \\mathbb{e}[x]^2\\), such that: \\begin{align} = \\mathbb{v}[y] + \\mathbb{e}[y]^2 + \\mathbb{v}[\\hat{f}(x_0)] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[y]^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} note that, \\(y = f(x_0) + \\epsilon[/\\)], donde [\\(]\\mathbb{e}[\\epsilon] = 0\\), thus it follows: \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + (\\mathbb{e}[f(x_0)] + \\mathbb{e}[\\epsilon])^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2(\\mathbb{e}[f(x_0)] + \\mathbb{e}[\\epsilon])\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[f(x_0)]^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[f(x_0)]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} we know that \\((a + b)^2 = a^2 + b^2 + 2ab\\) and that \\(\\mathbb{e}[f(x_0)] = f(x_0)\\), such that: \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + f(x_0)^2 + \\mathbb{e}[\\hat{f}(x_0)]^2 -2f(x_0)\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\left(\\mathbb{e}[\\hat{f}(x_0)] - f(x_0)\\right)^2 \\end{align} here the notation \\(\\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2]\\) defines the expected test mse, and refers expected to the average test mse that we would obtain if we repeatedly \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). the overall expected test mse can be computed by averaging \\(\\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2]\\) over all possible values of \\(x_0\\) in the test set. the previous equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. hence, we see that the expected test mse can never lie below \\(\\mathbb{v}[\\epsilon]\\), the irreducible error. the variance of the error terms, \\(\\mathbb{v}[\\epsilon]\\), is the variance of the target around its true mean \\(f(x_0)\\), and cannot be avoided no matter how well we estimate \\(f(x_0)\\), unless \\(\\sigma^2 = 0\\). variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. ideally the estimate for \\(f\\) should not vary too much between training sets. this is computed as the expected squared deviation of \\(\\hat{f}(x_0)\\) around its mean. bias refers to the error that is introduced by approximating a real-life problem by a simpler model. this quentifies the amount by which the average of our estimate differs from the true mean. as a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. as we increase the flexibility, the bias tends to initially decrease faster than the variance increases. consequently, the expected test mse declines. at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. in a real-life situation in which \\(f\\) is unobserved, it is generally not possible to explicitly compute the test mse, bias, or variance for a statistical learning method. training error consistently decreases with model complexity, typically dropping to zero if we increase the model complexity enough. a model with zero training error is overfit to the training dat and will typically generalize poorly. it is important to note that there are in fact two separate goals: model selection: estimating the performance of different models in order to choose the best model. model assessment: having chosen a final model, estimating its prediction error (generalization error) on new data. the training set is used to fit the models. the validation set is used to estimate prediction error for model selection. the test set is used for assessment of the generalization error $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T1/04_random_forests.html",
    "title": "Random Forests",
    "body": " index search search back random forests contents definition details of random forests variable importance proximity plots random forests and overfitting definition the idea in random forests (ilustrated on the image below) is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. this is achieved in the tree-growing process through random selection of the input variables. an average of \\(b\\) i.i.d. random variables, each with variance \\(\\sigma^2\\), has variance \\(\\frac{1}{b}\\sigma^2\\). if the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation Ï, the variance of the average is: \\begin{align} \\rho\\sigma^2 + \\frac{1 - \\rho}{b}\\sigma^2 \\end{align} as \\(b\\) increases, the second term disappears, but the first remains, and hence the size of the correlation of pairs of bagged trees limits the benefits of averaging. when growing a tree on a bootstrapped dataset before each split, select \\(m \\leq p\\) of the input variables at random as candidates for splitting. after \\(b\\) such trees \\(\\{t(x; \\theta_b)\\}_1^b\\) are grown, the random forest (regression) predictor is: \\begin{align} \\hat{f}_{rf}^b(x) = \\frac{1}{b}\\sum_{b=1}^b t(x; \\theta_b) \\end{align} where \\(\\theta_b\\) characterizes the bth random forest tree in terms of split variables, cutpoints at each node, and terminal-node values. intuitively, reducing \\(m\\) will reduce the correlation between any pair of trees in the ensemble. details of random forests for classification, the default value for \\(m\\) is \\(\\lfloor \\sqrt{p} \\rfloor\\) and the minimum node size is one. for regression, the default value for \\(m\\) is \\(\\lfloor \\frac{p}{3} \\rfloor\\) and the minimum node size is five. in practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters (hyperparamters). variable importance at each split in each tree, the improvement in the split-criterion is the importance measure attributed to the splitting variable, and is accumulated over all the trees in the forest separately for each variable. random forests also use the oob samples to construct a different variable importance measure. when the bth tree is grown, the oob samples are passed down the tree, and the prediction accuracy is recorded. then the values for the jth variable are randomly permuted in the oob samples, and the accuracy is again computed. the decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable \\(j\\) in the random forest. proximity plots in growing a random forest, an \\(n \\times n\\) proximity matrix is accumulated for the training data. such that the entry \\(ij\\) contains the number of trees for which the oob sample \\(x_i\\) and the oob sample \\(x_j\\) are on the same terminal node. this proximity matrix is then represented in two dimensions using multidimensional scaling like the following example: the proximity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier. random forests and overfitting when the number of variables \\(p\\) is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \\(m\\). at each split the chance can be small that the relevant variables will be selected. another claim is that random forests âcannot overfitâ the data. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T1/03_bagging.html",
    "title": "Bagging",
    "body": " index search search back bagging contents introduction bagging on regression trees bagging on decision trees out-of-bag error estimation variable importance measures advantages of ensemble models how to generate diversity ensemble algorithms bagging boosting staking model of experts introduction the decision trees discussed suffer from high variance. in contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets. bootstrap aggregation, or bagging, is a procedure for reducing the variance of a statistical learning method. recall that given a set of \\(n\\) independent observations \\(z_1, \\cdots, z_n\\) each with variance \\(\\sigma^2\\), the variance of the mean \\(\\overline{z}\\) of the observations is given by \\(\\frac{\\sigma^2}{n}\\). so, averaging a set of observations reduces variance. thus, to reduce the variance and increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. using \\(b\\) separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by: \\begin{align} \\hat{f}_{avg}(x) = \\frac{1}{b}\\sum_{b=1}^b \\hat{f}^b(x) \\end{align} for each bootstrap sample \\(z^{*b}, b = 1, 2, \\cdots, b\\), we fit our model, giving prediction \\(\\hat{f}^{*b}(x)\\). the bagging estimate is defined by: \\begin{align} \\hat{f}_{bag}(x) = \\frac{1}{b}\\sum_{b=1}^b \\hat{f}^{*b}(x) \\end{align} bagging on regression trees to apply bagging to regression trees, we simply construct \\(b\\) regression trees using \\(b\\) bootstrapped training sets, and average the resulting predictions. these trees are grown deep, and are not pruned. hence each individual tree has high variance, but low bias. averaging these b trees reduces the variance. bagging on decision trees how can bagging be extended to a classification problem where y is qualitative? for a given test observation, we can record the class predicted by each of the \\(b\\) trees, and take a majority vote. suppose our tree produces a classifier \\(\\hat{g}(x)\\) for a \\(k\\)-class response. then the bagged estimate \\(\\hat{f}_{bag}(x)\\) is a \\(k\\)-vector \\([p_1(x), p_2(x), \\cdots, p_k(x)]\\), with \\(p_k(x)\\) equal to the proportion of trees predicting class \\(k\\) at \\(x\\). the bagged classifier selects the class with the most votes from the \\(b\\) trees, \\(\\hat{g}_{bag}(x) = \\arg \\max_k \\hat{f}_{bag}(x)\\). often we require the class-probability estimates at \\(x\\). for many classifiers \\(\\hat{g}(x)\\) there is already an underlying function \\(\\hat{f}(x)\\) that estimates the class probabilities at \\(x\\) (for trees, the class proportions in the terminal node). an alternative bagging strategy is to average these instead. the number of trees \\(b\\) is not a critical parameter with bagging; using a very large value of \\(b\\) will not lead to overfitting. in practice we use a value of \\(b\\) sufficiently large that the error has settled down. note that bagging a good classifier can make it better, but bagging a bad classifier can make it worse. out-of-bag error estimation there is a very straightforward way to estimate the test error of a bagged model one can show that on average, each bagged tree makes use of around two-thirds of the observations. the observations not used to fit a given bagged tree are referred to as the out-of-bag (oob) observations. an oob prediction can be obtained for each of the \\(n\\) observations on the oob observation set, from which the overall oob mse (for a regression problem) or classification error (for a classification problem) can be computed. the resulting oob error is a valid estimate of the test error for the bagged model. variable importance measures although the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the rss (for bagging regression trees) or the gini index (for bagging classification trees) in the case of bagging regression trees, we can record the total amount that the rss is decreased due to splits over a given predictor, averaged over all \\(b\\) trees. for classification trees, we can add up the total amount that the gini index is decreased by splits over a given predictor, averaged over all \\(b\\) trees. advantages of ensemble models performance: it improves single models' perfomance. robustness: reduces predictions' variance. so it also improves the equilibrium between bias and variance. how to generate diversity manipulating instances: selecting a different subset of instances for each model. manipulating features: selecting a different subset of features for each model. manipulating models' definition: selecting different hyperparameters, optimization algorithm for ach model. hybridation: mix any of the previous practices. ensemble algorithms bagging models are trained concurrently with different data sets generated using bootstrapping. boosting construye mÃºltiples modelos (tÃ­picamente modelos del mismo tipo) secuenciales, cada uno de los cuales aprende a corregir los errores de predicciÃ³n de un modelo anterior en la cadena. el objetivo es desarrollar un modelo fuerte a partir de muchos modelos dÃ©biles especialmente diseÃ±ados que se combinan mediante votaciÃ³n simple o promediando. staking construye mÃºltiples modelos sobre el mismo conjunto de datos, tÃ­picamente modelos de diferentes tipos (modelos de nivel 0); y un modelo supervisado o meta modelo (modelo de nivel 1) que aprende cÃ³mo combinar mejor las predicciones de los modelos primarios. model of experts podemos dividir el espacio de caracterÃ­sticas de entrada en subespacios segÃºn algÃºn conocimiento de dominio del problema. luego se puede entrenar un modelo en cada subespacio del problema, convirtiÃ©ndose de hecho en un experto en el subproblema especÃ­fico. luego, un modelo aprende a quÃ© experto recurrir para predecir nuevos ejemplos en el futuro. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T1/02_bootstrapping.html",
    "title": "Bootstrapping",
    "body": " index search search back bootstrapping bootstrapping is a resampling technique used in statistics to estimate the sampling distribution of a statistic by sampling with replacement from the original dataset. the method is particularly useful when analytical methods for deriving the sampling distribution are complex or unavailable. here's a breakdown of the bootstrapping process: sampling with replacement: from the original dataset, randomly draw \\(n\\) samples with replacement. this means that each observation has an equal chance of being selected for the sample, and an observation may be selected multiple times. sample statistics: calculate the statistic of interest (e.g., mean, median, standard deviation, regression coefficient) on each bootstrapped sample. repeat: repeat steps \\(2\\) and \\(3\\) a large number of times to generate multiple bootstrap samples and their corresponding statistics. estimate sampling distribution: with the collection of bootstrap statistics, you can estimate the sampling distribution of the statistic of interest. this empirical distribution approximates the true sampling distribution of the statistic, providing information about its variability and uncertainty. inference: use the estimated sampling distribution to make inferences about the population parameter or to construct confidence intervals. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/04_gradient_boosting.html",
    "title": "Gradient Boosting",
    "body": " index search search back gradient boosting contents numerical optimization via gradient steepest descent gradient boosting implementations of gradient boosting step length characteristics numerical optimization via gradient imagine you have a machine learning model that makes predictions, but it's not perfect. gradient boosting is like a smart way to teach this model to make better predictions over time. instead of trying to fix all the prediction errors at once, gradient boosting focuses on correcting one error at a time. it does this by looking at the direction where the error is the steepest and making adjustments to improve the prediction in that direction. by repeating this process step by step, the model gradually gets better at making predictions, leading to more accurate results. so if you have the following function you want to optimize: \\begin{align} l(f) = \\sum_{i=1}^n l(y_i, f(x_i)) \\end{align} where \\(f \\in \\mathbb{r}^n\\) is the prediction function and its evaluation at each instance \\(x_i\\) are the parameteres we want to optimize: \\begin{align} f = \\{f(x_1), \\cdots, f(x_i), \\cdots, f(x_n)\\} \\end{align} therefore the optimization problem with respect to \\(f\\) can be summarized as follows: \\begin{align} \\hat{f} = \\arg \\min_f l(f) \\end{align} solving this entire problem at once may be challenging. to make it easier, numerical optimization procedures break down this big problem into smaller pieces, represented by component vectors. each component vector addresses a specific aspect of the problem. so: \\begin{align} f_{m} = \\sum_{m = 0}^m h_m, h_m \\in \\mathbb{r}^n \\end{align} where \\(f_m\\) represents the final model or prediction function obtained after m iterations or steps of the boosting algorithm. here \\(f_m\\) represents the model at iteration \\(m\\), whereas \\(h_m\\) represents the increment to the model at iteration \\(m\\) it is the component vector added to the current model to move towards the optimized solution. each \\(h_m\\) is induced based on the current parameter vector \\(f_{m-1}\\) and contributes to the overall model improvement. here is a simple layout of how the algorithm optimizes: at the beginning of the gradient boosting process, the initial model \\(f_0\\) is set to an initial guess. as the algorithm progresses through iterations (\\(m = 1, 2, \\cdots, m\\)), each step involves updating the model based on the gradient information to reduce errors in predictions. numerical optimization methods differ in their prescriptions for computing each increment vector \\(h_m\\). steepest descent steepest descent is a method used in optimization to find the minimum value of a function. this method chooses \\(h_m = \\rho_m g_m\\) where \\(\\rho_m\\) is a scalar and \\(g_m\\) is the gradient of \\(l(f_{m-1})\\), that is, the cost function evaluated at values predicted by the \"previous model\". the components of the gradient \\(g_m\\) are defined as follows: \\begin{align} g_{im} = \\left[\\frac{\\delta l(y_i, f(x_i))}{\\delta f(x_i)}\\right]_{f_m(x_i) = f_{m-1}(x_i)} \\end{align} the step length (kinda like the learning rate): \\begin{align} \\rho_m = \\arg \\min_{\\rho} l(f_{m-1} - \\rho g_m) \\end{align} thus, at each step, the predictor is updated as follows: \\begin{align} f_m = f_{m - 1} - \\rho_m g_m \\in \\mathbb{r}^n \\end{align} this updates \\(f_m\\) towards the direction of maximum descent at \\(l(f_{m-1})\\), which is why this is often interpreted as a greedy algorithm. gradient boosting gradient boosting aims to create a strong predictive model by combining multiple weak models. it starts with a simple model and gradually enhances it to minimize errors. at each iteration, a new tree model is fit to the negative gradient of the loss function. the predictions from these trees guide the model towards better predictions. using squared error to measure closeness, this leads us to: \\begin{align} \\tilde{\\theta}_m = \\arg \\min_{\\theta} \\sum_{i=1}^n (-g_{im} - t(x_i; \\theta))^2 \\end{align} this measures how close each prediction \\(t(x_i; \\theta)\\) is to the gradient \\(-g_{im}\\). the negative gradient of the loss function represents the direction in which the model's predictions need to be adjusted to reduce errors. by fitting a new tree to this negative gradient, the model learns how to correct its predictions to move closer to the actual target values. that is at each iteration, the new tree model focuses on capturing the errors or residuals of the current ensemble model. while the exact regions where the new tree makes corrections may not match perfectly with the original model's regions, they are close enough to serve the same purpose of improving the model's accuracy. here the original model is the ensemble model. the following figure summarizes the gradients for commonly used loss functions: implementations of gradient boosting algorithm \\(10.3\\) presents the generic gradient tree-boosting algorithm for regression. specific algorithms are obtained by inserting different loss criteria \\(l(y,f(x))\\). start with an initial model \\(f_0(x)\\) that minimizes the loss function \\(l(y, f(x))\\). for each boosting round \\(m = 1, \\cdots, m\\): calculate the negative gradient for each data point \\begin{align} r_{im} = -\\left[\\frac{\\delta l(y_i, f(x_i))}{\\delta f(x_i)}\\right]_{f(x_i) = f_{m-1}(x_i)} \\end{align} fit a regression tree to the gradients \\(r_{im}\\), which gives us the regions \\(r_{jm}, j = 1, 2, \\cdots, j_m\\) the step length \\(\\gamma\\) is determined by minimizing the loss using the previous model (\\(f_{m-1}\\)): \\begin{align} \\gamma_{jm} = \\arg \\min_{\\gamma} \\sum_{x_i \\in r_{jm}} l(y_i, f_{m-1}(x_i) + \\gamma) \\end{align} update the model by adding a new tree to the ensemble \\(f_m(x) = f_{m-1}(x) + \\gamma t(x; \\theta_m)\\), where \\(t(x; \\theta)\\) is the new tree model with parameters \\(\\theta_m\\) that corrects the errors in the previous model. the output of the ensemble model is defined as \\(\\hat{f}(x) = f_m(x)\\), that is as the sum of the weaker models. step length the step length \\(\\gamma\\) is crucial in determining how much each new tree should contribute to the ensemble model. it controls the impact of the new tree on the overall model's predictions. the line search aims to find the value of Î³ that minimizes the loss function: \\begin{align} l(f_{m-1} - \\gamma g_m) \\end{align} this means finding the optimal step length that results in the smallest possible loss when updating the model with the new tree. by minimizing the loss function with respect to \\(\\gamma\\), the algorithm is essentially performing a form of gradient descent. characteristics two basic tuning parameters are the number of iterations \\(m\\) and the sizes of each of the constituent trees \\(j_m, m = 1, 2, \\cdots, m\\). the original implementation of this algorithm was called mart for \"multiple additive regression trees\". $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/07_practice.html",
    "title": "Practice",
    "body": " index search search back practice contents boosting parameters building energy efficiency default gbm building energy efficiency tuned gbm mnist default gbm mnist tuned gbm boosting just like random forest, gbm is an ensemble method. imagine a data set just \\(10\\) examples and two numeric predictor variables, and we are trying to learn to distinguish between two possible classes: circle or cross. the very simplest decision tree we can make has just one node; i will represent it with a straight line in the following diagrams. it scored \\(60\\%\\): six right, four wrong. what we do now is train another very simple tree, but first we modify the training data to give the four rows it got wrong a higher weight. how much of a higher weight? that is where the \"gradient\" bit of gbm comes in. in the next figure the circles and crosses for the wrong items are bigger, and our next tree pays more attention to them. it got a different three items wrong so it still scores \\(60\\%\\). so, for our third tree, we tell it those four are more important; the one it has got wrong twice in a row is the biggest of all. if we stop training here, we end up with three weak models that scored \\(60\\%\\), \\(60\\%\\), and \\(80\\%\\), respectively. however, at least one of each of those three trees got every training row correct. you can see how they can work together to cover each other's weaknesses. gbm naturally focuses attention on the difficult rows in your training data, the ones that are hard to learn. that is good, but it can also be bad. if there is one outlier that each tree keeps getting wrong it is going to get boosted and boosted until it is bigger than the whole universe. this is bad when the data is a mistake instead of an outlier, as it distorts the model's accuracy. the mysterious? well, unlike (simple) decision trees, which can be really good at explaining their thinking, it becomes a bit of a black box. parameters n_trees: how many trees to make. max_depth: how deep are the trees allowed to be. learn_rate: controls the speed at which the model learns learn_rate_annealing: allows you to have the learn_rate start high, then gradually get lower as trees are added. min_rows: how many examples are needed to make a leaf node. low number might lead to overfitting. min_split_improvement: controls how much error improvement must be to perform a split. histogram_type: what type of histogram to use for finding optimal split points. nbins: for numerical columns, build a histogram of (at least) this many bins, then split at the best point. nbins_cat: for categorical columns, build a histogram of (at most) this many bins, then split at the best point. build_tree_one_node: run on one node only. building energy efficiency: default gbm this data set deals with the heating/cooling costs of various house designs. from h2o.estimators.gbm import h2ogradientboostingestimator m = h2ogradientboostingestimator(model_id=\"gbm_defaults\", nfolds=10) m.train(x, y, train) fifty trees were made, each of depth \\(5\\). on cross-validation data, the mse (mean squared error) is \\(2.462\\), and \\(r^2\\) is \\(0.962\\). under âvariable importancesâ (shown next), which can be seen with h2o.varimp(m) you will see it is giving x5 way more importance than any of the others; this is typical for gbm models. how about on the unseen data? m.model_performance(test) is saying mse is \\(2.318\\), better than on the training data. building energy efficiency: tuned gbm i decided to start, this time, with a big random grid search. the hyperparameters tuned are the following: max_depth: the default is \\(5\\), and we tried \\(5,10,15,20,25,30,40,50,60,75,90\\). the ninth best model was max_depth=75, so high values may not be bad, as such, but they donât appear to help. min_rows sample_rate col_sample_rate nbins what about ntrees? instead of trying to tune it, we set it high (\\(1000\\)) and used early stopping. more model results just confirmed the first impression: min_rows of \\(1\\) (or \\(2\\)) is effective with max_depth of \\(5\\), but really poor with higher values. min_rows of \\(10\\) is effective with any value of max_depth, but possibly \\(10\\) to \\(20\\) is best. curiously min_rows of \\(5\\) is mediocre. a sample_rate of \\(0.9\\) or \\(0.95\\) looks best, while there is still no clarity for col_sample_rate or nbins. let's see how it does on the test data, we obtain a mse of \\(1.640\\). this is way better than the default gbmâs 2.462, and also way better than the best tuned random forest model from the previous chapter. mnist: default gbm it is a multinomial classification, trying to look at the \\(784\\) pixels of a handwritten digit, and say which of \\(0\\) to \\(9\\) it is. m = h2o.estimators.h2ogradientboostingestimator(model_id=\"gbm_defaults\") m.train(x, y, train, validation_frame=valid) the confusion matrix on the training data (h2o.confusionmatrix(m)) shows an error rate of \\(2.08\\%\\), while on the validation data it is a bit higher at \\(4.82\\%\\). mse is \\(0.028\\) and \\(0.044\\), respectively. so we have a bit of overfitting on the training data, but not too much. on the test data the error this time is \\(4.44\\%\\) (mse is \\(0.048\\)); in other words, the validation and test sets are giving us similar numbers, which is good. mnist: tuned gbm as usual, the first thing i want to do is switch to using early stopping, so i can then give it lots of trees to work with, with the following parameters: stopping_tolerance = 0.001, stopping_rounds = 3, score_tree_interval = 10, ntrees = 400 just using this, with all other default settings, had some interesting properties training classification score was perfect after 140 trees. validation score was down to \\(2.83\\%\\). the mse and logloss of both the training data and validation data continued to fall, and so did the validation classification score. relative runtime kept increasing. that is, each new tree is taking longer. it finished up with 360 trees, with a very respectable \\(2.17\\%\\) error on the validation data. how can we improve that further? we know there is a lot of examples and variables, so we expect that lower sample ratios will be more effective. in terms of the learn_rate we know low is slower, but betterâ¦ and we have a lot of data. so we use a high (quick) learn_rate for the first grid or two, then lower it later on, once we start to home in on the best parameters. this is going to be a random grid search, because we are going to use a lot of parameters. the first discovery was that a high max_depth was very slow and no better than a shallow one. also min_rows=1 seemed poor. we also found that max_depth=20 was distinctly worse than max_depth=5. we also noticed that min_rows=10 seemed to be doing best, though it was less clear. reducing the three sample rates to \\(1\\) did seem to help, though there was not enough data to draw a confident conclusion. so, another try. we'll leave max_depth and min_rows at their defaults, and just concentrate on testing sampling rates. there was not that much clarity in the parameters, but the best two had col_sample_rate of \\(0.8\\) and sample_rate of \\(0.95\\), whereas sample_rate=0.5 was only chosen once, but was the worst of the nine. the default model with just early stopping added, would have come second best in the grid measured on classification error, but fourth on mse, and seventh on logloss, whereas the âtunedâ model is top on all metrics, so we have more confidence in selecting it. as a final step, we ran the chosen model on the test data and got an error rate of \\(2.33\\%\\). this compares to \\(4.44\\%\\) with the default settings. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/06_examples.html",
    "title": "Examples",
    "body": " index search search back examples contents california housing california housing the dataset consists of information from \\(20,460\\) neighborhoods in california. the target variable (\\(y\\)) is the median house value in each neighborhood. predictor variables include demographic factors like median income (medinc), housing density (house), average occupancy (aveoccup), location coordinates (longitude and latitude), and house attributes like average number of rooms (averooms) and bedrooms (avebedrms). there are thus a total of eight predictors, all numeric. we fit a gradient boosting model using the mart procedure, with \\(j = 6\\) terminal nodes. the test error is seen to decrease monotonically with increasing \\(m\\), more rapidly during the early stages and then leveling off to being nearly constant as iterations increase. thus, the choice of a particular value of \\(m\\) is not critical, as long as it is not too small. the next figure displays the relative variable importances for each of the eight predictor variables. not surprisingly, median income in the neighborhood is the most relevant predictor. longitude, latitude, and average occupancy all have roughly half the relevance of income, whereas the others are somewhat less influential on the following graphs we show single-variable partial dependence plots on the most relevant nonlocation predictors. note that the plots are not strictly smooth as a consequence of using tree-based models. the hash marks at the base of each plot delineate the deciles of the data distribution of the corresponding variables. so for example, \\(90%\\) of the data have a medinc value of less than \\(6\\). the partial dependence of median house value on median income is monotonic increasing. house value is generally monotonic decreasing with increasing average occupancy, except perhaps for average occupancy rates less than one. median house value is seen to have a very weak partial dependence on house age that is inconsistent with its importance ranking. this suggests that this weak main effect may be masking stronger interaction effects with other variables. the next graph shows the two-variable partial dependence of the fitted model on joint values of longitude and latitude: there is a very strong dependence of median house value on the neighborhood location in california. it can be viewed as representing an extra premium one pays for location. this premium is seen to be relatively large near the pacific coast especially in the bay area and los angelesâsan diego. in the northern, central valley, and southeastern desert regions of california, location costs considerably less. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/03_boosting_trees.html",
    "title": "Boosting Trees",
    "body": " index search search back boosting trees contents variable space partitioning optimization problem boosting trees optimization problem optimization objective finding optimal consants solution for regression trees solution for classification trees appendix greedy top-down recurisve partitioning algorithm variable space partitioning partitioning the predictor variable space into regions in boosting trees involves recursively splitting the data based on predictor variables to create distinct regions with associated constant values for making predictions. starting point: entire predictor variable space is considered as one large region, \\(r_1\\). decision making: at each step, a decision tree algorithm finds the best split based on a predictor variable \\(x_j\\) and a split point \\(s\\) that minimizes a certain criterion. this split divides region \\(r_j\\) into two subregions \\(r_{left}\\) and \\(r_{right}\\). splitting criteria: it can be represented as \\((j, s) = arg \\min_{j, s} [\\sum_{x_i \\in r_{left}} l(y_i, f(x_i)) + \\sum_{x_i \\in r_{right}} l(y_i, f(x_i)]\\) recursive process: this process is repeated recursively for each resulting subregion until a stopping criterion is met. terminal nodes: the final regions, or terminal nodes, are denoted as \\(r_j\\) and are assigned constant values \\(\\gamma_j\\) representing the prediction for data points falling into that region. constant assigment: each terminal node is associated with a constant value, resulting in a piecewise constant function, such that each tree can be denoted as: \\(t(x; \\theta) = \\sum_{j=1}^j \\gamma_j i(x \\in r_j)\\) where \\(i(\\cdot)\\) is the indication function and \\(\\theta = \\{r_j, \\gamma_j\\}_1^j\\) are the parameters. prediction: when making predictions for new data points, the algorithm determines the region \\(r_j\\) that the data point belongs to based on the predictor variables. the prediction for that data point is then based on the constant value \\(\\gamma_j\\) assigned to the corresponding region. optimization problem so as we have seen the optimization problem is defined, on a simplified manner, as follows: \\begin{align} \\hat{\\theta} = \\arg \\min_{\\theta} \\sum_{j=1}^j \\sum_{x_i \\in r_j} l(y_i, \\gamma_j) \\end{align} this is a combinatorial problem that we usually aproximate using suboptimal solutions. finding \\(\\gamma_j\\) given \\(r_j\\): we usually define \\(\\hat{\\gamma}_j = \\overline{y}_j\\) for regression problems. finding \\(r_j\\): this is the difficult part. we usually resort to a greedy, top-down recursive partitioning algorithm to find \\(r_j\\). boosting trees in boosting trees, terminal-node trees refer to the individual decision trees that make up the ensemble model. each terminal-node tree is denoted as \\(t(x; \\theta_m)\\) where \\(\\theta_m = \\{r_{jm}, \\gamma_{jm}\\}_1^{j_m}\\). the boosted tree model is an additive model, where each tree is added sequentially to improve the overall prediction. the model can be expressed as \\begin{align} f_m(x) = \\sum_{m=1}^m t(x; \\theta_m) \\end{align} where \\(m\\) represents the total number of trees in the ensemble. optimization problem the optimization problem for boosting trees involves finding the optimal regions and constants for each tree in the ensemble model. optimization objective the goal is to minimize the empirical risk: \\begin{align} \\hat{\\theta} = arg \\min_{\\theta} \\sum_{x_i \\in r_j} l(y_i, \\gamma_j) \\end{align} where \\(l(y_i, \\gamma_i)\\) represents the loss incurred for pedicting the target value \\(y_i\\) with constant \\(\\gamma_j\\) in region \\(r_j\\). finding optimal consants given the regions \\(r_{jm}\\) finding the optimal constant in \\(\\gamma_j\\) in each regions involves minimizing the loss function for the data points within that region: \\begin{align} \\hat{\\gamma}_{jm} = arg \\min_{\\gamma_{jm}} \\sum_{x_i \\in r_{jm}} l(y_i, f_{m - 1}(x_i) + \\gamma_{jm}) \\end{align} \\begin{align} \\hat{\\gamma}_{jm} = arg \\min_{\\gamma_{jm}} \\sum_{x \\in r_{jm}} l(y_i, f_{m - 1}(x_i) + t(x_i; \\theta_m)) \\end{align} finding the regions is difficult, and even more difficult than for a single tree. solution for regression trees for regressions trees the solution for boosted trees consists on choosing the regression tree that best predicts the current residuals \\(y_i - f_{m-1}(x_i)\\) and \\(\\hat{\\gamma}_{jm}\\) solution for classification trees for two-class classification and exponential loss, it gives rise to the adaboost method. it can be showin that given \\(r_{jm}\\) the solution is: \\begin{align} \\hat{\\gamma}_{jm} = \\log \\frac{\\sum_{x_i \\in r_{jm}} w_i^{(m)} i(y_i = 1)}{\\sum_{x_i \\in r_{jm}} w_i^{(m)} i(y_i = -1)} \\end{align} appendix greedy top-down recurisve partitioning algorithm a greedy, top-down recursive partitioning algorithm is a method used in decision tree construction to recursively split the predictor variable space into regions in a step-by-step manner. greedy approach: at each step, it makes the best split based on the available data without considering the impact of future splits. top down process: starts at the top with the entire predictor variable space considered as one region. it then recursively divides this space into smaller regions recursive partitioning: at each step the predictor variable space is divided into two or more subregions based on a splitting criterion. this process continues recursively for each resulting subregion until a stopping criterion is met. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/01_boosting.html",
    "title": "Boosting",
    "body": " index search search back boosting contents regression trees in machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, variance in supervised learning. a weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). in contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. before training each weak classifier, the data is sampled based on a distribution. this distribution initially assigns equal weight to each training example. after training, the weak classifier's accuracy on the training data is evaluated. the accuracy of the weak classifier determines its importance in the final model. higher accuracy leads to greater influence. re-weighting: after adding a weak learner, the data weights are adjusted, giving more weight to misclassified examples and reducing the weight of correctly classified ones. re-weighting helps subsequent weak learners to focus more on the examples that previous weak learners struggled with. regression trees boosting regression trees, often referred to as gradient boosting machines: what is the idea behind this procedure? given the current model, we fit a decision tree to the residuals \\(r_i\\) from the model rather than the outcome \\(y\\). each of these trees can be rather small, with just a few terminal nodes, determined by the parameter \\(d\\). by fitting small trees to the residuals, we slowly improve \\(\\hat{f}\\) in areas where it does not perform well. the shrinkage parameter \\(\\lambda\\) slows the process down even further. boosting has three tuning parameters: the number of trees \\(b\\): boosting can overfit if \\(b\\) is too large. the shrinkage parameter \\(\\lambda\\): this controls the rate at which boosting learns. the number \\(d\\) of splits in each tree, which controls the complexity of the ensemble. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/05_interpretability.html",
    "title": "Interpretability",
    "body": " index search search back interpretability contents importance of predictor variables decision trees additive models partial dependence plots example representation importance of predictor variables we attempt to discuss the relevance of predictor variables in a statistical modeling technique called boosting. decision trees we define the following as a measure of relevance for each predictor variable \\(x_{\\mathcal{l}}\\): \\begin{align} i_{\\mathcal{l}}^2(t) = \\sum_{t = 1}^{j - 1} \\hat{i}^2_t i(v(t) = \\mathcal{l}) \\end{align} this equation calculates the relevance of each predictor variable based on the squared improvements in error risk within the internal nodes of the tree. let's split each part of the equation: the term \\(\\hat{i}^2_t\\) represents the squared improvement in error risk at node \\(t\\) when splitting on the \\(x_{\\mathcal{l}}\\) predictor variable. the variable \\(v(t)\\) indicates which predictor variable is used for the split at node \\(t\\). each improvement is weighted by the indicator function \\(i(v(t) = \\mathcal{l})\\), which checks if the splitting variable at node \\(t\\) is the predictor variable of interest \\(x_{\\mathcal{l}}\\). this weighting ensures that only the improvements related to the predictor variable \\(x_{\\mathcal{l}}\\) are considered in the calculation. we sum these improvements over the \\(j - 1\\) internal nodes on the tree, which are not terminal nodes. additive models this importance measure is easily generalized to additive tree expansions: \\begin{align} i_{\\mathcal{l}}^2 = \\frac{1}{m}\\sum_{m = 1}^{m} i_{\\mathcal{l}}^2(t_m) \\end{align} due to the stabilizing effect of averaging, this measure turns out to be more reliable than the measure computed only over one tree. partial dependence plots partial dependence functions, by isolating the effects of selected variables, provides an interpretable analysis of their impact on the model's predictions, overcoming the challenges of information overload in high-dimensional spaces. let's define the partial dependency of \\(f(x)\\) on \\(x_s\\), \\begin{align} f_s(x_s) = \\mathbb{e}_{x_c}[f(x_s, x_c)] \\end{align} where: \\(x_s\\) is the subset of variables we want to study. \\(x_c\\) is the complement set, that is the rest of variables. \\(e_{x_c}\\) denotes the expectation operator with respect to the variables in the complement set \\(x_c\\). it averages the model's output over the variables in the complement set. \\(f\\) represents the model. \\(f_s(x_s)\\) represents the relationship between the subset of variables \\(x_s\\) and the model's output. the average can be estimated as follows: \\begin{align} \\overline{f}_s(x_s) = \\frac{1}{n} \\sum_{i=1}^n f(x_s, x_{i\\mathcal{c}}) \\end{align} we simply iterate over every data point on the training set, such that \\(x_{i\\mathcal{c}}\\) refers to the values of the variables in the complement set \\(x_c\\) for the \\(i\\)th data point. previously we measured the effects of \\(x_s\\) after accounting for the effects of the other variables \\(x_c\\) on \\(f(x)\\), they were not the effect of \\(x_s\\) on \\(f(x)\\) ignoring the effects of \\(x_c\\), that is given by: \\begin{align} \\tilde{f}_s(x_s) = \\mathbb{e}(f(x_s, x_c)|x_s) \\end{align} thus \\(\\tilde{f}_s(x_s) = \\overline{f}_s(x_s)\\) only if \\(x_s\\) and \\(x_c\\) are independent. example if we assume a purely additive effect, where the overall prediction is the sum of two components: \\begin{align} f(x) = h_1(x_s) + h_2(x_c) \\end{align} this implies that the effect of \\(x_s\\) on the prediction is independent of the other variables in \\(x_c\\). however if the prediction is defined as: \\begin{align} f(x) = h_1(x_s) \\cdot h_2(x_c) \\end{align} then this implies that the effect of \\(x_s\\) on the prediction is dependent on the values of the variables in \\(x_c\\). representation owing to the limitations of computer graphics, and human perception, the size of the subsets \\(x_s\\) must be small (\\(l \\approx 1, 2, 3\\)). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/AAII/T2/02_ada_boost.html",
    "title": "Ada Boost",
    "body": " index search search back ada boost contents definition characteristics toy example round 1 round 2 round 3 final round definition adaboost, also known as adaptive boosting, is a machine learning algorithm that combines multiple weak classifiers to create a strong classifier. let's break down the adaboost algorithm using the pseudocode shown in: initialization: adaboost starts by initializing the weights of all training examples equally \\(d_1(i) = \\frac{1}{n}\\) for \\(i = 1, 2, \\cdots, n\\). iterative process for \\(t = 1, \\cdots, t\\): adaboost iterates through rounds, where each round involves training a weak classifier \\(h_t: \\mathcal{x} \\rightarrow \\{-1, +1\\}\\) on the data \\(d_t\\). the algorithm adjusts the weights of the training examples based on the performance of the weak classifier: \\(d_{t+1} = \\frac{d_t(i) \\cdot e^{-\\alpha_ty_th_t(x_t)}}{z_t}\\), where \\(z_t\\) is a regularization term and \\(\\alpha_t\\) is the weight of \\(h_t\\) on the final ensemble model based on its accuracy. combining classifiers: after multiple rounds, adaboost combines all the weak classifiers into a final strong classifier. the final classifier makes predictions based on a weighted voting system using the predictions of the individual weak classifiers: \\(h(x) = sign(\\sum_{t=1}^t \\alpha_t h_t(x))\\) predictions: when making predictions on new data, adaboost uses the combined classifier to predict the class label based on the weighted votes of the weak classifiers characteristics the weak learning assumption means that we assume each weak classifier makes errors that are not too close to random guessing. so the the error \\(\\epsilon_t\\) is at most \\(\\frac{1}{2} - \\gamma\\) for some small positive constant \\(\\gamma\\). it can be proven that the training error of the c ombined classifier drops exponentially fas as a function of the number of weak classifiers combined, but it says nothing about the behaviour of its generalization error computed over the test data. toy example to illustrate how adaboost works, let us look at the tiny toy learning problem shown in the following picture: round 1 on round \\(1\\), we assign equal weights to all the examples. so \\(d1_(i) = \\frac{1}{n} = \\frac{1}{10}\\). the hypothesis \\(h_1\\) classifies incorrectly three points, so its error is \\(\\epsilon_1 = 0.3\\), so it follows that the weight assigned to this first model is \\(\\alpha_1 = 0.42\\). round 2 when constructing \\(d_2\\) we increment the weight of the three points misclassified by \\(h_1\\). and we define a new hypothesis \\(h_2\\) over this data, where we can see that it classifies correctly the three points misclassified by \\(h_1\\) however it still classifies incorrectly three other points. the error of this model is \\(\\epsilon_2 = 0.21\\) and thus the weight of this model is defined as \\(\\alpha_2 = 0.65\\). round 3 we modify the weights of the data taking into account the three points previously misclassified, augmenting their weight and decresing the weight of those correctly classified. this classifier misses none of the points misclassified by \\(h_1\\) and \\(h_2\\). final round the combined classifier \\(h\\) is defiend as a weigthed vote between \\(h_1\\), \\(h_2\\) and \\(h_3\\) where the weights are given by \\(\\alpha_1\\), \\(\\alpha_2\\) and \\(\\alpha_3\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/04_cnn.html",
    "title": "Redes Neuronales Convolucionales en VisiÃ³n Artificial",
    "body": " index search search back redes neuronales convolucionales en visiÃ³n artificial contents clasificaci n de im genes aumento de muestras redes pre-entrenadas extracci n de caracter sticas ajuste fino detecci n de objetos m todos regioncnn segmentaci n sem ntica m todos aplicaciones clasificaciÃ³n de imÃ¡genes una cnn para clasificaciÃ³n de imÃ¡genes estÃ¡ conformada por dos bloques: conjunto de capas que llevan a cabo la extracciÃ³n de caracterÃ­sitcas clasificador que toma como entrada las caracterÃ­sticas extraÃ­das por la red neuronal este tipo de redes necesita una gran cantidad de datos. en caso de no poseer un volumen elevado de datos se puede hacer uso de tÃ©cnicas como el aumento de muestras y el uso de redes pre-entrenadas. aumento de muestras consiste en crear nuevas muestras a partir de muestras existentes utilizando transformaciones aleatorias: rotaciones y translaciones. recorte o zoom. voltear horizontalmente (si no hay suposiciones de simetrÃ­a horizontal). aÃ±adir pequeÃ±as cantidades de ruido. en keras esto se puede llevar a cabo utilizando imagedatagenerator. redes pre-entrenadas si la red ha sido entrenada sobre un conjunto de datos lo suficientemente grande y general entonces, entonces podemos utilizarlo sobre clases no utilizadas en el entrenamiento original. existen dos tÃ©cnicas: extracciÃ³n de de caracterÃ­sticas: utiliza la red pre-entrenada para extraer caracterÃ­sticas de los nuevos datos. con las nuevas caracterÃ­sticas se entrena un clasificador desde cero. ajuste fino: utiliza unas cuantas capas de la red pre-entrenada y las entrena conjuntamente con el nuevo clasificador. extracciÃ³n de caracterÃ­sticas se puede hacer de dos maneras: aplicar la base convolutiva sobre el conjunto de datos y utlizarla para entrenar el clasificador. no permite el aumento de datos extender la base convolutiva con un nuevo clasificador y entrenar todo el conjunto. es mucho mÃ¡s lenta per permite el aumento de datos ajuste fino se lleva a cabo como sigue: se aÃ±ade la nueva red sobre la cnn pre-entrenada se bloquea la red pre-entrenada, de manera que sus pesos no cambian se entrena la nueva red se desbloquean algunas capas superior (que extraer las caracterÃ­sticas de mÃ¡s alto nivel), tal que sus pesos sÃ­ se pueden entrenar entrenar estas capas y la nueva red detecciÃ³n de objetos un sistema de detecciÃ³n de objetos debe producir el nombre de la clase asignada a la imagen, una caja de abarque (bounding box) y generalmente la probabilidad de que el objeto pertenezca a la clase. se presentan los siguientes retos a la hora de llevar a cabo la detecciÃ³n: oclusiones, cambios de puntos de vista y tamaÃ±os, objetos no rÃ­gidos y desenfoque por movimiento mÃ©todos antes del desarrollo de las redes neuronales se empleaban las cascadas de haar, que extraer caracterÃ­sticas por convoluciÃ³n con kernels suma de pÃ­xeles en negro menos en blanco que seguidamente se pasan a un clasificador entrenado. por otra parte, tenemos la detecciÃ³n basada en cnn que consiste en modificar nuestras redes cnn para no sÃ³lo clasificar, si no tambiÃ©n obtener la caja de abarque y la forma del objeto. dentro de estes distinguimos dos tipos: detectores de dos etapas, como por ejemeplo region cnn. son letos y no permiten su aplicaciÃ³n en tiempo real. detectore de una etapa, como por ejemplo single shot multibox detector. sÃ­ que permiten su aplicaciÃ³n en tiempo real. regioncnn se generan una serie de propuestas de caja de abarque a distintas escalas. se procesan las cajas utilizando una cnn pre-entrenada se clasifican utilizando un clasificador como svm se procesan las cajas utilizando regresiÃ³n lineal para ajustar las coordenadas segmentaciÃ³n semÃ¡ntica en la segmentaciÃ³n clÃ¡sica el objetivo consiste en agrupar pÃ­xeles contiguos de una categorÃ­a similar. la segmentaciÃ³n semÃ¡ntica se distingue de la segmentaciÃ³n clÃ¡sica en que intenta particionar la imagen en partes con significado y clasificarlas. mÃ©todos se utiliza una especie de gan, ya que tienen una red codificadora pre-entrenada seguida de una red decodificadora: la red codificadora aprende caracterÃ­sticas distintivas a baja resoluciÃ³n la red decodificadora proyecta semÃ¡nticamente las caracterÃ­sticas en el espacio de pÃ­xeles (alta resoluciÃ³n) aplicaciones aprendizaje de similaridad. subtitulado de imÃ¡genes. generaciÃ³n de imÃ¡genes. seguimiento en secuencias de imÃ¡genes. todo lo comentado aplicado a vÃ­deo y a imagen 3d. robots: odometrÃ­a y localizaciÃ³n y creaciÃ³n de mapas (slam) usando cÃ¡maras. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/01_foundation.html",
    "title": "T1. Fundamentos de las Redes Neuronales Profundas",
    "body": " index search search back t1. fundamentos de las redes neuronales profundas contents deep networks training deep networks the reasons for deep learning s popularity introducing popular open source libraries deep networks we could define deep learning as a class of machine learning techniques, where information is processed in hierarchical layers to understand representations and features from data in increasing levels of complexity. in practice, all deep learning algorithms are neural networks. with that in mind, let's look at the main classes of neural networks. the following list is not exhaustive, but it represents the vast majority of algorithms in use today: multi-layer perceptrons (mlps) convolutional neural networks (cnns) recurrent networks autoencoders training deep networks we can use different algorithms to train a neural network. but in practice, we almost always use stochastic gradient descent (sgd) and backpropagation. in the following section, we'll introduce momentum, the weight update rule is defined as follows: \\begin{align} w \\rightarrow w - \\lambda \\nabla (j(w)) \\end{align} where \\(\\lambda\\) is the learning rate. first we calculate the weight update value \\begin{align} \\delta w \\rightarrow \\mu\\delta w - \\lambda (\\nabla j(w)) \\end{align} we see that the first component, \\(\\mu\\delta w\\), is the momentum. the \\(\\delta w\\) represents the previous value of the weight update and \\(\\mu\\) is the coefficient, which wil determine how much the new value depends on the previous ones. then we update the weight: \\begin{align} w \\rightarrow w + \\delta w \\end{align} you may encounter other gradient descent optimizations, such as: nesterov momentum adadelta rmsprops adam the reasons for deep learning's popularity the first reason is, today, we have a lot more data than in the past. the second reason is the increased computing power. this is most visible in the drastically increased processing capacity of graphical processing units (gpus). neural networks are organized in such a way as to take advantage of the gpu's parallel architecture. introducing popular open source libraries the basic unit for data storage is the tensor. a tensor is a generalization of a matrix to higher dimensions. neural networks are represented as a computational graph of operations. the nodes of the graph represent the operations (weighted sum, activation function, and so on). the edges represent the flow of data. some common libraries: tensorflow keras: is a high-level neural net python library that runs on top of tensorflow, cntk or theano. pytorch: is a deep learning library based on torch. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/05_rnn.html",
    "title": "Redes Neuronales Recurrentes",
    "body": " index search search back redes neuronales recurrentes contents introducci n a las aplicaciones del deep learning para el nlp comparaci n entre enfoques cl sico y de deep learning enfoque cl sico enfoque deep learning arquitecturas ejemplos de deep learning para natural language processing clasificaci n de textos generaci n de textos resumen de textos traducci n b squeda y eliminaci n de duplicados otras aplicaciones introducciÃ³n a las aplicaciones del deep learning para el nlp comparaciÃ³n entre enfoques clÃ¡sico y de deep learning enfoque clÃ¡sico el enfoque clÃ¡sico se compone de los siguientes pasos: detecciÃ³n de idioma pre-procesado tokenizado etiquetado gramatical (pos) eliminaciÃ³n de stop-words etc. modelado extracciÃ³n de caracterÃ­sticas (entidades (ner), categorÃ­as (pos) ...) aplicaciÃ³n de algoritmos de ml etc. salida anÃ¡lisis de sentimientos clasificaciÃ³n de textos traducciÃ³n etc enfoque deep learning mientas que el enfoque basado en deep learning se compone de los siguientes pasos: pre-procesado tokenizado etiquetado gramatical (pos) eliminaciÃ³n de stop-words etc. representaciones distribuidas (word embeddings): transformaciÃ³n de palabras/secuencias en vectores que es la entrada que aceptan las redes neuronales. para ello se distinguen mÃ©todos como: word2vec, glove, etc. procesamiento en capas ocultas: no permite generar representaciÃ³n comprimida de la entradas. capa de salida anÃ¡lisis de sentimientos clasificaciÃ³n de textos traducciÃ³n etc arquitecturas para llevar a cabo natural languague processing (nlp) con deep learning podemos utilizar las siguientes arquitecturas: redes recurrentes (rnn) lstm (long short term memory) gru (gated recurrent units) redes convolucionales (cnn) autoencoders ejemplos de deep learning para natural language processing clasificaciÃ³n de textos para la clasificaciÃ³n de texto se define la siguiente estructura: capa de embedding: que transforma la secuencia de palabras en una tabla de vectores capturando la semÃ¡ntica de las mismas. componente de representaciÃ³n profunda: se utiliza rnn o cnn para obtener una representaciÃ³n comprimida de la entrada. parte totalmente conectada: transforma la representaciÃ³n comprimida en clases o puntuaciones para cada clase. ver el capÃ­tulo text classification using lstm de hands-on natural language processing with python. generaciÃ³n de textos se utilizan rnns para crear modelos generativos, tal que la generaciÃ³n se puede llevar a cabo en base a caracteres o a palabras. estas son capaces de aprender dependencias a largo plazo. ver el capÃ­tulo text generation and summarization using grus de hands-on natural language processing with python. resumen de textos distinguimos entre dos tipos: extractivos: se extraen frases o palabras clave. son simples y robustos y no permiten la parÃ¡frasis. abstractivos: la salida contiene texto no contenido en el original manteniendo el significado. ver el cap. text generation and summarization using grus de hands-on natural language processing with python. traducciÃ³n distinguimos distintos sistemas que efectÃºan la traducciÃ³n automÃ¡tica: sistemas expertos: se definen reglas lingÃ¼Ã­sticas y sintÃ¡cticas. traducciÃ³n estadÃ­stica: se aprenden reglas estadÃ­sticamente a partir de un gran conjunto de datos bilingÃ¼e. tal que define un modelo de traducciÃ³n que mapea textos de un lenguaje a otro. solo funciona bien traduciendo textos similares a los de entrenamiento y necesita gran cantidad de datos traducciÃ³n con redes neuronales: utilizan un sÃ³lo modelo que trabaja sobre segmentos de texto, no sÃ³lo sobre palabras o frases. ver el cap. machine translation using the attention-based model de hands-on natural language processing with python. bÃºsqueda y eliminaciÃ³n de duplicados se puede conseguir utilizando una cnn basada en caracteres, que proporciona la flexibilidad para entrenar modelos con caracteres desconocidos y ofrece mayor capacidad de generaliciÃ³n que los embeddings a nivel de palabra. ver el capÃ­tulo searching and deduplicating using cnns de hands-on natural language processing with python. otras aplicaciones preguntas-respuestas y chatbots reconocimiento de voz texto a voz $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/02/01_conv_nets.html",
    "title": "Convolutional Nets",
    "body": " index search search back convolutional nets contents deep convolutional neural network local receptive fields shared weights and bias pooling layer an example of dcnn lenet understanding the power of deep learning deep convolutional neural network a deep convolutional neural network (dcnn) consists of many neural network layers. two different types of layers, convolutional and pooling, are typically alternated local receptive fields if we want to preserve spatial information, we represent each image with a matrix of pixels. convolution operation: to encode the local structure is to connect a submatrix of adjacent input neurons (pixels) into one single hidden neuron belonging to the next layer. that single hidden neuron represents one local receptive field. we can encode more information by having overlapping submatrices. a feature map is the result of applying the convolution on the input data, on the previous example the matrix on the right would be one feature map. the kernel size is the size of each the submatrices, in the previous example \\(3 \\times 3\\). the stride is the number of elements between each submatrix. with a stide of \\(1\\) we obtain the following result: this convolutional layer is usually followed by a non-linear activation function (e.g. relu). shared weights and bias to detect the same feature independently from its location on the input we define the same weights for all the neurons on a layer. this way we force the neural net to search for relevant features everywhere on the input data, instead of searching for features on specific places on the input image. pooling layer it consists on using the spatial contiguity of the output from a single feature map and aggregate the values into a single output. on the following image max pooling is being performed. other common pooling operation is average pooling. an example of dcnn â lenet it is a family of convnets trained for recognizing mnist handwritten characters with robustness to simple geometric transformations and to distortion. it is defined as follows: on the low-layers we alternate convolution operations with max-pooling operations. (using carefully chosen local receptive fields and and shared weights). on higher levels are fully connected layers based on a traditional mlp with hidden layers and softmax as the output layer. understanding the power of deep learning deep networks always outperform the simple network and the gap is bigger when the number of examples provided for training is progressively reduced. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/02/02_rnn.html",
    "title": "Recurrent Neural Nets",
    "body": " index search search back recurrent neural nets contents simplernn cells rnn topologies vanishing and exploding gradients long short term memory lstm gated recurrent unit gru a recurrent neural network (rnn) is a class of neural networks that exploit the sequential nature of their input. such inputs could be text, speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it. simplernn cells rnn cells incorporate this dependence by having a hidden state, or memory. the value of the hidden state is a function of the value of the hidden state at the previous time step and the value of the input at the current time step. \\begin{align} h_t = \\phi(h_{t-1}, x_t) \\end{align} where \\(h_t\\) and \\(h_{t-1}\\) are the values of the hidden states at the time steps \\(t\\) and \\(t-1\\) and \\(x_t\\) is the values of the input at time \\(t\\). note that the equation is recursive at time \\(t\\) the cell has an input \\(x_t\\) and an output \\(y_t\\). part of the output \\(y_t\\) (the hidden state \\(h_t\\)) is fed back into the cell for use at a later time step \\(t+1\\). on the previous image we show the behaviour of a single cell unrolled. notice that the weight matrices \\(u\\), \\(v\\), and \\(w\\) are shared across the steps. we can also describe the computations within an rnn in terms of equations: \\begin{align} h_t = tanh(wh_{t-1} + ux_t) \\end{align} \\begin{align} y_t = sofmax(vh_t) \\end{align} rnn topologies rnns can be arranged in many ways to solve specific problems. in the basic topology, all input sequences are of the same length and an output is produced at each time step. another example of a many to many rnn could be a machine translation network shown on the many-to-many topology. these take in a sequence and produces another sequence. for example, the input could be a sequence in english and the output could be the translation in spanish. other variants are the one-to-many network, an example of which could be an image captioning network, where the input is an image and the output a sequence of words. similarly, an example of a many-to-one network could be a network that does sentiment analysis of sentences, where the input is a sequence of words and the output is a positive or negative sentiment. vanishing and exploding gradients training the rnn involves backpropagation, where the gradient at each output depends not only on the current time step, but also on the previous ones, this process is called backpropagation through time (bptt). during backpropagation (shown by dotted lines), the gradients of the loss with respect to the parameters \\(u\\), \\(v\\), and \\(w\\) are computed at each time step and the parameters are updated with the sum of the gradients. the following equation shows the gradient of the loss with respect to \\(w\\): \\begin{align} \\frac{\\delta l}{\\delta w} = \\sum_t \\frac{\\delta l_t}{\\delta w} \\end{align} let us now look at what happens to the gradient of the loss at the last time step (\\(t=3\\)) \\begin{align} \\frac{\\delta l_3}{\\delta w} = \\frac{\\delta l_3}{\\delta y_3} \\frac{\\delta y_3}{\\delta h_2} \\frac{\\delta h_2}{\\delta_w} \\end{align} the previous equation is simply deriving by applying the chain rule, where: the loss function \\(l_3\\) is defined as a function of \\(y_3\\), then \\(y_3 = softmax(vh_2)\\) and finally \\(h_2 = tanh(wh_1 + ux_1)\\) the gradient of the hidden state \\(h_2\\) with respect to \\(w\\) can be further decomposed as the sum of the gradient of each hidden state with respect to the previous one. \\begin{align} \\frac{\\delta l_3}{\\delta w} = \\sum_{t=0}^2 \\frac{\\delta l_3}{\\delta y_3} \\frac{\\delta y_3}{\\delta h_2} \\frac{\\delta h_2}{\\delta h_t}\\frac{\\delta h_t}{\\delta_w} \\end{align} finally, each gradient of the hidden state with respect to the previous one can be further decomposed as the product of gradients of the current hidden state against the previous one. \\begin{align} \\frac{\\delta l_3}{\\delta w} = \\sum_{t=0}^2 \\frac{\\delta l_3}{\\delta y_3} \\frac{\\delta y_3}{\\delta h_2} \\left(\\prod_{j=t+1}^2 \\frac{\\delta h_j}{\\delta h_{j-1}}\\right)\\frac{\\delta h_t}{\\delta_w} \\end{align} for example for \\(t = 3\\): \\begin{align} \\frac{\\delta l_4}{\\delta w} = \\frac{\\delta l_4}{\\delta y_4} \\frac{\\delta y_4}{\\delta h_3} \\left(\\prod_{j=4}^2 \\frac{\\delta h_j}{\\delta h_{j-1}}\\right)\\frac{\\delta h_4}{\\delta_w} \\end{align} \\begin{align} \\frac{\\delta l_4}{\\delta w} = \\frac{\\delta l_4}{\\delta y_4} \\frac{\\delta y_4}{\\delta h_3} \\left(\\frac{\\delta h_4}{\\delta h_3}\\frac{\\delta h_3}{\\delta h_2}\\frac{\\delta h_2}{\\delta h_1}\\right)\\frac{\\delta h_4}{\\delta_w} \\end{align} on general: \\begin{align} \\frac{\\delta l_i}{\\delta w} = \\sum_{t=0}^i \\frac{\\delta l_i}{\\delta y_i} \\frac{\\delta y_i}{\\delta h_{i-1}} \\left(\\prod_{j=t+1}^i \\frac{\\delta h_j}{\\delta h_{j-1}}\\right)\\frac{\\delta h_i}{\\delta_w} \\end{align} consider the case where the individual gradients of a hidden state with respect to the previous one is less than one. as we backpropagate across multiple time steps, the product of gradients get smaller and smaller, leading to the problem of vanishing gradients. similarly, if the gradients are larger than one, the products get larger and larger, leading to the problem of exploding gradients. the effect of vanishing gradients is that the gradients from steps that are far away do not contribute anything to the learning process, so the rnn ends up not learning long range dependencies. while there are a few approaches to minimize the problem of vanishing gradients, such as: proper initialization of the \\(w\\) matrix using a relu instead of tanh layers pre-training the layers using unsupervised methods the most popular solution is to use the lstm or gru architectures. long short term memory â lstm the lstm is a variant of rnn that is capable of learning long term dependencies. the line across the top of the diagram is the cell state c, and represents the internal memory of the unit. the line across the bottom is the hidden state. also, \\(i\\), \\(f\\), and \\(o\\) are the input, forget, and output gates. the forget gate defines how much of the previous state \\(h_{t-1}\\) you want to allow to pass through. the input gate defines how much of the newly computed state for the current input \\(x_t\\) you want to let through. the output gate defines how much of the internal state you want to expose to the next layer. the internal hidden state \\(g\\) is computed based on the current input \\(x_t\\) and the previous hidden state \\(h_{t-1}\\). such that: \\begin{align} i = \\sigma(w_ih_{t-1} + u_ix_t) \\end{align} \\begin{align} f = \\sigma(w_fh_{t-1} + u_fx_t) \\end{align} \\begin{align} o = \\sigma(w_oh_{t-1} + u_ox_t) \\end{align} \\begin{align} g = \\tanh(w_gh_{t-1} + u_gx_t) \\end{align} \\begin{align} c_t = (c_{t-1} \\otimes f) \\oplus (g \\otimes i) \\end{align} \\begin{align} h_t = tanh(c_t) \\otimes o \\end{align} one thing to realize is that an lstm is a drop-in replacement for a simplernn on the recurrent neural network. gated recurrent unit â gru this type of cell has two gates, an update gate \\(z\\), and a reset gate \\(r\\). the update gate defines how much previous memory to keep around. the reset gate defines how to combine the new input with the previous memory. the following equations define the gating mechanism in a gru: \\begin{align} z = \\sigma(w_zh_{t-1} + u_z x_t) \\end{align} \\begin{align} r = \\sigma(w_rh_{t-1} + u_r x_t) \\end{align} \\begin{align} c_t = tanh(w_c(h_{t-1} \\otimes r) + u_cx_t) \\end{align} \\begin{align} h_t = (z \\otimes c) \\oplus ((1 - z) \\otimes h_{t-1}) \\end{align} gru and lstm have comparable performance, while grus are faster to train and need less data to generalize in situations where there is enough data, an lstm's greater expressive power may lead to better results. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/02/04_autoencoders.html",
    "title": "Autoencoders",
    "body": " index search search back autoencoders contents autoencoders stacked autoencoders tying weights convolutional autoencoders recurrent autoencoders denoising autoencoders sparse autoencoders variational autoencoders generative adversarial networks the difficulties of traning gans deep convolutional gans progressive growing of gans style gans autoencoders an autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). an autoencoder learns two functions: an encoding function that transforms the input data a decoding function that recreates the input data from the encoded representation. the autoencoder learns dense representations (encoding) for a set of data. we can force the network to learn useful features adding different types of constraints, for example: defining the dense representation such that is has a lower dimensionality than the input data. adding noise to the input data (denoising autoencoders). the number of neurons in the output layer must be equal to the number of inputs. the outputs are often called the reconstructions because the cost function contains a reconstruction loss that penalizes the model when the reconstructions are different from the inputs. undercomplete autoencoder: the internal representation has a lower dimensionality than the input data. overcomplete autoencoder: the internal representation has a higher dimensionality than the input data. stacked autoencoders stacked autoencoders are said to be autoencoders that have multiple hidden layers. tying weights an autoencoder with tied weights has decoder weights that are the transpose of the encoder weights this reduces the number of parameters of the model, thus speeds up training and limits the risk of overfitting. convolutional autoencoders used with image data. the encoder is a regular cnn composed of convolutional layers and pooling layers. it reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). the decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions). recurrent autoencoders used with sequential data. the encoder is typically a sequence-to-vector rnn, which compresses the input sequence down to a single vector. the decoder is a vector-to-sequence rnn that does the reverse denoising autoencoders we want to add noise to the input data, and then train the network to be able to recover the original noise-free inputs. the noise can be pure gaussian noise added to the inputs, or it can be randomly switched-off inputs, just like in dropout. sparse autoencoders a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty. in most cases, we would construct our loss function by penalizing activations of hidden layers so that only a few nodes are encouraged to activate when a single sample is fed into the network. variational autoencoders they are probabilistic autoencoders as well as generative models. instead of directly producing a coding for a given input, the encoder produces a mean coding \\(\\mu\\) and a standard deviation \\(\\sigma\\). the actual coding is then sampled randomly from a gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). after that the decoder decodes the sampled coding normally. generative adversarial networks gans are composed of two neural networks: a generator that tries to generate data that looks similar to the training data a discriminator that tries to tell real data from fake data. takes either a fake image from the generator or a real image from the training set as input, and must guess whether the input image is fake or real. each training iteration is divided into two phases: we train the discriminator. a batch of data where half are real real images and the other half are fake images produced by the generator. the labels are set to \\(0\\) for fake images and \\(1\\) for real images, and the discriminator is trained on this labeled batch for one step. backpropagation only optimizes the weights of the discriminator. we train the generator: we only add fake images to the data, and all the labels are set to \\(1\\) (real). we want the generator to produce images that the discriminator will believe to be real. backpropagation only affects the weights of the generator. the generator and the discriminator compete against each other during training. the difficulties of traning gans it has been demonstrated that a gan can only reach a single nash equilibrium (we assume the training process to be finished): thatâs when the generator produces perfectly realistic images, and the discriminator is forced to guess (\\(50\\%\\) real, \\(50\\%\\) fake). nothing guarantees that the equilibrium will ever be reached. the biggest difficulty is called mode collapse: this is when the generatorâs outputs gradually become less diverse. such that the generator gets very good at generating data of a concrete kind, good enough to fool the discriminator, however it progressively start representing data of another kind and then forgets about the previous class of data. moreover, because the generator and the discriminator are constantly pushing against each other, their parameters may end up oscillating and becoming unstable. and since many factors affect these complex dynamics, gans are very sensitive to the hyperparameters. there are some techniques that aim to avoid this behaviour like: experience replay and mini-batch discrimination. experience replay: stores images on a buffer and the discriminator uses the images on this buffer as input for fake images. old images are then progressively replaces by newer images. mini-batch discrimination: it measures how similar are images on the batch, the discriminator uses this statistic to decide whether to reject the whole batch or not. deep convolutional gans these are gans based on deeper convolutional nets for larger images. progressive growing of gans it begins by generating images at low resolution, such as \\(4 \\times 4\\) pixels. the model is first trained on low-resolution images. once training stabilizes at this resolution, additional layers are added to the generator and discriminator to allow for the generation of higher-resolution images. after adding new layers, there is usually a transition phase where the model is trained on a mixture of images at the old and new resolutions. this gradual transition allows the model to adapt to the increased resolution without destabilizing the training process. once the training stabilizes at the new resolution, the transition phase ends, and the model continues to train exclusively on images at the higher resolution. increasing the resolution progressively allows the model to learn to capture both global and local features of the images more effectively. style gans what sets stylegans apart is the introduction of \"style\" into the generation process. in traditional gans, the generator takes random noise as input and directly generates images. in stylegans, the generator learns to separate the \"content\" of the image (e.g., facial features) from the \"style\" (e.g., lighting, color, texture). this allows for more fine-grained control over the generated images. the stylegan generator and discriminator models are trained using the progressive growing gan training method. stylegans consist of two main components: a mapping network and a synthesis network. the mapping network takes as input a latent vector (random noise) and maps it to an intermediate latent space, which controls the style of the generated image. the synthesis network then takes the intermediate latent representation and generates the final image. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/02/03_boltzmann.html",
    "title": "Boltzmann Based Networks",
    "body": " index search search back boltzmann based networks contents boltzmann machines restricted boltzmann machines contrastive divergence deep belief nets deep boltzmann machines boltzmann machines they are fully connected artificial neural networks, but they are based on stochastic neurons. the working of boltzmann machine is mainly inspired by the boltzmann distribution which says that the current state of the system depends on the energy of the system and the temperature at which it is currently operating. these neurons output \\(1\\) with some probability, given by the following equation: \\begin{align} p(s_i^{\\text{next step}} = 1) \\sigma\\left(\\frac{\\sum_{j=1}^n w_{i,j}s_j + b_i}{t}\\right) \\end{align} where: \\(s_j\\) is the \\(j\\)th neuron's state (\\(0\\) or \\(1\\)). \\(w_{i,j}\\) is the connection weight between the \\(i\\)th and \\(j\\)th neurons. note that \\(w_{i,i}\\) = 0. \\(b_i\\) is the ith neuronâs bias term. \\(n\\) is the number of neurons in the network. \\(t\\) is a number called the networkâs temperature; the higher the temperature, the more random the output. \\(\\sigma\\) is the logistic function. hence to implement these as neural networks, we use the energy models. the energy term was equivalent to the deviation from the actual answer. the higher the energy, the more the deviation. it has been thus important to train the model until it reaches a low-energy point. the nodes in boltzmann machines are simply categorized as visible and hidden nodes. the visible nodes take in the input. the same nodes which take in the input will return back the reconstructed input as the output. the energy function of the boltzmann machine is defined as follows: \\begin{aligned} e(v, h) = - \\sum_{i} v_ib_i - \\sum_k h_kb_k - \\sum_{i, j}v_iv_jw_{i,j} \\sum_{i,k}v_ih_kw_{i, k} - \\sum_{k,l}h_kh_kw_{k,l} \\end{aligned} where \\(v\\) are the visible units, \\(h\\) as the hidden units \\(b\\) is the bias and \\(w_{i, j}\\) are the weights between units \\(i\\) and \\(j\\). the probability of a joint configuration over both the visible unit and the hidden unit is as follows: \\begin{aligned} p(v,h) = \\frac{e^{-e(v,h)}}{\\sum_{m, n} e^{-e(m, n)}} \\end{aligned} and, for example, the probability distribution of visible units is obtained by marginalizing out hidden units: \\begin{aligned} p(v) = \\frac{\\sum_h e^{-e(v,h)}}{\\sum_{m, n} e^{-e(m, n)}} \\end{aligned} this can now be utilized to sample visible units. training a boltzmann machine means finding the parameters that will make the network approximate the training setâs probability distribution. so we have to obtain the parameters tha maximize the likelihood of the observed data. the traning algorithm runs as described: obtain the log likelihood function of visible units, by marginalizing the hidden units: \\begin{align} l(v|w) = \\log p(v|w) = \\log \\sum_h e^{-e_{v, h}} - \\log \\sum_{m, n} e^{-e_{m, n}} \\end{align} take the derivative of the log likelihood function as a function of \\(w\\): \\begin{align} \\frac{\\delta l(v|w)}{\\delta w} = \\frac{\\delta \\log \\sum_h e^{-e_{v, h}}}{\\delta \\sum_h e^{-e_{v, h}}} \\cdot \\frac{\\delta \\sum_h e^{-e_{v, h}}}{\\delta w} - \\frac{\\delta \\log \\sum_h e^{-e_{v, h}}}{\\delta \\sum_{m,n} e^{-e_{m, n}}} \\cdot \\frac{\\delta \\sum_Ì£{m,n} e^{-e_{m, n}}}{\\delta w} \\end{align} \\begin{align} = \\frac{1}{\\sum_h e^{-e_{v, h}}} \\cdot \\sum_h \\frac{\\delta e^{-e_{v,h}}}{\\delta w} - \\frac{1}{\\sum_{m,n} e^{-e_{m, n}}} \\cdot \\sum_{m,n} \\frac{\\delta e^{-e_{m,m}}}{\\delta w} \\end{align} \\begin{align} = \\frac{1}{\\sum_h e^{-e_{v, h}}} \\cdot \\sum_h -e^{-e_{v,h}} \\frac{\\delta e_{v,h}}{\\delta w} - \\frac{1}{\\sum_{m,n} e^{-e_{m, n}}} \\cdot \\sum_{m,n} -e^{e_{m,m}} \\frac{\\delta e_{m,m}}{\\delta w} \\end{align} \\begin{align} = -\\sum_h \\frac{e^{-e_{v,h}}}{\\sum_h e^{-e_{v, h}}} \\frac{\\delta e_{v,h}}{\\delta w} + \\sum_{m,n} \\frac{e^{e_{m,m}}}{\\sum_{m,n} e^{-e_{m, n}}} \\frac{\\delta e_{m,m}}{\\delta w} \\end{align} we know that: \\begin{align} p(h|v) = \\frac{p(v, h)}{p(v)} = \\frac{\\frac{e^{-e_{v, h}}}{\\sum_{m,n} e^{-e_{m, n}}}}{\\frac{\\sum_h e^{-e_{v, h}}}{\\sum_{m,n} e^{-e_{m, n}}}} \\end{align} by removing both \\(\\sum_{m,n} e^{-e_{m, n}}\\), we obtain: \\begin{align} = \\frac{e^{-e_{v, h}}}{\\sum_h e^{-e_{v, h}}} \\end{align} such that: \\begin{align} = -\\sum_h p(h|v) \\frac{\\delta e_{v,h}}{\\delta w} + \\sum_{m,n} p(m,n) \\frac{\\delta e_{m,m}}{\\delta w} \\end{align} and by de definition of the expected value \\(\\mathbb{e}(x) = \\sum_x x p(x)\\): \\begin{align} = - \\mathbb{e}_{p(h|v)}[\\frac{\\delta e_{v,h}}{\\delta w}] + \\mathbb{e}_Ì£{p(m,n)}[\\frac{\\delta e_{m,m}}{\\delta w}] \\end{align} computing these expectations is in general an intractable problem. he general approach for solving this problem is to use markov chain monte carlo (mcmc) to approximate these quantities: \\begin{align} \\frac{\\delta l(v|w)}{\\delta w} = - <s_i, s_j>_{p(h_{data}|v_{data})} + <s_i, s_j>_{p(h_{model}|v_{model})} \\end{align} here \\(<\\cdot, \\cdot>\\) denotes the expectation. restricted boltzmann machines an rbm is a boltzmann machine that only has connections between visible and hidden units. the energy function of the rbm is defined as follows: \\begin{align} e(v, h) = - \\sum_i v_ib_i - \\sum_k h_kb_k - \\sum_{i,k} v_i h_k w_{i,k} \\end{align} contrastive divergence this is a very efficient training algorithm for boltzmann machines. here is how it works: for each training instance \\(x\\), the algorithm starts by feeding it to the network by setting the state of the visible units to \\(x_1, \\cdots, x_n\\). compute the state of the hidden units by applying the output formula for a hidden neuron (see ), which gives us the vector \\(h\\), where \\(h_i\\) is the output of the ith neuron. next you compute the state of the visible units, by applying the same stochastic equation, which gives you vector \\(x'\\). once again you compute the state of the hidden units, which gives you a vector \\(h'\\). now you can update each connection weight by applying: \\begin{align} w_{i, j} = w_{i, j} + \\eta (xh^t - x'h'^t) \\end{align} the great benefit of this algorithm is that it does not require waiting for the network to reach thermal equilibrium. deep belief nets a deep belief net is an rbm where several layers of rbms can be stacked. such that the hidden units of the first-level rbm serve as the visible units for the second-layer rbm. you can train dbns one layer at a time using contrastive divergence, starting with the lower layers. their lower layers learn low-level features in the input data, while higher layers learn high-level features. thus it learns information in a hierarchical way. just like rbms, dbns are fundamentally unsupervised, but you can also train them in a semi-supervised manner by adding some visible units to represent the labels. the following describes the training process: rbm 1 is trained without supervision. rbm 2 is trained with rbm 1âs hidden units as inputs without supervision rbm 3 is trained using rbm 2âs hidden units as inputs, as well as extra visible units used to represent the target labels one advantage of this semisupervised approach is that you don't need much labeled training data. dbns can also work in reverse. if you activate one of the label units, the signal will propagate up to the hidden units of rbm 3, then down to rbm 2, and then rbm 1, and a new instance will be output by the visible units of rbm 1. this generative capability of dbns is quite powerful. for example, it has been used to automatically generate captions for images, and vice versa: first a dbn is trained (without supervision) to learn features in images, and another dbn is trained (again without supervision) to learn features in sets of captions (e.g., \"car\" often comes with \"automobile\"). then an rbm is stacked on top of both dbns and trained with a set of images along with their captions; it learns to associate high-level features in images with high-level features in captions. next, if you feed the image dbn an image of a car, the signal will propagate through the network, up to the top-level rbm, and back down to the bottom of the caption dbn, producing a caption. due to the stochastic nature of rbms and dbns, the caption will keep changing randomly a dbn, however, suffers from the following problems: inference in dbns is a problem because of the \"explaining away\" effect a dbn can only use greedy retraining and no joint optimization over all layers deep boltzmann machines the distinction between dbm and dbn from the previous section is that dbm information flows on bidirectional connections in the bottom layers. you can also train a dbm using contrastive divergence. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/index.html",
    "title": "Deep Learning",
    "body": " index search search back deep learning fundamentos de las redes neuronales profundas tipologÃ­as de las redes neuronales profundas convolutional nets recurrent neural networks boltzmann based networks autoencoders herramientas y estrategias de programaciÃ³n e implemetaciÃ³n de redes neuronales frameworks computaciÃ³n acelerada proveedores redes neuronales convolucionales en visiÃ³n artificial redes neuronales recurrentes servicios y proveedores de deep learning en la nube $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/03/03_proveedores.html",
    "title": "Proveedores",
    "body": " index search search back proveedores contents deep cognition h2o ai auto ml driverless ai big ml hay bÃ¡sicamente dos tipos de proveedores: locales: scikit-learn,tensorflow, deeplearning4j, pio, h2o, hadoop/spark, etc. orientados a la nube: abm, bigml, google ml, amazonml ,azureml , watson ibm, etc. arquitectura: deep cognition se trata de una plataforma que incorpora un ide visual que permite definir una red neuronal. se puede utilizar: en la nube en local en una mÃ¡quina virtual en una mÃ¡quina de azure h2o.ai su arquitecture se detalla en la siguiente imagen: en la parte superior vemos los lenguajes que soporta: seguidamente tenemos un bloque de tradcutores (rapids en c++ y scala en java): a continuaciÃ³n tenemos los algoritmos definidos asÃ­ como la herramienta de predcciÃ³n para h2o: en la siguiente imagen tenemos la parte de la gestiÃ³n de la computaciÃ³n que se lleva a cabo encima de clusters spark/hadoop o sobre la distribuciÃ³n standalone de h2o: auto ml permite evaluar modelos dado un conjunto de datos en base a una serie de mÃ©tricas: driverless ai permite desarrollar el pipeline completo de h2o de forma visual, tal que permite automatizar tareas. big ml se trata de una empresa espaÃ±ola. define algoritmos de clasificaciÃ³n, regresiÃ³n, anÃ¡lisis de clusters, detecciÃ³n de anomalÃ­as, descubrimiento de asociaciÃ³n y modelado. destaca sobretodo en el preprocesamiento de datos, visualizaciÃ³n y en la evaluaciÃ³n de modelo. no soporta ni cnn (no soporta capas de convoluciÃ³n ni de pooling) ni rnn. los parÃ¡metros soportados son los siguientes: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/03/01_frameworks.html",
    "title": "Frameworks para Deep Learning",
    "body": " index search search back frameworks para deep learning contents tensorflow arquitectura tensorflow tensorflow vs numpy modelo computacional de tensorflow lazy evaluation tensorflow hub operaciones theano keras tipos de modelos caffe torch pytorch fast ai aplicaciones de dl los frameworks principales son los siguientes: tensorflow torch/pytorch keras caffe theano (estÃ¡ aquÃ­ por motivos histÃ³ricos) una diferencia entre los distinos frameworks radica en la forma de especificar los modelos: a travÃ©s de ficheros de configuraciÃ³n (caffe, distbelief, cntk) a travÃ©s de cÃ³digo (torch/pytoch, theano, tensorflow) tensorflow arquitectura tensorflow presenta un nÃºcleo de bajo nivel (c++/cuda). ademÃ¡s se define un api python sencillo para definir el grÃ¡fico computacional, asÃ­ como apis de alto nivel (tf-learn, keras, etc) tensorflow vs numpy numpy no dispone de funciones/mÃ©todos para la creaciÃ³n de funciones de tensores y no computa automÃ¡ticamente sus derivadas. numpy no tiene soporte para gpu. modelo computacional de tensorflow tensorflow construye grafos donde cada nodo es un tensor y cada arista es una operaciÃ³n entre los tensores. de tal manera que, como vemos en la figura inferior, se pueden repartir las computaciones entre distintas gpus. lazy evaluation este grafo sÃ³lo encompasa la definiciÃ³n de las operaciones, de tal manera que no requiere de su ejecuciÃ³n. si no que la ejecuciÃ³n sÃ³lo se produce durante el entrenamiento. tensorflow hub se trata de un repositorio de modelo pre-entrenados. operaciones a continuaciÃ³n mostramos una serie de operaciones soportadas por tensorflow: theano se trata de otro framework, pionero en el uso de grafos computacionales. es una herramienta generalista, tal que podemos implementar cualquier tipo de algoirtmo sobre el framework. ademÃ¡s se puede especificar como backend a utilizar en keras, en lugar de tensorflow. sin embargo, finalizÃ³ su desarrollo a partir de la versiÃ³n 1.0. librerÃ­as que usan theano keras blocks lasagne sklearn-theano pymc 3 theano-rnn morb ademÃ¡s presenta las siguientes caracterÃ­sticas: permite la evaluaciÃ³n lazy del grafo (precursor de esta tÃ©nica). da soporte para gpu's. permite la diferenciaciÃ³n simbÃ³lica. keras keras puede ser utilizado con tensorflow o tambiÃ©n como una librerÃ­a adicional. ademÃ¡s presenta las siguientes ventajas: sencilla para comenzar, y sencilla para avanzar se ejecuta sobre theano y tensorflow disponibilidad de herramientas de visualizaciÃ³n (tensorboard) escrita de forma modular: fÃ¡cil de expandir suficientemente potente para escribir modelos serios pero tambiÃ©n presenta las siguiente desventajas: menos flexible menos tipos: no hay modelos rbm, por ejemplo. menos proyectos disponibles online que caffe soporte multi-gpu no del 100% la idea general para la creaciÃ³n de modelos/algoritmos sigue el siguiente esquema: preparar los tensores de entrada y salida crear la primera capa (layer) para manejar el tensor de entrada crear la Ãºltima capa (layer) para manejar el tensor de salida (targets) construir virtualmente cualquier modelo entre estas dos capas (hidden layers) las definiciones de los modelos pueden ser guardados y recuperados en formato json y en formato yaml. los parÃ¡metros tambiÃ©n pueden ser guardados y recuperados en formato h5. tipos de modelos keras soporta dos tipos de modelos: modelo secuencial api funcional: se usa para definir modelos complejos: modelos multi-output, grafos acÃ­clicos dirigidos (graph) o modelos con capa compartidas grafo (deprecado) caffe construido sobre c++, cuda. presenta una gran cantidad de modelos pre-entrenados la definiciÃ³n de modelos de hace de forma declarativa: cuÃ¡les son sus aplicaciones? object detection pixelwise prediction torch su backend estÃ¡ basado en c y en cuda. su frontend estÃ¡ escrito sobre lua. pytorch torch en python. fast.ai es muy similar a keras, fast.ai permite generar herramientas y modelos pre-entranados de manera muy sencilla. aplicaciones de dl visiÃ³n speech recognition nlp $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/03/02_computacion_acelerada.html",
    "title": "ComputaciÃ³n Acelerada",
    "body": " index search search back computaciÃ³n acelerada contents diferencias cpu gpu proveedores flujo de procesamiento en cuda plataformas tpu por qu utilizar tpus cu ndo deber amos utilizar una tpu versiones flujo de ejecuci n de tpus diferencias cpu/gpu una cpu tiene un nÃºmero limitado de cores, mientras que una gpu tiene un nÃºmero muy elevado de cores. una gpu tiene procesadores menos potentes (menos operaciones por ciclo), sin embargo tiene muchas mÃ¡s unidades lÃ³gicas-aritmÃ©ticas (alu), por lo que tiene mÃ¡s capacidad de cÃ¡lculo a coste de tener menos capacidad de manejo de almacenamiento. proveedores nvidia: se basa en la arquitecture compute unified device architecture (cuda). amd: se basa en una arquitectura mÃ¡s abierta, heterogeneous system architecture (hsa), que es multiplataforma. su arquitectura se puede utilizar con distintos proveedores, p.ej. nvidia. flujo de procesamiento en cuda plataformas tpu diseÃ±ado por google especialmente diseÃ±ado para operaciones matriciales y tensores. su uso fundamental es en el entrenamiento de redes neuronales y la inferencia. por quÃ© utilizar tpus? segÃºn google: son 30x mÃ¡s rÃ¡pidos que gpus y cpus. presentan una gran eficiencia energÃ©tica. las nn desarrolladas con tensorflow requieren muy pocas lÃ­neas de cÃ³digo. requieren menos tiempo -> menos dinero. cuÃ¡ndo deberÃ­amos utilizar una tpu? versiones hay dos versiones: v2: hbm de 8 gb/tpu core. 1mxu (128x128) por core. tpu pod, hasta 512 cores (4tb de memoria) v3: hbm de 16 gb/tpu core. 2 mxu (128x128) por core. tpu pod, hasta 2048 cores (32 tb de memoria) flujo de ejecuciÃ³n de tpus $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/2C_2C/DL/06_cloud_services.html",
    "title": "Servicios y Proveedores de Deep Learning en la Nube",
    "body": " index search search back servicios y proveedores de deep learning en la nube contents caracter sticas capacidades modelos de despliegue google cloud platform awm machine learning no se trata de una tecnologÃ­a, si no que es un servicio. se definen tres capas distintas de servicio: algunos ejemplos son: modelos de despliegue google cloud platform: machine learning engine aws machine learning microsoft azure: machine learning studio ibm watson machine learning y data studio caracterÃ­sticas it services catalog global network access instant elasticity chargeback common it-resouces pool capacidades saas (software as a service): se utiliza/alquila un servicio concreto, p.ej. one drive. paas (platform as a service): permite tener entornos preconfigurados para poder ejecutar ciertos conjuntos de aplicaciones. iaas (infraestructure as a service): se da acceso a la infraestructura, tal que te permite utilizarla para ejecutar los servicios que tu convengas. modelos de despliegue como modelos de despliegue tenemos: private cloud: se cierra el acceso para que sÃ³lo la propia infraestructura tenga acceso. community cloud: conjunto de nubes pÃºblicas que comparten recursos. hybrid cloud: tiene parte pÃºblica y tiene parte a la que se restringe el acceso. public cloud: permite el acceso desde la nube pÃºblica de internet. google cloud platform a continuaciÃ³n mostramos el ciclo completo de desarrollo de soluciones ml, y las herramientas de gcp asociadas y disponibles para cada una de ellas: en la fase de desarrollo se definen varias herramientas: data labeling service: se encarga del etiquetamiento correcto de los datos de forma semiautomÃ¡tica. deep learning vm image: proporciona imÃ¡genes virtuales sobre las cuales llevar a cabo el procesamiento. api platform notebook awm machine learning la estructura se divide en tres bloques, de menor a mayor abstracciÃ³n: infraestructura plataformas servicios how to use amazon sagemaker $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/index.html",
    "title": "Data Science Master",
    "body": " index search search back data science master 2c 2c aprendizaje automÃ¡tico ii deep learning modelos bayesianos jerÃ¡rquicos 3c 1c infraestructuras computacionales para procesamiento de datos masivos $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/index.html",
    "title": "Data Science",
    "body": " index search search back data science master machine learning stanford coursera artificial intelligence in robotics online training mobile robotics standalone topics ukf $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/index.html",
    "title": "Notes",
    "body": " index search search back notes math computer science data science music other $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/XML configuration file.html",
    "title": "Configure Spring Container with an XML file",
    "body": " index search search back configure spring container with an xml file first we create the config file <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <bean id=\"mycoach\" class=\"com.luv2code.springdemo.trackcoach\"> </bean> </beans> then we create the spring container in our application: package com.springdemo; /* class to create a spring container using xml files */ import org.springframework.context.support.classpathxmlapplicationcontext; public class myapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container by its id \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/IoC/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control the spring container (generally known as applicationcontext) has two main functions: create and manage objects (inversion of control) inject object's dependencies (dependency injection) so inversion control is externalizing the construction and management of objects which will be handled by and object factory. this is illustrated in the following image: myapp has the main method myapp asks spring to retrieve the appropiate object based on a configuration file or an annotation, instead of having to code it manually like: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\tcoach thecoach = new trackcoach(); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t} } where we have defined an interface coach that is implemented by both trackcoach and baseballcoach package com.springdemo; public interface coach { \tpublic string getdailyworkout(); \t } package com.springdemo; public class trackcoach implements coach { \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} } to avoid this approach we create a spring container. to configure a spring container we can use: xml configuration file (legacy) java annotations java source code however what is a spring bean? a \"spring bean\" is simply a java object. when java objects are created by the spring container, then spring refers to them as \"spring beans\". spring beans are created from normal java classes just like java objects. why do we specify the coach interface in getbean()? when we pass the interface to the method, behind the scenes spring will cast the object for you. context.getbean(\"mycoach\", coach.class) however, there are some slight differences than normal casting. behaves the same as getbean(string), but provides a measure of type safety by throwing a beannotofrequiredtypeexception if the bean is not of the required type. this means that classcastexception can't be thrown on casting the result correctly, as can happen with getbean(string). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Radio Buttons.html",
    "title": "Radio Buttons",
    "body": " index search search back radio buttons contents controller view model to pass and bind data from radio buttons to controllers an another views we use the form tag form:radiobutton which is surrounded by a form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and performs data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\tcountry: \t\t<!-- drop down list of country options --> \t\t<!-- we specify the variable where we store the selected value in the student object: which is country --> \t\t<form:select path=\"country\"> \t\t\t<!-- this is a list that was populated when we created the student object --> \t\t\t<!-- remember spring calls student.getcountryoptions() --> \t\t\t<form:options items=\"${student.countryoptions}\" /> \t\t</form:select> \t\t<br><br> \t\t<br><br> \t\tfavorite language: \t\t \t\t<!-- the \"path\" specifies the name of the property we are going to bind the radiobutton to, in this case \"favoritelanguage\" --> \t\t<!-- note these can also be populated from the student class or using a properties file --> \t\tjava <form:radiobutton path=\"favoritelanguage\" value=\"java\" /> \t\tc# <form:radiobutton path=\"favoritelanguage\" value=\"c#\" /> \t\tphp <form:radiobutton path=\"favoritelanguage\" value=\"php\" /> \t\truby <form:radiobutton path=\"favoritelanguage\" value=\"ruby\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \tthe student is confirmed: ${student.firstname} ${student.lastname} \t<br><br> \tselected coutry: ${student.country} ${student.lastname} \t<br><br> \t<!-- obtain the value using the binded variable inside the student object --> \tfavorite language: ${student.favoritelanguage} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t \tprivate string country; \t \tprivate linkedhashmap<string, string> countryoptions; \t \t// property we are going to bind to the radio buttons \tprivate string favoritelanguage; \t \tpublic student() { \t\t \t\t// populate country options: used iso country code \t\tcountryoptions = new linkedhashmap<>(); \t\t \t\tcountryoptions.put(\"br\", \"brazil\"); \t\tcountryoptions.put(\"fr\", \"france\"); \t\tcountryoptions.put(\"de\", \"germany\"); \t\tcountryoptions.put(\"in\", \"india\"); \t\tcountryoptions.put(\"us\", \"united states of america\");\t\t \t\t \t\t// we can also populate the favoritelanguage options from here \t\t// in the same manner we did with the country options \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getcountry() { \t\treturn country; \t} \tpublic void setcountry(string country) { \t\tthis.country = country; \t} \tpublic linkedhashmap<string, string> getcountryoptions() { \t\treturn countryoptions; \t} \t \t// setter and getter handlers for the new binded attribute \tpublic string getfavoritelanguage() { \t\treturn favoritelanguage; \t} \tpublic void setfavoritelanguage(string favoritelanguage) { \t\tthis.favoritelanguage = favoritelanguage; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Form Tags.html",
    "title": "Form Tags",
    "body": " index search search back form tags contents reference spring mvc form tags form tags are configurable an reusable: they can make use of data binding (you can automatically set and retrieve data from a java object) you can mix them in with you html web page some examples are: reference spring mvc form tags to use these tags in your web page you have to specify the spring namespace at the beginning of the jsp file: <!-- reference to the namespace --> <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head></head> <body> </body> </html> text fields drop down lists radio buttons checkbox $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Text Fields.html",
    "title": "Text Fields",
    "body": " index search search back text fields contents controller view model to pass and bind data from input text fields to controllers an another views we use the form tag form:input along with form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and perfoms data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<!-- note the modelattribute equals the attribute we added to the model in the controller--> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\t<!-- to retrieve the data this maps to student.getfirstname() --> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> when we submit spring will call student.setfirstname() and student.setlastname() to save the data in the student object, so we can retrieve it from our controller method. for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \t<!-- obtain data from the model: note we use the attribute's name (i.e. student) to access the object --> \tthe student is confirmed: ${student.firstname} ${student.lastname} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t\t \tpublic student() {} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Adding Data.html",
    "title": "Model",
    "body": " index search search back model contents example controller view the model is a container for the application data. so in your controller you can put anything in the model (strings, objects, info from db, etc). and then you view page (jsp) can access data from the model. example controller package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; @controller public class helloworldcontroller { // new a controller method to read form data and // add data to the model @requestmapping(\"/processformversiontwo\")\t // the httpservletrequest allows you to retrieve information from the request (like the parameters of a form) // the model is our model where we will store data public string parsestring(httpservletrequest request, model model) { // read the request parameter from the html form string thename = request.getparameter(\"studentname\"); // convert the data to all caps thename = thename.touppercase(); // create the message string result = \"yo! \" + thename; // add message attribute to the model model.addattribute(\"message\", result); \t\t return \"helloworld\"; } } view now, on the view, we can access the model data: <!doctype html> <html> <body> hello world of spring! <br><br> student name: ${param.studentname} <br><br> <!-- access model data by the attribute's name--> the message: ${message} </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Number Validation.html",
    "title": "Number Range Validation",
    "body": " index search search back number range validation contents add validation rule to bean perform validation in the controller display error on html in this section we will show how to perform a number range validation. add validation rule to bean we create a customer class, whose freepasses variable must be a number between 0 and 10. public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t// minimum value we will expect \t@min(value=0, message=\"must be greater than or equal to zero\") \t// maximum value we will expect \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate int freepasses; \t \t... perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the lastname attribute in the customer class --> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the freepasses attribute in the customer class --> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Read HTML Form Data.html",
    "title": "Read HTML Form Data",
    "body": " index search search back read html form data contents controller view the flow of our example will be the following: when the user accesses the url /showform, the browser will send a request to our controller, and our controller will return the corresponding view when the user hits submit on the form the action /processform is passed to the browser that will send a request to our controller, and our controller will process the request controller package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.requestmapping; @controller public class helloworldcontroller { \t// need a controller method to show the initial html form \t@requestmapping(\"/showform\") // the method name can be anything \tpublic string showform() { \t\treturn \"helloworld-form\"; \t} \t\t \t// need a controller method to process the html form \t@requestmapping(\"/processform\") \tpublic string processform() { \t\treturn \"helloworld\"; \t} \t\t } view we create web-inf/view/helloworld-form.jsp <!doctype html> <html> <head> \t<title>hello world - input form</title> </head> <body> <!-- the action is the request url --> \t<form action=\"processform\" method=\"get\"> \t\t<input type=\"text\" name=\"studentname\" \t\t\tplaceholder=\"what's your name?\" /> \t\t<input type=\"submit\" /> \t</form> </body> </html> and we create web-inf/view/helloworld-form.jsp <!doctype html> <html> <body> hello world of spring! <br><br> <!-- name of html form field from previous jsp view --> student name: ${param.studentname} </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Drop Down Lists.html",
    "title": "Drop Down Lists",
    "body": " index search search back drop down lists contents controller view model country options from a properties file to pass and bind data from drop down lists to controllers an another views we use the form tags form:select that encloses a set of options represented with form:option tags. and all these are surrounded by a form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and performs data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\tcountry: \t\t<!-- drop down list of country options --> \t\t<!-- we specify the variable where we store the selected value in the student object: which is country --> \t\t<form:select path=\"country\"> \t\t\t<!-- this is a list that was populated when we created the student object --> \t\t\t<!-- remember spring calls student.getcountryoptions() --> \t\t\t<form:options items=\"${student.countryoptions}\" /> \t\t</form:select> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \tthe student is confirmed: ${student.firstname} ${student.lastname} \t<!-- obtain the value saved in the coutry variable inside the student's object (corresponds to the selected value) --> \tselected coutry: ${student.country} ${student.lastname} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t \tprivate string country; \t \tprivate linkedhashmap<string, string> countryoptions; \t \tpublic student() { \t\t \t\t// populate country options: used iso country code \t\tcountryoptions = new linkedhashmap<>(); \t\t \t\tcountryoptions.put(\"br\", \"brazil\"); \t\tcountryoptions.put(\"fr\", \"france\"); \t\tcountryoptions.put(\"de\", \"germany\"); \t\tcountryoptions.put(\"in\", \"india\"); \t\tcountryoptions.put(\"us\", \"united states of america\");\t\t \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getcountry() { \t\treturn country; \t} \t// setter and getter handlers for the new binded attribute \tpublic void setcountry(string country) { \t\tthis.country = country; \t} \tpublic linkedhashmap<string, string> getcountryoptions() { \t\treturn countryoptions; \t} } country options from a properties file we create web-inf/countries.properties: br=brazil fr=france co=colombia in=india update configuration's file spring-mvc-dmo-servlet.xml header (to use a new set of spring tags: utils): <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xsi:schemalocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd\"> load the country options properties file in the spring configuration file, with a bean id equal to \"countryoptions\": <util:properties id=\"countryoptions\" location=\"classpath:../countries.properties\" /> inject properties inside our controller: @value(\"#{countryoptions}\") private map<string, string> countryoptions; add countryoptions as an attribute of the model inside the controller method: @requestmapping(\"/showform\") public string showform(model themodel) { // create a student object student student thestudent = new student(); // add student object to the model themodel.addattribute(\"student\", thestudent); // add the country options to the model themodel.addattribute(\"thecountryoptions\", countryoptions); return \"student-form\"; } update the view as follows: <form:select path=\"country\"> <form:options items=\"${thecountryoptions}\" /> </form:select> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Validation with Regular Expressions.html",
    "title": "Validation with Regular Expressions",
    "body": " index search search back validation with regular expressions contents add validation rule to bean perform validation in the controller display error on html in this section we will show how to perform a validation with regular expressions. add validation rule to bean we create a customer class, whose freepasses variable must be a number between 0 and 10. public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t@min(value=0, message=\"must be greater than or equal to zero\") \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate int freepasses; // define the regular expression for the postalcode attribute @pattern(regexp=\"^[a-za-z0-9]{5}\", message=\"only 5 chars/digits\") \tprivate string postalcode; \t \t... perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> <br><br> \t\tpostal code: <form:input path=\"postalcode\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the postalcode attribute in the customer class --> \t\t<form:errors path=\"postalcode\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Form Validation.html",
    "title": "Form Validation",
    "body": " index search search back form validation contents set up java has a standard bean validation api that defines a metadata model and an api for entity validation. here is a list of bean validation features you can check: required validate length validate numbers validate with regular expressions custom validation some annotations to perform the validation are the following: set up add hibernate's library (hibernate validator)for bean validation which is fully compliant with java's bean validation api. required validation number range validation validation with regular expressions handle string in integer field custom validation $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Required Validation.html",
    "title": "Required Validation",
    "body": " index search search back required validation contents add validation rule to bean perform validation in the controller display error on html in this section we will show how to perform a required validation. add validation rule to bean we create a customer class, whose lastname attribute must be non-null, that is, lastname is a required attribute: package com.springdemo.mvc; import javax.validation.constraints.notnull; import javax.validation.constraints.size; public class customer { \tprivate string firstname; \t // validation annotation \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} } note that if we wanted to make an integer required, we must use the wrapper java classes (i.e. integer), that will be able to handle empty strings as inputs and nulls. the primitive types will throw an exception. perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> <!-- we use the error form tag to display an error when the input is not valid --> <!-- the message shown equals the messages from both of the validation annotations defined for the lastname attribute in the customer class --> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Request Params and Request Mappings.html",
    "title": "Request Params and Request Mappings",
    "body": " index search search back request params and request mappings contents request params controller request mappings request params spring provides for a specific annotation that allows you to retrieve request parameters directly without using the httpservletrequest object. given the form: package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.requestparam; @controller public class helloworldcontroller { \t@requestmapping(\"/processformversionthree\")\t \tpublic string processformversionthree( // we use the annotation to obtain the parameter \t\t\t@requestparam(\"studentname\") string thename, \t\t\tmodel model) { \t\t\t\t \t\t// convert the data to all caps \t\tthename = thename.touppercase(); \t\t \t\t// create the message \t\tstring result = \"hey my friend from v3! \" + thename; \t\t \t\t// add message to the model \t\tmodel.addattribute(\"message\", result); \t\t\t\t \t\treturn \"helloworld\"; \t}\t } controller request mappings they serve as a parent mapping for the controller all request mappings on methods in the controller are relative for example: package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.requestparam; @controller // this is the request mapping for the controller @requestmapping(\"/hello\") public class helloworldcontroller { \t// both of these request mappings are relative to the parent mapping \t// that is the mapping translates to domain/hello/showform \t// need a controller method to show the initial html form \t@requestmapping(\"/showform\") \tpublic string showform() { \t\treturn \"helloworld-form\"; \t} \t\t \t// need a controller method to process the html form \t@requestmapping(\"/processform\") \tpublic string processform() { \t\treturn \"helloworld\"; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Handle String in Integer Field.html",
    "title": "Handle String Input in Integer Field",
    "body": " index search search back handle string input in integer field contents create a custom message specify properties file in configuration if we want to avoid the trace returned by errors like inputting the wrong data type (string instead of int), we can define a custom message that will override those messages. create a custom message create a properties file in resources/messages.properties // errortype.springmodelattributename.fieldname typemismatch.customer.freepasses=invalid number specify properties file in configuration we add the following in our configuration file spring-mvc-demo-servlet.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" \txmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" \txmlns:context=\"http://www.springframework.org/schema/context\" \txmlns:mvc=\"http://www.springframework.org/schema/mvc\" \txsi:schemalocation=\" \t\thttp://www.springframework.org/schema/beans \thttp://www.springframework.org/schema/beans/spring-beans.xsd \thttp://www.springframework.org/schema/context \thttp://www.springframework.org/schema/context/spring-context.xsd \thttp://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"> \t<context:component-scan base-package=\"com.luv2code.springdemo\" /> \t<mvc:annotation-driven/> \t<bean \t\tclass=\"org.springframework.web.servlet.view.internalresourceviewresolver\"> \t\t<property name=\"prefix\" value=\"/web-inf/view/\" /> \t\t<property name=\"suffix\" value=\".jsp\" /> \t</bean> \t <!-- load custom message resources --> <bean id=\"messagesource\" class=\"org.springframework.context.support.resourcebundlemessagesource\"> \t\t\t\t<!-- path where the properties file is stored --> <property name=\"basenames\" value=\"resources/messages\" /> </bean> </beans> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/MVC/Custom Validation.html",
    "title": "Custom Validation",
    "body": " index search search back custom validation contents create a custom java annotation create annotation clas create validator class add custom validation perform validation on controller display error on html create a custom java annotation create annotation clas package com.springdemo.mvc.validation; import java.lang.annotation.elementtype; import java.lang.annotation.retention; import java.lang.annotation.retentionpolicy; import java.lang.annotation.target; import javax.validation.constraint; import javax.validation.payload; // specify the class that holds the validation logic @constraint(validatedby = coursecodeconstraintvalidator.class) // where you can use this annotation: on a method or on a field @target( { elementtype.method, elementtype.field } ) @retention(retentionpolicy.runtime) // note the @interface (it is needed to create the annotation) public @interface coursecode { \t// define default course code \tpublic string value() default \"luv\"; \t \t// define default error message \tpublic string message() default \"must start with luv\"; \t \t// define default groups \tpublic class<?>[] groups() default {}; \t \t// define default payloads \tpublic class<? extends payload>[] payload() default {}; } create validator class this class holds the validation logic package com.springdemo.mvc.validation; import javax.validation.constraintvalidator; import javax.validation.constraintvalidatorcontext; // implements the previous constraintvalidator interface, with generics: <annotation interface, data type> public class coursecodeconstraintvalidator \timplements constraintvalidator<coursecode, string> { \tprivate string courseprefix; \t \t@override \tpublic void initialize(coursecode thecoursecode) { \t\t// obtain prefix from the \"value\" attribute of our annotation \t\tcourseprefix = thecoursecode.value(); \t} \t@override \t// called when we use the @valid annotation \tpublic boolean isvalid(string thecode, constraintvalidatorcontext theconstraintvalidatorcontext) { \t\tboolean result; \t\t \t\t// validation logic \t\tif (thecode != null) { \t\t\tresult = thecode.startswith(courseprefix); \t\t} \t\telse { \t\t\tresult = true; \t\t} \t\t \t\treturn result; \t} } add custom validation public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t@notnull(message=\"is required\") \t@min(value=0, message=\"must be greater than or equal to zero\") \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate integer freepasses; \t@pattern(regexp=\"^[a-za-z0-9]{5}\", message=\"only 5 chars/digits\") \tprivate string postalcode; \t // use our custom validation tag \t@coursecode(value=\"tops\", message=\"must start with tops\") \tprivate string coursecode; perform validation on controller package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> <br><br> \t\tpostal code: <form:input path=\"postalcode\" /> \t\t<form:errors path=\"postalcode\" cssclass=\"error\" /> \t\t<br><br> <!-- the message shown equals the messages from both of the validation annotations defined for the coursecode attribute in the customer class --> \t\t\tcourse code: <form:input path=\"coursecode\" /> \t\t\t<form:errors path=\"coursecode\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Maven/Additional Repositories.html",
    "title": "Additional Repositories",
    "body": " index search search back additional repositories as we have said, if maven does not find some dependency in your local repository it goes to the central repository to search for it. but what if the dependency is not in the central repository. then we have to define the repository in our pom.xml: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Maven/POM File Structure.html",
    "title": "POM File Structure",
    "body": " index search search back pom file structure contents project coordinates dependency coordinates find dependencies the pom file has the following structure: project metadata: information about the project dependencies: list of dependencies for the project plug-ins: additional custom tasks to run (junit tests, reports, etc) project coordinates project coordinates uniquely identify a project: where: group id: name of company, group or organization artifact id: name for the project version: a specific release version dependency coordinates to add a given dependency project, we need: group id artifact id optional: version (best practice to include the version) find dependencies search maven maven repository $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Maven/Maven Archetypes.html",
    "title": "Maven Archetypes",
    "body": " index search search back maven archetypes archetypes are used to create new maven projects, you can think of them as starter projects. some archetypes are: for standalone projects: maven-archetype-quickstart for web projects: maven-archetype-webapp $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Maven/Private Repositories.html",
    "title": "Private Repositories",
    "body": " index search search back private repositories if you want to create repositories with restricted access you can: set up your own private maven repository in your server, that is secure with credentials: id/password some maven repository manager products are: archiva artifactory nexus if you do not want to create your own server, there are also cloud based solutions like: package cloud my maven repo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Set Up.html",
    "title": "Set Up",
    "body": " index search search back set up requirements: jdk java application server (i.e. tomcat) java integrated development environment (ide) spring 5 jar files (download manually or use maven) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Custom Login Form.html",
    "title": "Custom Login Form",
    "body": " index search search back custom login form contents create the form login controller now we are going to configure the security of the access to web path in application, login, logout, etc: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t \t\tauth.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} \t@override \tprotected void configure(httpsecurity http) throws exception { // here is the control of the access to web path http.authorizerequests() // require authentication for every request .anyrequest().authenticated() // and for form login customize the login page shown .and() .formlogin() \t\t\t\t\t\t// custom jsp page .loginpage(\"/showmyloginpage\") \t\t\t\t\t\t// you do not need to create a method in your controller for this endpoint, it is handled by spring .loginprocessingurl(\"/authenticatetheuser\") .permitall(); \t\t \t} } create the form we create the login page /showmyloginpage as follows: <!-- reference the spring and jsp tags --> <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %> <html> <head> \t<title>custom login page</title> \t<style> \t\t.failed { \t\t\tcolor: red; \t\t} \t</style> </head> <body> <h3>my custom login page</h3> \t\t<!-- the form points to the endpoint specified preivously: \"authenticatetheuser\" --> \t\t<!-- contextpath is the domain of our app, i.e. localhost:8080 --> \t<form:form action=\"${pagecontext.request.contextpath}/authenticatetheuser\" \t\t\t method=\"post\"> \t\t<!-- check for login error --> \t\t<c:if test=\"${param.error != null}\"> \t\t\t<i class=\"failed\">sorry! you entered invalid username/password.</i> \t\t</c:if> \t\t<p> \t\t\tuser name: <input type=\"text\" name=\"username\" /> \t\t</p> \t\t<p> \t\t\tpassword: <input type=\"password\" name=\"password\" /> \t\t</p> \t\t<input type=\"submit\" value=\"login\" /> \t</form:form> </body> </html> note that spring appends a parameter error when the user fails to login. that is what we use as a condition to show our error message, that is, we check if param.error exists. also, spring security defines default names for login form fields: user name field: username password field: password login controller we also need a controller method for requests to /showmyloginpage: package com.springsecurity.demo.controller; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.getmapping; @controller public class logincontroller { \t@getmapping(\"/showmyloginpage\") \tpublic string showmyloginpage() { \t\t \t\t// this is the custom-login.jsp we created in the previous section \t\treturn \"custom-login\"; \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/JDBC Database Authentication.html",
    "title": "JDBC Database Authentication",
    "body": " index search search back jdbc database authentication contents set up database password encryption add dependiencies jdbc properties files spring security configuration spring security can read user account info from database by default, you have to follow spring security's predefined table schemas. you can customize the table schemas, but you will be responsible for writing the code to access the data. set up database the tables we have to create are the following: password encryption in spring security 5, passwords are stored using a specific format: {id}encodedpassword the id references the operation used to encrypt the password: noop: plain text. so the password is stored as follows in the database: {noop}test123 bcrypt: bcrypt password hashing. so the password is stored as follows in the database: {bcrypt}$2a$12$r9h/cipz0gi.urnnx3kh2opst9/pgbkqquzi.ss7kiugo2t0jwmuw etc. add dependiencies we define the dependencies in our pom.xmlfile that are needed to add support to connect to databases: \t\t<!-- add mysql and c3p0 support --> \t\t<dependency> \t\t\t<groupid>mysql</groupid> \t\t\t<artifactid>mysql-connector-java</artifactid> \t\t\t<version>8.0.16</version> \t\t</dependency> \t\t \t\t<dependency> \t\t\t<groupid>com.mchange</groupid> \t\t\t<artifactid>c3p0</artifactid> \t\t\t<version>0.9.5.4</version> \t\t</dependency> jdbc properties files inside /src/main/resources we create the properties file persistence-mysql.properties for our database connections: # # jdbc connection properties # jdbc.driver=com.mysql.jdbc.driver jdbc.url=jdbc:mysql://localhost:3306/spring_security_demo_plaintext?usessl=false jdbc.user=springstudent jdbc.password=springstudent # # connection pool properties # connection.pool.initialpoolsize=5 connection.pool.minpoolsize=5 connection.pool.maxpoolsize=20 connection.pool.maxidletime=3000 spring security configuration we have to modify our main configuration class, to include our database properties file and create the datasource package com.luv2code.springsecurity.demo.config; import java.beans.propertyvetoexception; import java.util.logging.logger; import javax.sql.datasource; import org.springframework.beans.factory.annotation.autowired; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.propertysource; import org.springframework.core.env.environment; import org.springframework.web.servlet.viewresolver; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.view.internalresourceviewresolver; import com.mchange.v2.c3p0.combopooleddatasource; @configuration @enablewebmvc @componentscan(basepackages=\"com.luv2code.springsecurity.demo\") @propertysource(\"classpath:persistence-mysql.properties\") public class demoappconfig { \t// set up variable to hold the properties \t@autowired \tprivate environment env; \t \t// set up a logger for diagnostics \tprivate logger logger = logger.getlogger(getclass().getname()); \t \t \t// define a bean for viewresolver \t@bean \tpublic viewresolver viewresolver() { \t\t \t\tinternalresourceviewresolver viewresolver = new internalresourceviewresolver(); \t\t \t\tviewresolver.setprefix(\"/web-inf/view/\"); \t\tviewresolver.setsuffix(\".jsp\"); \t\t \t\treturn viewresolver; \t} \t \t// define a bean for our security datasource \t \t@bean \tpublic datasource securitydatasource() { \t\t \t\t// create connection pool \t\tcombopooleddatasource securitydatasource \t\t\t\t\t\t\t\t\t= new combopooleddatasource(); \t\t\t\t \t\t// set the jdbc driver class \t\ttry { // obtain driver from properties file \t\t\tsecuritydatasource.setdriverclass(env.getproperty(\"jdbc.driver\")); \t\t} catch (propertyvetoexception exc) { \t\t\tthrow new runtimeexception(exc); \t\t} \t\t \t\t // obtain database info from properties file \t\tlogger.info(\">>> jdbc.url=\" + env.getproperty(\"jdbc.url\")); \t\tlogger.info(\">>> jdbc.user=\" + env.getproperty(\"jdbc.user\")); \t\t \t\t \t\t// set database connection props \t\tsecuritydatasource.setjdbcurl(env.getproperty(\"jdbc.url\")); \t\tsecuritydatasource.setuser(env.getproperty(\"jdbc.user\")); \t\tsecuritydatasource.setpassword(env.getproperty(\"jdbc.password\")); \t\t \t\t// set connection pool props \t\tsecuritydatasource.setinitialpoolsize( \t\t\t\tgetintproperty(\"connection.pool.initialpoolsize\")); \t\tsecuritydatasource.setminpoolsize( \t\t\t\tgetintproperty(\"connection.pool.minpoolsize\")); \t\tsecuritydatasource.setmaxpoolsize( \t\t\t\tgetintproperty(\"connection.pool.maxpoolsize\")); \t\tsecuritydatasource.setmaxidletime( \t\t\t\tgetintproperty(\"connection.pool.maxidletime\")); \t\t \t\treturn securitydatasource; \t} \t \t// need a helper method \t// read environment property and convert to int \t \tprivate int getintproperty(string propname) { \t\t \t\tstring propval = env.getproperty(propname); \t\t \t\t// now convert to int \t\tint intpropval = integer.parseint(propval); \t\t \t\treturn intpropval; \t} } now in our security configuration we do two things: inject the datasource we defined previouly that holds authentication information tell spring to use jdbc for authentication package com.springsecurity.demo.config; import javax.sql.datasource; import org.springframework.beans.factory.annotation.autowired; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t// add a reference to our security data source \t@autowired \tprivate datasource securitydatasource; \t \t \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// use jdbc authentication \t\tauth.jdbcauthentication().datasource(securitydatasource); \t\t \t} \t@override \tprotected void configure(httpsecurity http) throws exception { \t\thttp.authorizerequests() \t\t\t.antmatchers(\"/\").hasrole(\"employee\") \t\t\t.antmatchers(\"/leaders/**\").hasrole(\"manager\") \t\t\t.antmatchers(\"/systems/**\").hasrole(\"admin\") \t\t\t.and() \t\t\t.formlogin() \t\t\t\t.loginpage(\"/showmyloginpage\") \t\t\t\t.loginprocessingurl(\"/authenticatetheuser\") \t\t\t\t.permitall() \t\t\t.and() \t\t\t.logout().permitall() \t\t\t.and() \t\t\t.exceptionhandling().accessdeniedpage(\"/access-denied\"); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Java Configuration.html",
    "title": "Java Configuration",
    "body": " index search search back java configuration contents web app initializer we are going to show the demoappconfig.java that holds the base configuration of our application: package com.springsecurity.demo.config; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.web.servlet.viewresolver; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.view.internalresourceviewresolver; // tell spring this is a configuration file @configuration // enables annotations @enablewebmvc // search for components in \"com.springsecurity.demo\" package @componentscan(basepackages=\"com.springsecurity.demo\") public class demoappconfig { \t// define a bean for viewresolver \t@bean \tpublic viewresolver viewresolver() { \t\t \t\tinternalresourceviewresolver viewresolver = new internalresourceviewresolver(); \t\t \t\tviewresolver.setprefix(\"/web-inf/view/\"); \t\tviewresolver.setsuffix(\".jsp\"); \t\t \t\treturn viewresolver; \t} } as you can see we have defined a viewresolver that prepends /web-inf/view/ to every view, and appends .jsp to every view. web app initializer spring mvc provides support for web app initialization, and makes sure your code is automatically detected. your code is used to initialize the servlet container. as an example: package com.springsecurity.demo.config; import org.springframework.web.servlet.support.abstractannotationconfigdispatcherservletinitializer; public class myspringmvcdispatcherservletinitializer extends abstractannotationconfigdispatcherservletinitializer { \t@override \tprotected class<?>[] getrootconfigclasses() { \t\t// todo auto-generated method stub \t\treturn null; \t} \t@override \t// tell spring where the configuration for the servlet is \tprotected class<?>[] getservletconfigclasses() { \t\treturn new class[] { demoappconfig.class }; \t} \t@override \t// map the servlet to the path \"/\" \tprotected string[] getservletmappings() { \t\treturn new string[] { \"/\" }; \t} } here is the correspondence with the xml servlet configuration file: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Basic Security.html",
    "title": "Basic Security",
    "body": " index search search back basic security contents create security spring initializer create spring security configuration configuration add users passwords and roles create security spring initializer spring security provides support for security initialization. your security code is used to initialize the servlet container. there is a special class to register the spring security filters. you need this class for the spring security filters to \"activate\". next we show an example: package com.springsecurity.demo.config; import org.springframework.security.web.context.abstractsecuritywebapplicationinitializer; public class securitywebapplicationinitializer \t\t\t\t\t\textends abstractsecuritywebapplicationinitializer { } create spring security configuration (@configuration) now we create our spring security configuration file: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; // tell spring this is a configuration file @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication (this is for test purposes only, you would usually retrieve this information encrypted from the database) \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t // use the authenticationmanagerbuilder given by spring to handle authentication \t\tauth \t\t\t.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} } add users, passwords and roles $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Cross Site Request Forgery.html",
    "title": "Cross Site Request Forgery",
    "body": " index search search back cross site request forgery contents how to see the csrf token spring security protects against cross-site request forgery. csrf is a security attack where a website tricks you into executing an action on a web application that you are currently logged in. protection from this type of attack is embedded in the spring security filters. this protection is enabled by default. spring security uses the synchronizer token pattern, where each request includes a session cookie and a randomly generated token. so for request processing, spring security verifies the token before processing. how to use it? for form submissions use \"post\" instead of \"get\" the spring security tag <form:form> automatically adds the csrf token. if you do not use the tag, you must manually add the csrf token. if you do not add the token you get an error message: 403 forbidden, and further information about how the token cannot be null. how to see the csrf token? when your jsp with the <form:form> tag is processed into an html page, you will be able to see the token inside the form tag: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Display User and Roles.html",
    "title": "Display User and Roles",
    "body": " index search search back display user and roles contents add jsp tag library as dependency jsp page in this section we are going to show how to display in our jsp files the user id and its role: add jsp tag library as dependency first we add to our pom.xml file the jsp tag library: \t\t<!-- add spring security taglibs support --> \t\t<dependency> \t\t <groupid>org.springframework.security</groupid> \t\t <artifactid>spring-security-taglibs</artifactid> \t\t <version>${springsecurity.version}</version> \t\t</dependency>\t jsp page then add the tag library to the jsp page, and we use its tags to access the user id and its role: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!-- add tag library --> <%@ taglib prefix=\"security\" uri=\"http://www.springframework.org/security/tags\" %> <html> <head> \t<title>luv2code company home page</title> </head> <body> \t<h2>luv2code company home page</h2> \t<hr> \t<p> \twelcome to the luv2code company home page! \t</p> \t<hr> \t<!-- display user name and role --> \t<p> \t\tuser: <security:authentication property=\"principal.username\" /> \t\t<br><br> \t\trole(s): <security:authentication property=\"principal.authorities\" /> \t</p> \t<hr> \t<!-- add a logout button --> \t<form:form action=\"${pagecontext.request.contextpath}/logout\" \t\t\t method=\"post\"> \t\t<input type=\"submit\" value=\"logout\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Security/Log Out.html",
    "title": "Log Out",
    "body": " index search search back log out contents configuration log out button we are going to show in this section how to add the logout functionality to our spring application. configuration to our existing configuration we add: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t \t\tauth.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} \t@override \tprotected void configure(httpsecurity http) throws exception { // here is the control of the access to web path http.authorizerequests() // require authentication for every request .anyrequest().authenticated() .and() .formlogin() .loginpage(\"/showmyloginpage\") .loginprocessingurl(\"/authenticatetheuser\") .permitall(); // add logout functionality .and() .logout().permitall() \t\t \t} } the default url for logging out is /logout. log out button now we create the logout button in our home page: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>luv2code company home page</title> </head> <body> \t<h2>luv2code company home page</h2> \t<hr> \t<p> \twelcome to the luv2code company home page! \t</p> \t<!-- add a logout button: it point to \"/logout\" endpoint --> \t<form:form action=\"${pagecontext.request.contextpath}/logout\" \t\t\t method=\"post\"> \t\t<input type=\"submit\" value=\"logout\" /> \t</form:form> </body> </html> note that the logout logic is handled directly by spring, what it does is: invalidate the user's http session and remove cookies, etc. sends the user back to the login page appends a logout parameter: ?logout $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Bean Scopes/Life Cycle.html",
    "title": "Bean Life Cycle",
    "body": " index search search back bean life cycle contents define methods configure hooks in the configuration file main method notes the bean life cycle is as follows: as you can see you can add method/hooks: add custom code during bean initialization calling business logic methods setting up handles to resources (db, sockets, etc) add custom code during bean destruction calling business logic methods clean up handles to resources (db, sockets, etc) define methods first of all we define the methods in our bean: package com.springdemo; public class trackcoach implements coach { \tprivate fortuneservice fortuneservice; \tpublic trackcoach() { \t\t \t} \t \tpublic trackcoach(fortuneservice fortuneservice) { \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn \"just do it: \" + fortuneservice.getfortune(); \t} \t// add an init method \tpublic void domystartupstuff() { \t\tsystem.out.println(\"trackcoach: inside method domystartupstuff\"); \t} \t \t// add a destroy method \tpublic void domycleanupstuffyoyo() { \t\tsystem.out.println(\"trackcoach: inside method domycleanupstuffyoyo\");\t\t \t} } configure hooks in the configuration file once the initialization and clean-up methods have been defined, we configure them in our configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- define your beans here --> \t \t<!-- define the dependency --> \t<bean id=\"myfortuneservice\" \t class=\"com.springdemo.happyfortuneservice\"> \t</bean> \t \t<!-- note the new tag \"scope\" --> \t<bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\" \t\tinit-method=\"domystartupstuff\" \t\tdestroy-method=\"domycleanupstuffyoyo\">\t \t\t \t\t<!-- set up constructor injection --> \t\t<constructor-arg ref=\"myfortuneservice\" /> \t</bean> </beans> main method now in our app, we create the bean to check that our methods are being called: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class beanlifecycledemoapp { \t \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"beanlifecycle-applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// close the context \t\tcontext.close(); \t} } notes when using xml configuration, i want to provide additional details regarding the method signatures of the init-method and destroy-method . access modifier: the method can have any access modifier (public, protected, private) return type: the method can have any return type. however, \"void' is most commonly used. if you give a return type just note that you will not be able to capture the return value. as a result, \"void\" is commonly used. method name: the method can have any method name. arguments: the method can not accept any arguments. the method should be no-arg. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Bean Scopes/Bean Scopes and Life cycle.html",
    "title": "Spring Bean Scopes and Life Cycle",
    "body": " index search search back spring bean scopes and life cycle scope life cycle $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Bean Scopes/Scope.html",
    "title": "Bean Scopes",
    "body": " index search search back bean scopes contents intro specify scope in xml config file main method intro the scope of a bean refers to the life cycle of the bean: how long does it live how many instances are created how is the bean shared the default scope of the bean is a singleton: the spring container creates only one instance of the bean it is cached in memory all requests to the bean will return a shared reference to the same bean other scopes are: a singleton scope is good for stateless data a prototype scope is good for stateful data (the container returns a new bean for each request). note that for this type of bean, spring does not call the destroy method. specify scope in xml config file <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <!-- note the new tag \"scope\" --> <bean id=\"mycoach\" class=\"com.springdemo.trackcoach\" scope=\"prototype\">\t <!-- set up constructor injection --> <constructor-arg ref=\"myfortuneservice\" /> </bean> </beans> main method now, from our application we do: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class beanscopedemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"beanscope-applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\tcoach alphacoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// check if they are the same \t\tboolean result = (thecoach == alphacoach); \t\t \t\t// print out the results \t\tsystem.out.println(\"\\npointing to the same object: \" + result); \t\t \t\tsystem.out.println(\"\\nmemory location for thecoach: \" + thecoach); \t\tsystem.out.println(\"\\nmemory location for alphacoach: \" + alphacoach + \"\\n\"); \t \t\t// close the context \t\tcontext.close(); \t} } observe, the result variable should be set to false, because we are using the prototype scope. also the values of the memory location for the two objects should be distinct for that same reason. however if we were using scope=\"singleton\", then result should be true, and both objects should have the same memory location. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Spring Rest/Spring Rest Controller.html",
    "title": "Spring Rest Controller",
    "body": " index search search back spring rest controller contents hello world configuration general servlet initializer controller spring web mvc provides support for spring rest. for that we use a new annotation called restcontroller which is an extension of controller and handles rest requests and responses. spring rest will also automatically convert java pojos to json as long as the jackson project is on the classpath or pom.xml. hello world to exemplify how to set up a rest controller in spring we will create an application that upong request sends back a hello world! message: configuration first of all, make sure you have the jackson project, mvc and rest and also servlet libraries as a maven dependency or as a library in your classpath. \t<dependencies> \t\t<!-- add spring mvc and rest support --> \t\t<dependency> \t\t\t<groupid>org.springframework</groupid> \t\t\t<artifactid>spring-webmvc</artifactid> \t\t\t<version>5.0.5.release</version> \t\t</dependency> \t\t \t\t<!-- add jackson for json converters --> \t\t<dependency> \t\t\t<groupid>com.fasterxml.jackson.core</groupid> \t\t\t<artifactid>jackson-databind</artifactid> \t\t\t<version>2.9.9.2</version> \t\t</dependency> \t\t<!-- add servlet support for \t\t\t spring's abstractannotationconfigdispatcherservletinitializer --> \t\t<dependency> \t\t\t<groupid>javax.servlet</groupid> \t\t\t<artifactid>javax.servlet-api</artifactid> \t\t\t<version>3.1.0</version> \t\t</dependency> \t\t<!-- add support for jsp ... get rid of eclipse error -->\t\t\t\t \t\t<dependency> \t\t\t<groupid>javax.servlet.jsp</groupid> \t\t\t<artifactid>javax.servlet.jsp-api</artifactid> \t\t\t<version>2.3.1</version> \t\t</dependency> \t\t\t\t \t</dependencies> general we create a configuration class as follows: package com.springdemo.config; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.config.annotation.webmvcconfigurer; // mark it as a configuration class @configuration @enablewebmvc // enable component scanning in our source code @componentscan(\"com.springdemo\") public class demoappconfig implements webmvcconfigurer { } servlet initializer we have to specify the configuration of our servlet, for this we extend abstractannotationconfigdispatcherservletinitializer: package com.springdemo.config; import org.springframework.web.servlet.support.abstractannotationconfigdispatcherservletinitializer; public class myspringmvcdispatcherservletinitializer extends abstractannotationconfigdispatcherservletinitializer { \t@override \tprotected class<?>[] getrootconfigclasses() { \t\t// todo auto-generated method stub \t\treturn null; \t} \t@override \tprotected class<?>[] getservletconfigclasses() { // specify our config class \t\treturn new class[] { demoappconfig.class }; \t} \t@override \tprotected string[] getservletmappings() { \t\treturn new string[] { \"/\" }; \t} } controller for this we need to create our server with the controller that handles this request: package com.springdemo.rest; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; @restcontroller @requestmapping(\"/test\") public class demorestcontroller { \t// add code for the \"/hello\" endpoint \t \t@getmapping(\"/hello\") \tpublic string sayhello() { \t\treturn \"hello world!\"; \t} \t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Spring Rest/POJOs as JSON.html",
    "title": "POJOs as JSON",
    "body": " index search search back pojos as json contents create pojo create service to test converting pojos to json we are going to create a service that allows us to retrieve a list of students: create pojo we are going to create the student entity: package com.springdemo.entity; public class student { \tprivate string firstname; \tprivate string lastname; \t \tpublic student() { \t\t \t} \tpublic student(string firstname, string lastname) { \t\tthis.firstname = firstname; \t\tthis.lastname = lastname; \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t } create service we now code the logic that handles the controller. package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.luv2code.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t \t \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t \t \t \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\treturn thestudents.get(studentid); \t\t \t} } note that the endpoint \"/students/{studentid}\" has a path variable studentid $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Spring Rest/JSON Data Binding.html",
    "title": "Java JSON Data Binding",
    "body": " index search search back java json data binding contents set up create pojo class main app nested objects ignore unknwon properties data binding is the process of converting json data to a java pojo (the conversion goes both ways) data binding is the same as serialization/deserialization and marshalling/unmarshalling. spring uses the jackson project behind the scenes which handles data binding between json and java pojos. for conversion we use object mapper by default jackson will call appropiate getter and setter methods to populate a pojo from a json or to create a json object from a pojo. to convert from json to java, jackson calls the setter methods to convert from java to json, jackson calls the getter methods set up add jackson project as a dependency in the maven file: \t<dependencies> \t\t<!-- todo: add your dependency here --> \t\t<dependency> \t\t\t<groupid>com.fasterxml.jackson.core</groupid> \t\t\t<artifactid>jackson-databind</artifactid> \t\t\t<version>2.10.0.pr1</version> \t\t</dependency>\t \t\t\t \t</dependencies> create pojo class we now create the class we are going to convert to json (serialize): package com.jackson.json.demo; public class student { \tprivate int id; \tprivate string firstname; \tprivate string lastname; \tprivate boolean active; \t \tpublic student() { \t\t \t} \t \tpublic int getid() { \t\treturn id; \t} \t \tpublic void setid(int id) { \t\tthis.id = id; \t} \t \tpublic string getfirstname() { \t\treturn firstname; \t} \t \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \t \tpublic string getlastname() { \t\treturn lastname; \t} \t \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t \tpublic boolean isactive() { \t\treturn active; \t} \t \tpublic void setactive(boolean active) { \t\tthis.active = active; \t} \t } main app now, to test it we are going to create a student object by reading from a json object: package com.jackson.json.demo; import java.io.file; import com.fasterxml.jackson.databind.objectmapper; public class driver { \tpublic static void main(string[] args) { \t\t \t\ttry { \t\t\t// create object mapper \t\t\tobjectmapper mapper = new objectmapper(); \t\t\t \t\t\t// read json file and map/convert to java pojo: \t\t\t// data/sample-lite.json \t\t\t \t\t\tstudent thestudent = mapper.readvalue( \t\t\t\t\t\tnew file(\"data/sample-lite.json\"), student.class); \t\t} \t\tcatch (exception exc) { \t\t\texc.printstacktrace(); \t\t} \t} } nested objects but, how can we read nested properties inside a json file, like the following: { \t\"id\": 14, \t\"firstname\": \"mario\", \t\"lastname\": \"rossi\", \t\"active\": true, \t\"address\": { \t\t\"street\": \"100 main st\", \t\t\"city\": \"philadelphia\", \t\t\"state\": \"pennsylvania\", \t\t\"zip\": \"19103\", \t\t\"country\": \"usa\" \t}, \t\"languages\" : [\"java\", \"c#\", \"python\", \"javascript\"] } as you can see the address property has properties inside it. what we are going to do is create a new attribute address inside the student object, which will be a pojo object in itself. package com.jackson.json.demo; public class student { \tprivate int id; \tprivate string firstname; \tprivate string lastname; \tprivate boolean active; \t \tprivate address address; \t \tprivate string[] languages; \t \tpublic student() { \t\t \t} \t \tpublic int getid() { \t\treturn id; \t} \t \tpublic void setid(int id) { \t\tthis.id = id; \t} \t \tpublic string getfirstname() { \t\treturn firstname; \t} \t \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \t \tpublic string getlastname() { \t\treturn lastname; \t} \t \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t \tpublic boolean isactive() { \t\treturn active; \t} \t \tpublic void setactive(boolean active) { \t\tthis.active = active; \t} \tpublic address getaddress() { \t\treturn address; \t} \tpublic void setaddress(address address) { \t\tthis.address = address; \t} \tpublic string[] getlanguages() { \t\treturn languages; \t} \tpublic void setlanguages(string[] languages) { \t\tthis.languages = languages; \t} \t } we also need to create the address class: package com.jackson.json.demo; public class address { \tprivate string street; \tprivate string city; \tprivate string state; \tprivate string zip; \tprivate string country; \t \tpublic address() { \t\t \t} \tpublic string getstreet() { \t\treturn street; \t} \tpublic void setstreet(string street) { \t\tthis.street = street; \t} \tpublic string getcity() { \t\treturn city; \t} \tpublic void setcity(string city) { \t\tthis.city = city; \t} \tpublic string getstate() { \t\treturn state; \t} \tpublic void setstate(string state) { \t\tthis.state = state; \t} \tpublic string getzip() { \t\treturn zip; \t} \tpublic void setzip(string zip) { \t\tthis.zip = zip; \t} \tpublic string getcountry() { \t\treturn country; \t} \tpublic void setcountry(string country) { \t\tthis.country = country; \t} } ignore unknwon properties to ignore properties from the json file that cannot be mapped to an attribute in the pojo we use the annotation: package com.jackson.json.demo; @jsonignoreproperties(ignoreunkown=true) public class student { \tprivate int id; \tprivate string firstname; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Spring Rest/Exception Handling.html",
    "title": "Exception Handling",
    "body": " index search search back exception handling contents create error response class create exception class rest service with exception global exception handler in this section we are going to show how to create an error page to display when there is an error on a request. create error response class package com.springdemo.rest; public class studenterrorresponse { \tprivate int status; \tprivate string message; \tprivate long timestamp; \tpublic studenterrorresponse() { \t\t \t} \t \tpublic studenterrorresponse(int status, string message, long timestamp) { \t\tthis.status = status; \t\tthis.message = message; \t\tthis.timestamp = timestamp; \t} \tpublic int getstatus() { \t\treturn status; \t} \tpublic void setstatus(int status) { \t\tthis.status = status; \t} \tpublic string getmessage() { \t\treturn message; \t} \tpublic void setmessage(string message) { \t\tthis.message = message; \t} \tpublic long gettimestamp() { \t\treturn timestamp; \t} \tpublic void settimestamp(long timestamp) { \t\tthis.timestamp = timestamp; \t} \t \t } create exception class package com.springdemo.rest; public class studentnotfoundexception extends runtimeexception { \tpublic studentnotfoundexception(string message, throwable cause) { \t\tsuper(message, cause); \t} \tpublic studentnotfoundexception(string message) { \t\tsuper(message); \t} \tpublic studentnotfoundexception(throwable cause) { \t\tsuper(cause); \t} \t } rest service with exception what we need to know is: define an exception handler method with @exceptionhandler annotation the exception handler will return a response entity response entity is a wrapper for the http response object resposneentity provides a fine-grained control to specify: http status code http headers response body package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.exceptionhandler; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t \t \t \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\t// check the studentid against list size \t\tif ( (studentid >= thestudents.size()) || (studentid < 0) ) {\t\t\t \t\t\tthrow new studentnotfoundexception(\"student id not found - \" + studentid); \t\t} \t\t \t\treturn thestudents.get(studentid); \t\t \t} // tag it as an exception handling method \t@exceptionhandler // type of response body exception type to handle \tpublic responseentity<studenterrorresponse> handleexception(studentnotfoundexception exc) { \t\t \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t // json error object \t\terror.setstatus(httpstatus.not_found.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t // return response with the error object and the status code \t\treturn new responseentity<>(error, httpstatus.not_found); \t } // another exception handler \t@exceptionhandler // catch any exception thrown \tpublic responseentity<studenterrorresponse> handleexception(exception exc) { \t\t \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t \t\terror.setstatus(httpstatus.bad_request.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\treturn new responseentity<>(error, httpstatus.bad_request); \t}\t } global exception handler instead of having the exception handling methods in every controller, we defined them globally. for that we use controlleradvice that acts as a filter between the requests and the controller. it: pre-processes requests to controllers post-processes responses to handle exceptions so, we create a class with the @controlleradvice annotation: package com.springdemo.rest; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.controlleradvice; import org.springframework.web.bind.annotation.exceptionhandler; @controlleradvice public class studentrestexceptionhandler { \t// add exception handling code here \t// add an exception handler using @exceptionhandler \t@exceptionhandler \tpublic responseentity<studenterrorresponse> handleexception(studentnotfoundexception exc) { \t\t \t\t// create a studenterrorresponse \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t \t\terror.setstatus(httpstatus.not_found.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\t// return responseentity \t\treturn new responseentity<>(error, httpstatus.not_found); \t} \t \t// add another exception handler ... to catch any exception (catch all) \t@exceptionhandler \tpublic responseentity<studenterrorresponse> handleexception(exception exc) { \t\t \t\t// create a studenterrorresponse \t\tstudenterrorresponse error = new studenterrorresponse(); \t\terror.setstatus(httpstatus.bad_request.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\t// return responseentity\t\t \t\treturn new responseentity<>(error, httpstatus.bad_request); \t} \t } and now we modify the controller to make use of this paradigm: package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.exceptionhandler; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\t// check the studentid against list size \t\tif ( (studentid >= thestudents.size()) || (studentid < 0) ) { \t\t\tthrow new studentnotfoundexception(\"student id not found - \" + studentid); \t\t} \t\t\t \t\treturn thestudents.get(studentid); \t}\t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Boot Project Structure.html",
    "title": "Spring Boot Project Structure",
    "body": " index search search back spring boot project structure contents application properties static content testing application properties by default, spring boot will load properties from: application.properties in the src project directory. we inject it in our code the same way we did it with spring static content by default, spring boot wil load static resources from \"/static\" directory testing unit tests are stored on the src directory under the /test folder $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Boot Starters.html",
    "title": "Spring Boot Starters",
    "body": " index search search back spring boot starters contents spring boot starter parent spring boot staters offer a curated list of dependencies that are grouped together and tested by the spring development team. so now, if your application depends on the web and security module and also uses thymeleaf and jpa, you add the following dependencies: ... \t<dependencies> <!-- web --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-web</artifactid> \t\t</dependency> <!-- security --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-security</artifactid> \t\t</dependency> <!-- jpa --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-data-jpa</artifactid> \t\t</dependency> <!-- thymeleaf --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-thymeleaf</artifactid> \t\t</dependency> \t\t<!-- add support for automatic reloading --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-devtools</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... spring boot starter parent this is a special starter that provides defaults: default compiler level utf-8 source encoding you include it in your pom.xml file as follows: ... \t<parent> \t\t<groupid>org.springframework.boot</groupid> \t\t<artifactid>spring-boot-starter-parent</artifactid> \t\t<version>2.1.2.release</version> \t\t<relativepath/> <!-- lookup parent from repository --> \t</parent> \t<dependencies> \t... \t</dependencies> ... if you want to override a default, you use properties: ... \t<parent> \t\t<groupid>org.springframework.boot</groupid> \t\t<artifactid>spring-boot-starter-parent</artifactid> \t\t<version>2.1.2.release</version> \t\t<relativepath/> <!-- lookup parent from repository --> \t</parent> \t \t<!-- override default java version --> \t<properties> \t\t<java.version>1.8</java.version> \t</properties> \t<dependencies> \t... \t</dependencies> ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Data JPA.html",
    "title": "Spring Data JPA",
    "body": " index search search back spring data jpa contents create repository use repository with jpa api we created a employee dao, however what if we need to create one for each entity we manage. then we would duplicate a lot of code, because the calls to the api are basically the same. that is what we use spring data jpa, we plug in the entity type and the primary key to the dao, and spring creates it an manages it for us. create repository so now the employee dao is as follows: package com.springboot.cruddemo.dao; import org.springframework.data.jpa.repository.jparepository; import com.springboot.cruddemo.entity.employee; public interface employeerepository extends jparepository<employee, integer> { } use repository and the employee service is: package com.springboot.cruddemo.service; import java.util.list; import java.util.optional; import org.springframework.beans.factory.annotation.autowired; import org.springframework.stereotype.service; import com.springboot.cruddemo.dao.employeerepository; import com.springboot.cruddemo.entity.employee; @service public class employeeserviceimpl implements employeeservice { // here we make use of the above implemented repository \tprivate employeerepository employeerepository; \t \t@autowired \tpublic employeeserviceimpl(employeerepository theemployeerepository) { \t\temployeerepository = theemployeerepository; \t} \t \t@override \tpublic list<employee> findall() { \t\treturn employeerepository.findall(); \t} \t@override \tpublic employee findbyid(int theid) { \t\toptional<employee> result = employeerepository.findbyid(theid); \t\t \t\temployee theemployee = null; \t\t \t\tif (result.ispresent()) { \t\t\ttheemployee = result.get(); \t\t} \t\telse { \t\t\t// we didn't find the employee \t\t\tthrow new runtimeexception(\"did not find employee id - \" + theid); \t\t} \t\t \t\treturn theemployee; \t} \t@override \tpublic void save(employee theemployee) { \t\temployeerepository.save(theemployee); \t} \t@override \tpublic void deletebyid(int theid) { \t\temployeerepository.deletebyid(theid); \t} } this employeeservice implements the interface: package com.springboot.cruddemo.service; import java.util.list; import com.springboot.cruddemo.entity.employee; public interface employeeservice { \tpublic list<employee> findall(); \t \tpublic employee findbyid(int theid); \t \tpublic void save(employee theemployee); \t \tpublic void deletebyid(int theid); \t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Application Properties.html",
    "title": "Application Properties",
    "body": " index search search back application properties contents configuring the spring boot server core web actuator properties security data properties by default spring boot reads information from a standard properties file in src/main/resources/application.properties. you can define any custom properties in this file and your app can access properties using the annotation @value(we have done this before). configuring the spring boot server some properties offered by spring are: core # log levels severity mapping logging.level.org.springframework=debug logging.level.org.hibernate=trace logging.level.org.luv2code=info # log file name logging.file=date.log web # http server port server.port=7070 # context path of the application server.servlet.context-path=/my-app # default http session timeout server.servlet.session.timeout=15m actuator properties # endpoints to include by name or wildcard management.endpoints.web.exposure.include=* # endpoints to exclude by name or wildcard management.endpoints.web.exposure.exclude=beans,mapping security # default username spring.security.user.name=admin # password for default user spring.security.user.password=mypass data properties # jdbc url of the database spring.datasource.url=jdbc:mysql://localhost:3306/myapp # login username of the database spring.datasource.username=alba # login password of the database spring.datasource.password=testpass $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Boot DevTools.html",
    "title": "Spring Boot DevTools",
    "body": " index search search back spring boot devtools spring boot dev tools automatically restart your application when code is updated. the only thing you need to do is add the module to the dependencies: ... \t<dependencies> \t\t<!-- add support for automatic reloading --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-devtools</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Boot Actuator.html",
    "title": "Spring Boot Actuator",
    "body": " index search search back spring boot actuator contents add security spring boot actuator automatically exposes endpoints to monitor and manage your application. you only need to add the dependency to you pom.xml file: ... \t<dependencies> ... \t\t<!-- add support for spring boot actuator --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-actuator</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... the endpoints are prefixed by /actuator, some of them are: /health: health information about your application /info: information about your project. by default it return an empty json object. you can add info through application.properties as follows: info.app.name=my super cool app info.app.description=a crazy fun app, yoohoo! info.app.version=1.0.0 /auditevents: audit events for your application /beans: list of all beans registered in the spring application context /mappings: list of all @requestmapping path by default only /health and /info are exposed, to expose all actuator endpoints you need to specify on application.properties (you can also specify only the ones you want separated by commas): management.endpoints.web.exposure.include=* add security first you need to add spring security as a dependency in your pom.xml: ... \t<dependencies> \t\t... <!-- security --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-security</artifactid> \t\t</dependency> \t\t... \t</dependencies> ... now, when we access some endpoints like /actuator/beans spring will prompt a login to grant access to the endpoint. the default user name is \"user\" the password will be printed on the console where you start the application to override these defaults edit the application.properties file as follows: spring.security.user.name=alba spring.security.user.password=mypassword we can also exclude endpoints by adding the following declarations to the application.properties file: management.endpoints.web.exposure.exclude=health,info $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/SpringBoot/Spring Data REST.html",
    "title": "Spring Data Rest",
    "body": " index search search back spring data rest contents configuration sorting what if we want to also reduce the code for creating api, that is, what if spring could create a rest api for us using our jparepository, such that it would expose all of the basic rest api crud features automatically. what does it do? it scans your project fro jparepository it exposes rest apis for each entity type for your jparepository so now, we can remove our employee services and our rest controllers, because it is handled automatically by spring. the only thing needed is adding spring data rest as a dependency: \t<dependencies> ... \t\t<!-- add dependency for spring data rest --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-data-rest</artifactid> \t\t</dependency> ... \t</dependencies> to sum up, now in your application you only will have: your entity: employee the corresponding jpa repository: employeerepository dependency main application the first one applies to each entity your application has. spring data rest is hateoas compliant (the responses include metadata about itself). configuration you can specify the name of the endpoint that is exposed (by the default is the plural of the entity) with: @repositoryrestresource(path=\"members\") public interface employeerepository extends jparepository<employee, integer> { } the default number of elements returned are 20, then we can use pagination to retrieve the next ones with query parameters (?page=0). some properties available to tweak in application.properties are: spring.data.rest.base-path: base path used to expose repository resources spring.data.rest.default-page-size: default size pages spring.data.rest.max-page-size: maximum size of pages sorting you can sort by the property names of your entity. on the employee example we have firstname, lastname and email, therefore we can do: http://localhost:8080/employees?sort=firstname or http://localhost:8080/employees?sort=firstname,desc $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Dependency Injection/Injecting Literal Values.html",
    "title": "Injecting Literal Values",
    "body": " index search search back injecting literal values contents define the attributes configuration file main method to inject concrete attributes into our beans: define the attributes first we define the attributes emailaddress and team in the object. also we create the set and get methods for both of them: package com.luv2code.springdemo; public class cricketcoach implements coach { \tprivate fortuneservice fortuneservice; \t \t// add new fields for emailaddress and team \tprivate string emailaddress; \tprivate string team; \t \t\t \tpublic cricketcoach() { \t\tsystem.out.println(\"cricketcoach: inside no-arg constructor\"); \t} \t /* setters and getters */ \tpublic string getemailaddress() { \t\treturn emailaddress; \t} \tpublic void setemailaddress(string emailaddress) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setemailaddress\"); \t\tthis.emailaddress = emailaddress; \t} \tpublic string getteam() { \t\treturn team; \t} \tpublic void setteam(string team) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setteam\"); \t\tthis.team = team; \t} /* setter injection */ \tpublic void setfortuneservice(fortuneservice fortuneservice) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setfortuneservice\"); \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice fast bowling for 15 minutes\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } configuration file now we define the properties in the configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- define your beans here --> \t<!-- define the dependency --> \t<bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> \t</bean> \t \t<bean id=\"mycoach\" \t\tclass=\"com.springdemo.trackcoach\">\t \t\t<!-- set up constructor injection --> \t\t<constructor-arg ref=\"myfortuneservice\" /> \t</bean> \t \t<bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> \t <!-- set up setter injection --> \t <!-- ref: references the id of the bean we define previously --> \t <!-- name: name of the setter method set<name>, where the first \t letter of the name is capitalized --> \t <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> \t\t<!-- inject literal values, where name is the name of the attribute in the bean \t\tand value is the value to set the value to --> \t <property name=\"emailaddress\" value=\"email@email.com\" /> \t <property name=\"team\" value=\"best team\" /> \t</bean> </beans> main method now in the main method of our app we can call the getters and setters for these new attributes: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// retrieve attribute values \t\tsystem.out.println(thecoach.getteam()); \t\tsystem.out.println(thecoach.getemailaddress()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Dependency Injection/Inject Values From a Properties File.html",
    "title": "Inject Values from the Properties Files",
    "body": " index search search back inject values from the properties files contents create the properties file load the properties file main method create the properties file let's define our properties inside a properties file sport.properties: foo.email=myeasycoach@email.com foo.team=royal challengers bangalore load the properties file now we load the properties file using the context tag inside our config file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- load the properties file: sport.properties --> <context:property-placeholder location=\"classpath:sport.properties\"/> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\">\t \t<!-- set up constructor injection --> \t<constructor-arg ref=\"myfortuneservice\" /> </bean> <bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> <!-- set up setter injection --> <!-- ref: references the id of the bean we define previously --> <!-- name: name of the setter method set<name>, where the first letter of the name is capitalized --> <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> <!-- inject literal values, where name is the name of the attribute in the bean and value is the value to set the value to --> <!-- note that we are now referencing the values from the properties file --> <property name=\"emailaddress\" value=\"${foo.email})\" /> <property name=\"team\" value=\"${foo.team}\" /> </bean> </beans> main method in the main method, we create our object as usual, and if we invoke the getter methods, we retrieve the values passed in the property file: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// retrieve attribute values from property file \t\tsystem.out.println(thecoach.getteam()); \t\tsystem.out.println(thecoach.getemailaddress()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Dependency Injection/Constructor Injection.html",
    "title": "Constructor Injection",
    "body": " index search search back constructor injection contents create dependency object establish dependency configuration file main method now we will show an example where the baseballcoach has fortuneservice as a dependency. so, first we create the dependency interface as follows: create dependency object package com.springdemo; public interface fortuneservice { \tpublic string getfortune(); \t } next we create the dependency class than implements the interface: package com.springdemo; public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } establish dependency let's also update the coach interface to add a method getdailyfortune (note that all classes that implement this interface have to implement this new method): package com.springdemo; public interface coach { \tpublic string getdailyworkout(); \t \tpublic string getdailyfortune(); } now create a constructor for the dependency in the class that has the dependency package com.springdemo; public class baseballcoach implements coach { \t// define a private field for the dependency \tprivate fortuneservice fortuneservice; \t \t// define a constructor for dependency injection \tpublic baseballcoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"spend 30 minutes on batting practice\"; \t} \t@override \tpublic string getdailyfortune() {\t\t \t\t// use my fortuneservice to get a fortune\t\t \t\treturn fortuneservice.getfortune(); \t} } configuration file finally define the dependency in the configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <!-- bean with the dependency --> <bean id=\"mycoach\" class=\"com.springdemo.trackcoach\">\t <!-- set up constructor injection, note ref=id of bean --> <constructor-arg ref=\"myfortuneservice\" /> </bean> </beans> behind the scenes, spring framework does: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\t// create object \t\t// from the bean with id = myfortuneservice in the config file \t\thappyfortuneservice myfortuneservice = new happyfortuneservice(); \t\t \t\t// add dependency via constructor \t\t// from the bean with id = mycoach in the config file \t\ttrackcoach mycoach = new trackcoach(fortuneservice); \t} } main method we do not need to make any modifications to the app, when we create the coach bean using spring, the framework deals with the dependency injection: package com.luv2code.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class hellospringapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container (with the dependency) \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// let's call our new method for fortunes \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Dependency Injection/Dependency Injection.html",
    "title": "Dependency Injection",
    "body": " index search search back dependency injection contents injection types the dependencies of the objects are managed by the spring container object factory: so instead of having to build the object and all of its dependencies, the spring factory will do this work for you. injection types there are several injection types in spring. the more common are: constructor injection setter injection injecting literal values inject values from a properties file $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Dependency Injection/Setter Injection.html",
    "title": "Setter Injection",
    "body": " index search search back setter injection contents create dependency object define dependency configuration file main method inject dependencies by calling setter methods on your class create dependency object refer to create dependency object define dependency we include a setter method that takes the dependency as an argument like: package com.springdemo; public class cricketcoach implements coach { \tprivate fortuneservice fortuneservice;\t \t \t// create a no-arg constructor \tpublic cricketcoach() { \t\tsystem.out.println(\"cricketcoach: inside no-arg constructor\"); \t} \t \t// our setter method \tpublic void setfortuneservice(fortuneservice fortuneservice) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setfortuneservice\"); \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice fast bowling for 15 minutes\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } configuration file <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\">\t \t<!-- set up constructor injection --> \t<constructor-arg ref=\"myfortuneservice\" /> </bean> <bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> <!-- set up setter injection --> <!-- ref: references the id of the bean we define previously --> <!-- name: name of the setter method set<name>, where the first letter of the name is capitalized --> <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> </bean> </beans> behind the scenes, spring framework does: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\t// create object \t\t// from the bean with id = myfortuneservice in the config file \t\thappyfortuneservice myfortuneservice = new happyfortuneservice(); \t\t \t\t// from the bean with id = mycricketcoach in the config file \t\tcricketcoach mycricketcoach = new cricketcoach(fortuneservice); \t\t// add dependency via setter \t\tmycricketcoach.setfortuneservice(myfortuneservice); \t} } main method now, on the main method of our spring app, we create the object by reading the config file, and spring automatically injects the dependency via the setter method: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/After Advice.html",
    "title": "After Advice",
    "body": " index search search back after advice this advice runs always when the method is completed (like a finally clause inside a try catch). for example if we want to always run the advice afterfinallyfindaccountsadvice when the method findaccounts inside accountdao finishes: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.after; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t@after(\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\") \tpublic void afterfinallyfindaccountsadvice(joinpoint thejoinpoint) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @after (finally) on method: \" \t\t\t\t\t\t\t+ method); \t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/AfterThrowing Advice.html",
    "title": "AfterThrowing Advice",
    "body": " index search search back afterthrowing advice this advice is run whenever the target object throws and execption. for example: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.luv2code.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@afterthrowing( \t\t\tpointcut=\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\", \t\t\t// define the name of the parameter that holds the exception object \t\t\tthrowing=\"theexc\") \tpublic void afterthrowingfindaccountsadvice( \t\t\t\t\tjoinpoint thejoinpoint, throwable theexc) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @afterthrowing on method: \" + method); \t\t \t\t// log the exception \t\tsystem.out.println(\"\\n=====>>> the exception is: \" + theexc); \t \t} } in this code sample we have the advice afterthrowingfindaccountsadvice that is run whenever the method findaccounts inside accountdao throws an exception. we also make use of the throwing attribute that lets us map the exception object to a parameter inside our advice. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/Before Advice.html",
    "title": "Before Advice",
    "body": " index search search back before advice contents add dependencies create target object spring configuration create aspect with before main app we use the tag @before to execute some code before we call the target object function: add dependencies we have to download the aspectj jar file, because spring aop depends on some on their framework's classes create target object we create a dao object: package com.aopdemo.dao; import org.springframework.stereotype.component; @component public class accountdao { \tpublic void addaccount() { \t\tsystem.out.println( \t\t\tgetclass() \t\t\t+ \": doing my db work: adding an account\" \t\t); \t} } spring configuration we now have to enable aop proxying in our app configuration: package com.aopdemo; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.enableaspectjautoproxy; @configuration // enable proxying to add before advice @enableaspectjautoproxy @componentscan(\"com.aopdemo\") public class democonfig { } create aspect with @before now it is time to create an aspect with @before advice: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t// this is where we add all of our related advices for logging \t// here we specify we want to run this code before calling the \t// object method public void addaccount \t@before(\"execution(public void addaccount())\") \tpublic void beforeaddaccountadvice() { \t\tsystem.out.println(\"\\n=====>>> executing @before advice on addaccount()\"); \t} } main app we now create a demo app: package com.aopdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; import com.aopdemo.dao.accountdao; public class maindemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(democonfig.class); \t\t \t\t// get the bean from spring container \t\taccountdao theaccountdao = context.getbean(\"accountdao\", accountdao.class); \t\t \t\t// call the business method \t\ttheaccountdao.addaccount(); \t\t// do it again! \t\tsystem.out.println(\"\\nlet's call it again!\\n\"); \t\t \t\t// call the business method again \t\ttheaccountdao.addaccount(); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/Around Advice.html",
    "title": "Around Advice",
    "body": " index search search back around advice contents exception handling this type of advice is always called before and after the target object. when using the @around advice we have access to a reference of a proceeding join point. which is a handle to the target method, and will let us execute the taget method. so for example if we want to measure the performance of the getfortunemethod: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.proceedingjoinpoint; import org.aspectj.lang.annotation.after; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.around; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\t// now, let's execute the method \t\tobject result = theproceedingjoinpoint.proceed(); \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} } the advice aroundgetfortune is called before the getfortune is called, then it proceeds to call from inside the advice and we measure how long does the method take to run. exception handling inside an advice, to handle exceptions you can: handle the exception inside the advice \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\ttry { \t\t\tresult = theproceedingjoinpoint.proceed(); \t\t} catch (exception e) { \t\t\t// log the exception \t\t\tmylogger.warning(e.getmessage()); \t\t\t \t\t\t// give users a custom messagee \t\t\tresult = \"major accident! but no worries, \" \t\t\t\t\t+ \"your private aop helicopter is on the way!\"; \t\t} \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} simply rethrow the exception \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\ttry { \t\t\tresult = theproceedingjoinpoint.proceed(); \t\t} catch (exception e) { \t\t\t// log the exception \t\t\tmylogger.warning(e.getmessage()); \t\t\t// rethrow exception \t\t\tthrow e; \t\t} \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/Pointcut Expressions.html",
    "title": "Pointcut Expressions",
    "body": " index search search back pointcut expressions contents execution pointcut match methods match parameters a pointcut expression is a predicate expression that tells spring when to apply a given advice. spring aop uses aspectj's pointcut expression language. execution pointcut the expression pattern is the following: execution(modifiers-pattern? return-type-pattern declaring-type-pattern? method-name-pattern(param-pattern) throws-pattern?) modifiers-pattern?: spring aop only supports public or * return-type-pattern: void, boolean, string, list<costumer>, etc declaring-type-pattern?: the class name method-name-pattern(param-pattern): method name to match, and parameters type to match throws-pattern?: exception types to match if the parameter is optional it is followed by an ?. you can also add wildcards inside the patterns. match methods some examples are: match concrete method inside a class: @before(\"execution(public void com.aopdemo.dao.accountdao.addaccount())\") match a method inside any class: @before(\"execution(public void addaccount())\") match any method that starts with add: @before(\"execution(public void add*())\") match all methods inside a given package: @before(\"execution(* com.aopdemo.dao.*.*(..))\") the first * denotes the return type, it can be anything the second * denotes the class name, it can be anything inside the package the third * denotes the method name, it can be anything lastly, .. denotes the param-type, there can be 0 or more parameters match parameters there are the following parameter pattern wildcards: (): matches a method with no arguments (*): matches a method with one argument of any type (..): matches a method with 0 or more arguments of any type for example: match addaccount methods with no arguments: @before(\"execution(* addaccount())\") match addacount methods with one account parameter: @before(\"execution(* addaccount(com.aopdemo.account))\") match addacount methods with any number of parameters: @before(\"execution(* addaccount(*))\") $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/AfterReturning Advice.html",
    "title": "AfterReturning Advice",
    "body": " index search search back afterreturning advice this advice is run after the method is done executing, and it executed successfully. the flow of this advice is the following: so for example, if you want to have an advice run everytime we call the findaccounts method inside a concrete class, and we also want to print out the result we obtained we do the following: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t\t \t// add a new advice for @afterreturning on the findaccounts method \t@afterreturning( \t\t\tpointcut=\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\", // this is the parameter name of the list of accounts returned by findaccounts \t\t\treturning=\"result\") \tpublic void afterreturningfindaccountsadvice( \t\t\t\t\tjoinpoint thejoinpoint, list<account> result) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @afterreturning on method: \" + method); \t\t\t\t \t\t// print out the results of the method call \t\tsystem.out.println(\"\\n=====>>> result is: \" + result); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/Pointcut Declarations.html",
    "title": "Pointcut Declarations",
    "body": " index search search back pointcut declarations contents create pointcut declaration reuse pointcut declaration combine pointcut declarations how can we reuse a pointcut expression? we need to: create a pointcut declaration apply the pointcut declaration to the advices we want create pointcut declaration we define the pointcut declaration with the pointcut annotation and we bind it to an arbitrary method. package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} } reuse pointcut declaration to reuse this declaration we simply call the method that is bound to the pointcut declaration: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} \t // reuse declaration \t@before(\"fordaopackage()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t // reuse declaration \t@before(\"fordaopackage()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} \t } combine pointcut declarations how can we apply multiple pointcut expressions to a single advice? well we can combine pointcut expressions using logic operators: and (&&) or (||) not (!) for example: @before(\"expressionone() && expressiontwo()\") @before(\"expressionone() || expressiontwo()\") @before(\"expressionone() && !expressiontwo()\") imagine we want to execute an advice for every method in the package except for getters and setters, then we do: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} \t \t// create pointcut for getter methods \t@pointcut(\"execution(* com.aopdemo.dao.*.get*(..))\") \tprivate void getter() {} \t \t// create pointcut for setter methods \t@pointcut(\"execution(* com.aopdemo.dao.*.set*(..))\") \tprivate void setter() {} \t \t// create pointcut: include package ... exclude getter/setter \t@pointcut(\"fordaopackage() && !(getter() || setter())\") \tprivate void fordaopackagenogettersetter() {} \t \t@before(\"fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t \t@before(\"fordaopackagenogettersetter()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/AOP/Control Aspect Order.html",
    "title": "Control Aspect Order",
    "body": " index search search back control aspect order contents refactor and order log to cloud aspect logging aspect analytics aspect how do we control the order of advices being applied when they all match the pointcut expressions? to control order we should: refactor: place advices in separate aspects control order on aspects using the @order annotation refactor and order we are going to create three aspects separate from each other as follows: so with the ordering the aspect flow looks something like this: log to cloud aspect package com.luv2code.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set order @order(1) public class mycloudlogasyncaspect { \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void logtocloudasync() { \t\tsystem.out.println(\"\\n=====>>> logging to cloud in async fashion\");\t\t \t} } logging aspect package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set the order @order(2) public class mydemologgingaspect { \t \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t } analytics aspect package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set the order @order(3) public class myapianalyticsaspect { \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Field Injection.html",
    "title": "Field Injection",
    "body": " index search search back field injection contents define dependency as component specify dependency field injection allows you to inject dependencies by setting field values on your class directly (even private ones). this is accomplished by using java reflection. for this, we need to configure the autowired annotation as follows: apply it directly to the field which saves us from using setter methods for dependency injection. define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface @autowired \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Method Injection.html",
    "title": "Method Injection",
    "body": " index search search back method injection contents define dependency as component specify dependency one thing to note is that you can add dependency injection on any method you want, does not have to be a setter method: define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency we now create a setter method in our class for the injection: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired public anymethod(fortuneservice fortuneservice){ this.fortuneservice = fortuneservice; } \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/index.html",
    "title": "Java Annotations",
    "body": " index search search back java annotations java annotations are special labels added to classes. they provide metadata about the class, and can be processed at compile time or run-time for special processing. we use annotations to minimize the xml configuration. spring scans the classes to find beans and configure them internally (as we have done with the xml configuration). in order to use this approach we need to: enable component scanning in our spring configuration file and add the @component annotation to our class inversion of control dependency injection scopes life cycles $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Constructor Injection.html",
    "title": "Constructor Injection",
    "body": " index search search back constructor injection contents define dependency as component specify dependency define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired \tpublic tenniscoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Qualifier Annotation.html",
    "title": "Qualifier",
    "body": " index search search back qualifier contents define dependency as component specify dependency qualifier in constructor in order to specify which specific implementation of an interface we want to use, when this interface is implemented by several beans, then we use the qualifier annotation. the qualifier annotation can be used in any dependency injection implementation: constructor injection (has different syntax) setter injection method injection field injection define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \t \t// we tell spring to search for beans (classes with @component annotation) \t// that implement the fortuneservice interface whose name is \"happyfortuneservice\" \t// (note this is the default name of the component if you set one explicitly you \t// will have to specify that one in the qualifier annotation) \t@autowired \t@qualifier(\"happyfortuneservice\") \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } qualifier in constructor package com.springdemo; import org.springframework.beans.factory.annotation.autowired; import org.springframework.beans.factory.annotation.qualifier; import org.springframework.stereotype.component; @component public class tenniscoach implements coach { private fortuneservice fortuneservice; // define a default constructor public tenniscoach() { system.out.println(\">> tenniscoach: inside default constructor\"); } @autowired public tenniscoach(@qualifier(\"happyfortuneservice\") fortuneservice thefortuneservice) { system.out.println(\">> tenniscoach: inside constructor using @autowired and @qualifier\"); fortuneservice = thefortuneservice; } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Scopes.html",
    "title": "Scopes",
    "body": " index search search back scopes to explicitly specify scopes with java annotations you do as follows: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.context.annotation.scope; @component @scope(\"singleton\") public class tenniscoach implements coach { ... or package com.springdemo; import org.springframework.stereotype.component; import org.springframework.context.annotation.scope; @component @scope(\"prototype\") public class tenniscoach implements coach { ... refer to more information about scopes are in bean scopes: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control contents enable component scanning add component annotation to classes main method let's see how to make us of inversion of control with annotations: enable component scanning we remove all of the beans we defined, and enable component scanning: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- add entry to enable component scanning --> \t<context:component-scan base-package=\"com.springdemo\" /> </beans> now spring will scan recursively all of the files in this package. add @component annotation to classes we add the @component annotation to our classes (note we do not add it to the interfaces like coach). package com.springdemo; import org.springframework.stereotype.component; @component // we can also set the explicit name like // @component(\"mytenniscoach\") public class tenniscoach implements coach { \t \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} } note that we can name the component explicitly or by default. main method in our application we do not really need to change anything. we create our bean the same way we did before. the only thing to note is that if we set the name of the component explicitly, then when we instantiate the bean, we should refer to it by said name. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\t// if we set the name explicitly \t\tcoach thecoach = context.getbean(\"mytenniscoach\", coach.class); // else \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Life Cycles.html",
    "title": "Life Cycle",
    "body": " index search search back life cycle contents notes to define methods to add when the beans is constructed or destroyed we use the postconstruct and predestroy annotation. package com.springdemo; public class trackcoach implements coach { \t \tprivate fortuneservice fortuneservice; \t \tpublic trackcoach() { \t\t \t} \t \tpublic trackcoach(fortuneservice fortuneservice) { \t\tthis.fortuneservice = fortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} \t \t@override \tpublic string getdailyfortune() { \t\treturn \"just do it: \" + fortuneservice.getfortune(); \t} \t \t// run when the bean is done creating \t@postconstruct \tpublic void domystartupstuff() { \t\tsystem.out.println(\"trackcoach: inside method domystartupstuff\"); \t} \t \t// run before the bean is destroyed \t@predestroy \tpublic void domycleanupstuffyoyo() { \t\tsystem.out.println(\"trackcoach: inside method domycleanupstuffyoyo\");\t\t \t} } refer to more information about scopes are in bean life cycle: notes access modifier: the method can have any access modifier (public, protected, private) return type: the method can have any return type. however, \"void' is most commonly used. if you give a return type just note that you will not be able to capture the return value. as a result, \"void\" is commonly used. method name: the method can have any method name. arguments: the method can not accept any arguments. the method should be no-arg. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Dependency Injection.html",
    "title": "Dependency Injection",
    "body": " index search search back dependency injection contents which dependency to use we will introduce dependency injection with annotation using autowiring: spring looks for a class that matches the attribute type (call or interface) (i.e. fortuneservice) spring will inject it automatically if there are multiple implementations: tell spring which specific bean to use with the qualifier annotation constructor injection setter injection method injection field injection inject using properties file qualifier annotation which dependency to use choose a style and stay consistent in your project. you get the same functionality regardless of the type of dependency injection you use. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Setter Injection.html",
    "title": "Setter Injection",
    "body": " index search search back setter injection contents define dependency as component specify dependency define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency we now create a setter method in our class for the injection: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired public setfortuneservice(fortuneservice fortuneservice){ this.fortuneservice = fortuneservice; } \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Annotations/Inject using Properties File.html",
    "title": "Inject properties file using Java annotations",
    "body": " index search search back inject properties file using java annotations contents create a properties file load the properties inject values this solution will show you how inject values from a properties file using annotations. the values will no longer be hard coded in the java code. create a properties file we create new text file: src/sport.properties foo.email=myeasycoach@luv2code.com foo.team=silly java coders load the properties we load the properties in the configuration xml file. for that we add the line: <context:property-placeholder location=\"classpath:sport.properties\"/> inject values lastly we inject the properties values into our bean like so: @value(\"${foo.email}\") private string email; @value(\"${foo.team}\") private string team; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Hibernate/Database Operations.html",
    "title": "Database Operations",
    "body": " index search search back database operations contents save java object read java object query java object update java objects delete java objects save java object to save a java object: public ... { try {\t\t\t \t// create a student object \tstudent tempstudent = new student(\"paul\", \"doe\", \"paul@luv2code.com\"); \t \t// start a transaction \tsession.begintransaction(); \t \t// save the student object \tsession.save(tempstudent); \t \t// commit transaction \tsession.gettransaction().commit(); } finally { \tfactory.close(); } } read java object public ... { try {\t\t\t // from the student created and saved previously // find out the student's id: primary key // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); // retrieve student based on the id: primary key system.out.println(\"\\ngetting student with id: \" + tempstudent.getid()); // get from the db by the primary key of the student student mystudent = session.get(student.class, tempstudent.getid()); // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } query java object hibernate has a query language for retrieving objects: hql which is similar to sql. public class querystudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t // start a transaction session.begintransaction(); // note we use the java object name for the table name // and the name of the attribute in the class for the name // of the column (firstname istd of first_name) // query students: lastname='doe' or firstname='daffy' thestudents = session.createquery(\"from student s where\" + \" s.lastname='doe' or s.firstname='daffy'\").getresultlist(); // query students where email like '%gmail.com' thestudents = session.createquery(\"from student s where\" \t\t+ \" s.email like '%gmail.com'\").getresultlist(); // commit transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } update java objects public class updatestudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t\t\t\t\t\t // update one student int studentid = 1; // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); student mystudent = session.get(student.class, studentid); // update name of student mystudent.setfirstname(\"scooby\"); // commit the transaction session.gettransaction().commit(); // update several students session = factory.getcurrentsession(); session.begintransaction(); // update email for all students system.out.println(\"update email for all students\"); session.createquery(\"update student set email='foo@gmail.com'\") \t.executeupdate(); \t\t\t // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } } delete java objects public class deletestudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t\t\t\t\t\t int studentid = 1; \t\t\t // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); // retrieve student based on the id: primary key student mystudent = session.get(student.class, studentid); // delete the student session.delete(mystudent); // delete student id=2 session.createquery(\"delete from student where id=2\").executeupdate(); // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Hibernate/Eager vs Lazy Loading.html",
    "title": "Eager vs Lazy Loading",
    "body": " index search search back eager vs lazy loading contents default fetch types specify fetch type on entity avoid closed session exception eager: fetches all data all at once (with dependencies of the entity) lazy: fetches required data only lazy loading is usually preferred, that is only load data when absolutely needed. the flow of lazy loading is: load the main entity first load dependent entities on demand note than when using lazy loading you need an open hibernate session, else if you close the session and try to retrieve the data hibernate will throw an exception. default fetch types mapping defaul fetch type @onetoone fetchtype.eager @onetomany fetchtype.lazy @manytoone fetchtype.eager @manytomany fetchtype.lazy specify fetch type on entity we can specify the fetching type on the entity as follows: @entity @table(name=\"instructor\") public class instructor { \t@id \t@generatedvalue(strategy=generationtype.identity) \t@column(name=\"id\") \tprivate int id; \t \t@onetoone(cascade=cascadetype.all) \t@joincolumn(name=\"instructor_detail_id\") \tprivate instructordetail instructordetail; \t // specify fetch type (only load the courses on demand, their retrieval // is delayed) \t@onetomany(fetch=fetchtype.lazy, \t\t\t mappedby=\"instructor\", \t\t\t cascade= {cascadetype.persist, cascadetype.merge, \t\t\t\t\t\t cascadetype.detach, cascadetype.refresh}) \tprivate list<course> courses; ... avoid closed session exception to avoid the error we use the join fetch (we do override lazy loading with eager loading) of hql: public class fetchjoindemo { public static void main(string[] args) { // create session factory sessionfactory factory = ... // create session session session = factory.getcurrentsession(); try {\t\t\t // start a transaction session.begintransaction(); // hibernate query with hql to avoid exception of lazy loading when closing session // get the instructor from db int theid = 1; query<instructor> query = \t\tsession.createquery(\"select i from instructor i \" \t\t\t\t\t\t+ \"join fetch i.courses \" \t\t\t\t\t\t+ \"where i.id=:theinstructorid\", \t\t\t\tinstructor.class); // set parameter on query query.setparameter(\"theinstructorid\", theid); // execute query and get instructor instructor tempinstructor = query.getsingleresult(); system.out.println(\"luv2code: instructor: \" + tempinstructor);\t // commit transaction session.gettransaction().commit(); // close the session session.close(); system.out.println(\"\\nluv2code: the session is now closed!\\n\"); // get courses for the instructor system.out.println(\"luv2code: courses: \" + tempinstructor.getcourses()); system.out.println(\"luv2code: done!\"); } finally { // add clean up code session.close(); factory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Configuration/Load Properties from File.html",
    "title": "Load Properties from File",
    "body": " index search search back load properties from file contents create the file load the file inject values in order to inject values read from a properties file we do the following: create the file first, we create the file sport.properties foo.email=myeasycoach@luv2code.com foo.team=awesome java coders load the file now, we load the file from our configuration class: package com.springdemo; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.propertysource; import org.springframework.context.support.propertysourcesplaceholderconfigurer; @configuration @propertysource(\"classpath:sport.properties\") public class sportconfig { \t \t// define bean for our sad fortune service \t@bean \tpublic fortuneservice sadfortuneservice() { \t\treturn new sadfortuneservice(); \t} \t \t// define bean for our swim coach and inject dependency \t@bean \tpublic coach swimcoach() { \t\tswimcoach myswimcoach = new swimcoach(sadfortuneservice()); \t\t \t\treturn myswimcoach; \t} \t } inject values we inject the values at field level in our bean: package com.springdemo; import org.springframework.beans.factory.annotation.value; public class swimcoach implements coach { \tprivate fortuneservice fortuneservice; \t@value(\"${foo.email}\") \tprivate string email; \t \t@value(\"${foo.team}\") \tprivate string team; ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Configuration/index.html",
    "title": "Spring Configuration with Java",
    "body": " index search search back spring configuration with java we are now going to use java to configure our application instead of using xml, to do that we follow the next steps: create a java class and annotate as @configuration add component scanning support with @componentscan (optional), which is xml we did as: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" ....> \t<!-- add entry to enable component scanning --> \t<context:component-scan base-package=\"com.springdemo\" /> </beans> in the main app read the spring java configuration class configuration with java inversion of control load properties from file dependency injection $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Configuration/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control contents create the bean define the bean in the configuration class main method to define a bean, we now use our configuration class: create the bean package com.springdemo; // note there are no special annotations public class swimcoach implements coach { \tprivate fortuneservice fortuneservice; \tpublic swimcoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"swim 1000 meters as a warm up.\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } we also create the sadfortuneservice bean: package com.springdemo; import org.springframework.stereotype.component; @component public class sadfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is a sad day :(\"; \t} } define the bean in the configuration class package com.springdemo; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.configuration; @configuration public class sportconfig { \t \t// define bean for our sad fortune service \t@bean \tpublic fortuneservice sadfortuneservice() { \t\treturn new sadfortuneservice(); \t} \t \t// define bean for our swim coach and inject dependency // without springs dependency injection \t@bean \tpublic coach swimcoach() { \t\tswimcoach myswimcoach = new swimcoach(sadfortuneservice()); \t\t \t\treturn myswimcoach; \t} \t } the @bean annotation tells spring that we are creating a bean component manually. we didn't specify a scope so the default scope is singleton. public coach swimcoach(){ specifies that the bean will bean id of \"swimcoach\". the @bean annotation will intercept any requests for \"swimcoach\" bean. since we didn't specify a scope, the bean scope is singleton. so now in our main method: main method package com.luv2code.springdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; public class javaconfigdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(sportconfig.class); \t\t \t\t// get the bean from spring container by its id \t\tcoach thecoach = context.getbean(\"swimcoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// call method to get the daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Java Configuration/Configuration With Java.html",
    "title": "Configuration With Java",
    "body": " index search search back configuration with java contents create configuration class load the configuration class create configuration class package com.springdemo; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; // 1. define configuration class @configuration // 2. add component scanning support @componentscan(\"com.springdemo\") public class sportconfig { \t } load the configuration class package com.springdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; public class javaconfigdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(sportconfig.class); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// call method to get the daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/Spring/Spring Framework.html",
    "title": "Spring Framework",
    "body": " index search search back spring framework contents spring projects components: core container beans (define entities) core (management of beans) spel: spring expression language (annotations) context (store entities) aop (aspect oriented programming): allows you to create application wide services like messaging, logging, security, etc. and add this functionality to your objects in a declarative fashion. data access layer: establishes the connection with the database jdbc helper classes orm: provides hook to hibernate transactions oxm jms (java message service) for async messages web layer: all web related classes, holds all of the spring mvc framework servlet websocket web portlet test layer: supports tdd: unit integration mock spring projects spring modules built on top of the core spring framework: spring boot spring cloud spring batch etc spring projects $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Event Basics.html",
    "title": "Event Basics",
    "body": " index search search back event basics list of all possible events to define an event we have to specify: attribute: like onclick, onmousehover, etc. eventhandler: the function to apply. this can be specified as a reference or as an in-line function. next, we present an example: import react from 'react' const book = ({ title, author }) => { const clickhandler = () => {alert('hello!!')} return ( <article classname='book'> <!-- here we have the eventhandler as an in-line function --> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <!-- here we have the eventhandler as a reference --> <button type=\"button\" onclick={clickhandler}>this is a button</button> </article> ); }; export default book to pass an argument to the eventhandler we have to use a lambda function, else when we load the application will invoke the function clickhandler(author) import react from 'react' const book = ({ title, author }) => { const clickhandler = (author) => {alert(author)} return ( <article classname='book'> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <!-- wrap function with an in-line function --> <button type=\"button\" onclick={() => clickhandler(author)}>this is a button</button> </article> ); }; export default book we can also access the event object from within the function, like: import react from 'react' const book = ({ title, author }) => { // you can always access the event object from an eventhandler const clickhandler = (author, e) => {console.log(e)} return ( <article classname='book'> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <button type=\"button\" onclick={() => clickhandler(author)}>this is a button</button> </article> ); }; export default book $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Controlled Inputs.html",
    "title": "Controlled Inputs",
    "body": " index search search back controlled inputs contents multiple inputs let's see how to handle inputs in a form using react: import react, { usestate } from 'react'; const controlledinputs = () => { const [firstname, setfirstname] = usestate(''); const [people, setpeople] = usestate([]); const handlesubmit = (e) => { // avoid the default behaviour in submit which re-renders the page e.preventdefault(); // our own logic for the submit action if (firstname) { // create new person object const person = { id: new date().gettime().tostring(), firstname }; // update our state (remember you need to spred the people state variable we have before, else the you would // override the people state variable and it would be assigned to only the person object) setpeople((people) => { return [...people, person]; }); // set to empty, so the value of the input is the empty string setfirstname(''); } else { // no values to create new person console.log('empty values'); } }; return ( <> <article> {/*event handler for the submit event*/} <form onsubmit={handlesubmit}> <div > <label htmlfor='firstname'>name : </label> <input type='text' id='firstname' name='firstname' {/*set the value of the input, it updates every time we change the input*/} value={firstname} {/*event handler for the change event: use a lambda function to pass the event e and get the value in the input*/} onchange={(e) => setfirstname(e.target.value)} /> </div> <button type='submit'>add person</button> </form> {/*show each person in the people array */} {people.map((person, index) => { const { id, firstname } = person; return ( <div classname='item' key={id}> <h4>{firstname}</h4> </div> ); })} </article> </> ); }; export default controlledinputs; multiple inputs how can we define an event handler for the onchange event that is generic, instead of defining one for each input? to showcase this scenario, we will use the same code as before, but with two new inputs. all of the inputs have the same onchange event handler. import react, { usestate } from 'react'; const controlledinputs = () => { // create a new state variable person, that holds the properties of the person we are currently creating const [person, setperson] = usestate({ firstname: '', email: '', age: '' }); // array of people we have already created const [people, setpeople] = usestate([]); // generic event handler const handlechange = (e) => { // obtain the name of the input/state variable const name = e.target.name; // obtain the new value for the input const value = e.target.value; // update the value of the property for the current person setperson({ ...person, [name]: value }); }; const handlesubmit = (e) => { e.preventdefault(); if (person.firstname && person.email && person.age) { const newperson = { ...person, id: new date().gettime().tostring() }; setpeople([...people, newperson]); setperson({ firstname: '', email: '', age: '' }); } }; return ( <> <article classname='form'> <form> <div classname='form-control'> <label htmlfor='firstname'>name : </label> <input type='text' id='firstname' name='firstname' // access the firstname of the person object value={person.firstname} // generic event handler onchange={handlechange} /> </div> <div classname='form-control'> <label htmlfor='email'>email : </label> <input type='email' id='email' name='email' // access the email of the person object value={person.email} // generic event handler onchange={handlechange} /> </div> <div classname='form-control'> <label htmlfor='age'>age : </label> <input type='number' id='age' name='age' // access the age of the person object value={person.age} // generic event handler onchange={handlechange} /> </div> <button type='submit' classname='btn' onclick={handlesubmit}> add person </button> </form> </article> <article> {people.map((person) => { const { id, firstname, email, age } = person; return ( <div key={id} classname='item'> <h4>{firstname}</h4> <p>{email}</p> <p>{age}</p> </div> ); })} </article> </> ); }; export default controlledinputs; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Get State.html",
    "title": "Get State",
    "body": " index search search back get state in order to access the state saved in our state, we do the following: import react from \"react\"; import { useselector } from \"react-redux\"; function profile() { // use the useselector hook const user = useselector((state) => state.user.value); return ( <div style={{ color: themecolor }}> <h1> profile page</h1> <!--obtain the user state--> <p> name: {user.name} </p> <p> age: {user.age}</p> <p> email: {user.email}</p> </div> ); } export default profile; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Prop Drilling.html",
    "title": "Prop Drilling",
    "body": " index search search back prop drilling prop drilling refers to the scenario where we have to pass props to anidated components recursively. next up, we show and example import react, { usestate } from 'react'; // data import { data } from '../../../data'; // outer component const propdrilling = () => { // state passed as a prop const [people, setpeople] = usestate(data); // event handler passed as a prop const removeperson = (id) => { setpeople((people) => { return people.filter((person) => person.id !== id); }); }; return ( <section> <h3>prop drilling</h3> {/* pass props to the list elements */} <list people={people} removeperson={removeperson} /> </section> ); }; // middle component const list = ({ people, removeperson }) => { return ( <> {people.map((person) => { {/* pass props to the singleperson elements */} return ( <singleperson key={person.id} {...person} removeperson={removeperson} /> ); })} </> ); }; // inner component const singleperson = ({ id, name, removeperson }) => { return ( <div classname='item'> <h4>{name}</h4> <button onclick={() => removeperson(id)}>remove</button> </div> ); }; export default propdrilling; in these cases we can use the context api $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Custom Hooks.html",
    "title": "Custom Hooks",
    "body": " index search search back custom hooks customs hooks allow us to avoid duplicating code that uses hooks and essentially in different places of your code. for example, the fetching function is very common, so we create a usefetch hook. when you define a custom hook, that is, if you define a function outside a component that uses hooks, you will have to name it use<functionname>, else you will get an error. import react, { usestate, useeffect } from 'react' // import custom hook import { usefetch } from './2-usefetch' const url = 'https://course-api.com/javascript-store-products' const example = () => { // values returned by usefetch const { loading, products } = usefetch(url) return ( <div> <h2>{loading ? 'loading...' : 'data'}</h2> </div> ) } export default example import { usestate, useeffect, usecallback } from 'react'; export const usefetch = (url) => { // state within the hook const [loading, setloading] = usestate(true); const [products, setproducts] = usestate([]); // functionality of the hook const getproducts = usecallback(async () => { const response = await fetch(url); const products = await response.json(); setproducts(products); setloading(false); }, [url]); // run whenever the url or the getproducts function changes useeffect(() => { getproducts(); }, [url, getproducts]); // values returned by the custom hook return { loading, products }; }; note we are using the hook usecallback (refer to performance optimization), we do this because we are specifying getproducts as a dependency for useeffect. however getproducts is created every time the state changes. so when we call useeffect, we change the state, and therefore create the function getproducts, which triggers useeffect, thus the state changes, and we create getproducts, and so on and so forth. to avoid this, we use usecallback, which will create the function whenever any of the dependencies in the list change. so this means, now getproducts is only created when the url changes. this allows us to avoid the infinite loop we ran into before. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Performance Optimization.html",
    "title": "Performance Optimization",
    "body": " index search search back performance optimization contents react memo usecallback usememo even though react is fast by default (you do not need to use it), we can use different optimization techniques (mind, they do add their own cost): react.memo react.memo stores a component, and only re-renders if the props of the component change (it memoizes the component). in the next example, that means that we only re-render biglist if products change, thus, we do not re-render any singleproduct component unless products change. import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' const index = () => { const { products } = usefetch(url) const [count, setcount] = usestate(0) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <biglist products={products} /> </> ) } // each time a prop or the state changes, the component re-renders, so all // the elements of the list are processed again. // however if we use react.memo we only re-render the component if products change const biglist = react.memo(({ products }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> </article> ) } export default index usecallback what happens if we pass a function to biglist, well if the state changes (whichever variable of the state) then the function is created again, and so the function is different. which means the props of biglist list changes, and causes react.memo to re-render the entire component. that is why we use usecallback. usecallback allows us to define when to create a function, by specifying the dependencies like we did with useeffect: if the dependency is []: then only create in the first render if there are variables in the []: create whenever those variables change if there is nothing: create always. refer to customs hooks for an use case of usecallback inside the custom hook usefetch. import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' const index = () => { const { products } = usefetch(url); const [count, setcount] = usestate(0); const [cart, setcart] = usestate(0); // we only create this function when we update the cart value // that is we memoize the function const addtocart = usecallback(() => { setcart(cart + 1) }, [cart]) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <biglist products={products} addtocart={addtocart}/> </> ) } // each time a prop or the state changes, the component re-renders. because now // addtocart is define with usecallback, the re-render is not triggered const biglist = react.memo(({ products, addtocart }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} addtocart={addtocart} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields, addtocart }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> <button onclick={addtocart}>add to cart</button> </article> ) } export default index; usememo note that this hook deals with values (which is the traditional functionality of the idea of memoizing), whilst react.memo look for changes in the props. in the next example we create a function that returns a value, and we memoize the function, so it only computes the value whenever the products change (the argument of the function), else it returns the value stored before: import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' // define the function we are going to memoize const calculatemostexpensive = (data) => { return ( data.reduce((total, item) => { const price = item.fields.price if (price >= total) { total = price } return total }, 0) / 100 ) } const index = () => { const { products } = usefetch(url); const [count, setcount] = usestate(0); const [cart, setcart] = usestate(0); const addtocart = usecallback(() => { setcart(cart + 1) }, [cart]) // memoize the function with usememo const mostexpensive = usememo(() => calculatemostexpensive(products), [ products, ]) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <!-- show most expensive product --> <h1>most expensive : ${mostexpensive}</h1> <biglist products={products} addtocart={addtocart}/> </> ) } const biglist = react.memo(({ products, addtocart }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} addtocart={addtocart} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields, addtocart }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> <button onclick={addtocart}>add to cart</button> </article> ) } export default index; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Children in Props.html",
    "title": "Children in Props",
    "body": " index search search back children in props you can nest content inside your component. if we have the following: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' const singlebook = { title: 'book title', author: 'book author' } reactdom.render( <book {...singlebook}> <p> i am nested!</p> </book>, document.getelementbyid('root') ); you can access the nested object from your component: import react from 'react' // de-structure the children prop const book = ({ title, author, children }) => { return ( <article classname='book'> <h1>{title}</h1> <h4>{author}</h4> {children} </article> ); }; export default book $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/List of components.html",
    "title": "List of Components",
    "body": " index search search back list of components react has one restriction for list of objects, and that is: they have to have a key. so, for example: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' // data to create book object const books = [ { id: '1', title: 'book title', author: 'book author' }, { id: '2', title: 'book title', author: 'book author' }, ] const booklist = books.map((book) => { // de-structure book object return <book key={book.id} {...book} />; }) reactdom.render( <div> booklist </div>, document.getelementbyid('root') ); $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/React Router.html",
    "title": "React Router",
    "body": " index search search back react router contents links in react routing behaves differently than in traditional html applications: it does not fetch the html page from the server, it is done in the client side. there is no re-rendering even though we change the url. here we have an example: import react from 'react'; // react router import { browserrouter as router, route, switch } from 'react-router-dom'; // pages import home from './home'; import about from './about'; import people from './people'; import error from './error'; import person from './person'; // navbar import navbar from './navbar'; const reactroutersetup = () => { return ( <router> <navbar /> <!-- with the switch component only the first one that matches is displayed --> <switch> <!-- match the path exactly, else this will be rendered always along the other components --> <route exact path='/'> <!-- component to display --> <home /> </route> <!-- match the path --> <route path='/about'> <about /> </route> <!-- match the path --> <route path='/people'> <people /> </route> <!-- match the path and pass id as a parameter --> <!-- specify children property because it will be a list of components --> <route path='/person/:id' children={<person />}></route> <!-- match any path (this is only displayed when the other paths do not match if we use the switch component)--> <route path='*'> <error /> </route> </switch> </router> ); }; export default reactroutersetup; links how do we navigate through our application, well by using links. so, for example, in the navbar: import react from 'react'; import { link } from 'react-router-dom'; const navbar = () => { return ( <nav> <ul> <li> <!-- specify the path --> <link to='/'>home</link> </li> <li> <!-- specify the path --> <link to='/about'>about</link> </li> <li> <!-- specify the path --> <link to='/people'>people</link> </li> </ul> </nav> ); }; export default navbar; to pass a parameter to the link we can do the following: import react, { usestate } from 'react'; import { data } from '../../../data'; import { link } from 'react-router-dom'; const people = () => { // list of people const [people, setpeople] = usestate(data); return ( <div> <h1>people page</h1> {people.map((person) => { return ( <div key={person.id} classname='item'> <h4>{person.name}</h4> <!-- specify the path and pass the id of the current person as a parameter --> <link to={`/person/${person.id}`}>learn more</link> </div> ); })} </div> ); }; export default people; now in the person component, we can fetch the parameter: import react, { usestate, useeffect } from 'react'; import { data } from '../../../data'; import { link, useparams } from 'react-router-dom'; const person = () => { // state const [name, setname] = usestate('default name'); // useparams hook to fetch the parameter // the name of the parameter (id), is specified in the \"route\" component // in our case the path to person was: /person/:id const { id } = useparams(); useeffect(() => { const newperson = data.find((person) => person.id === parseint(id)); setname(newperson.name); }, []); return ( <div> <h1>{name}</h1> <!-- go to the previous page of the list of people --> <link to='/people' classname='btn'> back to people </link> </div> ); }; export default person; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/JSX Rules.html",
    "title": "JSX Rules",
    "body": " index search search back jsx rules always return something always return a single element or div, section, article or react.fragment (does not create a div) enclosing the element use camelcase for html attribute use classname instead of class close every element $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/File Structure.html",
    "title": "File Structure",
    "body": " index search search back file structure node_modules: folder that contains all of the dependencies package.json: is the manifest file for the project scripts start: runs the development server build: creates a production version for the project inside a folder called build, where the optimized files resulting of the build are stored. the rest of the files created by create-react-app are mostly useless: app.js app.css app.test.js logo.svg serviceworker.js setuptests.js also all of the contents of index.js can be removed. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/CSS in JSX.html",
    "title": "CSS in JSX",
    "body": " index search search back css in jsx we can define the style inside jsx, for that we use the prop style. the first curly braces takes us back to javascript, and the second are to specify the creation of an object. also note that we do not write font-size but we use the react convention of writing fontsize const author = () => ( <h4 style={{fontsize: '1px'}}> test </h4> ); this level has higher preference (overrides) than the css imported from a css file. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Conditional Rendering.html",
    "title": "Conditional Rendering",
    "body": " index search search back conditional rendering contents short circuit evlauation ternary operators in the following example we show how we can have react display different elements conditionally: import react, { usestate, useeffect } from 'react'; const url = 'https://api.github.com/users/quincylarson'; const multiplereturns = () => { const [isloading, setisloading] = usestate(true); const [iserror, setiserror] = usestate(false); const [user, setuser] = usestate('default user'); // fetch data useeffect(() => { fetch(url) .then((resp) => { if (resp.status >= 200 && resp.status <= 299) { return resp.json(); } else { // update the control state variables setisloading(false); setiserror(true); throw new error(resp.statustext); } }) .then((user) => { const { login } = user; setuser(login); // update the control state variables setisloading(false); }) .catch((error) => console.log(error)); }, []); // different display depending on the state of the get if (isloading) { return ( <div> <h1>loading...</h1> </div> ); } if (iserror) { return ( <div> <h1>error....</h1> </div> ); } return ( <div> <h1>{user}</h1> </div> ); }; export default multiplereturns; short circuit evlauation now, let's see an example of short circuit evaluation in action: import react, { usestate } from 'react'; const shortcircuit = () => { const [text, settext] = usestate(''); const [iserror, setiserror] = usestate(false); // if text is falsy, then return 'hello world' // else return text // const firstvalue = text || 'hello world'; // if text is true, then return 'hello world' // else return text // const secondvalue = text && 'hello world'; return ( <> {/*if text is false, return h1 with 'john doe value'*/} <h1>{text || 'john doe'}</h1> {/*if text is true, return h1 with 'john doe value'*/} {text && <h1>'john doe'</h1>} </> ); }; export default shortcircuit; ternary operators we can also use ternary operators to render conditionally in react. import react, { usestate } from 'react'; const shortcircuit = () => { const [iserror, setiserror] = usestate(false); return ( <> <button classname='btn' onclick={() => setiserror(!iserror)}> toggle error </button> {/*check the value of iserror, if is error is true, return the first value after the ? else return the second value*/} {iserror ? ( <p>there is an error...</p> ) : ( <div> <h2>there is no error</h2> </div> )} </> ); }; export default shortcircuit; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Context API.html",
    "title": "Context API",
    "body": " index search search back context api context api and usecontext allows us to resolve the issue of the prop drilling. the context has two components: the provider: works as a distributer the consumer we use them as follows: import react, { usestate, usecontext } from 'react'; import { data } from '../../../data'; // create context object const personcontext = react.createcontext(); const contextapi = () => { // state saved in the context const [people, setpeople] = usestate(data); // event handler saved in the context const removeperson = (id) => { setpeople((people) => { return people.filter((person) => person.id !== id); }); }; return ( {/*wrap the components in the context provider, so all the nested components have access to the variables defined in the context object*/} <personcontext.provider value={{ removeperson, people }}> <h3>context api / usecontext</h3> <list /> </personcontext.provider> ); }; const list = () => { // obtain data from the context with the usecontext hook const maindata = usecontext(personcontext); return ( <> {maindata.people.map((person) => { return <singleperson key={person.id} {...person} />; })} </> ); }; const singleperson = ({ id, name }) => { // obtain data from the context with the usecontext hook const { removeperson } = usecontext(personcontext); return ( <div classname='item'> <h4>{name}</h4> <button onclick={() => removeperson(id)}>remove</button> </div> ); }; export default contextapi; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Start in indexjs.html",
    "title": "Start in index.js",
    "body": " index search search back start in index.js keep in mind, index.js is the entry point: first of all refer to file structure, and then basically remove everything from index.js, and replace it for: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; we use the reactdom module to make use of the react dom api, which let's us render components, etc. next we call reactdom.render() to output our html: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; function component() { return ( <h4> hi! </h4> ); } reactdom.render( <component/>, document.getelementbyid(\"root\") ) note the function must start with a capital letter the tag that encloses the component must be closed, so either: <component/> or <component></ component> we use document.getelementbyid(\"root\"), this tells react where to place the component inside the html $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/ComputerScience/WebDev/React/Properties Hooks.html",
    "title": "Properties of Hooks",
    "body": " index search search back properties of hooks all the hooks have the following properties: they start with the word use the component where they are created must be named in uppercase they cannot be invoked inside a function/component body. you cannot call hooks conditionally $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/Rices/arch_install.html",
    "title": "Arch Linux Installation",
    "body": " index search search back arch linux installation contents set up install set up set the keyboard layout: $ loadkeys es augment the size of the iso image: $ mount -o remount,size=1g /run/archiso/cowspace download git: $ pacman -syy && pacman -s git configure git to store the credentials: $ git config --global credential.helper store clone the repository: git clone https://github.com/albamr09/archinstaller.git now, you are good to go to start the installation process. install place yourself inside the root of the repository: cd archinstaller check out the configuration file, in case some values do not make sense to you: cat install_scripts/config.sh if you are satisfied with the configuration, simply execute: cd install_scripts && ./install.sh this will cause the installation to begin. it is mostly automatic, but sometimes you will have to enter a password here and there. so do not just let it execute by itself, because there are timeouts that will cause the installation to hault with an error. once this finished, reboot your computer. when the computer is up and running again, you will be met with a very minimal login interface. log in with you user, and execute the following: cd /install_scripts && ./post_install this script prompts you to connect to a wifi access point. it also sets up some needed services (like lightdm!) and removes all the installation files used from you system so it is nice an clean. well, now your arch linux is ready to go! $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/VimWiki/index.html",
    "title": "Vim Wiki",
    "body": " index search search back vim wiki key bindings convert current file to html: ,wh see html file in browser: ,whh more info on vimwiki latex inline \\(a = 1\\) equation \\begin{align} a \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/index.html",
    "title": "Other",
    "body": " index search search back other rices arch linux installation misc vimwiki macos vm $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/MacOS/index.html",
    "title": "Mac OS VM",
    "body": " index search search back mac os vm source initial setup ubuntu/debian: $ sudo apt-get install qemu uml-utilities virt-manager git \\ wget libguestfs-tools p7zip-full make dmg2img -y fedora: $ sudo dnf install @virutalization start libvirt service: $ sudo systemctl start libvirt $ sudo systemctl enable libvirt add user to the kvm and libvirt groups (might be needed). $ sudo usermod -ag kvm $(whoami) $ sudo usermod -ag libvirt $(whoami) $ sudo usermod -ag input $(whoami) note: re-login after executing this command. now edit /etc/libvirt/qemu.conf and set user and group to your user. clone this repository on your qemu system. files from this repository are used in the following steps. $ cd ~ $ git clone --depth 1 --recursive https://github.com/kholia/osx-kvm.git $ cd osx-kvm note: with this you are installing your vm on $home. fetch macos installer. $ ./fetch-macos-v2.py on this step select monterey. convert the downloaded basesystem.dmg file into the basesystem.img file. $ dmg2img -i basesystem.dmg basesystem.img create a virtual hdd image where macos will be installed. if you change the name of the disk image from mac_hdd_ng.img to something else, the boot scripts will need to be updated to point to the new image name. $ qemu-img create -f qcow2 mac_hdd_ng.img 128g be aware that the machine can easily reach that amount of memory. installation cli method (primary). just run the opencore-boot.sh script to start the installation process. $ ./opencore-boot.sh before installing go to disk utility inside the machine and erase the partition we are going to use for the virtual machine (the one that is roughly 128gb). for that click on erase and select mac os extended (journaled). once the erasing procedure is done, you can start the installation normally. edit macos-libvirt-catalina.xml file and change the various file paths (search for changeme strings in that file). the following command should do the trick usually. $ sed \"s/changeme/$user/g\" macos-libvirt-catalina.xml > macos.xml $ virt-xml-validate macos.xml create a vm on virt-manager by running the following command. $ virsh --connect qemu:///system define macos.xml launch virt-manager and start the macos virtual machine. post-installation open virt-manager, select macos and edit cpus and memory so the virtual machine does not lag incredibly. permissions bug (might only happen in fedora) if you get an error when starting the machine related to permissions, they are solved with: $ sudo setenforce permissive if they are related with selinux. if that is the case, refer to. undo the previous command with: $ sudo setenforce enforcing on your $home directory try to fix with: $ sudo chcon -r -u system_u -r object_r -t svirt_image_t osx-kvm/ screen resolution execute the virtual machine and press esc inmmediately. select device management option and change ovmf to 1920x1080p resolution. enter the virtual machine, once it has been booted open a terminal and write: $ diskutil list select the disk where the efi partition is location $ sudo diskutil mount disk1s1 $ vi /volumes/efi/efi/oc/config.plist and edit the entry under resolution to be 1920x1080@32. reboot the machine. once rebooted go to system preferences > displays and check show all resolutions and select 1920x1080. connect to physical iphone open virtual manager, and enter the configuration of the machine. click on add hardware and select usb host, now edit the xml entry just created and substitue the content with: <hostdev mode=\"subsystem\" type=\"usb\" managed=\"yes\"> <source> <vendor id=\"0x05ac\"/> <product id=\"0x12a8\"/> </source> <address type=\"usb\" bus=\"0\" port=\"1\"/> </hostdev> where vendor id and product id is obtained through lsusb on the host machine: $ lsusb ... bus 001 device 004: id 05ac:12a8 apple, inc. iphone 5/5c/5s/6/se ... keyboard is locked if the keyboard seems to be captured when the machine starts, remove the entry on the machine hardware configuration that has this content or similar (this is my keyboard's smart card, may not apply to your case.) <hostdev mode=\"subsystem\" type=\"usb\" managed=\"yes\"> <source> <vendor id=\"0x04f2\"/> <product id=\"0x1469\"/> </source> <address type=\"usb\" bus=\"0\" port=\"2\"/> </hostdev> optimization source only the following are actually important: add more video memory open virtual manager, select macos machine and open the configuration. locate vga and change the xml entry so that vgamem has the value 65536. skip the gui login screen (at your own risk!) $ defaults write com.apple.loginwindow autologinuser -bool true disable spotlight indexing on macos to heavily speed up virtual instances. $ sudo mdutil -i off -a enable performance mode # check if enabled (should contain `serverperfmode=1`) $ nvram boot-args # turn on $ sudo nvram boot-args=\"serverperfmode=1 $(nvram boot-args 2>/dev/null | cut -f 2-)\" disable heavy login screen wallpaper $ sudo defaults write /library/preferences/com.apple.loginwindow desktoppicture \"\" reduce motion & transparency (could be faulty) defaults write com.apple.accessibility differentiatewithoutcolor -int 1 defaults write com.apple.accessibility reducemotionenabled -int 1 defaults write com.apple.universalaccess reducemotion -int 1 defaults write com.apple.universalaccess reducetransparency -int 1 defaults write com.apple.accessibility reducemotionenabled -int 1 to undo any of this changes refer to the reference material. gpu passthrough to be continued $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_02.html",
    "title": "The Law of Cosines and Area Formulas",
    "body": " index search search back the law of cosines and area formulas contents triangle side length restriction derivation of the law of cosines how to resolve oblique triangles area formulas heron s area formula other area formula triangle side length restriction in any triangle, the sum of the lengths of any two sides must be greater than the length of the remaining side. derivation of the law of cosines let \\(abc\\) be any oblique triangle. let \\(b\\) be a vertex at the origin and the side \\(bc\\) be along the positive x-axis (figure 10). let \\((x, y)\\) be the coordinates of vertex \\(a\\), then: \\begin{align} \\sin b = \\frac{y}{c} \\end{align} and \\begin{align} \\cos b = \\frac{x}{c} \\end{align} such that: \\begin{align} y = c \\sin b \\end{align} and \\begin{align} x = c \\cos b \\end{align} so the coordiantes for \\(a\\) become \\((c \\cos b, c \\sin b)\\). point \\(c\\) has coordinates \\((a, 0)\\) and \\(ac\\) has length \\(b\\). so if we apply the distance formula we obtain: \\begin{align} b = \\sqrt{(c \\cos b - a)^2 + (c \\sin b - 0)^2} \\end{align} \\begin{align} b^2 = (c \\cos b - a)^2 + (c \\sin b - 0)^2 \\end{align} \\begin{align} b^2 = c^2 \\cos^2 b + a^2 - 2ac\\cos b + c^2 \\sin^2 b \\end{align} \\begin{align} b^2 = c^2 (\\cos^2 b + \\sin^2 b) + a^2 - 2ac\\cos b \\end{align} \\begin{align} b^2 = c^2 (1) + a^2 - 2ac\\cos b \\end{align} \\begin{align} b^2 = c^2 + a^2 - 2ac\\cos b \\end{align} if we place \\(a\\) or \\(c\\) at the origin we obtain: \\begin{align} a^2 = b^2 + c^2 - 2bc\\cos b \\end{align} \\begin{align} c^2 = a^2 + b^2 - 2ab\\cos b \\end{align} how to resolve oblique triangles four cases can occur in solving an oblique triangle: case 1: one side and two angles are known use the angle sum formula (\\(a + b + c = 180Âº\\))to find the remaining angle use the law of sines to find the remaining sides case 2: two sides and one angle (not in-between) are known (ambiguous case, there may be no triangle, one triangle or two triangles) use the law of sines to find an angle use the angle sum formula to find the remaining angle use the law of sines to find the remaining side if two triangles exist, repeat step 2 and 3 case 3: two sides and the included angle are known use the law of cosines to find the third side use the law of sines to find the smaller of the two remaining angle use the angle sum formula to find the remaining angle case 4: three sides are known use the law of cosines to find the largest angle use the law of sines to find either of the two remaining angles use the angle sum formula to find the remaining angle area formulas heron's area formula given a triangle with sides of length \\(a\\), \\(b\\) and \\(c\\), its semiperimeter is: \\begin{align} s = \\frac{1}{2}(a + b + c) \\end{align} and the area of the triangle is: \\begin{align} \\mathcal{a} = \\sqrt{s(s-a)(s - b)(s - c)} \\end{align} other area formula if we know the measures of two sides and the angle between them, we can find the area of the triangle. we know: \\begin{align} \\mathcal{a} = \\frac{1}{2} bh \\end{align} where \\(b\\) is the base and \\(h\\) is the height, that can be computed as follows: \\begin{align} \\sin a = \\frac{h}{c} \\leftrightarrow h = c \\sin a \\end{align} therefore: \\begin{align} \\mathcal{a} = \\frac{1}{2}bh = \\frac{1}{2}bc \\sin a \\end{align} since the labels for the vertices in triangle abc could be rearranged, other area formulas can be written: \\begin{align} \\mathcal{a} = \\frac{1}{2}ab \\sin c \\end{align} \\begin{align} \\mathcal{a} = \\frac{1}{2}ac \\sin b \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/07_03.html",
    "title": "The Conic Sections and Nonlinear Systems",
    "body": " index search search back the conic sections and nonlinear systems contents characteristics eccentricity parabola ellipse circle hyperbola nonlinear systems characteristics the conic sections in this chapter have equations that can be written in the form: \\begin{align} ax^2 + dx + cy^2 + ey + f = 0 \\end{align} where either \\(a\\) or \\(c\\) must be nonzero. the special characteristics of each conic section are summarizeed in the following table. the chart summarizes our work with conic sections: eccentricity a conic is the set of all points \\(p(x, y)\\) in a plane such that the ratio of the distance from \\(p\\) to a fixed point and the distance from \\(p\\) to a fixed line is constant. the constant ratio is called the ecccentricity of the conic, written \\(e\\). parabola if the conic is a parabola, then by definition, the distances \\(d(p, f)\\) and \\(d(p, d)\\) are equal, thus every parabola has eccentricity \\(1\\). ellipse for an ellipse, eccentricity is a measure of its \"roundness\". the constant ratio in the definition is \\(e = \\frac{c}{a}\\), where \\(c\\) is the distance from the center of the figure to a focus and \\(a\\) is the distance from the center to a vertex. by the definition of an ellipse, \\(a^2 > b^2\\) and \\(c = \\sqrt{a^2 - b^2}\\). thus, for the ellipse: \\begin{align} 0 < c < a \\end{align} divide by \\(a\\): \\begin{align} 0 < \\frac{c}{a} < 1 \\end{align} where \\(e = \\frac{c}{a}\\): \\begin{align} 0 < e < 1 \\end{align} circle in the circle the foci coincide with the center such that \\(a = b\\) and \\(c = \\sqrt{a^2 - b^2} = 0\\) and therefore \\(e = \\frac{c}{a} = 0\\). hyperbola the hyperbola also has eccentricity \\(e = \\frac{c}{a}\\). by definition \\(c = \\sqrt{a^2 + b^2} > a\\), so: \\begin{align} 0 < a < c \\end{align} divide by \\(a\\): \\begin{align} 0 < 1 < \\frac{a}{c} \\end{align} \\begin{align} 0 < 1 < e \\end{align} therefore \\(e > 1\\). such that narrow hyperbolas have \\(e\\) near \\(1\\) and wide hyperbolas have a large value of \\(e\\). nonlinear systems a nonlinear system of equations can have any number of solutions. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/09_02.html",
    "title": "Sum and Difference Identities",
    "body": " index search search back sum and difference identities contents cosine sum and difference identities sine and tangent sum and difference identities cosine sum and difference identities let \\(s\\) and \\(q\\) be the points where the terminal sides of angles \\(a\\) and \\(b\\), respectively, intersect the circle. locate point \\(r\\) on the unit circle so that angle \\(por\\) equals the difference \\(a - b\\). see figure 5. we know: \\(q\\) has coordinates \\((\\cos b, \\sin b)\\) \\(s\\) has coordinates \\((\\cos a, \\sin a)\\) \\(r\\) has coordinates \\((\\cos (a - b), \\sin (a - b))\\) angle \\(soq\\) also equals \\(a - b\\). since the central angles \\(soq\\) and \\(por\\) are equal, chords \\(pr\\) and \\(sq\\) are equal. by the distance formula: \\begin{align} \\sqrt{[\\cos (a - b) - 1]^2 + [\\sin(a - b) - 0]^2} = \\sqrt{(\\cos a - \\cos b)^2 + (\\sin a - \\sin b)^2} \\end{align} \\begin{align} [\\cos (a - b) - 1]^2 + [\\sin(a - b) - 0]^2 = (\\cos a - \\cos b)^2 + (\\sin a - \\sin b)^2 \\end{align} \\begin{align} \\cos^2 (a - b)+ 1 - 2\\cos(a - b) + \\sin^2(a - b) = \\cos^2 a + \\cos^2 b - 2\\cos a \\cos b + \\sin^2 a + \\sin^2 b - 2\\sin a \\sin b \\end{align} because \\(\\sin^2 x + \\cos^2 x = 1\\) \\begin{align} 1 + 1 - 2\\cos(a - b) = 1 + 1 - 2\\cos a \\cos b - 2\\sin a \\sin b \\end{align} \\begin{align} -2\\cos(a - b) = - 2\\cos a \\cos b - 2\\sin a \\sin b \\end{align} \\begin{align} \\cos(a - b) = \\cos a \\cos b + \\sin a \\sin b \\end{align} for \\(\\cos (a + b)\\), rewrite \\(a + b\\) as \\(a - (- b)\\), and use the identity for \\(\\cos (a - b)\\): \\begin{align} \\cos(a + b) = \\cos(a - (-b)) = \\cos a \\cos (-b) + \\sin a \\sin (-b) \\end{align} given \\(\\cos (-x) = \\cos x\\) and \\(\\sin -x = - \\sin x\\), then: \\begin{align} = \\cos a \\cos b - \\sin a \\sin b \\end{align} sine and tangent sum and difference identities use the cofunction identity \\(\\sin \\theta = \\cos (\\frac{\\pi}{2} - \\theta)\\) and replace \\(\\theta\\) with \\(a + b\\): \\begin{align} \\sin (a + b) = \\cos \\left[\\frac{\\pi}{2} - (a + b)\\right] \\end{align} \\begin{align} = \\cos \\left[(\\frac{\\pi}{2} - a) - b)\\right] \\end{align} \\begin{align} = \\cos \\left(\\frac{\\pi}{2} - a\\right)\\cos b + \\sin \\left(\\frac{\\pi}{2} - a\\right) \\sin b \\end{align} we apply the cofunction identity again for \\(\\cos \\left(\\frac{\\pi}{2} - a\\right)\\) and \\(\\sin \\left(\\frac{\\pi}{2} - a\\right)\\): \\begin{align} = \\sin a\\cos b + \\cos a \\sin b \\end{align} now we write \\(\\sin (a - b)\\) as \\(\\sin[a + (-b)]\\) and use the identity for \\(\\sin (a + b)\\): \\begin{align} \\sin (a - b) = \\sin (a + (-b)) = \\sin a\\cos (-b) + \\cos a \\sin (-b) \\end{align} given \\(\\cos (-x) = \\cos x\\) and \\(\\sin -x = - \\sin x\\), then: \\begin{align} = \\sin a\\cos b - \\cos a \\sin b \\end{align} to derive the identity for \\(\\tan (a + b)\\), proceed as follows \\begin{align} \\tan (a + b) = \\frac{\\sin (a + b)}{\\cos (a + b)} \\end{align} \\begin{align} = \\frac{\\sin a \\cos b + \\cos a \\sin b}{\\cos a \\cos b - \\sin a \\sin b} \\end{align} we multiply the numerator and denominator by \\(\\frac{1}{\\cos a \\cos b}\\): \\begin{align} = \\frac{\\frac{\\sin a \\cos b + \\cos a \\sin b}{\\cos a \\cos b}}{\\frac{\\cos a \\cos b - \\sin a \\sin b}{\\cos a \\cos b}} \\end{align} \\begin{align} = \\frac{\\frac{\\sin a \\cos b}{\\cos a \\cos b} + \\frac{\\cos a \\sin b}{\\cos a \\cos b}}{\\frac{\\cos a \\cos b}{\\cos a \\cos b} - \\frac{\\sin a \\sin b}{\\cos a \\cos b}} \\end{align} \\begin{align} = \\frac{\\frac{\\sin a}{\\cos a} + \\frac{\\sin b}{\\cos b}}{1 - \\frac{\\sin a \\sin b}{\\cos a \\cos b}} \\end{align} \\begin{align} = \\frac{\\tan a + \\tan b}{1 - \\tan a \\tan b} \\end{align} replacing \\(b\\) with \\(-b\\) and the fact that \\(\\tan (-b) = -\\tan b\\) gives the identity for the tangent of the difference of two numbers: \\begin{align} \\tan (a + b) = \\tan (a + (-b))= \\frac{\\tan a + \\tan (-b)}{1 - \\tan a \\tan (-b)} \\end{align} \\begin{align} = \\frac{\\tan a - \\tan b}{1 + \\tan a \\tan b} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/07_02.html",
    "title": "Ellipses and Hyperbolas",
    "body": " index search search back ellipses and hyperbolas contents ellipse standard forms of equations for ellipses translations of ellipses hyperbola standard forms of equations for hyperbolas translations of hyperbolas ellipse an ellipse is the set of all points in a plane, the sum of whose distances from two fixed points is constant. each fixed point is called a focus (plural, foci) of the ellipse. an ellipse has two axes of symmetry: the major axis (the longer one) and the minor axis (the shorter one). the foci are always located on the major axis. the midpoint of the major axis is the center of the ellipse, and the endpoints of the major axis are the vertices of the ellipse. a chord through a focus and perpendicular to the major axis is called a latus rectum. the graph of an ellipse is not the graph of a function. given an ellipse with center at the origin, foci \\(f(c, 0)\\) and \\(fâ²(-c, 0)\\), and vertices \\(v(a, 0)\\) and \\(vâ²(-a, 0)\\). from the previous image we know that the distance from \\(v\\) to \\(f\\) is \\(a - c\\) and the distance from \\(v\\) to \\(fâ²\\) is \\(a + c\\). the sum of these distances is \\(2a\\). since \\(v\\) is on the ellipse, all othe points must satisfy this distance, such that for any point \\(p(x, y)\\) on the ellipse: \\begin{align} d(p, f) + d(p, f') = 2a \\end{align} \\begin{align} \\sqrt{(x - c)^2 + y^2} + \\sqrt{(x + c)^2 + y^2} = 2a \\end{align} \\begin{align} \\sqrt{(x - c)^2 + y^2} = 2a - \\sqrt{(x + c)^2 + y^2} \\end{align} we now square each side: \\begin{align} (x - c)^2 + y^2 = 4a^2 - 4a\\sqrt{(x + c)^2 + y^2} + (x + c)^2 + y^2 \\end{align} \\begin{align} x^2 + c^2 - 2xc + y^2 = 4a^2 - 4a\\sqrt{(x + c)^2 + y^2} + x^2 + c^2 + 2xc + y^2 \\end{align} \\begin{align} - 4xc = 4a^2 - 4a\\sqrt{(x + c)^2 + y^2} \\end{align} \\begin{align} - xc = a^2 - a\\sqrt{(x + c)^2 + y^2} \\end{align} \\begin{align} xc + a^2 = a\\sqrt{(x + c)^2 + y^2} \\end{align} we square both sides again: \\begin{align} (xc + a^2)^2 = a^2\\left((x + c)^2 + y^2\\right) \\end{align} \\begin{align} x^2c^2 + a^4 + 2xca^2 = a^2\\left(x^2 + c^2 + 2xc + y^2\\right) \\end{align} \\begin{align} x^2c^2 + a^4 + 2xca^2 = a^2x^2 + a^2c^2 + 2xca^2 + a^2y^2 \\end{align} \\begin{align} x^2c^2 + a^4 = a^2x^2 + a^2c^2 + a^2y^2 \\end{align} \\begin{align} x^2c^2 - a^2x^2 - a^2y^2 = - a^4 + a^2c^2 \\end{align} \\begin{align} x^2(c^2 - a^2)- a^2y^2 = a^2 (-a^2 + c^2) \\end{align} we multiply both sides by \\(-1\\): \\begin{align} x^2(a^2 - c^2) + a^2y^2 = a^2 (a^2 - c^2) \\end{align} we divide both sides by \\(a^2(a^2 - c^2)\\): \\begin{align} \\frac{x^2}{a^2} + \\frac{y^2}{(a^2 - c^2)} = 1 \\end{align} which gives us the standard form equation for the ellipse with center on the origin, vertices \\((\\pm a, 0)\\) and foci \\((\\pm c, 0)\\). since \\(b(0, b)\\) is on the ellipse, then: \\begin{align} d(b, f) + d(b + f') = 2a \\end{align} \\begin{align} \\sqrt{(-c)^2 + b^2} + \\sqrt{c^2 + b^2} = 2a \\end{align} \\begin{align} 2\\sqrt{c^2 + b^2} = 2a \\end{align} we square both sides \\begin{align} 4(c^2 + b^2) = 4a^2 \\end{align} \\begin{align} c^2 + b^2 = a^2 \\end{align} \\begin{align} b^2 = a^2 - c^2 \\end{align} therefore, by sustuting on the ellipse formula we obtain: \\begin{align} \\frac{x^2}{a^2} + \\frac{y^2}{b^2} = 1 \\end{align} standard forms of equations for ellipses the ellipse with center at the origin and equation: \\begin{align} \\frac{x^2}{a^2} + \\frac{y^2}{b^2} = 1, (a > b > 0) \\end{align} has vertices \\((\\pm a, 0)\\), endpoints of the minor axis \\((0, \\pm b)\\) and foci \\((\\pm c, 0)\\) where \\(c^2 = a^2 - b^2\\) the ellipse with center at the origin and equation \\begin{align} \\frac{x^2}{b^2} + \\frac{y^2}{a^2} = 1, (a > b > 0) \\end{align} has vertices \\((0, \\pm a)\\), endpoints of the minor axis \\((\\pm b, 0)\\) and foci \\((0, \\pm c)\\) where \\(c^2 = a^2 - b^2\\) translations of ellipses an ellipse with center at \\((h, k)\\) where \\(b^2 = a^2 - c^2\\) with \\(a > b > 0\\) and \\(c > 0\\) satisfies one of the following equations: major axis: horizontal, foci \\((h \\pm c, k)\\) and vertices \\((h \\pm a, k)\\): \\begin{align} \\frac{(x - h)^2}{a^2} + \\frac{(y - k)^2}{b^2} = 1 \\end{align} major axis: vertical, foci \\((h, k \\pm c)\\) and vertices \\((h, k \\pm a)\\): \\begin{align} \\frac{(x - h)^2}{b^2} + \\frac{(y - k)^2}{a^2} = 1 \\end{align} hyperbola a hyperbola is the set of all points in a plane such that the absolute value of the difference of the distances from two fixed points is constant. the two fixed points are called the foci of the hyperbola. suppose a hyperbola has center at the origin and foci at \\(f'(- c, 0)\\) and \\(f(c, 0)\\). the midpoint of the segment \\(fâ²f\\) is the center of the hyperbola, and the points \\(v'(- a, 0)\\) and \\(v(a, 0)\\) are the vertices of the hyperbola. the line segment v'v is the transverse axis of the hyperbola. a chord through a focus and perpendicular to an extension of the transverse axis is a latus rectum. standard forms of equations for hyperbolas the hyperbola with center at the origin and equation \\begin{align} \\frac{x^2}{a^2} - \\frac{y^2}{b^2} = 1 \\end{align} has vertices \\((\\pm a, 0)\\), asymptotes \\(y = \\pm \\frac{b}{a}x\\) and foci \\((\\pm c, 0)\\) where \\(c^2 = a^2 + b^2\\). the hyperbola with center at the origin and equation \\begin{align} \\frac{y^2}{a^2} - \\frac{x^2}{b^2} = 1 \\end{align} has vertices \\((0, \\pm a)\\), asymptotes \\(y = \\pm \\frac{a}{b}x\\) and foci \\((0, \\pm c)\\) where \\(c^2 = a^2 + b^2\\). to explain the concept of asymptotes, we can start with the first equation for a hyperbola, where the foci are on the x-axis, and solve for \\(y\\): \\begin{align} \\frac{x^2}{a^2} - \\frac{y^2}{b^2} = 1 \\end{align} \\begin{align} \\frac{y^2}{b^2} = \\frac{x^2}{a^2} - 1 \\end{align} \\begin{align} \\frac{y^2}{b^2} = \\frac{x^2 - a^2}{a^2} \\end{align} \\begin{align} \\frac{y}{b} = \\pm \\frac{1}{a} \\sqrt{x^2 - a^2} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} \\sqrt{x^2 - a^2} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} \\frac{x}{x} \\sqrt{x^2 - a^2} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} x \\sqrt{\\frac{x^2 - a^2}{x^2}} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} x \\sqrt{\\frac{x^2}{x^2} - \\frac{a^2}{x^2}} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} x \\sqrt{1 - \\frac{a^2}{x^2}} \\end{align} so when \\(x \\rightarrow \\infty\\) we know that \\(\\frac{a^2}{x^2} \\rightarrow 0\\) and thus: \\begin{align} y = \\pm \\frac{b}{a} x \\sqrt{1} \\end{align} \\begin{align} y = \\pm \\frac{b}{a} x \\end{align} which defines the asymptotes of the hyperbola. the lines are the extended diagonals of the rectangle whose vertices are \\((a, b)\\), \\((- a, b)\\), \\((a, - b)\\), and \\((- a, - b)\\). this rectangle is called the fundamental rectangle of the hyperbola. if the foci are on the y-axis the hyperbola is defined as follows: \\begin{align} y = \\pm \\frac{a}{b} x \\end{align} translations of hyperbolas a hyperbola with center \\((h, k)\\), where \\(c^2 = a^2 + b^2\\) is defined in one of two ways: traverse axis is horizontal, vertices are \\((h \\pm a, k)\\), foci are \\((h \\pm c, k)\\) and asymptotes are \\(y = \\pm \\frac{b}{a}(x - h) + k\\) \\begin{align} \\frac{(x - h)^2}{a^2} - \\frac{(y - k)^2}{b^2} = 1 \\end{align} traverse axis is verticsl, vertices are \\((k, h \\pm a)\\), foci are \\((k, h \\pm c)\\) and asymptotes are \\(y = \\pm \\frac{a}{b}(x - h) + k\\) \\begin{align} \\frac{(y - k)^2}{a^2} - \\frac{(x - h)^2}{b^2} = 1 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_05.html",
    "title": "The Circular Functions",
    "body": " index search search back the circular functions contents circular functions in figure 72, we start at the point \\((1, 0)\\) and measure an arc of length \\(s\\). if \\(s > 0\\), then the arc is measured in a counterclockwise direction, and if \\(s < 0\\), then the direction is clockwise. let the endpoint of this arc be at the point \\((x, y)\\). the circle in figure 72 is a unit circle âit has center at the origin and radius \\(1\\) unit. we know that \\(s = r \\theta\\), with \\(r = 1\\) then \\(s = \\theta\\). thus the trigonometric functions of angle \\(\\theta\\) in radians found by choosing a point \\((x, y)\\) on the unit circle can be rewritten as functions of the arc length \\(s\\), they are called circular functions. circular functions for any real number \\(s\\) represented by a directed arc on the unit circle \\(x^2 + y^2 = 1\\), the following definitions hold. \\begin{align} \\sin s = \\frac{y}{r} = y \\end{align} \\begin{align} \\cos s = \\frac{x}{r} = x \\end{align} \\begin{align} \\tan s = \\frac{y}{x}, x \\neq 0 \\end{align} \\begin{align} \\csc s = \\frac{r}{y} = \\frac{1}{y}, y \\neq 0 \\end{align} \\begin{align} \\sec s = \\frac{r}{x} = \\frac{1}{x}, x \\neq 0 \\end{align} \\begin{align} \\cot s = \\frac{x}{y}, y \\neq 0 \\end{align} so circular function values of real numbers are obtained in the same manner as trigonometric function values of angles measured in radians. we can use the following figure to easily obtain exact solutions to some angles: the diagram shown in figure 82 illustrates a correspondence that relates the right triangle ratio definitions of the trigonometric functions and the unit circle interpretation. the arc \\(sr\\) is the first-quadrant portion of the unit circle, and the standard-position angle \\(poq\\) is designated \\(\\theta\\). by definition, the coordinates of \\(p\\) are \\((cos \\theta, sin \\theta)\\). the six trigonometric functions of \\(\\theta\\) can be interpreted as lengths of line segments. for \\(\\cos \\theta\\) and \\(\\sin \\theta\\), use right triangle \\(poq\\) and right-triangle ratios: \\begin{align} \\cos \\theta = \\frac{\\text{adyacent}}{\\text{hypotenuse}} = \\frac{oq}{op} = \\frac{oq}{1} = oq \\end{align} \\begin{align} \\sin \\theta = \\frac{\\text{opposite}}{\\text{hypotenuse}} = \\frac{pq}{op} = \\frac{pq}{1} = pq \\end{align} for \\(\\tan \\theta\\) and \\(\\sec \\theta\\), use right triangle \\(vor\\) and right-triangle ratios: \\begin{align} \\tan \\theta = \\frac{\\text{opposite}}{\\text{adyacent}} = \\frac{vr}{or} = \\frac{vr}{1} = vr \\end{align} \\begin{align} \\sec \\theta = \\frac{\\text{hypotenuse}}{\\text{adyacent}} = \\frac{ov}{or} = \\frac{ov}{1} = ov \\end{align} for \\(\\csc \\theta\\) and \\(\\cot \\theta\\), first note that \\(us\\) and \\(or\\) are parallel. thus angle \\(suo\\) is equal to \\(\\theta\\): \\begin{align} \\csc \\theta = \\frac{\\text{hypotenuse}}{\\text{opposite}} = \\frac{ou}{os} = \\frac{ou}{1} = ou \\end{align} \\begin{align} \\cot \\theta = \\frac{\\text{adjacent}}{\\text{opposite}} = \\frac{us}{os} = \\frac{us}{1} = us \\end{align} figure 83 illustrates the results found above. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/09_03.html",
    "title": "Further Identities",
    "body": " index search search back further identities contents double-number identities product-to-sum identities sum-to-product identities half-number identities double-number identities the double-number identities, or double-angle identities, result from the sum identities when \\(a = b\\) so that \\(a + b = a + a = 2a\\). \\begin{align} \\cos 2a = \\cos(a + a) = \\cos a \\cos a - \\sin a \\sin a \\end{align} \\begin{align} = \\cos^2 a - \\sin^2 a \\end{align} two other common forms of this identity are obtained by substitution: \\begin{align} \\cos 2a = \\cos^2 a - \\sin^2 a = (1 - \\sin^2 a) - \\sin^2 a \\end{align} \\begin{align} 1 - 2\\sin^2 a \\end{align} and \\begin{align} \\cos 2a = \\cos^2 a - \\sin^2 a = \\cos^2 a - (1 - \\cos^2 a) \\end{align} \\begin{align} = 2\\cos^2 a - 1 \\end{align} we find \\(\\sin 2a\\) with the identity for \\(\\sin (a + b)\\) \\begin{align} \\sin 2a = \\sin (a + a) = \\sin a \\cos a + \\sin a \\cos a \\end{align} \\begin{align} = 2\\sin a \\cos a \\end{align} similarly, we use the identity for \\(\\tan (a + b)\\) to find \\(\\tan 2a\\). \\begin{align} \\tan 2a = \\tan (a + a) = \\frac{\\tan a + \\tan a}{1- \\tan a \\tan a} \\end{align} \\begin{align} = \\frac{2 \\tan a}{1 - \\tan^2 a} \\end{align} product-to-sum identities adding the identities for \\(\\cos (a + b)\\) and \\(\\cos (a - b)\\) gives the following: \\begin{align} \\cos (a + b) + \\cos (a - b) = \\cos a \\cos b - \\sin a \\sin b + \\cos a \\cos b + \\sin a \\sin b \\end{align} \\begin{align} = 2\\cos a \\cos b \\end{align} thus: \\begin{align} 2 \\cos a \\cos b = \\left[\\cos(a + b) + \\cos(a - b)\\right] \\end{align} \\begin{align} \\cos a \\cos b = \\frac{1}{2} \\left[\\cos(a + b) + \\cos(a - b)\\right] \\end{align} substracting \\(\\cos(a - b)\\) from \\(\\cos (a + b)\\) gives the following: \\begin{align} \\cos (a + b) - \\cos (a - b) = \\cos a \\cos b - \\sin a \\sin b - \\cos a \\cos b - \\sin a \\sin b \\end{align} \\begin{align} = -2 \\sin a \\sin b \\end{align} thus: \\begin{align} - 2 \\sin a \\sin b = \\left[\\cos(a + b) - \\cos(a - b)\\right] \\end{align} \\begin{align} 2 \\sin a \\sin b = -\\frac{1}{2} \\left[\\cos(a + b) - \\cos(a - b)\\right] \\end{align} \\begin{align} \\sin a \\sin b = \\frac{1}{2} \\left[\\cos(a - b) - \\cos(a + b)\\right] \\end{align} adding the identities for \\(\\sin (a + b)\\) and \\(\\sin (a - b)\\) gives the following: \\begin{align} \\sin(a + b) + \\sin (a - b) = \\sin a \\cos b + \\sin b \\cos a + \\sin a \\cos b - \\sin b \\cos a \\end{align} \\begin{align} = 2 \\sin a \\cos b \\end{align} thus: \\begin{align} 2 \\sin a \\cos b = \\sin (a + b) + \\sin (a - b) \\end{align} \\begin{align} \\sin a \\cos b = \\frac{1}{2} \\left[\\sin (a + b) + \\sin (a - b)\\right] \\end{align} substracting \\(\\sin (a - b)\\) from \\(\\sin (a + b)\\) gives the following: \\begin{align} \\sin(a + b) - \\sin (a - b) = \\sin a \\cos b + \\sin b \\cos a - \\sin a \\cos b + \\sin b \\cos a \\end{align} \\begin{align} = 2 \\sin b \\cos a \\end{align} thus: \\begin{align} 2 \\sin b \\cos a = \\sin (a + b) - \\sin (a - b) \\end{align} \\begin{align} \\sin b \\cos a = \\frac{1}{2} \\left[\\sin (a + b) - \\sin (a - b)\\right] \\end{align} sum-to-product identities from the previous identities we can derive another group of identities. for the sum of sines we have: \\begin{align} \\sin a + \\sin b = \\sin (\\frac{2a}{2} + \\frac{b - b}{2}) + \\sin (\\frac{2b}{2} + \\frac{a - a}{2}) \\end{align} \\begin{align} = \\sin (\\frac{(a + b) + (a - b)}{2}) + \\sin (\\frac{(a + b) - (a - b)}{2}) \\end{align} if \\(x = \\frac{a + b}{2}\\) and \\(y = \\frac{a - b}{2}\\), then \\begin{align} = \\sin (x + y) + \\sin (x - y) \\end{align} by the product to sum identity we have: \\begin{align} = 2 (\\sin x \\cos y) = 2 \\sin(\\frac{a + b}{2}) \\cos(\\frac{a - b}{2}) \\end{align} for the difference of sines we have: \\begin{align} \\sin a - \\sin b = \\sin (\\frac{2a}{2} + \\frac{b - b}{2}) - \\sin (\\frac{2b}{2} + \\frac{a - a}{2}) \\end{align} \\begin{align} = \\sin (\\frac{(a + b) + (a - b)}{2}) - \\sin (\\frac{(a + b) - (a - b)}{2}) \\end{align} if \\(x = \\frac{a + b}{2}\\) and \\(y = \\frac{a - b}{2}\\), then \\begin{align} = \\sin (x + y) - \\sin (x - y) \\end{align} by the product to sum identity we have: \\begin{align} = 2 (\\cos x \\sin y) = 2 \\cos(\\frac{a + b}{2}) \\sin(\\frac{a - b}{2}) \\end{align} for the sum of cosines we have: \\begin{align} \\cos a + \\cos b = \\cos (\\frac{2a}{2} + \\frac{b - b}{2}) + \\cos (\\frac{2b}{2} + \\frac{a - a}{2}) \\end{align} \\begin{align} = \\cos (\\frac{(a + b) + (a - b)}{2}) + \\cos (\\frac{(a + b) - (a - b)}{2}) \\end{align} if \\(x = \\frac{a + b}{2}\\) and \\(y = \\frac{a - b}{2}\\), then \\begin{align} = 2 \\cos (x + y) + \\cos (x - y) \\end{align} by the product to sum identity we have: \\begin{align} = 2 (\\cos x \\cos y) = 2 \\cos(\\frac{a + b}{2}) \\cos(\\frac{a - b}{2}) \\end{align} for the difference of cosines we have: \\begin{align} \\cos a - \\cos b = \\cos (\\frac{2a}{2} + \\frac{b - b}{2}) - \\cos (\\frac{2b}{2} + \\frac{a - a}{2}) \\end{align} \\begin{align} = \\cos (\\frac{(a + b) + (a - b)}{2}) - \\cos (\\frac{(a + b) - (a - b)}{2}) \\end{align} if \\(x = \\frac{a + b}{2}\\) and \\(y = \\frac{a - b}{2}\\), then \\begin{align} = \\cos (x + y) - \\cos (x - y) \\end{align} by the product to sum identity we have: \\begin{align} = -2 (\\sin x \\sin y) = -2 \\sin(\\frac{a + b}{2}) \\sin(\\frac{a - b}{2}) \\end{align} half-number identities we derive identities for \\(\\sin \\frac{a}{2}\\), \\(\\cos \\frac{a}{2}\\) and \\(\\tan \\frac{a}{2}\\). these are known as half-number identities, or half-angle identities. we derive the identity for \\(\\cos \\frac{a}{2}\\) as follows: \\begin{align} \\cos 2x = 2 \\cos^2 x - 1 \\end{align} \\begin{align} \\cos 2x + 1 = 2 \\cos^2 x \\end{align} \\begin{align} \\cos x = \\pm \\sqrt{\\frac{1 + \\cos 2x}{2}} \\end{align} now we replace \\(x\\) with \\(\\frac{a}{2}\\), such that: \\begin{align} \\cos \\frac{a}{2} = \\pm \\sqrt{\\frac{1 + \\cos a}{2}} \\end{align} we derive the identity for \\(\\sin \\frac{a}{2}\\) as follows: \\begin{align} \\cos 2x = 1 - 2 \\sin^2 x \\end{align} \\begin{align} \\frac{1 - \\cos 2x}{2} = \\sin^2 x \\end{align} \\begin{align} \\sin x = \\pm \\sqrt{\\frac{1 - \\cos 2x}{2}} \\end{align} now we replace \\(x\\) with \\(\\frac{a}{2}\\) to obtain: \\begin{align} \\sin \\frac{a}{2} = \\pm \\sqrt{\\frac{1 - \\cos a}{2}} \\end{align} an identity for \\(\\tan \\frac{a}{2}\\) comes from the half-number identities for sine and cosine. \\begin{align} \\tan {\\frac{a}{2}} = \\frac{\\sin \\frac{a}{2}}{\\cos \\frac{a}{2}} \\end{align} \\begin{align} = \\frac{\\pm \\sqrt{\\frac{1 - \\cos a}{2}}}{\\pm \\sqrt{\\frac{1 + \\cos a}{2}}} \\end{align} \\begin{align} = \\pm \\sqrt{\\frac{\\frac{1 - \\cos a}{2}}{\\frac{1 + \\cos a}{2}}} = \\pm \\sqrt{\\frac{1 - \\cos a}{1 + \\cos a}} \\end{align} we derive an alternative identity for \\(\\tan \\frac{a}{2}\\) by using double-number identities. \\begin{align} \\tan \\frac{a}{2} = \\frac{\\sin \\frac{a}{2}}{\\cos \\frac{a}{2}} \\end{align} we multiply by \\(2 \\cos \\frac{a}{2}\\) on both the numerator and the denominator: \\begin{align} = \\frac{2 \\sin \\frac{a}{2} \\cos \\frac{a}{2}}{2 \\cos^2 \\frac{a}{2}} \\end{align} we apply the double-number identity \\(\\sin 2a = 2 \\sin a \\cos a\\) and \\(\\cos 2a = 2 \\cos^2 a - 1\\): \\begin{align} = \\frac{\\sin \\left[2 \\frac{a}{2}\\right]}{1 + \\cos\\left[2 \\frac{a}{2}\\right]} \\end{align} \\begin{align} = \\frac{\\sin a}{1 + \\cos a} \\end{align} if we multiply this identity by \\(\\frac{1 - \\cos a}{1 - \\cos a}\\) we obtain: \\begin{align} = \\frac{(\\sin a)(1 - \\cos a)}{(1 + \\cos a) (1 - \\cos a)} \\end{align} \\begin{align} = \\frac{(\\sin a)(1 - \\cos a)}{1 - \\cos^2 a} \\end{align} given \\(\\cos^2 x + \\sin^2 x = 1\\): \\begin{align} = \\frac{(\\sin a)(1 - \\cos a)}{\\sin^2 a} \\end{align} \\begin{align} = \\frac{1 - \\cos a}{\\sin a} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_05.html",
    "title": "Determinants and Cramer's Rule",
    "body": " index search search back determinants and cramer's rule contents [[#determinants of \\(2 \times 2\\) matrices|determinants of \\(2 \times 2\\) matrices]] [[#determinants of \\(3 \times 3\\) matrices|determinants of \\(3 \times 3\\) matrices]] cofactor [[#cramer's rule for \\(2 \times 2\\) systems|cramer's rule for \\(2 \times 2\\) systems]] determinants of \\(2 \\times 2\\) matrices the determinant of a \\(2 \\times 2\\) matrix \\(a\\) is a real number defined as \\(det(a) = a_{11} a_{22}â a_{21} a_{12}\\). determinants of \\(3 \\times 3\\) matrices the determinant of a \\(3 \\times 3\\) matrix \\(a\\) is a real number defined as \\(det(a) = (a_{11}a_{22}a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21}a_{32}) â (a_{31}a_{22}a_{13} + a_{32} a_{23} a_{11} + a_{33} a_{21}a_{12})\\). cofactor let \\(m_{ij}\\) be the minor for element \\(a_{ij}\\) in an \\(n \\times n\\) matrix. the cofactor of \\(a_{ij}\\), written \\(a_{ij}\\), is defined as follows. \\begin{align} a_{ij} = (-1)^{i + j} \\cdot m_{ij} \\end{align} cramer's rule for \\(2 \\times 2\\) systems the solution of the system: \\begin{align} a_1x + b_1y = c_1 \\end{align} \\begin{align} a_2x + b_2y = c_2 \\end{align} is given by: \\begin{align} x = \\frac{d_x}{d}, y = \\frac{d_y}{d} \\end{align} where: \\begin{align} d_x = det( \\begin{bmatrix} c_1 & b_1 \\\\ c_2 & b_2 \\\\ \\end{bmatrix}) \\end{align} \\begin{align} d_y = det( \\begin{bmatrix} a_1 & c_1 \\\\ a_2 & c_2 \\\\ \\end{bmatrix}) \\end{align} \\begin{align} d = det( \\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ \\end{bmatrix}) \\neq 0 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_02.html",
    "title": "Solutions of Linear Systems in Three Variables",
    "body": " index search search back solutions of linear systems in three variables we can extend the ideas of systems of equations in two variables to linear equations of the form: \\begin{align} ax + by + cz = d \\end{align} considering the possible intersections of the planes representing three equations in three unknowns shows that the solution set of such a system may be either a single ordered triple \\((x, y, z)\\), an infinite set of ordered triples (dependent equations), or the empty set (an inconsistent system). the following steps can be used to solve a linear system with three variables. eliminate a variable from any two of the equations. eliminate the same variable from a different pair of equations. eliminate a second variable using the resulting two equations in two variables to get an equation with just one variable. find the values of the remaining variables by substitution. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_03.html",
    "title": "Solution of Linear Systems by Row Transformations",
    "body": " index search search back solution of linear systems by row transformations contents matrix row transformations row echelon method reduced row echelon method special cases solving linear systems of equations can be streamlined by using matrices. consider a system of three equations and three unknowns. \\begin{align} a_1 x + b_1y + c_1z = d_1 \\end{align} \\begin{align} a_2 x + b_2y + c_2z = d_2 \\end{align} \\begin{align} a_3 x + b_3y + c_3z = d_3 \\end{align} can be written as the following augmented matrix: \\begin{align} \\begin{bmatrix} a_1 & b_1 & c_1 & d_1 \\\\ a_2 & b_2 & c_2 & d_2 \\\\ a_3 & b_3 & c_3 & d_3 \\\\ \\end{bmatrix} \\end{align} matrix row transformations for any augmented matrix of a system of linear equations, the following row transformations will result in the matrix of an equivalent system. any two rows may be interchanged. the elements of any row may be multiplied by a nonzero real number. any row may be changed by adding to its elements a multiple of the corresponding elements of another row. row echelon method the echelon (triangular) form of an augmented matrix has 1s down the diagonal from upper left to lower right and 0s below each 1. once a system of linear equations is in echelon form, back-substitution can be used to find the solution set. the row echelon method uses matrices to solve a system of linear equations. start by obtaining a 1 as the first entry in the first column and then transform all entries below it to a 0. continue through the columns obtaining a 1 as the second entry in the second column (zeros below), the third entry in the third column (zeros below), and so on. repeat this process to row echelon form. the following matrix is an augmented matrix in row echelon form: \\begin{align} \\begin{bmatrix} 1 & 2 & 3 & 4\\\\ 0 & 5 & 6 & 7\\\\ 0 & 0 & 0 & 9\\\\ \\end{bmatrix} \\end{align} reduced row echelon method the reduced row echelon form has 1s along the main diagonal and 0s both below and above. for example \\begin{align} \\begin{bmatrix} 1 & 1 & 1 & 6\\\\ 2 & -1 & 1 & 5\\\\ 3 & 1 & -1 & 9\\\\ \\end{bmatrix} \\end{align} by using row transformations, this augmented matrix can be transformed to \\begin{align} \\begin{bmatrix} 1 & 0 & 0 & 3\\\\ 0 & 1 & 0 & 2\\\\ 0 & 0 & 1 & 1\\\\ \\end{bmatrix} \\end{align} which represents \\(x = 3, y = 2, z = 1\\). there is no need for back-substitution with reduced row echelon form. special cases whenever a row of the augmented matrix is of the form \\begin{align} \\begin{bmatrix} 0 & 0 & \\cdots & a\\\\ \\end{bmatrix} \\end{align} where \\(a \\neq 0\\) the system is inconsistent and there will be no solution. a row of the matrix of a linear system in the form: \\begin{align} \\begin{bmatrix} 0 & 0 & \\cdots & 0\\\\ \\end{bmatrix} \\end{align} indicates that the equations of the system are dependent. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_03.html",
    "title": "Vectors and Their Applications",
    "body": " index search search back vectors and their applications contents basic terminology interpretation of vectors properties of parallelograms vector operations vector notation using i and j dot product and the angle between vectors properties of the dot product geometric interpretation of the dot product basic terminology vector quantitites are defined by their magnitude and their direction. they are represented by a directed line segment, called a vector, whose length represents the magnitude. when two letters (i.e. op) name a vector, the first indicates the initial point, while the second one indicates the terminal point. the magnitude of a vector \\(\\textbf{op}\\) is written \\(|\\textbf{op}|\\) two vectors are equal if and only if they both have the same direction and the same magnitude. the sum of two vectors is also a vector. there are two ways to geometrically find the sum of two vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\). place the initial point of vector \\(\\textbf{b}\\) at the terminal point of vector \\(\\textbf{a}\\) (figure 22). the vector with the same initial point as \\(\\textbf{a}\\) and the same terminal point as \\(\\textbf{b}\\) is the sum \\(\\textbf{a} + \\textbf{b}\\) use the parallelogram rule. place vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\) so that their initial points coincide (figure 23). then complete the parallelogram. the diagonal of the parallelogram with the same initial point as \\(\\textbf{a}\\) and \\(\\textbf{b}\\) is the sum \\(\\textbf{a} + \\textbf{b}\\). vector addition is commutative. for every vector \\(\\textbf{v}\\) there is a vector \\(-\\textbf{v}\\) that has the same magnitude as \\(\\textbf{v}\\) but opposite direction, and is called the opposite of \\(\\textbf{v}\\) (figure 24). the sum of \\(\\textbf{v}\\) and \\(-\\textbf{v}\\) hsa magnitude \\(0\\) and is called the zero vector. to substract vector \\(\\textbf{b}\\) from vector \\(\\textbf{a}\\) find the vector sum \\(\\textbf{a} + (-\\textbf{b})\\) (figure 25). the product of a scalar \\(k\\) and a vector \\(\\textbf{u}\\) is called scalar multiplication. the vector \\(k\\textbf{u}\\) has magnitude \\(k|\\textbf{u}|\\). the vector \\(k\\textbf{u}\\) has the same direction as \\(\\textbf{u}\\) if \\(k > 0\\) and opposite direction if \\(k < 0\\) (figure 26). interpretation of vectors a vector with its initial point at the origin in a rectrangular coordinate system is called a position vector. the position vector \\(\\textbf{u}\\) with its endpoint at the point \\((a, b)\\) is written \\(\\langle a, b \\rangle\\). every vector in the real plane corresponds to an ordered pair of real numbers. geometrically a vector is a directed line segment and algebraically it is an ordered pair, where \\(a\\) is the horizontal component and \\(b\\) is the vertical component of vector \\(\\textbf{u}\\). figure 27 shows the vector \\(\\textbf{u} = \\langle a, b \\rangle\\). the positive angle between the x-axis and the position vector is called the direction angle of the vector. the maginutde of vector \\(\\textbf{u} = \\langle a, b \\rangle\\) is given by: \\begin{align} |\\textbf{u}| = \\sqrt{a^2 + b^2} \\end{align} the direction angle \\(\\theta\\) satisfies \\(\\tan \\theta = \\frac{b}{a}\\) where \\(a \\neq 0\\). a vector \\(\\textbf{u}\\) with magnitude \\(|\\textbf{u}|\\) and direction angle \\(\\theta\\) has as horizontal component: \\begin{align} a = |\\textbf{u}| \\cos \\theta \\end{align} and as vertical component: \\begin{align} b = |\\textbf{u}| \\sin \\theta \\end{align} therefore \\(\\textbf{u} = \\langle a, b \\rangle = \\langle |\\textbf{u}| \\cos \\theta, |\\textbf{u}| \\sin \\theta \\rangle\\) properties of parallelograms a parallelogram is quadrilateral whose opposite sides are parallel the opposite sides and opposite angles of a parallelogram are equal, and adjacent angles of a parallelogram are supplementary. the diagonals of a parallelogram bisect each other, but do not necessarily bisec the angles of the parallelogram. vector operations let \\(a, b, c, d\\) and \\(k\\) be real numbers: \\begin{align} \\langle a, b \\rangle + \\langle c, d \\rangle = \\langle a + c, b + d \\rangle \\end{align} \\begin{align} k\\langle a, b \\rangle = \\langle ka, kb \\rangle \\end{align} \\begin{align} \\langle a, b \\rangle - \\langle c, d \\rangle= \\langle a, b \\rangle + -\\langle c, d \\rangle = \\langle a - c, b - d \\rangle \\end{align} a unit vector is a vector that has magnitude \\(1\\). two important unit vectors are: \\begin{align} \\textbf{i} = \\langle 1, 0 \\rangle \\end{align} \\begin{align} \\textbf{j} = \\langle 0, 1 \\rangle \\end{align} vector notation using \\(i\\) and \\(j\\) if \\(\\textbf{v} = \\langle a, b \\rangle\\) then \\(\\textbf{v} = a \\textbf{i} + b \\textbf{j}\\) dot product and the angle between vectors the dot product (or inner product) of two vectors \\(\\textbf{u} = \\langle a, b \\rangle\\) and \\(\\textbf{v} = \\langle c, d \\rangle\\) id denoted as \\(\\textbf{u} \\cdot \\textbf{v}\\) and given by the following: \\begin{align} \\textbf{u} \\cdot \\textbf{v} = ac + bd \\end{align} properties of the dot product for all vectors \\(\\textbf{u}\\), \\(\\textbf{v}\\) and \\(\\textbf{w}\\) and real numbers \\(k\\) the following hold: \\(\\textbf{u} \\cdot \\textbf{v} = \\textbf{v} \\cdot \\textbf{u}\\) \\(\\textbf{u} \\cdot (\\textbf{v} + \\textbf{w}) = \\textbf{u} \\cdot \\textbf{v} + \\textbf{u} \\cdot \\textbf{w}\\) \\((\\textbf{u} + \\textbf{v}) \\cdot \\textbf{w} = \\textbf{u} \\cdot \\textbf{w} + \\textbf{v} \\cdot \\textbf{w}\\) \\((k \\textbf{u}) \\cdot \\textbf{v} = k(\\textbf{u} \\cdot \\textbf{v}) = \\textbf{u} \\cdot (k \\textbf{v})\\) \\(0 \\cdot \\textbf{u} = 0\\) \\(\\textbf{u} \\cdot \\textbf{u} = |\\textbf{u}|^2\\) the dot product of two vectors can be positive, \\(0\\) or negative based on the angle between them. if \\(\\textbf{a} \\cdot \\textbf{b} = 0\\) for two nonzero vectors \\(\\textbf{a}\\) and \\(\\textbf{b}\\), then \\(\\cos \\theta = 0\\) and \\(\\theta = 90Âº\\). thus \\(\\textbf{a}\\) and \\(\\textbf{b}\\) are perpendiular or orthogonal vectors (figure 40). geometric interpretation of the dot product if \\(\\theta\\) is the angle between the two nonzero vectors \\(\\textbf{u}\\) and \\(\\textbf{v}\\), where \\(0Âº \\leq \\theta \\leq 180Âº\\) then the following holds: \\begin{align} \\textbf{u} \\cdot \\textbf{v} = |\\textbf{u}||\\textbf{v}| \\cos \\theta \\end{align} or equivalently \\begin{align} \\cos \\theta = \\frac{\\textbf{u}\\textbf{v}}{|\\textbf{u}||\\textbf{v}|} \\end{align} for angles \\(\\theta\\) between \\(0Âº\\) and \\(180Âº\\), the following table shows the relationship between \\(\\cos \\theta\\), the dot product and \\(\\theta\\). \\(\\cos \\theta\\) dot product angle \\(\\theta\\) between vectors positive positive acute \\(0\\) \\(0\\) right negative negative obtuse $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_03.html",
    "title": "Right Triangles and Evaluating Trigonometric Functions",
    "body": " index search search back right triangles and evaluating trigonometric functions contents right-triangle definitions of the trigonometric functions trigonometric function values of special angles cofunction identities reference angles [[#finding trigonometric function values for a nonquadrantal angle \\(\theta\\)|finding trigonometric function values for a nonquadrantal angle \\(\theta\\)]] right-triangle definitions of the trigonometric functions figure 41 shows an acute angle \\(a\\) in standard position. the side of length \\(y\\) in figure 41 is called the side opposite angle \\(a\\), and the side of length \\(x\\) is called the side adjacent to angle \\(a\\). such that: \\begin{align} \\sin a = \\frac{y}{r} = \\frac{\\text{side opposite}}{\\text{hypotenuse}} \\end{align} \\begin{align} \\cos a = \\frac{x}{r} = \\frac{\\text{side adjacent}}{\\text{hypotenuse}} \\end{align} \\begin{align} \\tan a = \\frac{y}{x} = \\frac{\\text{side opposite}}{\\text{side adjacent}} \\end{align} \\begin{align} \\csc a = \\frac{r}{y} = \\frac{\\text{hypotenuse}}{\\text{side opposite}} \\end{align} \\begin{align} \\sec a = \\frac{r}{x} = \\frac{\\text{hypotenuse}}{\\text{side adjacent}} \\end{align} \\begin{align} \\cot a = \\frac{x}{y} = \\frac{\\text{side adjacent}}{\\text{side opposite}} \\end{align} trigonometric function values of special angles certain special angles, such as \\(30Âº, 45Âº\\) and \\(60Âº\\) occur so often that they deserve special study. see figure 43(a), for convenience the length of the sides is \\(2\\). bisecting one angle of this equilateral triangle leads to two right triangles, each of which has angles of \\(30Â°, 60Â°\\), and \\(90Â°\\), as shown in figure 43(b). let \\(x\\) represent the length of the longer leg: \\begin{align} 2^2 = 1^2 + x^2 \\end{align} \\begin{align} 4 = 1 + x^2 \\end{align} \\begin{align} 3 = x^2 \\end{align} \\begin{align} \\sqrt{3} = x \\end{align} therefore the hypotenuse is \\(2\\), the side opposite is \\(1\\) and the side adjacent is \\(\\sqrt{3}\\). from the definition of the trigonometric functions: \\begin{align} \\sin 30Âº = \\frac{\\text{side opposite}}{\\text{hypotenuse}} = \\frac{1}{2} \\end{align} \\begin{align} \\cos 30Âº = \\frac{\\text{side adjacent}}{\\text{hypotenuse}} = \\frac{\\sqrt{3}}{2} \\end{align} \\begin{align} \\tan 30Âº = \\frac{\\text{side opposite}}{\\text{side adjacent}} = \\frac{\\sqrt{3}}{3} \\end{align} \\begin{align} \\csc 30Âº = \\frac{\\text{hypotenuse}}{\\text{side opposite}} = \\frac{2}{1} = 2 \\end{align} \\begin{align} \\sec 30Âº = \\frac{\\text{hypotenuse}}{\\text{side adjacent}} = \\frac{2\\sqrt{3}}{3} \\end{align} \\begin{align} \\cot 30Âº = \\frac{\\text{side adjacent}}{\\text{side opposite}} = \\frac{\\sqrt{3}}{1} = \\sqrt{3} \\end{align} for \\(60Âº\\) refer to figure 44, such that hypotenuse is \\(2\\), side opposite is \\(\\sqrt{3}\\) and side adjacent is \\(1\\). therefore from the definition of the trigonometric functions: \\begin{align} \\sin 60Âº = \\frac{\\text{side opposite}}{\\text{hypotenuse}} = \\frac{\\sqrt{3}}{2} \\end{align} \\begin{align} \\cos 60Âº = \\frac{\\text{side adjacent}}{\\text{hypotenuse}} = \\frac{1}{2} \\end{align} \\begin{align} \\tan 60Âº = \\frac{\\text{side opposite}}{\\text{side adjacent}} = \\frac{\\sqrt{3}}{1} = \\sqrt{3} \\end{align} \\begin{align} \\csc 60Âº = \\frac{\\text{hypotenuse}}{\\text{side opposite}} = \\frac{2\\sqrt{3}}{3} \\end{align} \\begin{align} \\sec 60Âº = \\frac{\\text{hypotenuse}}{\\text{side adjacent}} = \\frac{2}{1} = 2 \\end{align} \\begin{align} \\cot 60Âº = \\frac{\\text{side adjacent}}{\\text{side opposite}} = \\frac{\\sqrt{3}}{3} \\end{align} for \\(45Âº\\) we start with a \\(45Âº-45Âº\\) right triangle as shown in figure 45. this triangle has two equal sides whose length is \\(1\\) unit. therefore, by the pythagoread theorem: \\begin{align} 1^2 + 1^2 = r^2 \\end{align} \\begin{align} r = \\sqrt{2} \\end{align} that is the hypotenuse is \\(\\sqrt{2}\\), the side opposite is \\(1\\) and the side adjacent is \\(1\\), so from the definition of the trigonometric functions: \\begin{align} \\sin 45Âº = \\frac{\\text{side opposite}}{\\text{hypotenuse}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} \\end{align} \\begin{align} \\cos 45Âº = \\frac{\\text{side adjacent}}{\\text{hypotenuse}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} \\end{align} \\begin{align} \\tan 45Âº = \\frac{\\text{side opposite}}{\\text{side adjacent}} = \\frac{1}{1} = 1 \\end{align} \\begin{align} \\csc 45Âº = \\frac{\\text{hypotenuse}}{\\text{side opposite}} = \\frac{\\sqrt{2}}{1} = \\sqrt{2} \\end{align} \\begin{align} \\sec 45Âº = \\frac{\\text{hypotenuse}}{\\text{side adjacent}} = \\frac{\\sqrt{2}}{1} = \\sqrt{2} \\end{align} \\begin{align} \\cot 45Âº = \\frac{\\text{side adjacent}}{\\text{side opposite}} = \\frac{1}{1} = 1 \\end{align} cofunction identities in a right triangle \\(abc\\) with right angle \\(c\\), the acute angles \\(a\\) and \\(b\\) are complementary. see figure 46. the length of the side opposite angle \\(a\\) is \\(a\\), and the length of the side opposite angle \\(b\\) is \\(b\\). the length of the hypotenuse is \\(c\\). in this triangle, \\(\\sin a = \\frac{a}{c}\\) and \\(\\cos b\\) is also equal to \\(\\frac{a}{c}\\). similar reasoning yields the following. \\begin{align} \\tan a = \\frac{a}{b} = \\cot b \\end{align} \\begin{align} \\sec a = \\frac{c}{b} = \\csc b \\end{align} if these identities follow we say \\(\\sin\\) and \\(\\cos\\) are cofunctions, as well as \\(\\tan\\) and \\(\\cot\\) and \\(\\sec\\) and \\(\\csc\\). since angles \\(a\\) and \\(b\\) are complementary \\(a + b = 90Âº\\), that is \\(b = 90Âº - a\\), therefore: \\begin{align} \\sin a = \\cos b = \\cos (90Âº - a) \\end{align} this is a cofunction identity, the rest are as follows: given an acute angle \\(a\\) in degrees: \\begin{align} \\cos a = \\sin b = \\sin (90Âº - a) \\end{align} \\begin{align} \\tan a = \\cot b = \\cot (90Âº - a) \\end{align} \\begin{align} \\csc a = \\sec b = \\sec (90Âº - a) \\end{align} \\begin{align} \\sec a = \\csc b = \\csc (90Âº - a) \\end{align} \\begin{align} \\cot a = \\tan b = \\tan (90Âº - a) \\end{align} given an acute angle \\(a\\) is radians: \\begin{align} \\sin a = \\cos b = \\cos \\left(\\frac{\\pi}{2} - a\\right) \\end{align} \\begin{align} \\cos a = \\sin b = \\sin \\left(\\frac{\\pi}{2} - a\\right) \\end{align} \\begin{align} \\tan a = \\cot b = \\cot \\left(\\frac{\\pi}{2} - a\\right) \\end{align} \\begin{align} \\csc a = \\sec b = \\sec \\left(\\frac{\\pi}{2} - a\\right) \\end{align} \\begin{align} \\sec a = \\csc b = \\csc \\left(\\frac{\\pi}{2} - a\\right) \\end{align} \\begin{align} \\cot a = \\tan b = \\tan \\left(\\frac{\\pi}{2} - a\\right) \\end{align} reference angles a reference angle for an angle \\(\\theta\\), written \\(\\theta'\\), is the positive acute angle made by the terminal side of angle \\(\\theta\\) and the x-axis. if an angle \\(\\theta\\) is negative or has measure greater than \\(360Â°\\), its reference angle is found by first finding its coterminal angle that is between \\(0Â°\\) and \\(360Â°\\). finding trigonometric function values for a nonquadrantal angle \\(\\theta\\) if \\(\\theta > 360Â°\\), or if \\(\\theta < 0Â°\\), then find a coterminal angle. find the reference angle \\(\\theta'\\). find the trigonometric function values for reference angle \\(\\theta'\\). determine the correct signs for the values, given by the quadrant of \\(\\theta\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_06.html",
    "title": "Solution of Linear Systems by Matrix Inverses",
    "body": " index search search back solution of linear systems by matrix inverses contents multiplicative inverses of square matrices using determinants to find inverses solving linear systems using inverse matrices multiplicative inverses of square matrices in a similar way, if \\(a\\) is an \\(n \\times n\\) matrix, then its multiplicative inverse, written \\(a^{-1}\\), must satisfy both: \\begin{align} aa^{-1} = a^{-1}a = i_n \\end{align} this result means that only a square matrix can have a multiplicative inverse. the inverse matrix of an \\(n \\times n\\) matrix \\(a\\) (if it exists) can be found analytically by first forming the augmented matrix \\([a|i_n]\\) such that \\(ax = i_n\\), thus \\(x = a^{-1}\\). this means you are solving \\(n\\) systems of linear equations of the form \\(ax_i = i_{n_i}\\). this system is solved by performing matrix row operations, until the left side of the augmented matrix becomes the identity matrix. the resulting augmented matrix can be written as \\([i_n|a^{-1}]\\), where the right side of the matrix is \\(a^{-1}\\). if \\(a^{-1}\\) exists, then it is unique. if \\(a^{-1}\\) does not exist, then \\(a\\) is a singular matrix. using determinants to find inverses if \\begin{align} a = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\end{align} and \\(det(a) \\neq 0\\) then \\begin{align} a^{-1} = \\frac{1}{det(a)}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} \\end{align} if \\(det(a) = 0\\), then \\(a^{â1}\\) does not exist and \\(a\\) is a singular matrix. solving linear systems using inverse matrices to solve the matrix equation \\(ax = b\\), first see if \\(a^{-1}\\) exists. assuming that it does, use the facts that \\(a^{-1}a = i\\) and \\(ix = x\\). \\begin{align} ax = b \\end{align} \\begin{align} a^{-1}(ax) = a^{-1}b \\end{align} \\begin{align} (a^{-1}a)x = a^{-1}b \\end{align} \\begin{align} ix = a^{-1}b \\end{align} \\begin{align} x = a^{-1}b \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_02.html",
    "title": "Trigonometric Functions and Fundamental Identities",
    "body": " index search search back trigonometric functions and fundamental identities contents trigonometric functions function values of quadrantal angles reciprocal identities signs and ranges of function values pythagorean identities quotient identities trigonometric functions let \\((x, y)\\) be a point other than the origin on the terminal side of an angle \\(\\theta\\) instandard position. the distance from the point to the origin is \\(r = \\sqrt{x^2 + y^2}\\). the six trigonometric functions of \\(\\theta\\) are as follows: \\begin{align} \\sin \\theta = \\frac{y}{r} \\end{align} \\begin{align} \\cos \\theta = \\frac{x}{r} \\end{align} \\begin{align} \\tan \\theta = \\frac{y}{x}, x \\neq 0 \\end{align} \\begin{align} \\csc \\theta = \\frac{r}{y} \\end{align} \\begin{align} \\sec \\theta = \\frac{r}{x} \\end{align} \\begin{align} \\cot \\theta = \\frac{x}{y}, y \\neq 0 \\end{align} \\(\\sin \\theta = \\frac{y}{r}\\) is the same no matter which point is used to find it. refer to figure 28, which shows an angle \\(\\theta\\) and two distinct points on its terminal side. point \\(p\\) has coordinates \\((x, y)\\), and point \\(p'\\) with coordinates \\((x', y')\\). let \\(r\\) be the length of the hypotenuse of triangle \\(opq\\), and let \\(r'\\) be the length of the hypotenuse of triangle \\(op'q'\\). since corresponding sides of similar triangles are in proportion: \\begin{align} \\frac{y}{r} = \\frac{y'}{r'} = \\sin \\theta \\end{align} we can also find the trigonometric function values of an angle if we know the equation of the line coinciding with the terminal ray: \\begin{align} ax + by = 0 \\end{align} by choosing any point on the ray, we can find the trigonometric function values of the angle. in general, it is true that \\(m = \\tan \\theta\\). function values of quadrantal angles conditions for undefined function values if the terminal side of the quadrantal angle lies along the y-axis (\\(x\\) equals zero), then the tangent and secant functions are undefined. if the terminal side of the quadrantal angle lies along the x-axis (\\(y\\) equals zero), then the cotangent and cosecant functions are undefined. reciprocal identities the definitions of the trigonometric functions were written to illustrate that certain function pairs are reciprocals of each other. since: \\begin{align} \\sin \\theta = \\frac{y}{r} \\end{align} and \\begin{align} \\csc \\theta = \\frac{r}{y} \\end{align} then: \\begin{align} \\sin \\theta = \\frac{1}{\\csc \\theta} \\end{align} therefore, the reciprocal identities are listed below: \\begin{align} \\sin \\theta = \\frac{1}{\\csc \\theta} \\end{align} \\begin{align} \\cos \\theta = \\frac{1}{\\sec \\theta} \\end{align} \\begin{align} \\tan \\theta = \\frac{1}{\\cot \\theta} \\end{align} \\begin{align} \\csc \\theta = \\frac{1}{\\sin \\theta} \\end{align} \\begin{align} \\sec \\theta = \\frac{1}{\\cos \\theta} \\end{align} \\begin{align} \\cot \\theta = \\frac{1}{\\tan \\theta} \\end{align} signs and ranges of function values a point \\((x, y)\\) in quadrant ii has \\(x < 0\\) and \\(y > 0\\). this makes the values of sine and cosecant positive for quadrant ii angles, while the other four functions take on negative values. similar results can be obtained for the other quadrants, as summarized here. in figure 37 we can see that as the measure of the angle increases, \\(y\\) increases, but never exceeds \\(r\\), so \\(y \\leq r\\). in a similar way, angles in quadrant iv suggest that \\(-r \\leq y\\). therefore: \\begin{align} -r \\leq y \\leq r \\end{align} \\begin{align} -1 \\leq \\frac{y}{r} \\leq 1 \\end{align} \\begin{align} -1 \\leq \\sin \\theta \\leq 1 \\end{align} similar reasoning leads to the following: \\begin{align} -1 \\leq \\cos \\theta \\leq 1 \\end{align} the tangent of an angle is defined as \\(\\frac{y}{x}\\). it is possible that \\(x < y\\), \\(x = y\\), or \\(x > y\\). for this reason, \\(\\frac{y}{x}\\) can take any value, so \\(\\tan \\theta\\) can be any real number, as can \\(\\cot \\theta\\). the functions \\(\\sec \\theta\\) and \\(\\csc \\theta\\) are reciprocals of the functions \\(\\cos \\theta\\) and \\(\\sin \\theta\\), respectively, making the following true: \\begin{align} \\sec \\theta \\leq -1 \\text{ or } \\sec \\theta \\geq 1 \\end{align} and \\begin{align} \\csc \\theta \\leq -1 \\text{ or } \\csc \\theta \\geq 1 \\end{align} pythagorean identities given \\(x^2 + y^2 = r^2\\), then: \\begin{align} \\frac{x^2}{r^2} + \\frac{y^2}{r^2} = \\frac{r^2}{r^2} \\end{align} \\begin{align} \\left(\\frac{x}{r}\\right)^2 + \\left(\\frac{y}{r}\\right)^2 = 1 \\end{align} \\begin{align} \\left(\\cos \\theta\\right)^2 + \\left(\\sin \\theta\\right)^2 = 1 \\end{align} similarly: \\begin{align} \\frac{x^2}{x^2} + \\frac{y^2}{x^2} = \\frac{r^2}{x^2} \\end{align} \\begin{align} 1 + \\left(\\frac{y}{x}\\right)^2 = \\left(\\frac{r}{x}\\right)^2 \\end{align} \\begin{align} 1 + \\left(\\tan \\theta\\right)^2 = \\left(\\sec \\theta\\right)^2 \\end{align} similarly: \\begin{align} \\frac{x^2}{y^2} + \\frac{y^2}{y^2} = \\frac{r^2}{y^2} \\end{align} \\begin{align} \\left(\\frac{x}{y}\\right)^2 + 1 = \\left(\\frac{r}{y}\\right)^2 \\end{align} \\begin{align} \\left(\\cot \\theta\\right)^2 + 1 = \\left(\\csc \\theta \\right)^2 \\end{align} quotient identities consider the quotient of \\(\\sin \\theta\\) and \\(\\cos \\theta\\): \\begin{align} \\frac{\\sin \\theta}{\\cos \\theta} = \\frac{\\frac{y}{r}}{\\frac{x}{r}} = \\frac{y}{x} = \\tan \\theta \\end{align} where \\(\\cos \\theta \\neq 0\\). similarly: \\begin{align} \\frac{\\cos \\theta}{\\sin \\theta} = \\frac{\\frac{x}{r}}{\\frac{y}{r}} = \\frac{x}{y} = \\cot \\theta \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/07_04.html",
    "title": "Parametric Equations",
    "body": " index search search back parametric equations contents graphing a parabola defined parametrically a plane curve is a set of points \\((x, y)\\) such taht \\(x= f(t)\\), \\(y = g(t)\\) and \\(f\\) and \\(g\\) are both continuous on an interval \\(i\\). the equations \\(x = f(t)\\) and \\(y = g(t)\\) are parametric equations with parameter \\(t\\). graphing a parabola defined parametrically graph the plane curve \\(x = t^2, y = 2t + 3\\) for \\(t \\in [-3, 3]\\). make a table of corresponding values of \\(t\\), \\(x\\), and \\(y\\) over the domain of \\(t\\). \\(t\\) \\(x\\) \\(y\\) ---- --- ---- \\(-3\\) \\(9\\) \\(-3\\) \\(-2\\) \\(4\\) \\(-1\\) \\(-1\\) \\(1\\) \\(1\\) \\(0\\) \\(0\\) \\(3\\) \\(1\\) \\(1\\) \\(5\\) \\(2\\) \\(4\\) \\(7\\) \\(3\\) \\(9\\) \\(9\\) then plot the points. to find an equivalent rectangular equation, we eliminate the parameter \\(t\\). \\begin{align} y = 2t + 3 \\end{align} \\begin{align} y - 3 = 2t \\end{align} \\begin{align} \\frac{y - 3}{2} = t \\end{align} now we substitute the result in the first equation \\(x = t^2\\): \\begin{align} x = t^2 = \\left(\\frac{y - 3}{2}\\right)^2 = \\frac{(y - 3)^2}{4} = \\frac{1}{4}(y-3)^2 \\end{align} this is indeed an equation of a horizontal parabola that opens to the right. because \\(t\\) is in \\([3, -3]\\), \\(x\\) is in \\([3, 0]\\), and \\(y\\) is in \\([-3, 9]\\). the rectangular equation must be given with its restricted domain as: \\begin{align} x = \\frac{1}{4}(y-3)^2, \\text{ for } x \\in [0, 9] \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/index.html",
    "title": "A Graphical Approach to Algebra and Trigonometry",
    "body": " index search search back a graphical approach to algebra and trigonometry linear functions, equations and inequalities real numbers and the rectangular coordinate system introduction to relations and functions linear functions equations of lines and linear models linear equations and inequalities applications of linear functions analysis of graphs of functions graphs of basic functions and relations: symmetry vertical and horizontal shifts of graphs stretching, shrinking and reflecting graphs absolute value functions piecewise-defined functions opertions and composition polynomial functions complex numbers quadratic functions and graphs quadratic equations and inequalities high-degree polynomial functions and graphs topics in the theory of polynomial functions (i) topics in the theory of polynomial functions (ii) polynomial equations and inequalities; further applications and models rational, power and root functions rational functions and graphs (i) rational functions and graphs (ii) rational equations, inequalities, models and applications functions defined by powers and roots equations, inequalities, and applications involving root functions inverse, exponential and logarithmic functions inverse functions exponential functions logarithms and their properites logarithmic functions exponential and logarithmic equations and inequalities further applications and modeling with exponential and logarithmic functions systems and matrices systems of equations solutions of linear systems in three variables solution of linear systems by row transformations matrix properties and operations determinants and cramer's rule solution of linear systems by matrix inverses systems of inequalities and linear programming partial fractions analytic geometry and nonlinear systems circles and parabolas ellipses and hyperbolas the conic sections and nonlinear systems parametric equations trigonometric functions and applications. angles and their measures trigonometric functions and fundamental identities right triangles and evaluating trigonometric functions applications of right triangles the circular functions graph of the sine and cosine functions graphs of the other circular functions harmonic motion trigonometric identities and equations. trigonometric identities sum and difference identities further identities the inverse circular functions applications of trigonometry and vectors the law of sines the law of cosines and area formulas vectors and their applications trigonometric (polar) form of complex numbers powers and roots of complex numbers polar equations and graphs more parametric equations further topics in algebra sequences and series arithmetric sequences and series geometric sequences and series counting theory the binomial theorem mathematical induction probability appendix b. vectors in space c. polar form of conic sections d. rotation of axes $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_06.html",
    "title": "Graphs of the Sine and Cosine Functions",
    "body": " index search search back graphs of the sine and cosine functions contents periodic function graph of the sine function amplitude period guidelines for sketching graphs of the sine and cosine functions translations and transformations further guidelines for sketching graphs of the sine and cosine functions periodic function a periodic function is a function \\(f\\), such that: \\begin{align} f(x) = f(x + np) \\end{align} for every real number \\(x\\) in the domain of \\(f\\), every integer \\(n\\) and some positive real number \\(p\\). the least possible value of \\(p\\) is the period of the function. graph of the sine function see figure 85, and trace along the circle to verify the results shown in the table. this graph is called a sine wave or sinusoid. amplitude the graph of \\(y = a \\sin x\\) or \\(y = a \\cos x\\), with \\(a \\neq 0\\), will have the same shape as the graph of \\(y = \\sin x\\) or \\(y = \\cos x\\), respectively, except with range \\([-|a|, |a|]\\). the amplitude is \\(|a|\\). no matter what the value of the amplitude, the periods of \\(y = a \\sin x\\) and \\(y = a \\cos x\\) are still \\(2\\pi\\). period in general, the graph of a function of the form \\(y = \\sin bx\\) or \\(y = \\cos bx\\), for \\(b > 0\\), will have a period different from \\(2\\pi\\) when \\(b \\neq 1\\). we know that \\(bx\\) ranges from \\(0\\) to \\(2\\pi\\), therefore: \\begin{align} 0 \\leq bx \\leq 2\\pi \\end{align} \\begin{align} 0 \\leq x \\leq \\frac{2\\pi}{b} \\end{align} therefore the period is \\(\\frac{2\\pi}{b}\\). by dividing the interval \\([0, \\frac{2\\pi}{b}]\\) into four equal parts, we obtain the values for which \\(\\sin bx\\) or \\(\\cos bx\\) is \\(-1\\), \\(0\\), or \\(1\\). guidelines for sketching graphs of the sine and cosine functions to graph \\(y = a \\sin bx\\) or \\(y = a \\cos bx\\), with \\(b > 0\\), follow these steps. find the period, \\(\\frac{2\\pi}{b}\\). divide the interval into four equal parts. evaluate the function for each of the five x-values. the points will be maximum points, minimum points, and x-intercepts. plot the points found in step 3, and join them with a sinusoidal curve having amplitude \\(|a|\\). translations and transformations in general, the graph of a function of the form: \\begin{align} y = f(x - d) \\end{align} is translated horizontally compared with the graph of \\(y = f(x)\\). the translation is \\(d\\) units to the right if \\(d > 0\\) and \\(|d|\\) units to the left if \\(d < 0\\). in general, the graph of a function of the form: \\begin{align} y = c + f(x) \\end{align} is translated vertically compared with the graph of \\(y = f(x)\\). the translation is \\(c\\) units up if \\(c > 0\\) and \\(|c|\\) units down if \\(c < 0\\). further guidelines for sketching graphs of the sine and cosine functions a function of the form \\(y = c + a \\sin [b(x â d)]\\) or \\(y = c + a \\cos [b(x â d)]\\) , \\(b > 0\\) can be graphed according to the following guidelines: method 1: find an interval whose length is one period \\(\\frac{2\\pi}{b}\\) by solving the three-part inequality \\(0 \\leq b(x - d) \\leq 2\\pi\\). divide the interval into four equal parts. evaluate the function for each of the five x-values. the points will be maximum points, minimum points, and points that intersect the line \\(y = c\\). plot the points in step 3, and join them with a sinusoidal curve having amplitude \\(|a|\\). method 2: graph \\(y = a \\sin bx\\) or \\(y = a \\cos bx\\). the amplitude of the function is \\(|a|\\), and the period is \\(\\frac{2\\pi}{b}\\). use translations to graph the desired function. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_08.html",
    "title": "Harmonic Motion",
    "body": " index search search back harmonic motion contents simple harmonic motion dampled oscillatory motion simple harmonic motion consider figure 119, suppose the point \\(p(x, y)\\) moves around the circle counterclockwise at a uniform angular speed \\(\\omega\\). assume that at time \\(t = 0\\), \\(p\\) is at \\((a, 0)\\). the angle swept out by ray op at time \\(t\\) is given by: \\begin{align} \\theta = \\omega t \\end{align} the coordinates of point \\(p\\) at time \\(t\\) are: \\begin{align} x = a \\cos \\theta = a \\cos \\omega t \\end{align} and \\begin{align} y = a \\sin \\theta = a \\sin \\omega t \\end{align} the number of oscillations, or cycles per unit of time, called the frequency, is the reciprocal o the period. the position of a point oscillating about an equilibrium position at time \\(t\\) is modeled by either: \\begin{align} s(t) = a \\cos \\omega t \\end{align} or \\begin{align} s(t) = a \\sin \\omega t \\end{align} where \\(a\\) and \\(\\omega\\) are constants with \\(\\omega > 0\\). the amplitude of the motion is \\(|a|\\), the period is \\(\\frac{2 \\pi}{\\omega}\\) and the frequency is \\(\\frac{\\omega}{2 \\pi}\\). dampled oscillatory motion up until now we disregarded the effect of friction, which causes the amplitude of the motion to diminish gradually. we say that the motion has been damped by the force of friction. most oscillatory motions are damped, and the decrease in amplitude follows the pattern of exponential decay. an example of damped oscillatory motion is given by the function: \\begin{align} s(t) = e^{-t} \\sin t \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_04.html",
    "title": "Applications of Right Triangles",
    "body": " index search search back applications of right triangles contents solving triangles angles of elevation or depression bearing the results of physical measurements are only approximately accurate and depend on the precision of the measuring instrument as well as the aptness of the observer. the digits obtained by actual measurement are called significant digits. solving triangles to solve a triangle means to find the measures of all the angles and sides of the triangle. angles of elevation or depression the angle of elevation from point \\(x\\) to point \\(y\\) (above \\(x\\)) is the acute angle formed by ray \\(xy\\) and a horizontal ray with endpoint at \\(x\\) (see figure 61). the angle of depression from point \\(x\\) to point \\(y\\) (below \\(x\\)) is the acute angle formed by ray \\(xy\\) and a horizontal ray with endpoint \\(x\\) (see figure 62). bearing when a single angle is given, such as \\(164Â°\\), it is understood that the bearing is measured in a clockwise direction from due north (see figure 64). the second method for expressing bearing starts with a northâsouth line and uses an acute angle to show the direction, either east or west, from this line (see figure 66). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_07.html",
    "title": "Graphs of the Other Circular Functions",
    "body": " index search search back graphs of the other circular functions contents graphs of the secant and cosecant functions graphs of the tangent and cotangent functions graphs of the secant and cosecant functions the secant function is undefined for odd multiples of \\(\\frac{\\pi}{2}\\) and has vertical asymptotes for such values. furthermore, since \\(\\sec (-x) = \\sec x\\), the secant function is even and its graph is symmetric with respect to the y-axis. figure 100 shows how the graphs of \\(y = \\cos x\\) and \\(y = \\sec x\\) are related the vertical asymptotes of the cosecant function are at \\(x\\)-values that are integer multiples of \\(\\pi\\). because \\(\\csc (-x) = -\\csc x\\), the cosecant function is odd and its graph is symmetric with respect to the origin. figure 102 shows how the graphs of \\(y = \\sin x\\) and \\(y = \\csc x\\) are related. guidelines for sketching graphs of the secant and cosecant functions to graph \\(y = a \\sec bx\\) or \\(y = a \\csc bx\\), with \\(b > 0\\), follow these steps: graph the reciprocal function as a guide. for \\(y = a \\sec bx\\), graph \\(y = a \\cos bx\\) for \\(y = a \\csc bx\\), graph \\(y = a \\sin bx\\) sketch the vertical asymptotes with equations \\(x = k\\), where \\((k, 0)\\) is an \\(x\\)-intercept of the graph of the guide function. sketch the graph of the desired function by drawing the typical u-shaped branches between the adjacent asymptotes. the branches will be above the graph of the guide function when the guide function values are positive and below the graph of the guide function when the guide function values are negative. graphs of the tangent and cotangent functions the tangent function is undefined for odd multiples of \\(\\frac{\\pi}{2}\\) and has vertical asymptotes for such values. furthermore, since \\(\\tan (-x) = -\\tan x\\), the tangent function is odd and its graph is symmetric with respect to the origin. the tangent function has period \\(\\pi\\). because \\(\\tan x = \\frac{\\sin x}{\\cos x}\\), tangent values are \\(0\\) when sine values are \\(0\\) (\\(x = 0\\)), and undefined when cosine values are \\(0\\) (\\(x = \\frac{\\pi}{2}\\) or \\(x = -\\frac{\\pi}{2}\\)). as x-values go from \\(-\\frac{\\pi}{2}\\) to \\(\\frac{\\pi}{2}\\), tangent values go from \\(- \\infty\\) to \\(\\infty\\). the graph of \\(y = \\tan x\\) is shown in figure 110. the cotangent function's vertical asymptotes are at x-values that are integer multiples of \\(\\pi\\). because \\(\\cot (-x) = -\\cot x\\), the cotangent function is odd and its graph is symmetric with respect to the origin. its graph is plotted on figure 112. the cotangent function also has period \\(\\pi\\). cotangent values are \\(0\\) when cosine values are \\(0\\) (\\(x = \\frac{\\pi}{2}\\)), and undefined when sine values are \\(0\\) (\\(x = 0\\) or \\(x = \\pi\\)). as \\(x\\)-values go from \\(0\\) to \\(\\pi\\), cotangent values go from \\(\\infty\\) to \\(-\\infty\\) and decrease throughout the interval. guidelines for sketching graphs of the tangent and cotangent functions to graph \\(y = a \\tan bx\\) or \\(y = a \\cot bx\\), with \\(b > 0\\), follow these steps: the period is \\(\\frac{\\pi}{b}\\). to locate two adjacent vertical asymptotes, solve the following equations for \\(x\\). for \\(y = a \\tan bx\\): \\(bx = - \\frac{\\pi}{2}\\) and \\(bx = \\frac{\\pi}{2}\\) for \\(y = a \\cot bx\\): \\(bx = 0\\) and \\(bx = \\pi\\) sketch the two vertical asymptotes. divide the interval formed by the vertical asymptotes into four. evaluate the function for the first-quarter point, midpoint, and thirdquarter point. join the points with a smooth curve. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_04.html",
    "title": "Matrix Properties and Operations",
    "body": " index search search back matrix properties and operations contents matrix addition multiplication of a matrix by a scalar matrix multiplication in general, a matrix with \\(m\\) rows and \\(n\\) columns has dimension \\(m \\times n\\). the number of rows is always given first certain matrices have special names. an \\(n \\times n\\) matrix is a square matrix of order n. also, a matrix with just one row is a row matrix, and a matrix with just one column is a column matrix. two matrices are equal if they have the same dimension and if corresponding elements, position by position, are equal a matrix containing only zeros as elements is called a zero matrix. matrix addition the sum of two \\(m \\times n\\) matrices \\(a\\) and \\(b\\) is the \\(m \\times n\\) matrix \\(a + b\\) in which each element is the sum of the corresponding elements of \\(a\\) and \\(b\\). only matrices with the same dimension can be added. multiplication of a matrix by a scalar the product of a scalar \\(k\\) and a matrix \\(a\\) is the matrix \\(ka\\), each of whose elements is \\(k\\) times the corresponding element of \\(a\\). matrix multiplication the product \\(ab\\) of an \\(m \\times n\\) matrix \\(a\\) and an \\(n \\times k\\) matrix \\(b\\) is an \\(m \\times k\\) matrix and is found as follows. to find the \\(i\\)th row, \\(j\\)th column element of \\(ab\\), multiply each element in the \\(i\\)th row of \\(a\\) by the corresponding element in the \\(j\\)th column of \\(b\\). the sum of these products gives the element of row \\(i\\), column \\(j\\) of \\(ab\\). \\begin{align} c_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj} \\end{align} the product ab can be found only if the number of columns of a is the same as the number of rows of b. the final product will have as many rows as a and as many columns as b. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_07.html",
    "title": "Systems of Inequalities and Linear Programming",
    "body": " index search search back systems of inequalities and linear programming contents solving linear inequalities graphing a linear inequality two methods for graphing an inequality soliving systems of inequalitites linear programming fundamental theorem of linear programming solving a linear programming problem a line divides a plane into three sets of points: the points of the line itself the points belonging to the two regions determined by the line. each of these two regions is called a half plane. the line is the boundary of each half plane. solving linear inequalities a linear inequality in two variables is an inequality of the form \\begin{align} ax + by \\leq c, \\end{align} where \\(a\\), \\(b\\), and \\(c\\) are real numbers with \\(a\\) and \\(b\\) not both equal to \\(0\\). (the symbol \\(\\leq\\) be replaced with \\(\\geq\\), \\(>\\) or \\(<\\)). graphing a linear inequality graph \\(x + 4y 7 > 4\\). the boundary here is the line \\(x + 4y = 4\\). since the points on this line do not satisfy \\(x + 4y = 4\\), make the line dashed. to decide which half plane represents the solution, solve for \\(y\\). such that \\(y > -\\frac{1}{4}x + 1\\) since \\(y\\) is greater than \\(-\\frac{1}{4}x + 1\\), the graph of the solution set is the half plane above the boundary two methods for graphing an inequality for a function \\(f\\), the graph of \\(y < f(x)\\) consists of all the points that are below the graph of y = Æ(x). the graph of \\(y > f(x)\\) consists of all the points that are above the graph of \\(y = f(x)\\). if the inequality is not or cannot be solved for \\(y\\), choose a test point not on the boundary. if the test point satisfies the inequality, the graph includes all points on the same side of the boundary as the test point. otherwise, the graph includes all points on the other side of the boundary. soliving systems of inequalitites the solution set of a system of inequalities is the intersection of the solution sets of its members graph the solution set of the system. \\begin{align} x > 6 - 2y \\end{align} \\begin{align} x^2 < 2y \\end{align} the next figures show the graphs for both inequalities as well as the solution set, that is the intersection of the regions that represent the solution set for each inequality. linear programming we use linear programming to find an optimum value. a linear programming problem typically needs the definition of the following concepts: the restrictions of the problem, usually a system of inequalitites, that conform the constraints. an objetive function, which is the function we aim to optimize. the region of feasible solutions that is the set of values for \\(x\\) and \\(y\\) that satisfy all constraints. fundamental theorem of linear programming if the optimal value for a linear programming problem exists, it occurs at a vertex of the region of feasible solutions. solving a linear programming problem write the objective function and all necessary constraints. graph the region of feasible solutions. identify all vertices (corner points). evaluate the objective function at each vertex. the solution is given by the vertex producing the optimal value of the objective function. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/08_01.html",
    "title": "Angles and Their Measures",
    "body": " index search search back angles and their measures contents basic terminology degree measure standard position and coterminal angles radian measure degree to radian conversion arc lengths and areas of sectors linear and angular speed appendix basic terminology two distinct points \\(a\\) and \\(b\\) determine a line called line \\(ab\\). the portion of the line between \\(a\\) and \\(b\\), including points \\(a\\) and \\(b\\), is segment \\(ab\\). the portion of line \\(ab\\) that starts at \\(a\\) and continues through \\(b\\) and on past \\(b\\) is called ray \\(ab\\). point \\(a\\) is the endpoint of the ray. in trigonometry, an angle consists of two rays in a plane with a common endpoint, or two line segments with a common endpoint. these two rays (or segments) are called the sides of the angle and the common endpoint is called the vertex of the angle. associated with an angle is its measure, generated by a rotation about the vertex. this measure is determined by rotating a ray starting at one side of the angle, called the initial side, to the position of the other side, called the terminal side. a counterclockwise rotation generates an angle with positive measure, while a clockwise rotation generates an angle with negative measure. degree measure the most common unit used to measure the size of angles is the degree. we assign \\(360\\) degrees to a complete rotation of a ray. an angle measuring between \\(0\\)Â° and \\(90\\)Â° is an acute angle. an angle measuring exactly \\(90\\)Â° is a right angle. an angle measuring more than \\(90\\)Â° but less than \\(180\\)Â° is an obtuse angle, and an angle of exactly \\(180\\)Â° is a straight angle. if the sum of the measures of two positive angles is \\(90Â°\\), the angles are called complementary and the angles are complements. two positive angles with measures whose sum is \\(180Â°\\) are supplementary and the angles are supplements. one minute, written \\(1'\\), is \\(\\frac{1}{60}\\) of a degree and one second, \\(1''\\), is \\(\\frac{1}{60}\\) of a minute.however angles are commonly measured in decimal degrees. standard position and coterminal angles an angle is in standard position if its vertex is at the origin and its initial side is along the positive x-axis. the angles in (a) and (b) are in standard position. an angle in standard position is said to lie in the quadrant in which its terminal side lies, such as angles with measures \\(90Â°\\), \\(180Â°\\), \\(270Â°\\), and so on, are called quadrantal angles. if the terminal side lies along an axis, then the angle does not lie in any quadrant. for example, an acute angle is in quadrant i (a) and an obtuse angle is in quadrant ii (b). the angles whose measures differ by a multiple of \\(360Âº\\) are called coterminal angles. radian measure an angle with vertex at the center of a circle that intercepts an arc on the circle equal in length to the radius of the circle has measure \\(1\\) radian. an angle \\(\\theta\\) whose vertex is at the center of a circle is called central angle. in general, if \\(\\theta\\) is a central angle in a circle of radius \\(r\\), and \\(\\theta\\) intercepts an arc of lengt \\(s\\), the radian measure of \\(\\theta\\) is \\(\\frac{s}{r}\\). degree to radian conversion we know that the circumference of a circle is given by \\(c = 2\\pi r\\), where \\(r\\) is the radius. this shows that the radius can be laid \\(2\\pi\\) times around the circle. therefore, an angle of \\(360Â°\\), which corresponds to a complete circle, intercepts an arc equal in length to \\(2\\pi\\) times the radius of the circle. thus, an angle of \\(360Âº\\) has measure \\(2\\pi\\). \\begin{align} 360Âº = 2\\pi \\text{ radians} \\end{align} to convert a degree measure to radians multiply the degree measure by \\(\\frac{2\\pi}{360} = \\frac{\\pi}{180}\\) to convert a radian measure to degrees multiply the radian measure by \\(\\frac{360}{2\\pi} = \\frac{180}{\\pi}\\) arc lengths and areas of sectors in the following figure angle \\(qop\\) has measure \\(1\\) radian and intercepts an arc of length \\(r\\). while angle \\(rot\\) has measure \\(\\theta\\) radians and intercepts an arc of length \\(s\\). since the lengths of the arcs are proportional to the measures of their central angles: \\begin{align} \\frac{s}{r} = \\frac{\\theta}{1} \\end{align} \\begin{align} s = r\\theta \\end{align} a sector of a circle is the portion of the interior of a circle intercepted by a central angle. the interior of a circle can be thought of as a sector intercepted by a central angle of measure \\(2\\pi\\) radians. if a central angle for a sector has measure \\(\\theta\\) radians, then the sector makes up the fraction \\(\\frac{\\theta}{2\\pi}\\) of a complete circle. the area inside a circle with radius \\(r\\) is \\(\\mathcal{a} = \\pi r^2\\), therefore, the area of the sector is given by the product of the fraction \\(\\frac{\\theta}{2\\pi}\\) and the total area: \\begin{align} \\mathcal{a} = \\frac{\\theta}{2\\pi}(\\pi r^2) = \\frac{1}{1}r^2 \\theta, \\theta \\text{ in radians} \\end{align} linear and angular speed suppose that point \\(p\\) moves at a constant speed along a circle of radius \\(r\\) and center \\(o\\). the measure of how fast the position of \\(p\\) is changing is called linear speed. if \\(v\\) represents linear speed, then: \\begin{align} v = \\frac{s}{t} \\end{align} where \\(s\\) is the length of the arc traced by point \\(p\\) in time \\(t\\). (this formula is just a restatement of \\(d = rt\\) with \\(s\\) as distance, \\(v\\) as rate (speed), and \\(t\\) as time.) as point \\(p\\) moves along the circle, ray \\(op\\) rotates around the origin, so the speed at which the measure ofthe angle changes is called angular speed, \\(\\omega\\) and is given by: \\begin{align} \\omega = \\frac{\\theta}{t}, \\theta \\text{ in radians} \\end{align} where \\(\\theta\\) is the measure of angle \\(pob\\). to relate linear and angular speeds we use the result \\(s = r\\theta\\), therefor: \\begin{align} v = \\frac{s}{t} = \\frac{r\\theta}{t} = r \\frac{\\theta}{t} = r \\omega \\end{align} appendix latitude gives the measure of a central angle with vertex at earth's center whose initial side goes through the equator and whose terminal side goes through the given location (basically like north-south orientation). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_01.html",
    "title": "The Law of Sines",
    "body": " index search search back the law of sines contents congruency and oblique triangles derivation of the law of sines ambiguous case congruency and oblique triangles we say two trigangles are congruent if their sides and angles are equal. we list the three congruency axioms: side-angle-side (sas): if two sides and the angle between them of a triangle equal two sides and the angles between them of a second triangle, then the triangles are congruent. angle-side-angle (asa): if two angles and the side between them of a triangle equal two angles and the side between them of a second triangle, then the triangles are congruent. side-side-side (sss): if three sides of a triangles are equal to three sides of a second triangle, then the triangles are congruent. an oblique triangle is a triangle that is not a right triangle. to solve an oblique triangle we need the following data: case 1: one side and two angles are known (saa or asa). case 2: two sides and one angle not included between the two sides are known (ssa). this may lead to zero, one or two triangles. this is called the ambiguous case. case 3: two sides and the angle included between are known (sas). case 4: thre sides are known (sss). cases 1 and 2 require the law of sines. cases 3 and 4 require the law of cosines. derivation of the law of sines given an acute triangle (figure 1(a)) or an obtuse triangle (figure 1(b)). we construct the perpendicular from \\(b\\) to side \\(ac\\). let \\(h\\) be the length of the perpendicular. then: \\begin{align} \\sin a = \\frac{h}{c} \\leftrightarrow h = c \\sin a \\end{align} \\begin{align} \\sin c = \\frac{h}{a} \\leftrightarrow h = a \\sin c \\end{align} since \\(h = c \\sin a = a \\sin c\\), then: \\begin{align} a \\sin c = c \\sin a \\end{align} \\begin{align} \\frac{a}{\\sin a} = \\frac{c}{\\sin c} \\end{align} by constructing perpendicular lines from the other vertices, it can be shown that: \\begin{align} \\frac{a}{\\sin a} = \\frac{b}{\\sin b} \\end{align} \\begin{align} \\frac{b}{\\sin b} = \\frac{c}{\\sin c} \\end{align} therefore: \\begin{align} \\frac{a}{\\sin a} = \\frac{b}{\\sin b} = \\frac{c}{\\sin c} \\end{align} ambiguous case if we are given the length of two sides and the angle opposite to one of them then zero, one or two such triangles could exist. this is what we call the ambiguous case. to solve this type of triangle we make use of the following facts: for any angle \\(\\theta\\) of a triangle, \\(0 < \\sin \\theta \\leq 1\\). given an angle \\(\\theta\\) on the triangle where \\(\\sin \\theta = 1\\), then \\(\\theta = 90Âº\\) and the triangle is a right triangle. for any angle \\(\\theta\\) it follows \\(\\sin \\theta = \\sin (180Âº - \\theta)\\) the smallest angle is opposite the shortest side, the largest angle is opposite the longest side and the midle-valued angle is opposite the intermediate side. we know that if \\(a\\) is acute then there are four possible outcomes, whilst if \\(a\\) is obtuse there are two possible outcomes. when \\(a\\) is acute we define the following cases, after applying the law of sines: if \\(\\sin b > 1\\) and \\(a < h< b\\) then there are \\(0\\) possible triangles, as the range of \\(\\sin\\) is \\([-1, 1]\\). if \\(\\sin b = 1\\) and \\(a = b\\) and \\(h < b\\) then there is \\(1\\) possible triangle. if \\(0 < \\sin b < 1\\) and \\(a \\leq b\\) there is \\(1\\) possible triangle. if \\(0 < \\sin b_1 < 1\\), \\(h < a < b\\), there is \\(1\\) possible triangle. note that \\(\\sin b_1 = \\sin (180Âº - b_1)\\), so if if, \\(a + b_2 < 180Âº\\) (which means there is a \\(c\\) such that \\(a + b_2 + c = 180Âº\\)) then there is another possible triangle. when \\(a\\) is obtuse we define the following cases, after applying the law of sines: if \\(\\sin b \\geq 1\\) and \\(a \\leq b\\), then there are zero possible triangles as the range of \\(\\sin\\)is \\([-1, 1]\\). if \\(0 < \\sin b < 1\\) and \\(a > b\\) then there is one possible triangle. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_08.html",
    "title": "Partial Fractions",
    "body": " index search search back partial fractions contents decomposition of rational expressions [[#partial fraction decomposition of \\(\frac{f(x)}{g(x)}\\)|partial fraction decomposition of \\(\frac{f(x)}{g(x)}\\)]] techniques for decomposition into partial fractions for linear factors for quadratic factors decomposition of rational expressions partial fraction decomposition of \\(\\frac{f(x)}{g(x)}\\) if \\(\\frac{f(x)}{g(x)}\\) is not a proper fraction (a fraction whose numerator is lesser degree than it denominator) divide \\(f(x)\\) by \\(g(x)\\). for example \\begin{align} \\frac{x^4 - 3x^3 + x^2 + 5x}{x^2 + 3} = x^2 - 3x - 2 + \\frac{14x + 6}{x^2 + 3} \\end{align} then appy the following steps to the remainder. factor the denominator \\(g(x)\\) into factors of the form \\((ax + b)^m\\) or \\((cx^2 + dx + e)^n\\). for each distinct linear factor \\((ax + b)\\), the decomposition must include the term \\frac{a}{ax + b}. for each repeated linear factor \\((ax + b)^m\\), the decomposition must include the terms: \\begin{align} \\frac{a_1}{(ax + b)} + \\frac{a_2}{(ax + b)^2} + \\cdots + \\frac{a_m}{(ax + b)^m} \\end{align} for each distinct quadratic factor \\((cx^2 + dx + e)\\), the decomposition must include the term \\(\\frac{bx + c}{cx2 + dx + e}\\) for each repeated quadratic factor \\((cx^2 + dx + e)^n\\), the decomposition must include the terms: \\begin{align} \\frac{b_1x + c_1}{(cx^2 + dx + e)} + \\frac{b_2x + c_2}{(cx^2 + dx + e)^2} + \\cdots + \\frac{b_nx + c_n}{(cx^2 + dx + e)^n} \\end{align} use algebraic techniques to solve for the constants in the numerators of the decomposition. techniques for decomposition into partial fractions for linear factors multiply each side of the resulting rational equation by the common denominator. substitute the zero of each factor into the resulting equation. for repeated linear factors, substitute as many other numbers as is necessary to find all the constants in the numerators. the number of substitutions required will equal the number of constants \\(a, b, \\cdots\\). for example: \\begin{align} \\frac{f(x)}{(x-1)(x-2)} = \\frac{a}{(x-1)} + \\frac{b}{(x - 2)} = \\frac{a(x-2) + b(x-1)}{(x-2)(x-1)} \\end{align} multiply both sides by \\((x-1)(x-2)\\): \\begin{align} f(x) = a(x-2) + b(x-1) \\end{align} solve for \\(x = 2\\): \\begin{align} f(2) = a(0) + b(1) \\end{align} and then solve for \\(x = 1\\): \\begin{align} f(1) = a(-1) + b(0) \\end{align} for quadratic factors multiply each side of the resulting rational equation by the common denominator. collect terms on the right side of the equation. equate the coefficients of like terms to get a system of equations. solve the system to find the constants in the numerators. for example: \\begin{align} \\frac{x^2 + 3x - 1}{(x + 1)(x^2 + 2)} = \\frac{a}{(x + 1)} + \\frac{bx + c}{(x^2 + 2)} \\end{align} multiply each side by \\((x + 1)(x^2 + 2)\\): \\begin{align} x^2 + 3x - 1 = a(x^2 + 2) + (bx + c)(x + 1) \\end{align} collect the terms on each side of the equation: \\begin{align} x^2 + 3x - 1 = ax^2 + a2 + bx^2 + bx + cx + c = x^2 (a + b) + x (b + c) + 2a + c \\end{align} equate the coefficients of like terms to get a system of equations: \\begin{align} (a + b) = 1 \\end{align} \\begin{align} (b + c) = 3 \\end{align} \\begin{align} 2a + c = -1 \\end{align} solving this system for \\(a\\), \\(b\\), and \\(c\\) would give the partial fraction decomposition. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/09_01.html",
    "title": "Trigonometric Identities",
    "body": " index search search back trigonometric identities contents fundamental identities reciprocal identities quotient identities pythagorean identities negative-number identities hits for verifying identities the unit circle is shown in figure 1. the arc length is given by \\(s = r\\theta\\) and the radius of the unit circle is \\(1\\), an angle \\(\\theta\\) (in radians) determines an arc of length \\(s = \\theta\\). the arc terminates at a point \\((x, y)\\). angle \\(\\theta\\) determines a corresponding arc of equal length terminating at the point \\((x, -y)\\). therefore: \\begin{align} \\sin \\theta = \\frac{y}{1} = y \\end{align} and \\begin{align} \\sin(-\\theta) = \\frac{-y}{1} = -y \\end{align} thus: \\begin{align} \\sin(-\\theta) = -\\sin \\theta \\end{align} also, by definition: \\begin{align} \\cos(-\\theta) = \\cos \\theta \\end{align} and: \\begin{align} \\tan(-\\theta) = \\frac{\\sin(-\\theta)}{\\cos(-\\theta)} = \\frac{-\\sin(\\theta)}{\\cos(\\theta)} = -\\frac{\\sin(\\theta)}{\\cos(\\theta)} \\end{align} so: \\begin{align} \\tan(-\\theta) = -\\tan \\theta \\end{align} fundamental identities reciprocal identities \\begin{align} \\cot(\\theta) = \\frac{1}{\\tan \\theta} \\end{align} \\begin{align} \\sec(\\theta) = \\frac{1}{\\cos \\theta} \\end{align} \\begin{align} \\csc(\\theta) = \\frac{1}{\\sin \\theta} \\end{align} quotient identities \\begin{align} \\tan(\\theta) = \\frac{\\sin \\theta}{\\cos \\theta} \\end{align} \\begin{align} \\cot(\\theta) = \\frac{\\cos \\theta}{\\sin \\theta} \\end{align} pythagorean identities \\begin{align} \\sin^2 \\theta + \\cos^2 \\theta = 1 \\end{align} \\begin{align} 1 + \\tan^2 \\theta = \\sec^2 \\theta \\end{align} \\begin{align} 1 + \\cot^2 \\theta = \\csc^2 \\theta \\end{align} negative-number identities \\begin{align} \\sin (-\\theta) = - \\sin \\theta \\end{align} \\begin{align} \\cos (-\\theta) = \\cos \\theta \\end{align} \\begin{align} \\tan (-\\theta) = -\\tan \\theta \\end{align} \\begin{align} \\csc (-\\theta) = -\\csc \\theta \\end{align} \\begin{align} \\sec (-\\theta) = \\sec \\theta \\end{align} \\begin{align} \\cot (-\\theta) = -\\cot \\theta \\end{align} hits for verifying identities learn the fundamental identities. try to rewrite the more complicated side of the equation. it is sometimes helpful to express all trigonometric functions in the equation in terms of sine and cosine. usually, any factoring, division involving complex fractions, or indicated algebraic operations should be performed. as you select substitutions, keep in mind the side you are not changing, because it represents your goal. if a fractional expression contains \\(1 + \\sin x\\), multiplying both numerator and denominator by \\(1 - \\sin x\\) would give \\(1 - \\sin^2 x\\), which could be replaced with \\(\\cos^2 x\\). note that verifying identities is not the same as solving equations. one strategy is to work with only one side and rewrite it to match the other side. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/07_01.html",
    "title": "Circles and Parabolas",
    "body": " index search search back circles and parabolas contents conic sections circle general form of the equation of a circle parabola parabola with a horizontal axis and vertex 0 0 equation forms for translated parabolas conic sections parabolas, circles, ellipses, and hyperbolas form a group of curves known as the conic sections, because they are the result of intersecting a cone with a plane. circle a circle is a set of points in a plane that are equidistant from a fixed point. the distance is called the radius of the circle, and the fixed point is called the center. suppose a circle has center \\((h, k)\\) and radius \\(r > 0\\). then the distance between the center \\((h, k)\\) and any point \\((x, y)\\) on the circle must equal \\(r\\). thus, an equation of the circle is as follows: \\begin{align} \\sqrt{(x - h)^2 + (y - k)^2} = r \\end{align} \\begin{align} (x - h)^2 + (y - k)^2 = r^2 \\end{align} therefore the centerâradius form of the equation of a circle with center \\((h, k)\\) and radius \\(r\\) is: \\begin{align} (x - h)^2 + (y - k)^2 = r^2 \\end{align} notice that a circle is the graph of a relation that is not a function. a circle with center \\((0, 0)\\) and radius \\(r\\) has equation: \\begin{align} x^2 + y^2 = r^2 \\end{align} general form of the equation of a circle for real numbers \\(c\\), \\(d\\), and \\(e\\), the equation: \\begin{align} x^2 + y^2 + cx + dy + e = 0 \\end{align} can have a graph that is a circle, that is a point, or that is empty (contains no points.) starting with an equation in this general form, we can work in reverse by completing the square to get an equation of the form: \\begin{align} (x - h)^2 + (y - k)^2 = m \\text{ for some } m \\end{align} there are three possibilities for the graph, based on the value of \\(m\\): if \\(m > 0\\), then \\(r^2 = m\\), and the equation represents a circle with radius \\(\\sqrt{m}\\). if \\(m = 0\\), the equation represents the single point \\((h, k)\\). if \\(m < 0\\), no points satisfy the equation and the graph is empty. parabola a parabola is a set of points in a plane equidistant from a fixed point and a fixed line. the fixed point is called the focus, and the fixed line the directrix, of the parabola. we can find an equation of a parabola from the preceding definition. let the directrix be the line \\(y = -c\\) and the focus be the point \\(f\\) with coordinates \\((0, c)\\). given a point \\(p\\) on the parabola, with coordinates \\((x, y)\\), using the distance formula gives the following result: \\begin{align} d(p, f) = d(p, d) \\end{align} \\begin{align} \\sqrt{(x - 0)^2 + (y - c)^2} = \\sqrt{(x - x)^2 + [y - (-c)]^2} \\end{align} \\begin{align} x^2 + (y - c)^2 = [y +c]^2 \\end{align} \\begin{align} x^2 + (y - c)^2 = (y + c)^2 \\end{align} \\begin{align} x^2 + y^2 + c^2 - 2cy = y^2 + c^2 + 2cy \\end{align} \\begin{align} x^2 = 4cy \\end{align} the focal chord through the focus and perpendicular to the axis of symmetry of a parabola is called the latus rectum, and has length \\(|4c|\\). to see this, note in the previous image that the endpoints of the chord are \\((-x, c)\\) and \\((x, c)\\). let \\(y = c\\) in the equation of the parabola and solve for \\(x\\). \\begin{align} x^2 = 4cy \\end{align} \\begin{align} x^2 = 4c^2 \\end{align} \\begin{align} x = |2c| \\end{align} the length of half the focal chord is |2c| (from x = 0 to x = 2c or x = -2c) , so its full length is |4c|. parabola with a horizontal axis and vertex \\((0, 0)\\) the parabola with focus \\((c, 0)\\) and directrix \\(x = -c\\) has equation: \\begin{align} y^2 = 4cx \\end{align} the parabola has vertex \\((0, 0)\\), horizontal axis \\(y = 0\\), and opens to the right if \\(c > 0\\) or to the left if \\(c < 0\\). notice that the graph of a parabola with a horizontal axis is not a function. equation forms for translated parabolas a parabola with vertex \\((h, k)\\) has an equation of the form: vertical axis: \\begin{align} (x - h)^2 = 4c(y - k) \\end{align} horizontal axis: \\begin{align} (y - k)^2 = 4c(x - h) \\end{align} where the focus is a distance \\(|c|\\) from the vertex: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/09_04.html",
    "title": "The Inverse Circular Functions",
    "body": " index search search back the inverse circular functions contents inverse sine function inverse cosine function inverse tangent function other inverse trigonometric functions inverse sine function applying the horizontal line test, we see that \\(y = \\sin x\\) does not define a one-to-one function. if we restrict the domain to the interval \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) the function is one-to-one and has an inverse function. see figure 16: \\(y = \\sin^{-1} x\\) or \\(y = \\arcsin x\\) means that \\(x = \\sin y\\), for \\(-\\frac{\\pi}{2} \\leq y \\leq \\frac{\\pi}{2}\\) inverse cosine function the function \\(y = \\cos^{-1} x\\) (or \\(y = \\arccos x\\)) is defined by restricting the domain of the function to the interval \\([0, \\pi]\\), as in figure 19. the graph of \\(y = \\cos^{-1} x\\) is shown in figure 20. \\(y = \\cos^{-1} x\\) or \\(y = \\arccos x\\) means that \\(x = \\cos y\\), for \\(0 \\leq y \\leq \\pi\\). inverse tangent function restricting the domain of the function \\(y = \\tan x\\) to the open interval \\((-\\frac{\\pi}{2}, \\frac{\\pi}{2})\\) yields a one-to-one function. figure 23 shows the graph of the restricted tangent function. figure 24 gives the graph of \\(y = \\tan^{-1} x\\). \\(y = \\tan^{-1} x\\) or \\(y = \\arctan x\\) means \\(x = \\tan y\\), for \\(-\\frac{\\pi}{2} < y < \\frac{\\pi}{2}\\) other inverse trigonometric functions \\(y = \\cot^{-1} x\\) or \\(y = arc\\cot x\\) means that \\(x = \\cot y\\), for \\(0 < y < \\pi\\). \\(y = \\sec^{-1} x\\) or \\(y = arc\\sec x\\) means that \\(x = \\sec y\\), for \\(0 \\leq y \\leq \\pi, y \\neq \\frac{\\pi}{2}\\). \\(y = \\csc^{-1} x\\) or \\(y = arc\\csc x\\) means that \\(x = \\csc y\\), for \\(-\\frac{\\pi}{2} \\leq y \\leq \\frac{\\pi}{2}, y \\neq 0\\) finding \\(\\cot^{-1} x\\), \\(\\sec^{-1} x\\), and \\(\\csc^{-1} x\\) can be achieved by expressing these functions in terms of \\(\\tan^{-1} x\\), \\(\\cos{-1} x\\) and \\(\\sin^{-1} x\\). if \\(y = \\sec^{-1} x\\), then \\(\\sec y = x\\), so it follows: \\begin{align} \\sec y = \\frac{1}{\\cos y} = x \\end{align} \\begin{align} \\cos y = \\frac{1}{x} \\end{align} therefore: \\begin{align} y = \\cos^{-1}\\left(\\frac{1}{x}\\right) \\end{align} for the cosecant function: \\begin{align} \\csc y = \\frac{1}{\\sin y} = x \\end{align} \\begin{align} \\sin y = \\frac{1}{x} \\end{align} therefore: \\begin{align} y = \\sin^{-1}\\left(\\frac{1}{x}\\right) \\end{align} finally the inverse cotangent function can be evaluated as \\(90Âº - \\tan^{-1}x\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/index.html",
    "title": "Math",
    "body": " index search search back math pre-calculus a graphical approach to algebra and trigonometry calculus calculus ealy transcendentals $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/Development.html",
    "title": "Development",
    "body": " index search search back development ordered by priority, one choose one at a time backend dev projects do the back end dev course from freecodecamp (max a week) continue the bookish project (add graphql) learn c++ tutorialspoints course (max a week) learn how to use cmake search for projects, some are: data structure data analysis with python course from freecodecamp data visualization course from freecodecamp to search for more project go to project ideas, there are project listings for a lot of languages, frameworks. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/index.html",
    "title": "Study",
    "body": " index search search back study study guides math cs development $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Computer Architecture.html",
    "title": "Computer Architecture",
    "body": " index search search back computer architecture contents resources computer systems a programmer s perspective nand2tetris resources computer systems: a programmer's perspective most courses go from chapter 1 to 6. do this course nand2tetris each chapter involves building a small piece of the overall system, from writing elementary logic gates in hdl, through a cpu and assembler, all the way to an application the size of a tetris game. the elements of computing systems (nand2tetris) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Operating Systems.html",
    "title": "Operating Systems",
    "body": " index search search back operating systems books operating systems: three easy pieces labs: xv6 labs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Mathematics for Computer Science.html",
    "title": "Mathematics for Computer Science",
    "body": " index search search back mathematics for computer science books mit lecture notes video lectures mit video lectures $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Algorithms and Data Structures.html",
    "title": "Algorithms and Data Structures",
    "body": " index search search back algorithms and data structures books the algorithm design manual video lectures skiena's or tim's on coursera practice: leetcode problem solving book after the manual: how to solve it $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/index.html",
    "title": "Computer Science",
    "body": " index search search back computer science source programming computer architecture algorithms and data structures mathematics for computer science operating systems computer networking databases languages and compilers distributed systems ai and machine learning ai and machine learning check the source. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Computer Networking.html",
    "title": "Computer Networking",
    "body": " index search search back computer networking books computer networking: a top-down approach video lectures standford: introduction to computer networking course labs: wireshark labs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Programming.html",
    "title": "Programming",
    "body": " index search search back programming contents notes alternatives books structure and interpretation of computer programs video lectures brian harveyâs sicp lectures resources for the berkley course code online on scheme scheme on arch (download mit/gnu scheme) plan 1 notes notes we recommend working through at least the first three chapters of sicp and doing the exercises. for additional practice, work through a set of small programming problems like those on exercism. alternatives same course but with python instead of scheme (stk) books: composing programs lectures: 61a taught by john denero at berkley. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Databases.html",
    "title": "Databases",
    "body": " index search search back databases contents data modelling start with the recording and the go through the book (paper compilation) video lectures cs186b berkley book readings in database systems data modelling book: data and reality: a timeless perspective on perceiving and managing information in our imprecise world. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Distributed Systems.html",
    "title": "Distributed Systems",
    "body": " index search search back distributed systems books practice oriented: designing data-intensive applications more traditional: distributed systems, 3rd edition $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Languages and Compilers.html",
    "title": "Languages and Compilers",
    "body": " index search search back languages and compilers contents notes books introductory: crafting interpreters as supplementary reference for video lectures: compilers: principles, techniques & tools video lecutres alex aikenâs, on edx notes for introductory book: we suggest taking the time to work through the whole thing, attempting whichever of the \"challenges\" sustain your interest. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/Math.html",
    "title": "Math",
    "body": " index search search back math contents first year first semester second semester second year first semester second semester third year first semester second semester fourth year first semester second semester first year first semester dicrete maths discrete mathematics with applications, 2nd edition by susanna s. epp discrete mathematics structures, 4th edition by kolman, busby and ross proof writing (very advanced, do not expect to master anything in these books) mathematical proofs: a transition to advanced mathematics by gary chartrand et. al (this one is better than the next one) an introduction to abstract mathematics by robert j. bond and wiliam j. keane second semester pre-algebra (refresh really basic math) ags pre-algebra (has solutions) fearon's pre-algebra (this one is better) college algebra (after the pre-algebra one, if the pre-algebra books are too easy skip onto these ones) college algebra by kaufmann (more begginer friendly) college algebra by blitzer second year first semester pre-calculus (once you are done with college algebra. if you know some basic algebra you can skip the college algebra and start in this section) a graphical approach to algebra and trigonometry by hornsby, lial, and rockswold. 6th edition (get the instructor's edition) second semester calculus calculus by james stewart, 5th edition (very famous book, to learn basic calculus. it has a lot of problems. used to teach calculus i, ii and iii) calculus by michael spivak, 3rd edition (it has less material but it is more advanced) third year first semester differential equations a first course in differential equations by zill ordinary differential equations with applications by andrews (it is easier, good for beginners) linear algebra (try to learn as much as possible) elementary linear algebra by howard anton (beginner friendly, with exercises) linear algebra by friedgber, insel, and spence (it is harder and more difficult to read. it is proof based) second semester statistics mathematical statistics by wackerly, mendenhall, and scheaffer a first course in probability by ross complex analysis (calculus with complex numbers. both are pretty much the same, very good beginner books) fundamentals of complex analysis by saff and snider, 3rd edition complex variables and applications by brown and churchill, 7th edition fourth year first semester real analysis (one of the hardest subjects) analysis 1 and analysis 2 by terrance tao (easier to read, but the other two are standard) advanced calculus by fitzpatrick principles of mathematical analysis by rudin elements ofanalysis by ross (expends a lot of time for proofs) abstract algebra (study of groups, rings and fields. very proof based) abstract algebra by saracino (very good for beginners) contemporary abstract algebra by gallian (also good for beginners) second semester topology (optional) introduction to topology by gamelin and greene (it has full solutions for all of the problems) combinatorics (optional) applied combinatorics by tucker naive set theory (optional) naive set theory by halmos functional analysis (optional) functional analysis by kreyszig graph theory (optional) graph theory by gould $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/index.html",
    "title": "Registry Index",
    "body": " index search search registry index notes study $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_04.html",
    "title": "Trigonometric (Polar) Form of Complex Numbers",
    "body": " index search search back trigonometric (polar) form of complex numbers contents the complex plane and vector representation trigonometric polar form converting from rectangular to trigonometric form produc theorem quotient theorem the complex plane and vector representation to graph a complex number we modify the rectangular coordinate system by calling the horizontal axis the real axis and the vertical axis the imaginary axis, such that we obtain the complex plane (figure 46). each complex number \\(a + bi\\) determines a unique position vector \\(\\textbf{op}\\) with initial point \\((0, 0)\\) and terminal point \\((a, b)\\). trigonometric (polar) form the next figure shows the complex number \\(x + yi\\) that corresponds to a vector \\(\\textbf{op}\\) with direction angle \\(\\theta\\) and magnitude \\(r\\), such that: \\begin{align} x = r \\cos \\theta \\end{align} \\begin{align} y = r \\sin \\theta \\end{align} \\begin{align} r = \\sqrt{x^2 + y^2} \\end{align} \\begin{align} \\tan \\theta = \\frac{y}{x}, x \\neq 0 \\end{align} if we substitute \\(x = r \\cos \\theta\\) and \\(y = r \\sin \\theta\\) into \\(x + yi\\) it gives: \\begin{align} x + yi = r \\cos \\theta + r \\sin \\theta i \\end{align} \\begin{align} = r (\\cos \\theta + i \\sin \\theta) \\end{align} this is called the trigonometric form (or polar form) of the complex number \\(x + yi\\). the expression \\(\\cos \\theta + i \\sin \\theta\\) is sometimes abbreviated \\(\\text{cis} \\theta\\), therefore \\(r(\\cos \\theta + i \\sin \\theta) = r \\text{cis} \\theta\\) the number \\(r\\) is the modulus or absolute value of \\(x + yi\\) and \\(\\theta\\) is the argument of \\(x + yi\\). converting from rectangular to trigonometric form sketch a graph of the number \\(x + yi\\) in the complex plane. find \\(r\\) by using the equation \\(r = \\sqrt{x^2 + y^2}\\). find \\(\\theta\\) by using th equation \\(\\tan \\theta = \\frac{y}{x}, x \\neq 0\\) choosing the quadrant indicated in (1). a fractal is a geometric figure with an endless self-similarity property. a fractal image repeats itself infinitely with ever decreasing dimensions. produc theorem given two complex number \\(x + yi\\) and \\(a + bi\\) such that their trigonometric form is given by: \\begin{align} x + yi = r(\\cos \\theta + i \\sin \\theta) \\end{align} and \\begin{align} a + bi = n(\\cos \\phi + i \\sin \\phi) \\end{align} if we multiply their trigonometric forms we obtain: \\begin{align} [r(\\cos \\theta + i \\sin \\theta)][n(\\cos \\phi + i \\sin \\phi)] \\end{align} \\begin{align} = r \\cdot n (\\cos \\theta + i \\sin \\theta)(\\cos \\phi + i \\sin \\phi) \\end{align} \\begin{align} = r \\cdot n (\\cos \\theta \\cdot \\cos \\phi + i \\sin \\theta \\cdot \\cos \\phi + \\cos \\theta \\cdot i \\sin \\phi + i^2 \\sin \\theta \\cdot \\sin \\phi) \\end{align} we know that \\(i^2 = -1\\), and we factor out \\(i\\). \\begin{align} = r \\cdot n (\\cos \\theta \\cdot \\cos \\phi + (-1) \\sin \\theta \\cdot \\sin \\phi + i (\\sin \\theta \\cdot \\cos \\phi + \\cos \\theta \\cdot \\sin \\phi)) \\end{align} \\begin{align} = r \\cdot n (\\cos \\theta \\cdot \\cos \\phi - \\sin \\theta \\cdot \\sin \\phi + i (\\sin \\theta \\cdot \\cos \\phi + \\cos \\theta \\cdot \\sin \\phi)) \\end{align} given \\(\\cos \\theta \\cdot \\cos \\phi - \\sin \\theta \\cdot \\sin \\phi = \\cos (\\theta + \\phi)\\) and \\(\\sin \\theta \\cdot \\cos \\phi + \\cos \\theta \\cdot \\sin \\phi = \\sin (\\theta + \\phi)\\) \\begin{align} = r \\cdot n (\\cos (\\theta + \\phi) + i \\sin (\\theta + \\phi)) \\end{align} in compact form this is written: \\begin{align} = r \\cdot n \\text{cis} (\\theta + \\phi) \\end{align} quotient theorem given two complex number \\(x + yi\\) and \\(a + bi\\) such that their trigonometric form is given by: \\begin{align} x + yi = r(\\cos \\theta + i \\sin \\theta) \\end{align} and \\begin{align} a + bi = n(\\cos \\phi + i \\sin \\phi) \\end{align} if we divide them we obtain: \\begin{align} \\frac{x + yi}{a + bi} = \\frac{(x + yi)(a - bi)}{(a + bi)(a - bi)} \\end{align} \\begin{align} = \\frac{ax + ayi - bxi -ybi^2}{a^2 - b^2i^2} = \\frac{ax + ayi - bxi + yb}{a^2 + b^2} \\end{align} \\begin{align} = \\frac{a(x + yi) + b(y - xi)}{a^2 + b^2} \\end{align} if we substitute their trigonometric forms, knowing that \\(a = n \\cdot \\cos \\phi\\), \\(b = n \\cdot \\sin \\phi\\), \\(x = r \\cdot \\cos \\theta\\) and \\(y = r \\cdot \\sin \\theta\\), then \\begin{align} = \\frac{(n \\cdot \\cos \\phi)(r \\cdot \\cos \\theta + i \\cdot r \\cdot \\sin \\theta) + (n \\cdot \\sin \\phi)(r \\cdot \\sin \\theta - i \\cdot r \\cdot \\cos \\theta)}{(n \\cdot \\cos \\phi)^2 + (n \\cdot \\sin \\phi)^2} \\end{align} we extract \\(n\\) and \\(r\\) as common factors and we expand the denominator: \\begin{align} = \\frac{n \\cdot r [\\cos \\phi \\cdot (\\cos \\theta + i \\cdot \\sin \\theta) + \\sin \\phi \\cdot (\\sin \\theta - i \\cdot \\cos \\theta)]}{n^2 (\\cos^2 \\phi + \\sin^2 \\phi)} \\end{align} we know that \\(\\cos^2 x + \\sin^2 x = 1\\). we multiply the elements on the numerator: \\begin{align} = \\frac{n \\cdot r (\\cos \\phi \\cdot \\cos \\theta + i \\cdot \\cos \\phi \\cdot \\sin \\theta + \\sin \\phi \\cdot \\sin \\theta - i \\sin \\phi \\cdot \\cos \\theta)}{n^2} \\end{align} we rearrange the elements on the numerator and extract \\(i\\) as a common factor: \\begin{align} = \\frac{n \\cdot r [\\cos \\theta \\cdot \\cos \\phi + \\sin \\theta \\cdot \\sin \\phi + i \\cdot (\\sin \\theta \\cdot \\cos \\phi - \\sin \\phi \\cdot \\cos \\theta)]}{n^2} \\end{align} given that \\(\\cos \\theta \\cdot \\cos \\theta + \\sin \\theta \\cdot \\sin \\phi = \\cos (\\theta - \\phi)\\) and \\(\\sin \\theta \\cdot \\cos \\phi - \\sin \\phi \\cdot \\cos \\theta = \\sin (\\theta - \\phi)\\): \\begin{align} = \\frac{n \\cdot r (\\cos (\\theta - \\phi) + i \\cdot \\sin (\\theta - \\phi))}{n^2} \\end{align} \\begin{align} = \\frac{r}{n} (\\cos (\\theta - \\phi) + i \\cdot \\sin (\\theta - \\phi)) \\end{align} in compact form this is written: \\begin{align} = \\frac{r}{n} \\text{cis} (\\theta - \\phi) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_05.html",
    "title": "Power and Roots of Complex Numbers",
    "body": " index search search back power and roots of complex numbers contents powers of complex numbers de moivre s theorem roots of complex numbers finding the roots of complex numbers powers of complex numbers (de moivreâs theorem) consider the following: \\begin{align} r(\\cos \\theta + i \\sin \\theta)^2 = [r(cos \\theta + i \\sin \\theta)][r(cos \\theta + i \\sin \\theta)] \\end{align} by the complex number product theorem: \\begin{align} = r^2 [cos (\\theta + \\theta) + i \\sin (\\theta + \\theta)] \\end{align} \\begin{align} = r^2 (\\cos 2\\theta + i \\sin 2 \\theta) \\end{align} so by de moivre's theorem: if \\(r(\\cos \\theta + i \\sin \\theta)\\) is a complex number and \\(n\\) is any positive integer, then the following holds: \\begin{align} [r(\\cos \\theta + i \\sin \\theta)]^n = r^n (\\cos n\\theta + i \\sin n\\theta) \\end{align} in compact form: \\begin{align} [r\\text{ cis} \\theta]^n = r^n \\text{ cis } n\\theta \\end{align} roots of complex numbers for a positive integer \\(n\\), the complex number \\(a + bi\\) an \\(n\\)th root of the complex number \\(x + yi\\) if the following holds: \\begin{align} (a + bi)^n = x + yi \\end{align} finding the roots of complex numbers to find the three complex cube roots of \\(8(\\cos 135Âº + i \\sin 135Âº)\\) we look for a comple number, \\(r(\\cos \\alpha + i \\sin \\alpha)\\) that satisfies: \\begin{align} [r (\\cos \\alpha + i \\sin \\alpha)]^3 = 8(\\cos 135Âº + i \\sin 135Âº) \\end{align} using de moivre's theorem the expression becomes: \\(r^3(\\cos 3\\alpha + i \\sin 3\\alpha) = 8(\\cos 135Âº + i \\sin 135Âº)\\) the first condition implies: \\begin{align} r^3 = 8 \\leftrightarrow r = 2 \\end{align} the second condition implies: \\begin{align} \\cos 3\\alpha = \\cos 135Âº \\end{align} \\begin{align} \\sin 3\\alpha = \\sin 135Âº \\end{align} such that \\(3\\alpha\\) must represent an angle that is coterminal with \\(135Âº\\), therefore: \\begin{align} 3 \\alpha = 135Âº + 360Âº k, k \\in \\mathbb{z} \\end{align} \\begin{align} \\alpha = \\frac{135Âº + 360Âº k}{3}, k \\in \\mathbb{z} \\end{align} if we let \\(k\\) take on integer values \\(0\\), \\(1\\) and \\(2\\): \\begin{align} \\text{if } k = 0 \\text{, then } \\alpha = 45Âº \\end{align} \\begin{align} \\text{if } k = 1 \\text{, then } \\alpha = 165Âº \\end{align} \\begin{align} \\text{if } k = 2 \\text{, then } \\alpha = 285Âº \\end{align} for \\(k > 2\\) we obtain angles bigger than \\(365Âº\\) that are coterminal with the identified solution. for example, for \\(k = 4\\) we obtain \\(\\alpha = 405Âº\\) that is coterminal with \\(45Âº\\). this previous example represents the \\(n\\)th root theorem that says: if \\(n\\) is a positive integer, \\(r\\) is a positive real number and \\(\\theta\\) is in degrees, then the nonzero complex number \\(r (\\cos \\theta + i \\sin \\theta)\\) has exactly \\(n\\) distinct \\(n\\)th roots, given by: \\begin{align} \\sqrt[n]{r} (\\cos \\alpha + i \\sin \\alpha) \\end{align} \\begin{align} \\sqrt[n]{r} \\text{ cis } \\alpha \\end{align} where: \\begin{align} \\alpha = \\frac{\\theta + 360Âº \\cdot k}{n}, k = 0, 1, \\cdots, n - 1 \\end{align} if \\(\\theta\\) is in radians then: \\begin{align} \\alpha = \\frac{\\theta + 2 \\pi \\cdot k}{n}, k = 0, 1, \\cdots, n - 1 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_06.html",
    "title": "Polar Equations and Graphs",
    "body": " index search search back polar equations and graphs contents polar coordinate system relationships between rectangular and polar coordinates graph of polar equations classifying polar equations circles lemniscates lima ons rose curves polar coordinate system the polar coordinate system is based on a point, called the pole, and a ray, called the polar axis. the polar axis is usually drawn in the direction of the positive x-axis. see figure 56. in figure 57, the pole has been placed at the origin of a rectangular coordinate system. the point \\(p\\) has rectangular coordinates \\((x, y)\\), directed angle \\(\\theta\\) and directed distance \\(r\\). the ordered pair \\((r, \\theta)\\) gives us the polar coordinates. figure 58 shows the rectangular axes over a polar coordinate grid. relationships between rectangular and polar coordinates if a point has rectangular coordinates \\((x, y)\\) and polar coordinates \\((r, \\theta)\\) then: \\begin{align} x = r \\cos \\theta \\end{align} \\begin{align} y = r \\sin \\theta \\end{align} \\begin{align} r^2 = x^2 + y^2 \\end{align} \\begin{align} \\tan \\theta = \\frac{y}{x}, x \\neq 0 \\end{align} note that a point in the plane can only have one pair of rectangular coordinates, however this same point can have infinitely many pairs of polar coordinates. for example \\((2, 30Âº) = (2, 390Âº) = (2, -330Âº) = (-2, 210Âº) = \\cdots\\) (see figure 62). graph of polar equations equations in \\(x\\) and \\(y\\) are called rectangluar (or cartesian) equations, so equations in \\(r\\) and \\(\\theta\\) are called polar equations. \\begin{align} r = 3 \\sin \\theta \\end{align} the rectangular forms of lines and circles can also be defined in terms of polar coordinates, usually obtained by solving for \\(r\\). for a line: \\begin{align} ax + by = c \\end{align} \\begin{align} a(r \\cos \\theta) + b(r \\sin \\theta) = c \\end{align} \\begin{align} r(a \\cos \\theta + b \\sin \\theta) = c \\end{align} \\begin{align} r = \\frac{c}{a \\cos \\theta + b \\sin \\theta} \\end{align} for a circle: \\begin{align} x^2 + y^2 = a^2 \\end{align} \\begin{align} r^2 = a^2 \\end{align} \\begin{align} r = \\pm \\sqrt{a^2} = \\pm a \\end{align} classifying polar equations the table summarizes common polar graphs and forms of their equations. circles \\begin{align} r = a \\cos \\theta \\end{align} \\begin{align} r = a \\sin \\theta \\end{align} lemniscates \\begin{align} r^2 = a^2 \\sin 2 \\theta \\end{align} \\begin{align} r^2 = a^2 \\cos 2 \\theta \\end{align} limaÃ§ons \\begin{align} r = a \\pm b \\sin \\theta \\end{align} \\begin{align} r = a \\pm b \\cos \\theta \\end{align} rose curves $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/10_07.html",
    "title": "More Parametric Equations",
    "body": " index search search back more parametric equations contents parametric equations with trigonometric functions the cycloid applications of parametric equations parametric equations with trigonometric functions if we use trigonometric functions in parametric equations, many interesting curves can be drawn as shown in the following figure: the cycloid the path traced by a fixed point on the circumference of a circle rolling along a line is called a cycloid. a cycloid is defined by: \\begin{align} x = at - a \\sin t \\end{align} \\begin{align} y = a - a \\cos t \\end{align} for \\(t\\) in \\((-\\infty, \\infty)\\). it has an interesting physical property. if a flexible cord goes through points \\(p\\) and \\(q\\), due to the force of gravity, a bead slides without friction along this path from \\(p\\) to \\(q\\). the path that requires least time takes the shape of an inverted cycloid. applications of parametric equations parametric equations are used to simulate motion. if a ball is thrown with an initial velocidy of \\(v_0\\) at an angle \\(\\theta\\) with the horizontal, it position \\((x, y)\\) can be modeled by the parametric equations: \\begin{align} x = (v_0 \\cos \\theta)t \\end{align} \\begin{align} y = (v_0 \\sin \\theta)t - 16t^2 + h \\end{align} where \\(t\\) is in seconds and \\(h\\) is the ball's initial height above the ground. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_01.html",
    "title": "Sequences and Series",
    "body": " index search search back sequences and series contents sequences series summation properties summation rules sequences a sequence is a function that has a set of natural numbers as its domain. instead of using a funcion notation to indicate a sequence \\(f\\), we use \\(a_n\\), where \\(a_n = f(n)\\). the elements in the range of a sequence are called the terms of the sequence, and they are ordered. the general term of the \\(n\\)th term of the sequence is \\(a_n\\). a sequence is a finite sequence if the domain is a finite set \\(\\{1, 2, 3, 4, \\cdots, n\\}\\), where \\(n\\) is a natural number. an infinite sequence has the set of a ll natural numbers as its domain. if the terms of an infinite sequence get closer to some real number, the sequence is said to be convergent and to converge to that real number (see figure 3). a sequence that does not converte to some number is divergent. some sequences are defined by a recursive definition, a definition in which each term is defined as an expression involving the previous term or terms. series the sum of the terms of a sequence is called a series. a finite series is an expression of the form: \\begin{align} s_n = a_1 + a_2 + a_3 + \\cdots + a_n = \\sum_{i=1}^n a_i \\end{align} an infinite series is an expression of the form: \\begin{align} s_{\\infty} = a_1 + a_2 + a_3 + \\cdots + a_n + \\cdots = \\sum_{i=1}^{\\infty} a_i \\end{align} summation properties if \\(a_1, a_2, a_3, \\cdots, a_n\\) and \\(b_1, b_2, b_3, \\cdots, b_n\\) are two sequences, and \\(c\\) is a constante, then for every positive integer \\(n\\), the following hold: \\begin{align} \\sum_{i=1}^n c = n c \\end{align} \\begin{align} \\sum_{i=1}^n ca_i = c \\sum_{i=1}^n a_i \\end{align} \\begin{align} \\sum_{i=1}^n (a_i + b_i) = \\sum_{i=1}^n a_i + \\sum_{i=1}^n b_i \\end{align} \\begin{align} \\sum_{i=1}^n (a_i - b_i) = \\sum_{i=1}^n a_i - \\sum_{i=1}^n b_i \\end{align} summation rules \\begin{align} \\sum_{i=1}^n i = 1 + 2 + \\cdots + n = \\frac{n(n + 1)}{2} \\end{align} \\begin{align} \\sum_{i=1}^n i^2 = 1^2 + 2^2 + \\cdots + n^2 = \\frac{n(n + 1)(2n + 1)}{6} \\end{align} \\begin{align} \\sum_{i=1}^n i^3 = 1^3 + 2^3 + \\cdots + n^3 = \\frac{n^2(n + 1)^2}{4} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_02.html",
    "title": "Arithmetic Sequences and Series",
    "body": " index search search back arithmetic sequences and series contents arithmetic sequences arithmetic series arithmetic sequences a sequence in which each term after the first is obtained by adding a fixed number, the common difference \\(d\\), to a previous term is an arithmetic sequence (or arithmetic progression). the common difference is obtained as: \\begin{align} d = a_{n + 1} - a_n \\end{align} in an arithmetic sequence with first term \\(a_1\\) and common difference \\(d\\), the \\(n\\)th term is: \\begin{align} a_n = a_1 + (n - 1)d \\end{align} to obtain the graph for that sequence: \\begin{align} a_n = a_1 + (n - 1)d \\end{align} \\begin{align} = a_1 + dn - d \\end{align} \\begin{align} = dn + (a_1 - d) \\end{align} \\begin{align} = dn + c \\end{align} where \\(c = a_1 - d\\). such that the points on the graph of an arithmetic sequence \\(f\\) are defined by \\(f(n) = dn + c\\). the following image shows such a graph: arithmetic series the sum of the terms of an arithmetic sequence is an arithmetic series. to obtain the general formula for the arithmetic series we first write the sum of the first \\(n\\) terms as: \\begin{align} s_n = a_1 + (a_1 + d) + (a_1 + 2d) + \\cdots + (a_1 + (n - 1)d) \\end{align} now we write the same sum in reverse order, beginnin with \\(a_n\\) and substracting \\(d\\): \\begin{align} s_n = a_n + (a_n - d) + (a_n - 2d) + \\cdots + (a_n - (n - 1)d) \\end{align} now we add both expressions: \\begin{align} 2 s_n = (a_1 + a_n) + (a_1 + a_n) + \\cdots + (a_1 + a_n) \\end{align} \\begin{align} 2 s_n = n(a_1 + a_n) \\end{align} \\begin{align} s_n = \\frac{n}{2}(a_1 + a_n) \\end{align} \\begin{align} s_n = \\frac{n}{2}(a_1 + a_1 + (n-1)d) \\end{align} \\begin{align} s_n = \\frac{n}{2}(2a_1 + (n-1)d) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_03.html",
    "title": "Geometric Sequences and Series",
    "body": " index search search back geometric sequences and series contents geometric sequences geometric series infinite geometric series geometric sequences a geometric sequence (or geometric progression) is a sequence in which each term after the first is obtained by multiplying the preceding term by a constant non-zero real number, called the common ratio \\(r\\). that is: \\begin{align} r = \\frac{a_{n+1}}{a_n} \\end{align} in a geometric sequence with first term \\(a_1\\) and common ratio \\(r\\), where neither of them are zero, then the \\(n\\)th term is: \\begin{align} a_n = a_1 r^{n-1} \\end{align} geometric series a geometric series is the sum of the terms of a geometric sequence. to find a formula of the sum \\(s_n\\) first we write the sum as follows: \\begin{align} s_n = a_1 + a_2 + \\cdots + a_n \\end{align} \\begin{align} = a_1 + a_1 r + \\cdots + a_1 r^{n-1} \\end{align} we multiply each side of the equation by \\(r\\) \\begin{align} rs_n = a_1r + a_1 r^2 + \\cdots + a_1 r^{n} \\end{align} and now we substract these two equations: \\begin{align} s_n - rs_n = a_1 + (a_1r - a_1r) + (a_1 r^2 - a_1 r^2) + \\cdots + (a_1 r^{n-1} - a_1 r^{n-1}) - a_1 r^{n} \\end{align} \\begin{align} s_n - rs_n = a_1 - a_1 r^{n} \\end{align} \\begin{align} s_n (1 - r) = a_1 - a_1 r^{n} \\end{align} \\begin{align} s_n = \\frac{a_1 - a_1 r^{n}}{1 - r}, r \\neq 1 \\end{align} infinite geometric series if a geometric sequence has first term \\(a_1\\) and common ratio \\(r\\), then: \\begin{align} s_n = \\frac{a_1 - a_1 r^{n}}{1 - r}, r \\neq 1 \\end{align} \\begin{align} s_n = \\frac{a_1(1 - r^{n})}{1 - r}, r \\neq 1 \\end{align} if \\(|r| < 1\\), then \\(\\lim_{n \\rightarrow \\infty}r^n = 0\\) and \\begin{align} \\lim_{n \\rightarrow \\infty} s_n = \\frac{a_1(1 - 0)}{1 - r} = \\frac{a_1}{1 - r} \\end{align} the quotient \\(\\frac{a_1}{1 - r}\\) is called the sum of the term of an infinite geometric sequence. the limit \\(\\lim_{n \\rightarrow \\infty} s_n\\) can be expressed as \\(s_{\\infty}\\) or \\(\\sum_{i=1}^{\\infty} a_i\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_01.html",
    "title": "Real Numbers and the Rectangular System",
    "body": " index search search back real numbers and the rectangular system contents sets of real numbers the rectangular coordinate system pythagorean theorem distance formula midpoint formula sets of real numbers several important sets of numbers are used in mathematics. some of these sets are listed in the following table set description natural numbers \\(\\{1, 2, 3, \\cdots\\}\\) whole numbers \\(\\{0, 1, 2, 3, \\cdots\\}\\) integers \\(\\{\\cdots, -1, 0, 1, 2, 3, \\cdots\\}\\) rational numbers \\(\\{\\frac{p}{q} \\shortmid p \\text{ and } q \\text{ are integers, } q \\neq 0\\}\\) irrational numbers \\(\\{x \\shortmid x \\text { is not rational}\\}\\) real numbers \\(\\{x \\shortmid x \\text { is a decimal number}\\}\\) the rectangular coordinate system if we place two number lines at right angles, intersecting at their origins, we obtain a two-dimensional rectangular coordinate system. this rectangular coordinate system is also called the cartesian coordinate system. pythagorean theorem in a right triangle, the sum of the squares of the lengths of the legs is equal to the square of the length of the hypotenuse. \\begin{align} a^2 + b^2 = c^2 \\end{align} distance formula suppose that \\(p(x_1, y_1)\\) and \\(r(x_2, y_2)\\) are two points in a coordinate plane. then the distance between \\(p\\) and \\(r\\), written \\(d(p, r)\\), is given by the distance formula. \\begin{align} d(p, r) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \\end{align} midpoint formula the midpoint \\(m\\) of the line segment with endpoints \\((x_1, y_1)\\) and \\((x_2, y_2)\\) has the following coordinates. \\begin{align} m = \\left(\\frac{x_1 + x_2}{2}, \\frac{y_1 + y_2}{2}\\right) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_02.html",
    "title": "Introduction to Relations and Functions",
    "body": " index search search back introduction to relations and functions contents set-builder notation and interval notation relations domain and range functions vertical line test function notation set-builder notation and interval notation the following chart summarizes set-builder notation, interval notation, and graphs of intervals of real numbers. it is assumed that \\(a < b\\). relations, domain, and range a relation is a set of ordered pairs. if we denote the ordered pairs of a relation by \\((x, y)\\), then the set of all \\(x\\)-values is called the domain of the relation and the set of all \\(y\\)-values is called the range of the relation. a relation can be represented by any of the following: a graph, as illustrated in figure 17 and figure 18 a table of \\(xy\\)-values, as shown in figure 18(a) an equation, such as \\(y = 2x\\) in figure 18(b) a mapping or diagram, as illustrated in figure 19 functions a function is a relation in which each element in the domain corresponds to exactly one element in the range. in a function, each \\(x\\)-value must correspond to exactly one \\(y\\)-value vertical line test if every vertical line intersects a graph in no more than one point, then the graph is the graph of a function. function notation to say that \\(y\\) is a function of \\(x\\) means that for each value of \\(x\\) from the domain of the function, there is exactly one value of \\(y\\). to emphasize that \\(y\\) is a function of \\(x\\), or that \\(y\\) depends on \\(x\\), it is common to write \\begin{align} y = f(x) \\end{align} with \\(f(x)\\) read \"f of x.\" this notation is called function notation. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_03.html",
    "title": "Linear Functions",
    "body": " index search search back linear functions contents linear function intercepts constant function slope of a line geometric orientation based on slope vertical line slope intercept form of the equation of a line linear function a function \\(f\\) defined by \\(f(x) = ax + b\\), where \\(a\\) and \\(b\\) are real numbers, is called a linear function. graphing linear equations by hand involves plotting points whose coordinates are solutions of the equation and then connecting them with a straight line. from geometry, we know that two distinct points determine a line. therefore, if we know the coordinates of two points, we can graph the line. unless otherwise specified, the domain of a linear function is the set of all real numbers. the range of a nonconstant linear function is also the set of all real numbers. intercepts to find the \\(x\\)-intercept of the graph of \\(y = ax + b\\), let \\(y = 0\\) and solve for \\(x\\) (assuming that \\(a \\neq 0\\)). to find the \\(y\\)-intercept, let \\(x = 0\\) and solve for \\(y\\). let \\(Æ\\) be a function. then any number \\(c\\) for which \\(f(c) = 0\\) is called a zero of the function \\(Æ\\). the point \\((c, 0)\\) is an \\(x\\)-intercept of the graph of \\(f\\). constant function a function \\(f(x) = b\\), where \\(b\\) is a real number, is called a constant function. its graph is a horizontal line with \\(y\\)-intercept \\((0, b)\\). for \\(b \\neq 0\\), it has no \\(x\\)-intercept. (every constant function is also linear.) slope of a line the slope \\(m\\) of the line passing through the points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) is: \\begin{align} m = \\frac{\\delta y}{\\delta x} = \\frac{y_2 - y_1}{x_2 - x_1} \\end{align} where \\(\\delta x = x_2 - x_1 \\neq 0\\) geometric orientation based on slope for a line with slope \\(m\\), if \\(m > 0\\) (i.e., slope is positive), the line rises from left to right. if \\(m < 0\\) (i.e., slope is negative), the line falls from left to right. if \\(m = 0\\) (i.e., slope is \\(0\\)), the line is horizontal. vertical line a vertical line with \\(x\\)-intercept \\((k, 0)\\) has an equation of the form \\(x = k\\). its slope is undefined. slopeâintercept form of the equation of a line in general, if \\(f(x) = ax + b\\), then the slope of the graph of \\(f(x)\\) is \\(a\\) and the \\(y\\)-coordinate of the \\(y\\)-intercept is b. to verify this fact, notice that \\(f(0) = a(0) + b = b\\). because the slope of the graph of \\(f(x) = ax + b\\) is \\(a\\), it is often convenient to use \\(m\\) rather than \\(a\\) in the general form of the equation. therefore, we can write either \\begin{align} f(x) = mx + b \\end{align} or \\begin{align} y = mx + b \\end{align} this equation for \\(f(x)\\) is generally called the slopeâintercept form of the equation of a line. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_04.html",
    "title": "Equations of Lines and Linear Models",
    "body": " index search search back equations of lines and linear models contents point slope form standard form of the equation of a line parallel and perpendicular lines linear models and regression point slope form figure 51 shows a line passing through the fixed point \\((x_1, y_1)\\) with slope \\(m\\). let \\((x, y)\\) be any other point on the line. by the slope formula: \\begin{align} m = \\frac{y - y_1}{x - x_1} \\end{align} \\begin{align} y - y_1 = m(x - x_1) \\end{align} this result is called the pointâslope form of the equation of a line. standard form of the equation of a line a linear equation written in the form: \\begin{align} ax + by = c \\end{align} where \\(a\\), \\(b\\), and \\(c\\) are real numbers (\\(a\\) and \\(b\\) not both \\(0\\)), is said to be in standard form. parallel and perpendicular lines two distinct nonvertical lines are parallel if and only if they have the same slope. two lines, neither of which is vertical, are perpendicular if and only if their slopes have product \\(-1\\). linear models and regression when data points are plotted in the \\(xy\\)-plane, the resulting graph is sometimes called a scatter diagram. scatter diagrams are often helpful for analyzing trends in data graphing calculators are capable of finding the line of \"best fit,\" for the data called the least-squares regression line, by using a technique taught in statistics courses known as least-squares regression. one common measure of the strength of the linear relationship in a data set is called the correlation coefficient, denoted \\(r\\), where \\(-1 \\leq r \\leq 1\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_05.html",
    "title": "Linear Equations and Inequalities",
    "body": " index search search back linear equations and inequalities contents solving linear equations in one variable identities and contradictions analytic approach graphical approaches to solving linear equations intersection-of-graphs method of graphical solution x-intercept method of graphical solution solving linear inequalities in one variable graphical approaches to solving linear inequalities intersection-of-graphs method of solution of a linear inequality x-intercept method of solution of a linear inequality solving linear equations in one variable a linear equation in the variable \\(x\\) is an equation that can be written in the form: \\begin{align} ax + b = 0, a \\neq 0 \\end{align} we use two distinct approaches to solving equations. the analytic approach, in which we use paper and pencil to transform complicated equations into simpler ones. the graphical approach, in which we often support our analytic solutions by using graphs or tables. the words root, solution, and zero all refer to the same basic concept. identities and contradictions a contradiction is an equation that has no solution. an identity is an equation that is true for all values in the domain of its variables. analytic approach one way to solve a given equation analytically is to rewrite it as a series of simpler equivalent equations, each of which has the same solution set as the given one. equivalent equations are obtained by using the properties of equality. graphical approaches to solving linear equations in general, if \\(f\\) and \\(g\\) are linear functions, then their graphs are lines that intersect at a single point, no point, or infinitely many points, as illustrated in figure 67. intersection-of-graphs method of graphical solution to solve the equation \\(f(x) = g(x)\\) graphically, graph \\begin{align} y_1 = f(x) \\end{align} and \\begin{align} y_2 = g(x) \\end{align} the \\(x\\)-coordinate of any point of intersection of the two graphs is a solution of the equation. x-intercept method of graphical solution to solve the equation \\(f(x) = g(x)\\) graphically, graph \\begin{align} y = f(x) - g(x) = f(x) \\end{align} the \\(x\\)-coordinate of any \\(x\\)-intercept of the graph of \\(y = f(x)\\) (or zero of the function \\(f\\)) is a solution of the equation. solving linear inequalities in one variable an inequality says that one expression is greater than, greater than or equal to, less than, or less than or equal to another. two inequalities with the same solution set are equivalent inequalities. inequalities are solved using the properties of inequality: for real numbers \\(a\\), \\(b\\), and \\(c\\): \\(a < b\\) and \\(a + c < b + c\\) are equivalent if \\(c > 0\\), then \\(a < b\\) and \\(ac < bc\\) are equivalent if \\(c < 0\\), then \\(a < b\\) and \\(ac > bc\\) are equivalent similar properties exist for \\(>, \\leq\\) and \\(\\geq\\). a linear inequality in the variable \\(x\\) is an inequality that can be written in one of the following forms, where \\(a \\neq 0\\). \\begin{align} ax + b > 0 \\end{align} \\begin{align} ax + b < 0 \\end{align} \\begin{align} ax + b \\geq 0 \\end{align} \\begin{align} ax + b \\leq 0 \\end{align} the solution set of a linear inequality is typically an interval of the real number line and can be expressed in interval notation. graphical approaches to solving linear inequalities intersection-of-graphs method of solution of a linear inequality suppose that \\(f\\) and \\(g\\) are linear functions. the solution set of \\(f(x) > g(x)\\) is the set of all real numbers \\(x\\) such that the graph of \\(f\\) is above the graph of \\(g\\). x-intercept method of solution of a linear inequality to solve \\(Æ(x) > g(x)\\), we can rewrite the inequality as \\(Æ(x) - g(x) > 0\\) or \\(f(x) > 0\\). such that the solution set of \\(f(x) > 0\\) is the set of all real numbers \\(x\\) such that the graph of \\(f\\) is above the \\(x\\)-axis. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/01_06.html",
    "title": "Applications of Linear Functions",
    "body": " index search search back applications of linear functions contents problem-solving strategies direct variation problem-solving strategies these steps may be helpful in solving application problems. read the problem and make sure you understand it. assign a variable to what you are being asked to find. write an equation that relates the quantities described in the problem. solve the equation and determine the solution to the posed question look back and check your solution. direct variation a common application involving linear functions deals with quantities that vary directly (or are in direct proportion). a number \\(y\\) varies directly with \\(x\\) if there exists a nonzero number \\(k\\) such that: \\begin{align} y = kx \\end{align} the number k is called the constant of variation. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_01.html",
    "title": "Graphs of Basic Functions and Relations: Symmetry",
    "body": " index search search back graphs of basic functions and relations: symmetry contents increasing decreasing and constant functions symmetry with respect to the y -axis symmetry with respect to the origin symmetry with respect to the x -axis even and odd functions the absolute value function increasing, decreasing, and constant functions suppose that a function \\(f\\) is defined over an open interval \\(i\\). \\(f\\) increases on \\(i\\) if, whenever \\(x_1 < x_2\\), \\(f(x_1) < Æ(x_2)\\) \\(f\\) decreases on \\(i\\) if, whenever \\(x_1 < x_2\\), \\(f(x_1) > Æ(x_2)\\) \\(f\\) is constant on \\(i\\) if for every \\(x_1\\), \\(x_2\\), \\(f(x_1) = Æ(x_2)\\) symmetry with respect to the \\(y\\)-axis if a function \\(f\\) is defined so that: \\begin{align} f(-x) = f(x) \\end{align} for all \\(x\\) in its domain, then the graph of \\(f\\) is symmetric with respect to the \\(y\\)-axis. symmetry with respect to the origin if a function \\(f\\) is defined so that: \\begin{align} f(-x) = -f(x) \\end{align} for all \\(x\\) in its domain, then the graph of \\(f\\) is symmetric with respect to the origin. symmetry with respect to the \\(x\\)-axis if replacing \\(y\\) with \\(-y\\) in an equation results in the same equation, then the graph is symmetric with respect to the \\(x\\)-axis. even and odd functions a function \\(f\\) is called an even function if \\(f(-x) = f(x)\\) for all \\(x\\) in the domain of \\(f\\). (its graph is symmetric with respect to the y-axis.) a function \\(f\\) is called an odd function if \\(f(-x) = -f(x)\\) for all \\(x\\) in the domain of \\(f\\). (its graph is symmetric with respect to the origin.) the absolute value function on a number line, the absolute value of a real number x, denoted \\(|x|\\), represents its undirected distance from the origin, \\(0\\). the absolute value function, \\(f(x) = |x|\\), pairs every real number with its absolute value and is defined as follows: \\begin{align} f(x) = |x| = \\begin{cases} x & \\text{ if } x \\geq 0 \\\\ -x & \\text{ if } x < 0 \\\\ \\end{cases} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_02.html",
    "title": "Vertical and Horizontal Shifts of Graphs",
    "body": " index search search back vertical and horizontal shifts of graphs contents vertical shifts horizontal shifts vertical shifts if \\(c > 0\\), then the graph of \\(y = f(x) + c\\) is obtained by shifting the graph of \\(y = f(x)\\) upward a distance of \\(c\\) units. the graph of \\(y = f(x) - c\\) is obtained by shifting the graph of \\(y = f(x)\\) downward a distance of \\(c\\) units. horizontal shifts if \\(c > 0\\), then the graph of \\(y = f(x - c)\\) is obtained by shifting the graph of \\(y = f(x)\\) to the right a distance of \\(c\\) units. the graph of \\(y = f(x + c)\\) is obtained by shifting the graph of \\(y = f(x)\\) to the left a distance of \\(c\\) units. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_03.html",
    "title": "Vertical and Horizontal Shifts of GraphsStretching, Shrinking and Reflecting Graphs",
    "body": " index search search back vertical and horizontal shifts of graphsstretching, shrinking and reflecting graphs contents vertical stretching vertical shrinking horizontal stretching and shrinking reflecting across an axis combining transformations of graphs vertical stretching if \\(c > 1\\), then the graph of \\(y = cf(x)\\) is a vertical stretching of the graph of \\(y = f(x)\\) by applying a factor of \\(c\\). in figure 25, we graphically interpret the preceding statement. notice that the graphs have the same \\(x\\)-intercepts vertical shrinking if \\(0 < c < 1\\), then the graph of \\(y = cf(x)\\) is a vertical shrinking of the graph of \\(y = f(x)\\) by applying a factor of \\(c\\). figure 27 shows a vertical shrink graphically. vertical stretching or shrinking does not change the \\(x\\)-intercepts of the graph but it can change the \\(y\\)-intercept. horizontal stretching and shrinking if \\(0 < c < 1\\), then the graph of \\(y = f(cx)\\) is a horizontal stretching of the graph of \\(y = f(x)\\). if \\(c > 1\\), then the graph of \\(y = f(cx)\\) is a horizontal shrinking of the graph of \\(y = f(x)\\). notice in figure 32 that horizontal stretching or shrinking can change the \\(x\\)-intercepts, but not the \\(y\\)-intercept. reflecting across an axis for a function \\(y = f(x)\\), the following are true. the graph of \\(y = âf(x)\\) is a reflection of the graph of \\(f\\) across the \\(x\\)-axis. the graph of \\(y = f(âx)\\) is a reflection of the graph of \\(f\\) across the \\(y\\)-axis. combining transformations of graphs we can create infinitely many functions from a basic function by stretching or shrinking, shifting upward, downward, left, or right, and reflecting across an axis. to determine the order in which the transformations are made, follow the conventional order of operations as they would be applied to a particular \\(x\\)-value. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_04.html",
    "title": "Absolute Value Functions",
    "body": " index search search back absolute value functions contents the graph of y f x properties of absolute value equations and inequalities involving absolute value the graph of \\(y = â£f(x)â£\\) the domain of \\(y = |f(x)|\\) is the same as the domain of \\(f\\), while the range of \\(y = |f(x)|\\) will be a subset of \\([0, \\infty)\\). properties of absolute value for all real numbers \\(a\\) and \\(b\\): \\(|ab| = |a| |b|\\) \\(|\\frac{a}{b}| = \\frac{|a|}{|b|}\\) \\(|a| = |-a|\\) \\(|a| + |b| \\geq |a + b|\\) equations and inequalities involving absolute value let \\(k\\) be a positive number. to solve \\(|ax + b| = k\\), solve: \\begin{align} a x + b = k, \\text{ or } ax + b = -k \\end{align} to solve \\(|ax + b| > k\\), solve: \\begin{align} a x + b > k, \\text{ or } ax + b < -k \\end{align} to solve \\(|ax + b| < k\\), solve: \\begin{align} a x + b < k, \\text{ or } ax + b > -k \\end{align} inequalities involving \\(\\leq\\) or \\(\\geq\\) are solved similarly. if two quantities have the same absolute value, they must either be equal to each other or be negatives of each other. this fact allows us to solve absolute value equations (and related inequalities) of the form \\(|ax + b| = |cx + d|\\) by solving: \\begin{align} ax + b = cx + d, \\text{ or } ax + b = -(cx + d) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_04.html",
    "title": "Counting Theory",
    "body": " index search search back counting theory contents fundamental principle of counting n-factorial permutations combinations fundamental principle of counting two events are independent events if neither influences the outcome of the other. if \\(n\\) independent events take place, with: \\begin{align} \\begin{matrix} m_1 \\text{ ways for event } 1 \\text{ to occur}, \\\\ m_2 \\text{ ways for event } 2 \\text{ to occur}, \\\\ \\cdot \\\\ \\cdot \\\\ \\cdot \\\\ m_n \\text{ ways for event } n \\text{ to occur}, \\\\ \\end{matrix} \\end{align} then there are: \\begin{align} m_1 \\times m_2 \\times \\cdots \\times m_n \\end{align} different ways for all \\(n\\) events to occur. n-factorial for any positive integer \\(n\\): \\begin{align} n! = n(n-1)(n-2)\\cdots (3)(2)(1) \\end{align} by definition \\(0! = 1\\). permutations a permutation of \\(n\\) elements taken \\(r\\) at a time if one of the possible orderings of \\(r\\) elements from a set of \\(n\\) elements. the number of permutations of \\(r\\) elements on a set of \\(n\\) elements its denoted by \\(p(n, r)\\) and is calculated as follows: \\begin{align} p(n, r) = n(n - 1)(n - 2)(n - r + 1) \\end{align} \\begin{align} = \\frac{n(n - 1)(n - 2)(n - r + 1)(n - r)(n - r - 1) \\cdots (2)(1)}{(n - r)(n - r - 1)\\cdots (2)(1)} \\end{align} \\begin{align} = \\frac{n!}{(n - r)!} \\end{align} altenative notations for \\(p(n, r)\\) are \\(p^n_r\\) and \\(_{n}p_{r}\\). combinations a subset of items selected without regard to order is called a combination. to evaluate \\(c(n, r)\\), we start with the number of permutations, given by \\(p(n, r)\\). to get rid of the repeat orderings we divide by \\(r!\\) (the number of ways to oder the subset of \\(r\\) elements). such that the number of combinations of \\(n\\) elements taken \\(r\\) at a time is obtained as: \\begin{align} c(n, r) = \\frac{p(n, r)}{r!} \\end{align} \\begin{align} = \\frac{n!}{(n - r)!r!} \\end{align} altenative notations for \\(c(n, r)\\) are \\(c^n_r\\) and \\(_{n}c_{r}\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_05.html",
    "title": "Piecewise-Defined Functions",
    "body": " index search search back piecewise-defined functions contents graphing piecewise-defined functions graphing piecewise-defined functions the absolute value function is a simple example of a function defined by different rules (formulas) over different subsets of its domain. such a function is called a piecewise-defined function. see figure 55. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/02_06.html",
    "title": "Operations and Composition",
    "body": " index search search back operations and composition contents operations on functions the difference quotient composition of functions operations on functions given two functions \\(Æ\\) and \\(g\\), for all values of \\(x\\) for which both \\(f(x)\\) and \\(g(x)\\) are defined: \\begin{align} (f + g)(x) = f(x) + g(x) \\end{align} \\begin{align} (f - g)(x) = f(x) - g(x) \\end{align} \\begin{align} (fg)(x) = f(x)g(x) \\end{align} \\begin{align} \\left(\\frac{f}{g}\\right)(x) = \\frac{f(x)}{g(x)}, g(x) \\neq 0 \\end{align} the domains of \\(f + g\\), \\(f - g\\), and \\(fg\\) include all real numbers in the intersection of the domains of \\(f\\) and \\(g\\), while the domain of \\(\\frac{f}{g}\\) includes those real numbers in the intersection of the domains of \\(f\\) and \\(g\\) for which \\(g(x) \\neq 0\\). the difference quotient suppose that the point \\(p\\) lies on the graph of \\(y = f(x)\\) as in figure 68, and suppose that \\(h\\) is a positive number. if we let \\((x, f(x))\\) denote the coordinates of \\(p\\) and \\((x + h, f(x + h))\\) denote the coordinates of \\(q\\), then the line joining \\(p\\) and \\(q\\) has slope: \\begin{align} m = \\frac{f(x + h) - f(x)}{(x + h) - x} = \\frac{f(x + h) - f(x)}{h} \\end{align} this expression, called the difference quotient. figure 68 shows the graph of the line pq (called a secant line. this slope is equal to the average rate of change of \\(f\\) from \\(x\\) to \\(x + h\\). composition of functions the diagram in figure 69 shows a function \\(Æ\\) that assigns, to each \\(x\\) in its domain, a value \\(Æ(x)\\). then another function \\(g\\) assigns, to each \\(f(x)\\) in the domain of \\(g\\), a value \\(g(f(x))\\). this two-step process takes an element \\(x\\) and outputs an element \\(g(f(x))\\). the function with \\(y\\)-values \\(g(f(x))\\) is called the composition of function \\(g\\) and \\(f\\), written \\(g \\circ f\\) and read \"\\(g\\) of \\(f\\)\". $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_01.html",
    "title": "Complex Numbers",
    "body": " index search search back complex numbers contents the imaginary unit i complex numbers simplifying powers of i complex conjugates the imaginary unit i the imaginary unit is defined as: \\begin{align} i = \\sqrt{-1} \\end{align} and therefore \\begin{align} i^2 = -1 \\end{align} complex numbers numbers of the form \\(a + bi\\), where \\(a\\) and \\(b\\) are real numbers, are called complex numbers. in the complex number \\(a + bi\\), \\(a\\) is the real part and \\(b\\) is the imaginary part. two complex numbers \\(a + bi\\) and \\(c + di\\) are equal provided that their real parts are equal and their imaginary parts are equal. for a complex number \\(a + bi\\), if \\(b = 0\\), then \\(a + bi =\\) a, which is a real number. if \\(a = 0\\) and \\(b \\neq 0\\), the complex number is a pure imaginary number. simplifying powers of i by definition, \\(i^1 = i\\) and \\(i^2 = -1\\). now, observe the following pattern. \\begin{align} i^1 = i \\end{align} \\begin{align} i^2 = -1 \\end{align} \\begin{align} i^3 = i^2 i^1 = -i \\end{align} \\begin{align} i^4 = i^2 i^2 = 1 \\end{align} such that: \\begin{align} (i^4)^n = 1 \\end{align} we can then simplify powers of \\(i\\) by considering the other factor. for example, \\begin{align} i^53 = i^52 i^1 = (i^4)^13 i^1 = i \\end{align} complex conjugates the conjugate of the complex number \\(a + bi\\) is \\(a - bi\\). their product is the sum of the squares of their real and imaginary parts. to find the quotient of two complex numbers in standard form, we multiply numerator and denominator by the conjugate of the denominator. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_02.html",
    "title": "Quadratic Functions and Graphs",
    "body": " index search search back quadratic functions and graphs contents quadratic function completing the square graphs of quadratic functions vertex formula extreme values quadratic function the function: \\begin{align} p(x) = ax^2 + bx + c \\end{align} where \\(a\\), \\(b\\), and \\(c\\) are real numbers, with \\(a \\neq 0\\), is called a quadratic function. quadratic functions, as well as linear functions, are examples of polynomial functions. completing the square to transform the quadratic function \\(p(x) = ax^2 + bx + c\\) into the form \\(p(x) = a(x - h)^2 + k\\), follow these steps. divide each side of the equation by \\(a\\) so the coefficient of \\(x^2\\) is \\(1\\). \\begin{align} \\frac{p(x)}{a} = x^2 + \\frac{b}{a}x + \\frac{c}{a} \\end{align} add \\(-\\frac{c}{a}\\) to each side \\begin{align} \\frac{p(x)}{a} - \\frac{c}{a} = x^2 + \\frac{b}{a}x \\end{align} completing the squares: add to each side the square of hald the coefficient of \\(x\\): \\(\\left(\\frac{b}{2a}\\right)^2\\) \\begin{align} \\frac{p(x)}{a} - \\frac{c}{a} + \\left(\\frac{b}{2a}\\right)^2 = x^2 + \\frac{b}{a}x + \\left(\\frac{b}{2a}\\right)^2 \\end{align} factor the right side as the square of a binomial and combine terms on the left. \\begin{align} \\frac{p(x)}{a} - \\frac{c}{a} + \\left(\\frac{b}{2a}\\right)^2 = (x + \\frac{b}{a})^2 \\end{align} isolate the term involving \\(p(x)\\) on the left. \\begin{align} \\frac{p(x)}{a} = (x + \\frac{b}{a})^2 + \\frac{c}{a} - \\left(\\frac{b}{2a}\\right)^2 \\end{align} \\begin{align} = (x + \\frac{b}{a})^2 + \\frac{c}{a} - \\frac{b^2}{4a^2} \\end{align} multiply each side by \\(a\\). \\begin{align} p(x) = a(x + \\frac{b}{a})^2 + c - \\frac{b^2}{4a} \\end{align} \\begin{align} = a(x + \\frac{b}{a})^2 + \\frac{4ac - b^2}{4a} \\end{align} graphs of quadratic functions recall from intercepts that the \\(y\\)-intercept of the graph of an equation is the point that has \\(x\\)-coordinate 0. for a parabola given in the form \\(p(x) = ax^2 + bx + c\\), the \\(y\\)-value of the \\(y\\)-intercept is \\(p(0) = c\\). consider the graph of \\(p(x) = a(x - h)^2 + k (a \\neq 0)\\), then: the graph is a parabola with vertex \\((h, k)\\) and vertical line \\(x = h\\) as its axis of symmetry. the graph opens upward if \\(a > 0\\) and downward if \\(a < 0\\). the graph is wider than the graph of \\(y = x^2\\) if \\(0 < |a| < 1\\) and narrower if \\(|a| > 1\\). vertex formula we can determine the coordinates of the vertex of the graph of a quadratic function by completing the square, as shown earlier. given the standard form of the quadratic function \\(p(x) = ax^2 + bx + c, where a \\neq 0\\), then: \\begin{align} p(x) = a\\left(x - \\left(-\\frac{b}{a}\\right)\\right)^2 + + \\frac{4ac - b^2}{4a} \\end{align} this equation shows that the vertex \\((h, k)\\) can be expressed in terms of \\(a, b\\) and \\(c\\), such that: \\begin{align} h = \\left(-\\frac{b}{a}\\right) \\end{align} and \\begin{align} k = \\frac{4ac - b^2}{4a} \\end{align} extreme values the vertex of the graph of \\begin{align} p(x) = ax^2 + bx + c \\end{align} is the lowest point on the graph of the function if \\(a > 0\\) and the highest point if \\(a < 0\\). such points are called extreme points (also extrema; singular, extremum). if \\(a > 0\\) then the vertex \\((h, k)\\) is called the minimum point of the graph. the minimum value of the function is \\(p(h) = k\\). if \\(a < 0\\) then the vertex \\((h, k)\\) is called the maximum point of the graph. the maximum value of the function is \\(p(h) = k\\). figure 14 illustrates these ideas. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_05.html",
    "title": "The Binomial Theorem",
    "body": " index search search back the binomial theorem contents binomial coefficient pascal s triangle the binomial theorem rth term of a binomial expansion binomial coefficient generalizing, we find that the coefficient for the term of the expansion of \\((x + y)^n\\) in which the variable part is \\(x^ry^{n -r}\\), where \\(r \\leq n\\) is: \\begin{align} \\binom{n}{r} = \\frac{n!}{r!(n - r)!} \\end{align} which is equivalent to \\(c(n ,r)\\). this number is called the binomial coefficient and is often written as \\(\\binom{n}{r}\\). pascal's triangle if we only write the coffiencients for the expansion of \\((x + y)^n\\) we obtain the following pattern: such that each number in the triangle is the sum of the two numbers directly above it. this triangular array of numbers is called pascal's triangle. the binomial theorem our observations about the expansion of \\((x + y)^n\\) are summarized as follows: there are \\(n + 1\\) terms in the expansion. the first term is \\(x^n\\) and the last term is \\(y^n\\) in each term, the exponent on \\(x\\) decreases by \\(1\\), and the exponent on \\(y\\) increases by \\(1\\). the sum of the exponents on \\(x\\) and \\(y\\) in any term is \\(n\\) the coefficient of the term with \\(x^ry^{n - r}\\) o \\(x^{n - r}y^r\\) is \\(\\binom{n}{r}\\) from these observations the binomial theorem is derived: for any positive integer \\(n\\): \\begin{align} (x + y)^n = x^n + \\binom{n}{1} x^{n-1}y + \\binom{n}{2}x^{n-2}y^2 + \\cdots + \\binom{n}{r}x^{n - r}y^r + \\cdots + \\binom{n}{n - 1}xy^{n - 1} + y^n \\end{align} \\begin{align} (x + y)^n = \\sum_{r=0}^n \\binom{n}{r}x^{n - r}y^r \\end{align} rth term of a binomial expansion the \\(r\\)th term of the binomial expansion of \\((x + y)^n\\), where \\(n \\geq r - 1\\) is: \\begin{align} \\binom{n}{r - 1} x^{n - (r - 1)} y^{r - 1} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_03.html",
    "title": "Quadratic Equations and Inequalities",
    "body": " index search search back quadratic equations and inequalities contents quadratic equation in one variable square root property quadratic formula and the discriminant solving quadratic inequalities quadratic equation in one variable an equation that can be written in the form: \\begin{align} ax^2 + bx + c = 0 \\end{align} where \\(a\\) and \\(b\\) are real numbers, with \\(a \\neq 0\\) is a quadratic equation in standard form. figure 25 shows possible numbers of \\(x\\)-intercepts of the graph of a quadratic function that opens upward similarly, figure 26 shows possible numbers of x-intercepts of the graph of a quadratic function that opens downward. thus, a quadratic equation can have zero, one, or two real solutions. square root property the solution set of \\(x^2 = k\\) is one of the following \\(\\{\\pm \\sqrt{k}\\}\\) if \\(k > 0\\) \\(\\{0\\}\\) if \\(k = 0\\) \\(\\{\\pm i\\sqrt{|k|}\\}\\) if \\(k < 0\\) quadratic formula and the discriminant there is a formula that can be used to solve any quadratic equation. to find it, we complete the square on the standard form of \\(ax^2 + bx + c = 0\\). \\begin{align} ax^2 + bx + c = 0 \\end{align} \\begin{align} x^2 + \\frac{b}{a}x + \\frac{c}{a} = 0 \\end{align} \\begin{align} x^2 + \\frac{b}{a}x = -\\frac{c}{a} \\end{align} \\begin{align} x^2 + \\frac{b}{a}x + \\left(\\frac{b}{2a}\\right)^2= -\\frac{c}{a} + \\left(\\frac{b}{2a}\\right)^2 \\end{align} \\begin{align} (x + \\frac{b}{2a})^2 = -\\frac{c}{a} + \\frac{b^2}{4a^2} \\end{align} \\begin{align} x + \\frac{b}{2a} = \\sqrt{-\\frac{c}{a} + \\frac{b^2}{4a^2}} \\end{align} \\begin{align} x + \\frac{b}{2a} = \\pm \\sqrt{\\frac{b^2 - 4ac}{4a^2}} \\end{align} \\begin{align} x + \\frac{b}{2a} = \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a} \\end{align} \\begin{align} x = - \\frac{b}{2a} \\pm \\frac{\\sqrt{b^2 - 4ac}}{2a} \\end{align} \\begin{align} x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\end{align} the expression \\(b^2 â 4ac\\) under the radical in the quadratic formula is called the discriminant. if \\(b^2 - 4ac > 0\\) then there are two real solutions. if \\(b^2 - 4ac = 0\\) then there is one real solutions. if \\(b^2 - 4ac < 0\\) then there are two non-real complex solutions. solving quadratic inequalities a quadratic inequality is an inequality that can be written in the form: \\begin{align} ax^2 + bx + c < 0 \\end{align} where \\(a\\), \\(b\\) and \\(c\\) are real numbers with \\(a \\neq 0\\). we can solve a quadratic inequality graphically, using the ideas shown in the following table. to solve a quadratic inequality analytically, follow these steps. solve the corresponding quadratic equation. identify the intervals determined by the solutions of the equation use a test value from each interval to determine which intervals form the solution set. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_05.html",
    "title": "High-Degree Polynomial Functions and Graphs",
    "body": " index search search back high-degree polynomial functions and graphs contents polynomial function cubic functions quartic functions extrema number of turning points end behaviour x-intercepts real zeros comprehensive graphs polynomial function a polynomial function of degree \\(n\\) in the variable \\(x\\) is a function of the form: \\begin{align} p(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1 x + a_0 \\end{align} where each \\(a_1\\) is a real number, \\(a_n \\neq 0\\) and \\(n\\) is a whole number. the behavior of the graph of a polynomial function is due largely to the value of the coefficient \\(a_n\\) and the parity of the exponent \\(n\\) on the term of greatest degree. for this reason, we will refer to \\(a_n\\) as the leading coefficient and to \\(a_n x_n\\) as the dominating term. the term \\(a_0\\) is the constant term of the polynomial function, and since \\(p(0) = a_0\\), it is the \\(y\\)-value of the \\(y\\)-intercept of the graph. cubic functions a polynomial function of the form: \\begin{align} p(x) = ax^3 + bx^2 + cx + d, a \\neq 0 \\end{align} is a cubic function. quartic functions a polynomial function of the form: \\begin{align} p(x) = ax^4 + bx^3 + cx^2 + dx + e, a \\neq 0 \\end{align} is a quartic function. extrema the graphs for polynomial may have turning points where the function changes from increasing to decreasing or vice versa. let \\(c\\) be in the domain of \\(p\\), then the following hold: \\(p(c)\\) is an absolute maximum if \\(p(c) \\geq p(x)\\) for all \\(x\\) in the domain of \\(p\\) \\(p(c)\\) is an absolute minimum if \\(p(c) \\leq p(x)\\) for all \\(x\\) in the domain of \\(p\\) \\(p(c)\\) is an local maximum if \\(p(c) \\geq p(x)\\) when \\(x\\) is near \\(c\\) \\(p(c)\\) is an local minimum if \\(p(c) \\leq p(x)\\) when \\(x\\) is near \\(c\\) number of turning points the number of turning points of the graph of a polynomial function of degree \\(n \\geq 1\\) is at most \\(n - 1\\). end behaviour suppose that \\(ax^n\\) is the dominating term of a polynomial function \\(p\\) of odd degree. if \\(a > 0\\), then as \\(x \\rightarrow \\infty\\), \\(p(x) \\rightarrow \\infty\\), and as \\(x \\rightarrow -\\infty\\), \\(p(x) \\rightarrow -\\infty\\). therefore, the end behavior of the graph is of the type shown in figure 52(a). if \\(a < 0\\), then as \\(x \\rightarrow \\infty\\), \\(p(x) \\rightarrow -\\infty\\), and as \\(x \\rightarrow -\\infty\\), \\(p(x) \\rightarrow \\infty\\). therefore, the end behavior of the graph is of the type shown in figure 52(b). suppose that \\(ax^n\\) is the dominating term of a polynomial function \\(p\\) of even degree. if \\(a > 0\\), then as \\(|x| \\rightarrow \\infty\\), \\(p(x) \\rightarrow \\infty\\). therefore, the end behavior of the graph is of the type shown in figure 53(a). if \\(a < 0\\), then as \\(|x| \\rightarrow \\infty\\), \\(p(x) \\rightarrow -\\infty\\). therefore, the end behavior of the graph is of the type shown in figure 53(b). x-intercepts (real zeros) the graph of a polynomial function of degree \\(n\\) will have at most \\(n\\) \\(x\\)-intercepts (real zeros). comprehensive graphs the most important features of the graph of a polynomial function are its intercepts, extrema, and end behavior. for this reason, a comprehensive graph of a polynomial function will exhibit the following features. all \\(x\\)-intercepts (if any) the \\(y\\)-intercept all extreme points (if any) enough of the graph to reveal the correct end behavior $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_06.html",
    "title": "Mathematical Induction",
    "body": " index search search back mathematical induction contents principle of mathematical induction method of proof by mathematical induction generalized principle of mathematical induction proof of the binomial theorem principle of mathematical induction let \\(s_n\\) be a statement concerning the positive integer \\(n\\). suppose that both of the following hold: \\(s_1\\) is true. for any positive integer \\(k\\), \\(k \\leq n\\) if \\(s_k\\) is true, then \\(s_{k+1}\\) is also true. then \\(s_n\\) is true for every positive integer \\(n\\). method of proof by mathematical induction prove that the statement is true for \\(n = 1\\). show that for any positive integer \\(k\\), if \\(s_k\\) is true then \\(s_{k+1}\\) is also true. generalized principle of mathematical induction let \\(s_n\\) be a statement concerning the positive integer \\(n\\). let \\(j\\) be a fixed positive integer. suppose that both of the following hold. \\(s_j\\) is true. for any positive integer \\(k\\), \\(k \\geq j\\), \\(s_k\\) implies \\(s_{k+1}\\) then \\(s_n\\) is true for all positive integers \\(n\\), where \\(n \\geq j\\). proof of the binomial theorem the binomial theorem can be proved by mathematical induction. that is, for any positive integer \\(n\\) and any numbers \\(x\\) and \\(y\\), \\begin{align} (x + y)^n = x^n + \\binom{n}{1} x^{n-1}y + \\binom{n}{2} x^{n-2}y^2 + \\cdots + \\binom{n}{r} x^{n - r}y^r + \\cdots + \\binom{n}{n-1}xy^{n-1} + y^n \\end{align} proof let \\(s_n\\) be the previous statement. begin by verifying \\(s_n\\) for \\(n = 1\\). \\begin{align} (x + y)^{n=1} = x + y = \\binom{1}{0}x^1y^0 + \\binom{1}{1} x^0y^1 \\end{align} now we assume \\(s_k\\), such that: \\begin{align} (x + y)^k = x^k + \\frac{k!}{1!(k - 1)!} x^{k-1}y + \\frac{k!}{2!(k - 2)!} x^{k-2}y^2 + \\cdots + \\frac{k!}{r!(k - r)!} x^{k - r}y^r + \\cdots + \\frac{k!}{(k - 1)!1!}xy^{k-1} + y^k \\end{align} we multiply the left side by \\((x + y)\\): \\begin{align} (x + y)(x + y)^k = x(x + y)^k + y(x + y)^k \\end{align} and we apply \\(s_k\\) \\begin{align} x(x + y)^k + y(x + y)^k = x\\left[x^k + \\frac{k!}{1!(k - 1)!} x^{k-1}y + \\frac{k!}{2!(k - 2)!} x^{k-2}y^2 + \\cdots + \\frac{k!}{r!(k - r)!} x^{k - r}y^r + \\cdots + \\frac{k!}{(k - 1)!1!}xy^{k-1} + y^k\\right] + y \\left[x^k + \\frac{k!}{1!(k - 1)!} x^{k-1}y + \\frac{k!}{2!(k - 2)!} x^{k-2}y^2 + \\cdots + \\frac{k!}{r!(k - r)!} x^{k - r}y^r + \\cdots + \\frac{k!}{(k - 1)!1!}xy^{k-1} + y^k\\right] \\end{align} \\begin{align} = \\left[x^{k + 1} + \\frac{k!}{1!(k - 1)!} x^{k}y + \\frac{k!}{2!(k - 2)!} x^{k-1}y^2 + \\cdots + \\frac{k!}{r!(k - r)!} x^{k - r + 1}y^r + \\cdots + \\frac{k!}{(k - 1)!1!}x^2y^{k-1} + xy^k\\right] + \\left[yx^k + \\frac{k!}{1!(k - 1)!} x^{k-1}y^2 + \\frac{k!}{2!(k - 2)!} x^{k-2}y^3 + \\cdots + \\frac{k!}{r!(k - r)!} x^{k - r}y^{r + 1} + \\cdots + \\frac{k!}{(k - 1)!1!}xy^{k} + y^{k+1}\\right] \\end{align} now we group the elements with the same terms: \\begin{align} = x^{k+1} + \\left[\\frac{k!}{1!(k - 1)!} x^{k}y + \\frac{k!}{0!k!} yx^k \\right] + \\left[\\frac{k!}{2!(k - 2)!} x^{k-1}y^2 + \\frac{k!}{1!(k - 1)!} x^{k-1}y^2 \\right] + \\cdots + \\left[\\frac{k!}{r!(k - r)!} x^{(k - r) + 1}y^r + \\frac{k!}{(r - 1)!(k - (r - 1))!}x^{(k - r) + 1}y^{r} \\right] + \\cdots + y^{k + 1} \\end{align} we first show that: \\begin{align} \\frac{k!}{r!(k - r)!} + \\frac{k!}{(r + 1)!(k - (r + 1))!} = \\binom{k + 1}{r + 1} \\end{align} by the definition of the factorial of a number: \\begin{align} = \\frac{k (k - 1) (k - 2) \\cdots (k - r) \\cdots 1}{(r + 1)!(k - r - 1)!} + \\frac{k (k - 1) (k - 2) \\cdots (k - r + 1) \\cdots 1}{r!(k - r)!} \\end{align} \\begin{align} = \\frac{k (k - 1) (k - 2) \\cdots (k - r)}{(r + 1)!} + \\frac{k (k - 1) (k - 2) \\cdots (k - r - 1)}{r!} \\end{align} \\begin{align} = \\frac{k (k - 1) (k - 2) \\cdots (k - r)}{(r + 1)!} + (r + 1)\\frac{k (k - 1) (k - 2) \\cdots (k - r + 1)}{(r + 1)!} \\end{align} \\begin{align} = \\frac{k(k - 1)(k - 2) \\cdots (k - r - 1) \\left[(k - r) + (r + 1)\\right]}{(r + 1)!} \\end{align} \\begin{align} = \\frac{k(k - 1)(k - 2) \\cdots (k - r - 1) \\left[k + 1\\right]}{(r + 1)!} \\end{align} \\begin{align} = \\frac{(k + 1)k(k - 1)(k - 2) \\cdots (k - r - 1)}{(r + 1)!} \\end{align} \\begin{align} = \\frac{(k + 1)k(k - 1)(k - 2) \\cdots (k - r - 1) \\cdots 1}{(r + 1)!(k - r)!} \\end{align} \\begin{align} = \\frac{(k + 1)!}{(r + 1)!((k + 1) - (r + 1))!} \\end{align} \\begin{align} = \\binom{k + 1}{r + 1} \\end{align} so now: \\begin{align} = x^{k+1} + \\left[\\frac{k!}{1!(k - 1)!} + \\frac{k!}{0!k!}\\right] x^{k}y + \\left[\\frac{k!}{2!(k - 2)!} + \\frac{k!}{1!(k - 1)!} \\right] x^{k-1}y^2 + \\cdots + \\left[\\frac{k!}{r!(k - r)!} + \\frac{k!}{(r - 1)!(k - (r - 1))!} \\right] x^{(k - r) + 1} y^r + \\cdots + y^{k + 1} \\end{align} by the previous proof \\(\\frac{k!}{r!(k - r)!} + \\frac{k!}{(r + 1)!(k - (r + 1))!} = \\binom{k + 1}{r + 1}\\), therefore: \\begin{align} = x^{k+1} + \\binom{k + 1}{1} x^{k}y + \\binom{k + 1}{2} x^{k-1}y^2 + \\cdots + \\binom{k + 1}{r!} x^{(k - r) + 1} y^r + \\cdots + y^{k + 1} \\end{align} \\begin{align} = x^{k+1} + \\binom{k + 1}{1} x^{(k + 1) - 1}y + \\binom{k + 1}{2} x^{(k + 1) - 2}y^2 + \\cdots + \\binom{k + 1}{r} x^{(k + 1) - r} y^r + \\cdots + y^{k + 1} \\end{align} \\begin{align} = \\sum_{r = 0}^{k + 1} \\binom{k + 1}{r}x^{(k + 1) - r}y^r \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_06.html",
    "title": "Topics in the Theory of Polynomial Functions (I)",
    "body": " index search search back topics in the theory of polynomial functions (i) contents intermediate value theorem division of polynomials by x k and synthetic division remainder theorem factor theorem division of any two polynomials intermediate value theorem if \\(p(x)\\) defines a polynomial function with only real coefficients, and if, for real numbers \\(a\\) and \\(b\\), the values \\(p(a)\\) and \\(p(b)\\) are opposite in sign, then there exists at least one real zero between \\(a\\) and \\(b\\). division of polynomials by x â k and synthetic division we can use long division to determine whether one whole number is a factor of another. if the degree \\(n\\) polynomial \\(p(x)\\) (where \\(n \\geq 1\\)) is divided by \\(x - k\\), then the quotient polynomial, \\(q(x)\\), has degree \\(n - 1\\). the remainder \\(r\\) is a constant (and may be \\(0\\)). the complete quotient for \\(\\frac{p(x)}{x - k}\\) may be written as: \\begin{align} \\frac{p(x)}{x - k} = q(x) + \\frac{r}{x - k} \\end{align} long division of a polynomial by a binomial of the form \\(x - k\\) can be condensed on the right, exactly the same division is shown without the variables. all the numbers in color on the right are repetitions of the numbers directly above them, so they can be omitted, as shown below on the left. since the coefficient of \\(x\\) in the divisor is always \\(1\\), it can be omitted, too the numbers in color on the left are again repetitions of the numbers directly above them. they may be omitted, as shown on the right. now the problem can be condensed. if the 3 in the dividend is brought down to the beginning of the bottom row, the top row can be omitted, since it duplicates the bottom row to simplify the arithmetic, we replace subtraction in the second row by addition and compensate by changing the \\(-3\\) at the upper left to its additive inverse, \\(3\\). remainder theorem if a polynomial \\(p(x)\\) is divided by \\(x - k\\), the remainder is equal to \\(p(k)\\). by the division algorithm for polynomials: \\begin{align} p(x) = q(x)(x-k) + r \\end{align} \\begin{align} p(k) = q(k)(k-k) + r = r \\end{align} factor theorem a polynomial \\(p(x)\\) has a factor \\(x - k\\) if and only if \\(p(k) = 0\\). by the remainder theorem: \\begin{align} p(k) = r \\end{align} where \\(r\\) is the remainder, which is necessarily \\(0\\). division of any two polynomials let \\(p(x)\\) and \\(d(x)\\) be two polynomials, with the degree of \\(d(x)\\) greater than zero and less than the degree of \\(p(x)\\). then there exist unique polynomials \\(q(x)\\) and \\(r(x)\\) such that: \\begin{align} \\frac{p(x)}{d(x)} = q(x) + \\frac{r(x)}{d(x)} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_07.html",
    "title": "Topics in the Theory of Polynomial Functions (II)",
    "body": " index search search back topics in the theory of polynomial functions (ii) contents conjugate zeros theorem fundamental theorem of algebra number of zeros theorem multiplicity rational zeros theorem descartes rule of signs boundedness theorem conjugate zeros theorem if \\(p(x)\\) is a polynomial function having only real coefficients, and if \\(a + bi\\) is a zero of \\(p(x)\\), then the conjugate \\(a - bi\\) is also a zero of \\(p(x)\\). fundamental theorem of algebra every function defined by a polynomial of degree \\(1\\) or more has at least one complex zero. number of zeros theorem a function defined by a polynomial of degree \\(n\\) has at most \\(n\\) distinct (unique) complex zeros. multiplicity the number of times a zero appears is referred to as the multiplicity of the zero. a zero \\(k\\) of a polynomial function has as multiplicity the exponent of the factor \\(x - k\\). if the zero has multiplicity one, the graph crosses the \\(x\\)-axis at the corresponding \\(x\\)-intercept as seen in figure 74(a) on the next page. if the zero has even multiplicity, the graph is tangent to the \\(x\\)-axis at the corresponding \\(x\\)-intercept (see figure 74(b)). if the zero has odd multiplicity greater than one, the graph crosses the \\(x\\)-axis and is tangent to the \\(x\\)-axis at the corresponding \\(x\\)-intercept. (see figure 74(c)). rational zeros theorem let \\(p(x) = a_nx^n + a_{n-1}x^{n - 1} + \\cdots + a_1x + a_0\\), where \\(a_n \\neq 0\\) and \\(a_0 \\neq 0\\), be a polynomial function with integer coefficients. if \\(\\frac{p}{q}\\) is a rational number written in lowest terms, and if \\(\\frac{p}{q}\\) is a zero of \\(p(x)\\), then \\(p\\) is a factor of the constant term \\(a_0\\), and \\(q\\) is a factor of the leading coefficient \\(a_n\\). proof \\(p(\\frac{p}{q}) = 0\\) since \\(\\frac{p}{q}\\) is a zero of \\(p(x)\\): we substitute \\(x\\) by \\(\\frac{p}{q}\\) \\begin{align} a_n \\left(\\frac{p}{q}\\right)^n + a_{n-1} \\left(\\frac{p}{q}\\right)^{n - 1} + \\cdots + a_{1} \\left(\\frac{p}{q}\\right) + a_0 = 0 \\end{align} \\begin{align} a_n \\left(\\frac{p^n}{q^n}\\right) + a_{n-1} \\left(\\frac{p^{n-1}}{q^{n-1}}\\right) + \\cdots + a_{1} \\left(\\frac{p}{q}\\right) + a_0 = 0 \\end{align} we multiply by \\(q^n\\) and we add \\(-a_0q^n\\) \\begin{align} a_n p^n + a_{n-1} p^{n-1}q + \\cdots + a_{1} p q^{n-1} = -a_0 + q^{n} \\end{align} we factor out \\(p\\) \\begin{align} p(a_n p^{n-1} + a_{n-1} p^{n-2}q + \\cdots + a_{1} q^{n-1}) = -a_0 + q^{n} \\end{align} thus, \\(-a_0q^n\\) equals the product of the two factors, \\(p\\) and \\((a_np^{n-1} + \\cdots + a_1q^{n-1})\\). for this reason, \\(p\\) must be a factor of \\(-a_0q^n\\). since it was assumed that \\(\\frac{p}{q}\\) is written in lowest terms, \\(p\\) and \\(q\\) have no common factor other than \\(1\\), so \\(p\\) is not a factor of \\(q^n\\). thus, \\(p\\) must be a factor of \\(a_0\\). descartes' rule of signs let \\(p(x)\\) be a polynomial function with real coefficients and a nonzero constant term, with terms in descending powers of \\(x\\). the number of positive real zeros either equals the number of variations in sign occurring in the coefficients of \\(p(x)\\) or is less than the number of variations by a positive even integer. the number of negative real zeros either equals the number of variations in sign occurring in the coefficients of \\(p(-x)\\) or is less than the number of variations by a positive even integer. boundedness theorem let \\(p(x)\\) be a polynomial function of degree \\(n \\geq 1\\) with real coefficients and with a positive leading coefficient. suppose \\(p(x)\\) is divided synthetically by \\(x - c\\). if \\(c > 0\\) and all numbers in the bottom row of the synthetic division are nonnegative, then \\(p(x)\\) has no zero greater than \\(c\\). if \\(c < 0\\) and the numbers in the bottom row of the synthetic division alternate in sign (with \\(0\\) considered positive or negative, as needed), then \\(p(x)\\) has no zero less than \\(c\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/03_08.html",
    "title": "Polynomial Equations and Inequalities; Further Applications and Models",
    "body": " index search search back polynomial equations and inequalities; further applications and models contents complex nth roots complex nth roots theorem complex nth roots if \\(n\\) is a positive integer and \\(k\\) is a nonzero complex number, then a solution of \\(x^n = k\\) is called an \\(n\\)th root of \\(k\\). complex nth roots theorem if \\(n\\) is a positive integer and \\(k\\) is a nonzero complex number, then the equation \\(x^n = k\\) has exactly \\(n\\) complex roots. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/04_01.html",
    "title": "Rational Functions and Graphs (I)",
    "body": " index search search back rational functions and graphs (i) contents rational function the reciprocal function rational function a function \\(f\\) of the form: \\begin{align} f(x) = \\frac{p(x)}{q(x)} \\end{align} where \\(p(x)\\) and \\(q(x)\\) are polynomials, with \\(q(x) \\neq 0\\), is called a rational function. the reciprocal function the simplest rational function with a variable denominator is the reciprocal function, defined as: \\begin{align} f(x) = \\frac{1}{x} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/04_02.html",
    "title": "Rational Functions and Graphs (II)",
    "body": " index search search back rational functions and graphs (ii) contents vertical and horizontal asymptotes graphic a rational function behavior of graphs of rational functions near vertical asymptotes behavior of graphs of rational functions near x-intercepts graphs with points of discontinuity vertical and horizontal asymptotes let \\(p(x)\\) and \\(q(x)\\) be polynomials. for the rational function \\(f(x) = \\frac{p(x)}{q(x)}\\), written in lowest terms, and for real numbers \\(a\\) and \\(b\\), if \\(|f(x)| \\rightarrow \\infty\\) as \\(x \\rightarrow a\\), then the line \\(x = a\\) is a vertical asymptote if \\(|f(x)| \\rightarrow b\\) as \\(x \\rightarrow \\infty\\), then the line \\(y = b\\) is a horizontal asymptote to find asymptotes of a rational function defined by a rational expression in lowest terms, use the following procedures: vertical asymptotes are found by determining the values of \\(x\\) that make the denominator, but not the numerator, equal to \\(0\\). other asymptotes: if the numerator has lesser degree than the denominator, then there is a horizontal asymptote \\(y = 0\\) . if the numerator and denominator have the same degree and the function is of the form: \\begin{align} f(x) = \\frac{a_nx^n + \\cdots + a_0}{b_nx^n + \\cdots + b_0}, \\text{ where } b_n \\neq 0 \\end{align} then dividing by \\(x_n\\) in the numberator and denominator produces de horizontal asymptote \\(y = \\frac{a_n}{b_n}\\). if the numerator is of degree exactly one greater than the denominator, then there may be an oblique (or slant) asymptote. to find it, divide the numerator by the denominator and disregard any remainder. set the (linear) polynomial portion of the quotient equal to \\(y\\) to find the equation of the asymptote. graphic a rational function let \\(f(x) = \\frac{p(x)}{q(x)}\\) be a function with the rational expression in lowest terms. to sketch its graph, follow these steps. find the domain and all vertical asymptotes. find any horizontal or oblique asymptote. find the \\(y\\)-intercept, if possible, by evaluating \\(f(0)\\) find the \\(x\\)-intercepts, if any, by solving \\(f(x) = 0\\). determine whether the graph will intersect its nonvertical asymptote \\(y = b\\) by solving \\(f(x) = b\\), where \\(b\\) is the \\(y\\)-value of the horizontal asymptote, or by solving \\(f(x) = mx + b\\), where \\(y = mx + b\\) is the equation of the oblique asymptote. plot selected points as necessary. behavior of graphs of rational functions near vertical asymptotes suppose that \\(f(x)\\) is a rational expression in lowest terms. if \\(n\\) is the largest positive integer such that \\((x - a)^n\\) is a factor of the denominator of \\(f(x)\\), the graph will behave in the manner illustrated near \\(a\\). behavior of graphs of rational functions near x-intercepts suppose that \\(f(x)\\) is a rational expression in lowest terms. if \\(n\\) is the largest positive integer such that \\((x - c)^n\\) is a factor of the numerator of \\(f(x)\\), the graph will behave in the manner illustrated near \\(c\\). graphs with points of discontinuity a rational function that has a common variable factor in the numerator and denominator is not in lowest terms. its graph usually has a hole, or point of discontinuity. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/04_03.html",
    "title": "Rational Equations, Inequalities, Models and Applications",
    "body": " index search search back rational equations, inequalities, models and applications contents solving rational equations and inequalities solving rational equations solving rational inequalities inverse variation joint variation solving rational equations and inequalities a rational equation (or rational inequality) is an equation (or inequality) with at least one term having a variable expression in a denominator or at least one term having a variable expression raised to a negative integer power. solving rational equations determine all values for which the rational equation has undefined expressions. to clear fractions, multiply each side of the equation by the least common denominator of all rational expressions in the equation. solve the resulting equation. reject any values found in step 1. solving rational inequalities rewrite the inequality, if necessary, so that \\(0\\) is on one side and there is a single rational expression on the other side. determine the values that will cause either the numerator or the denominator of the rational expression to equal \\(0\\). these values determine the intervals on the number line to consider. use a test value from each interval to determine which intervals form the solution set. inverse variation when two quantities vary inversely, an increase in one quantity results in a decrease in the other. let \\(x\\) and \\(y\\) denote two quantities and \\(n\\) be a positive number. then \\(y\\) is inversely proportional to the nth power of \\(x\\), or \\(y\\) varies inversely with the nth power of \\(x\\), if there exists a nonzero number \\(k\\) such that: \\begin{align} y = \\frac{k}{x^n} \\end{align} if \\(y = \\frac{k}{x}\\), then \\(y\\) is inversely proportional to \\(x\\), or \\(y\\) varies inversely with \\(x\\). joint variation let \\(m\\) and \\(n\\) be real numbers. then \\(z\\) varies jointly with the \\(n\\)th power of \\(x\\) and the \\(m\\)th power of \\(y\\) if a nonzero real number \\(k\\) exists such that \\begin{align} z = kx^ny^m \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/04_04.html",
    "title": "Functions Defined by Powers and Roots",
    "body": " index search search back functions defined by powers and roots contents power and root functions [[#graphs of \\(f(x) = \\sqrt[n]{ax + b}\\)]] power and root functions a function \\(f\\) of the form: \\begin{align} f(x) = x^b \\end{align} where \\(b\\) is a constant, is a power function. if \\(b = \\frac{1}{m}\\), for some integer \\(n \\geq 2\\), then \\(f\\) is a root function frequently, the domain of a power function \\(f\\) is restricted to nonnegative numbers. suppose the rational number \\(\\frac{p}{q}\\) is written in lowest terms. then the domain of \\(f(x) = x^{\\frac{p}{q}}\\) is all real numbers whenever \\(q\\) is odd and all nonnegative real numbers whenever \\(q\\) is even. if \\(b\\) is a positive irrational number, the domain of \\(f(x) = x^b\\) is all nonnegative real number. graphs of \\(f(x) = \\sqrt[n]{ax + b}\\) when \\(n\\) is even, the graph of the root function \\(f(x) = \\sqrt[n]{x}\\) resembles the graph of the square root function. when \\(n\\) is odd, the graph of the root function \\(f(x) = \\sqrt[n]{x}\\) resembles the graph of the cube root function. to determine the domain of a root function of the form: \\begin{align} f(x) = \\sqrt[n]{ax + b} \\end{align} we must note the parity of \\(n\\): if \\(n\\) is even in \\(\\sqrt[n]{ax + b}\\), then \\(ax + b\\) must be greater than or equal to \\(0\\). if \\(n\\) is odd in \\(\\sqrt[n]{ax + b}\\), then \\(ax + b\\) can be any real number. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/04_05.html",
    "title": "Equations, Inequalities, and Applications Involving Root Functions",
    "body": " index search search back equations, inequalities, and applications involving root functions contents equations and inequalities power property equations and inequalities power property if \\(p\\) and \\(q\\) are algebraic expressions, then every solution of the equation \\(p = q\\) is among the solutions of the equation \\(p^n = q^n\\), for any positive integer \\(n\\). when the power property is used to solve equations, the new equation may have more solutions than the original equation. we call these proposed solutions of the original equation. after applying the power property on equations that contain radicals or rational exponents, it is essential to check all proposed solutions in the original equation. to solve equations involving roots, follow these steps. isolate a term involving a root on one side of the equation raise each side of the equation to a positive integer power that will eliminate the radical or rational exponent. solve the resulting equation. (if a root is still present after step 2, repeat steps 1 and 2.) check each proposed solution in the original equation $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_01.html",
    "title": "Inverse Functions",
    "body": " index search search back inverse functions contents one-to-one functions horizontal line test inverse function finding the equation of the inverse of y Æ x geometric relationship between the graphs of f and f -1 important facts about inverses one-to-one functions a function \\(f\\) is a one-to-one function if, for elements \\(a\\) and \\(b\\) from the domain of \\(f\\): \\begin{align} a \\neq b \\end{align} implies \\begin{align} f(a) \\neq f(b) \\end{align} and by the contrapositive: \\begin{align} f(a) = f(b) \\end{align} implies \\begin{align} a = b \\end{align} a function that is either always increasing or always decreasing on its domain must be one-to-one. horizontal line test a function is one-to-one if every horizontal line intersects the graph of the function at most once. inverse function let \\(f\\) be a one-to-one function. then \\(g\\) is the inverse function of \\(f\\) and \\(f\\) is the inverse function of \\(g\\) if: \\begin{align} (f \\circ g)(x) = x \\text{ for every } x \\text{ in the domain of } g \\end{align} and \\begin{align} (g \\circ x)(x) = x \\text{ for every } x \\text{ in the domain of } f \\end{align} a special notation is often used for inverse functions. if \\(g\\) is the inverse function of \\(Æ\\), then \\(g\\) can be written as \\(f^{-1}\\) (read \"f-inverse\"). by the definition of an inverse function, the domain of \\(f\\) equals the range of \\(f^{-1}\\), and the range of \\(f\\) equals the domain of \\(f^{-1}\\). finding the equation of the inverse of y = Æ(x) for a one-to-one function \\(f\\) defined by an equation \\(y = f(x)\\), find the defining equation of the inverse as follows. (you may need to replace \\(f(x)\\) with \\(y\\) first. any restrictions on \\(x\\) and \\(y\\) should be considered.) interchange \\(x\\) and \\(y\\). solve for \\(y\\). replace \\(y\\) with \\(f^{-1}\\). geometric relationship between the graphs of \\(f\\) and \\(f^{-1}\\) if a function \\(f\\) is one-to-one, then the graph of its inverse \\(f^{-1}\\) is a reflection of the graph of \\(f\\) across the line \\(y = x\\). important facts about inverses if \\(f\\) is one-to-one, then \\(f^{-1}\\) exists the domain of \\(f\\) is equal to the range of \\(f^{-1}\\), and the range of \\(f\\) is equal to the domain of \\(f^{-1}\\). if the point \\((a, b)\\) lies on the graph of \\(f\\), then \\((b, a)\\) lies on the graph of \\(f^{-1}\\). the graphs of \\(f\\) and \\(f^{-1}\\) are reflections of each other across the line \\(y = x\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_02.html",
    "title": "Exponential Functions",
    "body": " index search search back exponential functions contents real-number exponents graphs of exponential functions exponential equations type 1 real-number exponents for any real number \\(a > 0\\), \\(a \\neq 1\\), the following statements are true: \\(a^{x}\\) is a unique real number for each real number \\(x\\). \\(a^b = a^c\\) if and only if \\(b = c\\). if \\(a > 1\\) and \\(m < n\\), then \\(a^m > a^n\\). graphs of exponential functions if \\(a > 0\\) and \\(a \\neq 1\\), then: \\begin{align} f(x) = a^x \\end{align} is the exponential function with base \\(a\\). the behavior of the graph of an exponential function depends, in general, on the magnitude of \\(a\\). as a becomes larger (\\(a > 1\\)), the graph becomes steeper moving to the right of the \\(y\\)-axis. (see figure 16(a)). if the base \\(a\\) is between \\(0\\) and \\(1\\), as a gets closer to \\(0\\), the graph becomes steeper moving to the left of the \\(y\\)-axis. (see figure 16(b)). exponential equations (type 1) on the equation \\(25^x = 125\\) the variable appears in the exponent, we refer to such an equation as a type 1 exponential equation (this is not any type of standard naming, it is just used on this manual). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_03.html",
    "title": "Logarithms and Their Properties",
    "body": " index search search back logarithms and their properties contents logarithm common logarithm natural logarithm properties of logarithms proofs change-of-base rule logarithm for all positive numbers \\(a\\), where \\(a \\neq 1\\), \\begin{align} a^y = x \\end{align} is equivalent to \\begin{align} y = \\log_a x \\end{align} the expression \\(\\log_a x\\) represents the exponent to which the base \\(a\\) must be raised in order to obtain \\(x\\). the number \\(a\\) is called the base of the logarithm, and \\(x\\) is called the argument of the expression. the argument of a logarithm must be a positive number. common logarithm base \\(10\\) logarithms are called common logarithms. the common logarithm of \\(x\\) is written \\(log x\\), where the base is understood to be \\(10\\). natural logarithm logarithms with base \\(e\\) are called natural logarithms. the natural logarithm of a positive number \\(x\\) is written \\(\\ln x\\). properties of logarithms for \\(a > 0\\), \\(a \\neq q\\), and any real number \\(k\\), the following hold. \\(log_a 1 = 0\\) \\(log_a a^k = k\\) \\(a^{\\log_a k} = k ,k > 0\\) product rule \\(\\log_a xy = \\log_a x + \\log_a y\\) quotient rule \\(\\log_a \\frac{x}{y} = \\log_a x - \\log_a y\\) power rule \\(\\log_a x^r = r\\log_a x\\) proofs property \\(1\\) is true because \\(a^0 = 1\\) for any nonzero value of \\(a\\). property \\(2\\) is verified by writing the equation in exponential form. by the definition of the logarithm, if \\(\\log_a a^k = k\\), then \\(a^k = a^k\\), which is true. property \\(3\\) is justified by the fact that \\(\\log_a k\\) is the value we have to raise \\(a\\) to obtain \\(k\\). if we raise \\(a\\) by \\(\\log_a k\\), then by the definition of the logarithm we obtain \\(k\\). the proof of property \\(4\\), the product rule, is as follows: let \\(m = \\log_a x\\) and \\(n = \\log_a y\\), then \\(a^m = x\\) and \\(a^n = y\\) by the definiton of a logarithm. if we multiply them: \\begin{align} a^m a^n = xy \\end{align} \\begin{align} a^{m + n} = xy \\end{align} by the definition of the logarithm: \\begin{align} \\log_a xy = m + n \\end{align} substituting \\(m = \\log_a x\\) and \\(n = \\log_a y\\) \\begin{align} \\log_a xy = \\log_a x + \\log_a y \\end{align} properties \\(5\\) and \\(6\\), the quotient and power rules, are proved in a similar way change-of-base rule for any positive real numbers \\(x\\), \\(a\\), and \\(b\\), where \\(a \\neq 1\\) and \\(b \\neq 1\\) \\begin{align} \\log_a x = \\frac{\\log_b x}{\\log_b a} \\end{align} let \\begin{align} y = \\log_a x \\end{align} by the definition of the logarithm: \\begin{align} a^y = x \\end{align} we apply the logarithm on both sides: \\begin{align} \\log_b a^y = \\log_b x \\end{align} by the power rule: \\begin{align} y \\log_b a = \\log_b x \\end{align} \\begin{align} y = \\frac{\\log_b x}{\\log_b a} \\end{align} substituting \\(y = \\log_a x\\) \\begin{align} \\log_a x = \\frac{\\log_b x}{\\log_b a} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_04.html",
    "title": "Logarithms Functions",
    "body": " index search search back logarithms functions contents logarithmic function graphs of logarithmic functions logarithmic function if \\(a > 0\\), \\(a \\neq 1\\), and \\(x > 0\\), then \\begin{align} f(x) = \\log_a x \\end{align} is the logarithmic function with base \\(a\\). the exponential function \\(f(x) = a^x\\), \\(a > 1\\), is increasing on its domain. if \\(0< a <1\\), the function is decreasing on its domain. thus, for all allowable bases \\(a\\), function \\(f\\) is one-to-one and has an inverse. we can find the rule for \\(f^{-1}\\) analytically: \\begin{align} f(x) = a^x \\end{align} \\begin{align} y = a^x \\end{align} \\begin{align} x = a^y \\end{align} by the definition of the logarithm: \\begin{align} y = \\log_a x \\end{align} \\begin{align} f^{-1}(x) = \\log_a x \\end{align} to confirm this, use properties of logarithms to show that \\((f \\circ f^{-1})(x) = x\\) and \\((f^{-1} \\circ f)(x) = x\\). \\begin{align} (f \\circ f^{-1})(x) = f(f^{-1}(x)) = a^{\\log_a x} = x \\end{align} \\begin{align} (f^{-1} \\circ f)(x) = f^{-1}(f(x)) = \\log_a a^x = x \\end{align} thus, the functions \\(f(x) = a^x\\) and \\(g(x) = log_a x\\) are inverse functions. graphs of logarithmic functions $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_05.html",
    "title": "Exponential and Logarithmic Equations and Inequalities",
    "body": " index search search back exponential and logarithmic equations and inequalities contents properties of logarithmic and exponential functions exponential equations and inequalities type 2 solving exponential and logarithmic equations properties of logarithmic and exponential functions for \\(b > 0\\) and \\(b \\neq 1\\): \\(b^x = b^y\\) if and only if \\(x = y\\). if \\(x > 0\\) and \\(y > 0\\), then \\(log_b x = log_b y\\) if and only if \\(x = y\\) exponential equations and inequalities (type 2) unlike a type 1 exponential equation (or inequality) a type 2 exponential equation (or inequality) is one in which the exponential expressions cannot easily be written as powers of the same base. for example: \\begin{align} 7^x = 12 \\end{align} solving exponential and logarithmic equations an exponential or logarithmic equation can be solved by changing the equation into one of the following forms, where \\(a\\) and \\(b\\) are real numbers, \\(a > 0\\), and \\(a \\neq 1\\). \\(a^{f(x)} = b\\): solve by taking a logarithm of each side. \\(\\log_a f(x) = \\log_a g(x)\\): the equation is satisfied when \\(f(x) = g(x)\\) \\(\\log_a f(x) = b\\): solve by changing to exponential form \\(f(x) = a^b\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/05_06.html",
    "title": "Further Applications and Modeling with Exponential and Logarithmic Functions",
    "body": " index search search back further applications and modeling with exponential and logarithmic functions contents physical science applications physical science applications a function of the form: \\begin{align} a(t) = a_0 e^{kt} \\end{align} where \\(a_0\\) represent the initial quantity present, \\(t\\) represents the time elapsed, \\(k > 0\\) represents the growth constant is called an exponential growth function. a function of the form: \\begin{align} a(t) = a_0 e^{-kt} \\end{align} is an exponential decay function. if a quantity decays exponentially, the amount of time that it takes to reach onehalf its original amount is called the half-life. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/06_01.html",
    "title": "Systems of Equations",
    "body": " index search search back systems of equations contents linear systems substitution method substitution method nonlinear systems linear systems any equation of the form: \\begin{align} a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b \\end{align} for real numbers \\(a_1, a_2, \\cdots, a_n\\) (not all zero) and \\(b\\) is a linear equation or a first-degree equation in \\(n\\) unknowns. a set of equations is called a system of equations. the solutions of a system of equations must satisfy every equation in the system. if all the equations in a system are linear, the system is a system of linear equations, or a linear system. there are three possible outcomes for the graph of a system of two linear equations in two variables: the graphs intersect at exactly one point, which gives the (single) ordered-pair solution of the system. the system is consistent and the equations are independent. (figure 1a) the graphs are parallel lines, so there is no solution and the solution set is \\(\\emptyset\\). the system is inconsistent and the equations are independent. see figure 1(b). the graphs are the same line, and there are infinitely many solutions. the system is consistent and the equations are dependent. see figure 1(c). substitution method in a system of two equations with two variables, the substitution method involves using one equation to find an expression for one variable in terms of the other, and then substituting this expression into the other equation of the system. substitution method another way to solve a system of two equations, the elimination method, uses multiplication and addition to eliminate a variable from one equation. systems that have the same solution set are called equivalent systems. the three transformations allowed are the following: interchange any two equations of the system. multiply or divide any equation of the system by a nonzero real number. replace any equation of the system by the sum of that equation and a multiple of another equation in the system. nonlinear systems a nonlinear system of equations is a system in which at least one of the equations is not a linear equation. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/11_07.html",
    "title": "Probability",
    "body": " index search search back probability contents basic concepts probability of event e complements and venn diagrams odds union of two events properties of probability binomial probability basic concepts given an experiment that has one or more outcomes that are equally likely to occur. then the set \\(s\\) of all possible outcomes of a given experiment is called the sample space for the experiment. for example, the sample space for the experiment of tossing a consists of the outcomes \\(h\\) (heads) and \\(t\\) (tails). this can be written in set notation as: \\begin{align} s = \\{h, t\\} \\end{align} any subset of the sample space is called an event. to represent the number of outcomes that belong to event \\(e\\) we use the notation \\(n(e)\\). the notation \\(p(e)\\) is used for the probability of an event \\(e\\). probability of event e in a sample space with equally likely outcomes, the probability of an event \\(e\\), written \\(p(e)\\) is given by: \\begin{align} p(e) = \\frac{n(e)}{n(s)} \\end{align} where \\(n(e)\\) is the number of outcomes in sample space \\(s\\) that belong to event \\(e\\), and \\(n(s)\\) is the total number of outcomes in sample space \\(s\\). complements and venn diagrams the set of all outcomes in the sample space that do not belong to event \\(e\\) is called the complement of \\(e\\), written \\(e'\\). by definition: \\begin{align} e \\cup e' = s \\end{align} \\begin{align} e \\cap e' = \\emptyset \\end{align} probability concepts can be illustrated using venn diagrams, as shown in figure 23. odds the odds in favor of an event \\(e\\) are expressed as the ratio of \\(p(e)\\) to \\(p(e')\\), or as the fraction \\(\\frac{p(e)}{p(e')}\\). if the odds favoring event \\(e\\) are \\(m\\) to \\(n\\), then: \\begin{align} p(e) = \\frac{m}{m + n} \\end{align} and \\begin{align} p(e') = \\frac{n}{m + n} \\end{align} union of two events two events \\(h\\) and \\(k\\) that cannot occur simultaneously are said to be mutually exclusive, therefore \\(h \\cap k = \\emptyset\\) is always true. this suggests the following proprety: for any events \\(e\\) and \\(f\\): \\begin{align} p(e \\text{ or } f) = p(e \\cup f) = p(e) + p(f) - p(e \\cap f) \\end{align} properties of probability for any events \\(e\\) and \\(f\\) the following hold: \\(0 \\leq p(e) \\leq 1\\) \\(p(\\text{a certain event}) = 1\\) \\(p(\\text{an impossible event}) = 0\\) \\(p(e') = 1 - p(e)\\) \\(p(e \\text{ or } f) = p(e \\cup f) = p(e) + p(f) - p(e \\cap f)\\) binomial probability an experiment that consists of repeated independent trials, which only has two outcomes (success and failure) is called a binomial experiment. let the probability of success in one trial be \\(p\\), such that the probability of failure becomes \\(1 - p\\). then the probability of \\(r\\) successes in \\(n\\) trials is: \\begin{align} \\binom{n}{r} p^r (1 - p)^r \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/B.html",
    "title": "Vectors in Space",
    "body": " index search search back vectors in space contents rectangular coordinates in space distance formula vectors in space vector definitions and operations angle between two vectors direction angles in space rectangular coordinates in space on a three dimensional space, we associate each point with an ordered triple \\((x, y, z)\\) (see figure 1). the region of three-dimensional space where are coordinates are positive is called the first octant. there are eight octants in all. distance formula if \\(p_1(x_1, y_1, z_1)\\) and \\(p_2(x_2, y_2, z_2)\\) are two points in a three-dimensional coordinate system, then the distance between \\(p_1\\) and \\(p_2\\) is given by: \\begin{align} d(p_1, p_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2} \\end{align} vectors in space we denote a vector \\(\\textbf{v}\\) in space with initial point \\(o\\) at the origin as: \\begin{align} \\textbf{v} = \\langle a, b, c \\rangle \\end{align} using the unit vectors \\(\\textbf{i} = \\langle 1, 0, 0 \\rangle\\), \\(\\textbf{j} = \\langle 0, 1, 0 \\rangle\\) and \\(\\textbf{k} = \\langle 0, 0, 1 \\rangle\\), we can represent \\(\\textbf{v}\\) as: \\begin{align} \\textbf{v} = a\\textbf{i} + b \\textbf{j} + c \\textbf{k} \\end{align} where the scalars \\(a\\), \\(b\\) and \\(c\\) are the components of vector \\(\\textbf{v}\\). not all vectors are position vectors, and they are computed differently. for example, the component form of vector \\(\\textbf{pq}\\) is represented as follows: \\begin{align} \\textbf{pq} = \\langle x_2 - x_1, y_2 - y_1, z_2 - z_1 \\rangle \\end{align} as figure 2 suggests, \\(\\textbf{pq}\\) is equal to the following position vector: \\begin{align} \\textbf{or} = (x_2 - x_1)\\textbf{i} + (y_2 - y_1)\\textbf{j} + (z_2 - z_1)\\textbf{k} \\end{align} vector definitions and operations if \\(\\textbf{v} = a\\textbf{i} + b\\textbf{j} + c\\textbf{k}\\) and \\(\\textbf{w} = d\\textbf{i} + e \\textbf{j} + f\\textbf{k}\\) are vectors and \\(g\\) is a scalar, the following hold. \\(\\textbf{v} = \\textbf{w}\\) if and only if \\(a = d\\), \\(b = e\\) and \\(c = f\\). \\(\\textbf{v} + \\textbf{w} = (a + d)\\textbf{i} + (b + e)\\textbf{j} + (c + f)\\textbf{k}\\) \\(\\textbf{v} - \\textbf{w} = (a - d)\\textbf{i} + (b - e)\\textbf{j} + (c - f)\\textbf{k}\\) \\(g\\textbf{v} = ga \\textbf{i} + gb \\textbf{j} + gc \\textbf{k}\\) \\(|\\textbf{v}| = \\sqrt{a^2 + b^2 + c^2}\\) \\(\\textbf{v} \\cdot \\textbf{w} = ad + be + cf\\) angle between two vectors if \\(\\theta\\) is the angle between two nonzero vectors \\(\\textbf{v}\\) and \\(\\textbf{w}\\), where \\(0Âº \\leq \\theta \\leq 180Âº\\), then: \\begin{align} \\cos \\theta = \\frac{\\textbf{v} \\cdot \\textbf{w}}{|\\textbf{v}||\\textbf{w}|} \\end{align} direction angles in space in three dimensions, a vector is determined by its magnitude and three direction angles. as shown in figure 3: \\(\\alpha\\) is the direction angle between \\(\\textbf{v}\\) and the positive \\(x\\)-axis \\(\\beta\\) is the direction angle between \\(\\textbf{v}\\) and the positive \\(y\\)-axis \\(\\gamma\\) is the direction angle between \\(\\textbf{v}\\) and the positive \\(z\\)-axis we can evaluate these angles using the expression for the cosine of the angle between two vectors. note that \\(\\textbf{i} = \\langle 1, 0, 0 \\rangle\\), \\(\\textbf{j} = \\langle 0, 1, 0 \\rangle\\) and \\(\\textbf{k} = \\langle 0, 0, 1 \\rangle\\), where each one has magnitude \\(1\\). for \\(\\textbf{v} = a\\textbf{i} + b \\textbf{j} + c\\textbf{k}\\): \\begin{align} \\cos \\alpha = \\frac{\\textbf{v} \\cdot \\textbf{i}}{|\\textbf{v}||\\textbf{i}|} = \\frac{a}{|\\textbf{v}|} \\end{align} \\begin{align} \\cos \\beta = \\frac{\\textbf{v} \\cdot \\textbf{j}}{|\\textbf{v}||\\textbf{j}|} = \\frac{b}{|\\textbf{v}|} \\end{align} \\begin{align} \\cos \\gamma = \\frac{\\textbf{v} \\cdot \\textbf{k}}{|\\textbf{v}||\\textbf{k}|} = \\frac{c}{|\\textbf{v}|} \\end{align} these quantities, \\(\\cos \\alpha\\), \\(\\cos \\beta\\) and \\(\\cos \\gamma\\) are called direction cosines. and they satisfy: \\begin{align} \\cos^2 \\alpha + \\cos^2 \\beta + \\cos^2 \\gamma = 1 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/C.html",
    "title": "Polar Form of Conic Sections",
    "body": " index search search back polar form of conic sections contents polar forms of conic sections polar forms of conic sections a polar equation of the form: \\begin{align} r = \\frac{ep}{1 \\pm e \\cos \\theta} \\end{align} or \\begin{align} r = \\frac{ep}{1 \\pm e \\sin \\theta} \\end{align} has a conic section as its graph. the eccentricity is \\(e\\) (where \\(e > 0\\)), and \\(|p|\\) is the distance between the pole (focus) and the directrix. we can verify that those equations satisfy the definition of a conic section. consider figure 1, where the directrix is vertical and \\(p > 0\\) units to the right of the focus \\(f(0, 0Âº)\\). let \\(p(r, \\theta)\\) be a point on the graph, then the distance between \\(p\\) and the directrix is obtained as: \\begin{align} pp' = |p - x| \\end{align} \\begin{align} = |p - r \\cos \\theta| \\end{align} we substitute \\(r\\) by \\(\\frac{ep}{1 \\pm e \\cos \\theta}\\). \\begin{align} = |p - \\left(\\frac{ep}{1 \\pm e \\cos \\theta} \\right) \\cos \\theta| \\end{align} \\begin{align} = |\\frac{(1 + e \\cos \\theta) - ep\\cos \\theta}{1 \\pm e \\cos \\theta} | \\end{align} \\begin{align} = |\\frac{p + ep \\cos \\theta - ep\\cos \\theta}{1 \\pm e \\cos \\theta}| \\end{align} \\begin{align} = |\\frac{p}{1 \\pm e \\cos \\theta}| \\end{align} given: \\begin{align} r = \\frac{ep}{1 \\pm e \\cos \\theta} \\end{align} then we can multiply each side by \\(\\frac{1}{e}\\) \\begin{align} \\frac{r}{e} = \\frac{p}{1 \\pm e \\cos \\theta} \\end{align} we substitute this expression for \\(\\frac{r}{e}\\): \\begin{align} pp' = |\\frac{p}{1 \\pm e \\cos \\theta}| = |\\frac{r}{e}| = \\frac{|r|}{|e|} = \\frac{|r|}{e} \\end{align} note that \\(e > 0\\), therefore \\(|e| = e\\). the distance between the pole and \\(p\\) is \\(pf = |r|\\), so the ratio of \\(pf\\) to \\(pp'\\) is: \\begin{align} \\frac{pf}{pp'} = \\frac{|r|}{\\frac{|r|}{e}} = e \\end{align} thus, by definition, the graph has eccentricity \\(e\\) and must be a conic. in the previous discussion, we assumed a vertical directrix to the right of the pole. there a re three other possible situations: equation directrix \\(r = \\frac{ep}{1 + e \\cos \\theta}\\) vertical, \\(p\\) units to the right of the pole \\(r = \\frac{ep}{1 - e \\cos \\theta}\\) vertical, \\(p\\) units to the left of the pole \\(r = \\frac{ep}{1 + e \\sin \\theta}\\) horizontal, \\(p\\) units above the pole \\(r = \\frac{ep}{1 - e \\sin \\theta}\\) horizontal, \\(p\\) units below the pole $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAA/D.html",
    "title": "Rotation of Axes",
    "body": " index search search back rotation of axes contents derivation of rotation equations angle of rotation equations of conics with xy-term derivation of rotation equations given a \\(xy\\)-coordinate system having origin \\(o\\). if we rotate the axes about \\(o\\) through an angle \\(\\theta\\), the new coordinate system is called a rotation of the \\(xy\\)-system. let \\(p\\) be any point other than the origin, with coordinates \\((x, y)\\) in the \\(xy\\)-system and \\((x', y')\\) in the \\(x'y'\\)-system (see figure 1). let \\(op = r\\) and \\(\\alpha\\) be the angle made by \\(op\\) and the \\(x'\\) axis. then the following holds: \\begin{align} \\cos (\\theta + \\alpha) = \\frac{oa}{r} = \\frac{x}{r} \\end{align} \\begin{align} \\sin (\\theta + \\alpha) = \\frac{ap}{r} = \\frac{y}{r} \\end{align} \\begin{align} \\cos (\\alpha) = \\frac{ob}{r} = \\frac{x'}{r} \\end{align} \\begin{align} \\sin (\\alpha) = \\frac{pb}{r} = \\frac{y'}{r} \\end{align} such that we can rewrite the statements as follows: \\begin{align} x = r \\cos (\\theta + \\alpha) \\end{align} \\begin{align} y = r \\sin (\\theta + \\alpha) \\end{align} \\begin{align} x' = r \\cos \\alpha \\end{align} \\begin{align} y' = r \\sin \\alpha \\end{align} therefore: \\begin{align} x = r \\cos (\\theta + \\alpha) \\end{align} \\begin{align} = r (\\cos\\theta \\cos\\alpha - \\sin\\theta \\sin \\alpha) \\end{align} \\begin{align} = \\cos\\theta (r\\cos\\alpha) - \\sin\\theta (r\\sin \\alpha) \\end{align} \\begin{align} = \\cos\\theta x' - \\sin\\theta y' \\end{align} the same goes for \\(y\\) \\begin{align} y = r \\sin (\\theta + \\alpha) \\end{align} \\begin{align} = r (\\sin\\theta \\cos\\alpha + \\cos\\theta \\sin \\alpha) \\end{align} \\begin{align} = \\sin\\theta (r\\cos\\alpha) + \\cos\\theta (r\\sin \\alpha) \\end{align} \\begin{align} = \\sin\\theta x' + \\cos\\theta y' \\end{align} angle of rotation the \\(xy\\)-term is removed from the standard equation: \\begin{align} ax^2 + bxy + cy^2 + dx + ey + f = 0 \\end{align} by a rotation of the axes through an angle \\(\\theta\\), \\(0Âº < \\theta < 90Âº\\), where: \\begin{align} \\cot 2\\theta = \\frac{a - c}{b} \\end{align} equations of conics with xy-term if the standard second-degree equation: \\begin{align} ax^2 + bxy + cy^2 + dx + ey + f = 0 \\end{align} has a graph, it will be one of the following: a circle or an ellipse (or a point) if \\(b^2 - 4ac < 0\\) a parabola (or one line or two parallel lines) if \\(b^2 - 4ac = 0\\) a hyperbola (or two intersecting lines) if \\(b^2 - 4ac > 0\\) a straight line if \\(a = b = c = 0\\) and \\(d \\neq 0\\) or \\(e \\neq 0\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/index.html",
    "title": "Infraestructuras Computacionales para Procesamiento de Datos Masivos",
    "body": " index search search back infraestructuras computacionales para procesamiento de datos masivos mÃ³dulo 1 tema 1. introducciÃ³n a big data tema 2. el nÃºcleo de hadoop tema 3. programaciÃ³n mapreduce tema 4. inyecciÃ³n extracciÃ³n y serializaciÃ³n/deserializaciÃ³n de los datos mÃ³dulo 2 tema 1. introducciÃ³n a apache spark tema 2. programaciÃ³n en aplicaciones spark tema 3. librerÃ­as/componentes de spark tema 4. configuraciÃ³n, monitorizaciÃ³n y optimizaciÃ³n de spark mÃ³dulo 3 tema 1. introducciÃ³n a las arquitecturas de procesamiento de streams: lambda y kappa tema 2. componentes tecnolÃ³gicos de adquisiciÃ³n y transmisiÃ³n/distribuciÃ³n de eventos: kafka tema 3. procesamiento de streams: apache spark streaming mÃ³dulo 4 tema 1. proveedores de soluciones: aws $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/01_01.html",
    "title": "Four Ways to Represent a Function",
    "body": " index search search back four ways to represent a function contents representation of functions functions arise whenever one quantity depends on another. a symbol that represents an arbitrary number in the domain of a function \\(f\\) is called an independent variable. a symbol that represents a number in the range of f is called a dependent variable. one way to visualize a function is to think of it as a machine (see figure 2). another way to picture a function is by an arrow diagram as in figure 3. perhaps the most useful method for visualizing a function is its graph (see figure 4). if \\(f\\) is a function with domain \\(d\\), then its graph is the set of ordered pairs \\begin{align} \\{(x, f(x)) | x \\in d\\} \\end{align} representation of functions we consider four different ways to represent a function: verbally: by a description in words. numerically: by a table of values. visually: by a graph. algebraically: by an explicit formula. a function with an explicit formula that approximates the behavior of a given \"true\" function its called a mathematical model. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/index.html",
    "title": "Calculus Ealy Transcendentals",
    "body": " index search search back calculus ealy transcendentals functions and models four ways to represent a function mathematical models: a catalog of essential functions new functions from old functions exponential functions inverse functions and logarithms limits and derivatives the tangent and velocity problems the limit of a function calculating limits using the limits laws the precise definition of a limit continuity limits at infinity. horizontal asymptotes derivatives and rates of change the derivative as a function $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/01_02.html",
    "title": "Mathematical Models: A Catalog of Essential Functions",
    "body": " index search search back mathematical models: a catalog of essential functions contents mathematical model algebraic function faimilies of essential functions and their graphs mathematical model a mathematical model is a mathematical description (generally by the means of a function or an equation) of a real-world phenomenon. its purpose is to understan the phenomenon and perhaps make predictions about future behaviour. the process of mathematical modeling is a follows: formulate a mathematical model by identifying the independent and dependent variables and making assumptions that simplify the phenomenon. apply the mathematics that we know to derive mathematical conclusions. take those mathematics conclusions and interpret them as information about the original real-world phenomenon. test our predictions against real-world data. this process is illustrated on the following figure. if there is no physical law or principle to help us formulate a model, we construct an empirical model that is a model that captures the basic trend of the data points. algebraic function a function \\(f\\) is an algebraic function if it can be constructed using algebraic operations (such as addition, substraction, multiplication, division and taking roots). functions that are not algebraic are called transcendental, these include trigonometric, exponential and logarithmic functions. faimilies of essential functions and their graphs on the following table we show a summary of graphs of some families of essential functions. function form graph linear function \\(f(x) = mx + b\\) power function \\(f(x) = x^n\\) root function \\(f(x) = \\sqrt[n]{x}\\) reciprocal function \\(f(x) = \\frac{1}{\\sqrt[n]{x}}\\) exponential function \\(f(x) = b^x\\) logarithmic function \\(f(x) = \\log_b x\\) trigonimetric functions \\(f(x) = \\sin x\\) trigonimetric functions \\(f(x) = \\cos x\\) trigonimetric functions \\(f(x) = \\tan x\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/01_03.html",
    "title": "New Functions from Old Functions",
    "body": " index search search back new functions from old functions see the following for the theory on function transformations: vertical and horizontal shifts of graphs stretching, shrinking and reflecting graphs and for more information on operations and composition. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/01_04.html",
    "title": "Exponential Functions",
    "body": " index search search back exponential functions contents the number e the definition of an exponential function. the number e figures 12 and 13 show the tangent lines to the graphs of \\(y = 2^x\\) and \\(y = 3^x\\) at the point \\((0, 1)\\). the slopes for these tangent lines are \\(m \\approx 0.7\\) and \\(m \\approx 1.1\\) respectively. some formulas of calculus will be simplified if we choose the base \\(b\\) so that the slope of the tangent line to \\(y = b^x\\) at \\((0, 1)\\) is exactly \\(1\\) (see figure 14). such a base is denoted by the letter \\(e\\). this notation was chosen by the swiss mathematician leonhard euler in 1727. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/01_05.html",
    "title": "Inverse Functions and Logarithms",
    "body": " index search search back inverse functions and logarithms contents inverse functions cancellation equations graphing the inverse logarithmic functions natural logarithms inverse trigonometric functions inverse sine function inverse cosine function inverse tangent function other inverse functions inverse functions we know that a function is one-to-one graphically by applying the horizontal line test. any function \\(f\\) has an inverse if and only if \\(f\\) is one-to-one. cancellation equations given a function \\(f: a \\rightarrow b\\) whose inverse \\(f^{-1}\\) exists, then its cancellation equations are defined as follows: \\begin{align} f^{-1}(f(x)) = x, \\forall x \\in a \\end{align} \\begin{align} f(f^{-1}(x)) = x, \\forall x \\in b \\end{align} on the first cancellation equation \\(f^{-1}\\) undoes what \\(f\\) does, and viceversa for the second equation. graphing the inverse the process of finding the inverse gives us a method for obtaining the graph of \\(f^{-1}\\) from the graph of \\(f\\). since \\(f(a) = b\\) if and only if \\(f^{-1}(b) = a\\) then the point \\((a, b)\\) is on the graph of \\(f\\) if and only if the point \\((b, a)\\) is on the graph \\(f^{-1}\\). but we get the point \\((b, a)\\) from \\((a, b)\\) by reflecting about the line \\(y = x\\) (see figure 8). so the graph of \\(f^{-1}\\) is obtained by reflecting the graph of \\(f\\) about the line \\(y = x\\). logarithmic functions if \\(b > 0\\) and \\(b \\neq 1\\) then the exponential function \\(f(x) = b^x\\) is either increasing or decreasing, and so it is one-to-one. therefore its inverse exists and is called the logarithmic function with base \\(b\\): \\begin{align} \\log_b x = y \\leftrightarrow b^y = x \\end{align} if we apply the cancellation equations we obtain: \\begin{align} \\log_b(b^x) = x, \\forall x \\in \\mathbb{r} \\end{align} \\begin{align} b^{\\log_b x} = x, \\forall x > 0 \\end{align} the logarithmic function has domain \\((0, \\infty)\\) and range \\(\\mathbb{r}\\). its graph is the reflection of the graph of \\(y = b^x\\) about the line \\(y = x\\) (see figure 11). natural logarithms the logarithm with base \\(e\\) is called the natural logarithm and is denoted as: \\begin{align} \\log_e x = \\ln x \\end{align} if we apply the cancellation equations we obtain: \\begin{align} \\ln(e^x) = x, x \\in \\mathbb{r} \\end{align} \\begin{align} e^{\\ln x} = x, x > 0 \\end{align} in particular if we set \\(x = 1\\) we get: \\begin{align} \\ln e = 1 \\end{align} therefore: \\begin{align} x^r = \\left(e^{\\ln(x)}\\right)^r = e^{r \\ln(x)} \\end{align} inverse trigonometric functions trigonometric functions are not one-to-one, as their are periodic function. however if their domain to an interval that \"lats\" one period of the function then it is one-to-on. inverse sine function the inverse of the sine function is denoted by \\(\\sin^{-1}\\) or \\(\\arcsin\\), and it is defined as: \\begin{align} \\sin^{-1}(x) = y \\leftrightarrow \\sin y = x, -\\frac{\\pi}{2} \\leq y \\leq \\frac{\\pi}{2} \\end{align} as you can see the domain has been restricted to \\([-\\frac{\\pi}{2}, \\frac{-pi}{2}]\\) (see figure 18). by the cancellation equations we obtain: \\begin{align} \\sin^{-1}(\\sin x) = x, -\\frac{\\pi}{2} \\leq x \\leq \\frac{\\pi}{2} \\end{align} \\begin{align} \\sin(\\sin^{-1} x) = x, -1 \\leq x \\leq 1 \\end{align} the inverse sine function has domain \\([-1, 1]\\) and range \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) and its graph is shown on figure 20. inverse cosine function the inverse of the cosine function is denoted by \\(\\cos^{-1}\\) or \\(\\arccos\\), and it is defined as: \\begin{align} \\cos^{-1}(x) = y \\leftrightarrow \\cos y = x, 0 \\leq y \\leq \\pi \\end{align} as you can see the domain has been restricted to \\([0, \\pi]\\) (see figure 21). by the cancellation equations we obtain: \\begin{align} \\cos^{-1}(\\cos x) = x, 0 \\leq x \\leq \\pi \\end{align} \\begin{align} \\cos(\\cos^{-1} x) = x, -1 \\leq x \\leq 1 \\end{align} the inverse cosine function has domain \\([-1, 1]\\) and range \\([0, \\pi]\\) and its graph is shown on figure 22. inverse tangent function the inverse of the tangent function is denoted by \\(\\tan^{-1}\\) or \\(\\arctan\\), and it is defined as: \\begin{align} \\tan^{-1}(x) = y \\leftrightarrow \\tan y = x, -\\frac{\\pi}{2} \\leq y \\leq \\frac{\\pi}{2} \\end{align} as you can see the domain has been restricted to \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) (see figure 23). by the cancellation equations we obtain: \\begin{align} \\tan^{-1}(\\tan x) = x, -\\frac{\\pi}{2} \\leq x \\leq \\frac{\\pi}{2} \\end{align} \\begin{align} \\tan(\\tan^{-1} x) = x, -1 \\leq x \\leq 1 \\end{align} the inverse tangent function has domain \\([-1, 1]\\) and range \\([-\\frac{\\pi}{2}, \\frac{\\pi}{2}]\\) and its graph is shown on figure 25. we know that the lines \\(x = \\pm \\frac{\\pi}{2}\\) are vertical asymptotes of the graph of the tangent function. since the graph of \\(\\tan^{-1}\\) is obtained by reflecting the graph of the restricted tangnet function about the line \\(y = x\\), the the lines \\(y = \\frac{\\pi}{2}\\) and \\(y = -\\frac{\\pi}{2}\\) are horizontal asymptotes of the graph of \\(\\tan^{-1}\\). other inverse functions see for information about the remaning inverse trigonometric functions. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_01.html",
    "title": "The Tangent and Velocity Problems",
    "body": " index search search back the tangent and velocity problems contents the tangent problem the velocity problem the tangent problem we can think of a tangent to a curve as a line that touches the curve an follows the same direction as the curve at the point of contact. let's look at the next example, where we want to find an equation for the tangent line to the parabola \\(y = x^2\\) at point \\(p(1, 1)\\). to do so we only need to know the slope \\(m\\), however to find the slope of the line we need two points on the curve, and as of now we only have \\(p(1, 1)\\). but observe that we can compute an approximation by choosing a nearby point \\(q(x, x^2)\\) (see figure 2) and computing the slope \\(m_{pq}\\) of the secant line \\(pq\\). (a secant line if a line that intersects a curve more than once): \\begin{align} m_{pq} = \\frac{x^2 - 1}{x - 1} \\end{align} on the following tables we see that the closer \\(q\\) is to \\(p\\), that is the closer \\(x\\) is to \\(1\\), then the closer \\(m_{pq}\\) is to \\(2\\). \\(x\\) \\(m_{pq}\\) \\(2\\) \\(3\\) \\(1.5\\) \\(2.5\\) \\(1.1\\) \\(2.1\\) \\(1.01\\) \\(2.01\\) \\(1.001\\) \\(2.001\\) \\(x\\) \\(m_{pq}\\) \\(0\\) \\(1\\) \\(0.5\\) \\(1.5\\) \\(0.9\\) \\(1.9\\) \\(0.99\\) \\(1.99\\) \\(0.999\\) \\(1.999\\) this suggests that the slope of the tangent line is the limit of the slopes of the secant lines: \\begin{align} \\lim_{q \\rightarrow p} m_{pq} = m \\end{align} \\begin{align} \\lim_{x \\rightarrow 1} \\frac{x^2 - 1}{x - 1} = 2 \\end{align} assuming that the slope of the tangent is indeed \\(2\\), we define the equation of the tangent line through \\((1, 1)\\) as: \\begin{align} y - 1 = 2(x - 1) \\end{align} \\begin{align} y = 2x - 1 \\end{align} figure 3 illustrates the process that occurs on this example. as \\(q\\) approaches \\(p\\) the secant lines rotate about \\(p\\) and approach the tangent line \\(\\mathcal{l}\\). another method to approximate the slope of the tangent line at \\(p\\) is to measure the sides of a triangle \\(abc\\) as in figure 5: this gives an estimate of the slope of the tangent line as: \\begin{align} - \\frac{ab}{bc} \\approx -\\frac{8.0 - 5.4}{0.06 - 0.02} = -65.0 \\end{align} the velocity problem the instantaneous velocity is defined to be the limiting value of the average velocities over shorter and shorter time periods. thus there is a close connection between the tangent problem and the velocity problem. if we draw the grapf of the distance function and we consider points \\(p(5, f(5))\\) and \\(q(5 + h, f(5 + h))\\) (see figure 6). then the slope of the secant line \\(pq\\) is: \\begin{align} m_{pq} = \\frac{f(5 + h) - f(5)}{(5 + h) - 5} \\end{align} which is the same as the average velocity over the time interval \\([5, 5 + h]\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_02.html",
    "title": "The Limit of a Function",
    "body": " index search search back the limit of a function contents limits one-sided limits how can a limit fail to exist infinite limits vertical asymptotes vertical asymptotes limits suppose \\(f(x)\\) is defined when \\(x\\) is near the number \\(a\\), then we write: \\begin{align} \\lim_{x \\rightarrow a} f(x) = l \\end{align} if we can make the values of \\(f(x)\\) arbitrarily close to \\(l\\) by restricting \\(x\\) to be sufficiently close to \\(a\\) (on either side of \\(a\\)) but not equal to \\(a\\) (see figure 1). an alternative notation is: \\begin{align} f(x) \\rightarrow l \\text{ as } x \\rightarrow a \\end{align} notice that we never consider \\(x = a\\). in fact, \\(f(x)\\) need not even be defined when \\(x = a\\). the only thing that matters is how \\(f\\) is defined near \\(a\\). one-sided limits the notation \\(t \\rightarrow 0^{-}\\) indicates that we consider only values of \\(t\\) that are less than 0. likewise, \\(t \\rightarrow 0^{+}\\) indicates that we consider only values of \\(t\\) that are greater than 0. we write: \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = l \\end{align} and say that the left-hand limit of \\(f(x)\\) as \\(x\\) approaches \\(a\\) is equal to \\(l\\) if we can make the values of \\(f(x)\\) arbitrarily clsoe to \\(l\\) by restricting \\(x\\) to be sufficiently close to \\(a\\) with \\(x\\) less than \\(a\\). we write: \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = l \\end{align} and say that the right-hand limit of \\(f(x)\\) as \\(x\\) approaches \\(a\\) is equal to \\(l\\) if we can make the values of \\(f(x)\\) arbitrarily clsoe to \\(l\\) by restricting \\(x\\) to be sufficiently close to \\(a\\) with \\(x\\) greater than \\(a\\). see figure 6 for a graphical representation. by both definitions, we see that the following is true: \\begin{align} \\lim_{x \\rightarrow a} f(x) = l \\leftrightarrow \\lim_{x \\rightarrow a^{-}} f(x) = l \\text{ and } \\lim_{x \\rightarrow a^{+}} f(x) = l \\end{align} how can a limit fail to exist? a limit fails to exist at a number \\(a\\) if the left- and right-hand limits are not equal. when guessing the value of a limit it is easy to guess the wrong value if we use inapproapriate values of \\(x\\). it is also hard to know when to stop calculating value (sometimes calculators and computers give the wrong values due to precission issues). infinite limits: vertical asymptotes another way a limit at a number \\(a\\) can fail to exist is when the function values grow arbitrarily large (in absolute value) as \\(x\\) approaches \\(a\\). let \\(f\\) be a function defined on both sides of \\(a\\), except possible at \\(a\\) itself, then: \\begin{align} \\lim_{x \\rightarrow a} f(x) = \\infty \\end{align} means that the values of \\(f(x)\\) can be arbitrarily large by taking \\(x\\) sufficiently close to \\(a\\), but not equal to \\(a\\) (see figure 10). let \\(f\\) be a function defined on both sides of \\(a\\), except possible at \\(a\\) itself, then: \\begin{align} \\lim_{x \\rightarrow a} f(x) = -\\infty \\end{align} means that the values of \\(f(x)\\) can be arbitrarily large negative by taking \\(x\\) sufficiently close to \\(a\\), but not equal to \\(a\\) (see figure 11). this does not mean that the limit exists, it simply expresses the particular way in which the limit does not exist. similar definitions can be given for one-sided limits: \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = -\\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = -\\infty \\end{align} these are illustrated on the following figure: vertical asymptotes the vertical line \\(x = a\\) is called a vertical asymptotes of the curve \\(y = f(x)\\) if at least one of the following statements is true: \\begin{align} \\lim_{x \\rightarrow a} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a} f(x) = -\\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = -\\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = -\\infty \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_03.html",
    "title": "Calculating Limits Using the Limits Laws",
    "body": " index search search back calculating limits using the limits laws contents properties of limits evaluating limits by direct substitution limit equality using one-sided limits the squeeze theorem properties of limits supporse that \\(c\\) is a constant and the limits: \\begin{align} \\lim_{x \\rightarrow a} f(x) \\end{align} \\begin{align} \\lim_{x \\rightarrow a} g(x) \\end{align} exist. then: sum law \\begin{align} \\lim_{x \\rightarrow a} [f(x) + g(x)] = \\lim_{x \\rightarrow a} f(x) + \\lim_{x \\rightarrow a} g(x) \\end{align} difference law \\begin{align} \\lim_{x \\rightarrow a} [f(x) - g(x)] = \\lim_{x \\rightarrow a} f(x) - \\lim_{x \\rightarrow a} g(x) \\end{align} constant multiple law \\begin{align} \\lim_{x \\rightarrow a} [cf(x)] = c\\lim_{x \\rightarrow a} f(x) \\end{align} product law \\begin{align} \\lim_{x \\rightarrow a} [f(x) g(x)] = \\lim_{x \\rightarrow a} f(x) \\cdot \\lim_{x \\rightarrow a} g(x) \\end{align} quotient law \\begin{align} \\lim_{x \\rightarrow a} [\\frac{f(x)}{g(x)}] = \\frac{\\lim_{x \\rightarrow a} f(x)}{\\lim_{x \\rightarrow a} g(x)} \\end{align} if we use the product law repeteadly with \\(g(x) = f(x)\\), then we arrive at the power law: \\begin{align} \\lim_{x \\rightarrow a} \\left[f(x)\\right]^n = [\\lim_{x \\rightarrow a} f(x)]^n \\end{align} root law \\begin{align} \\lim_{x \\rightarrow a} \\sqrt[n]{f(x)} = \\sqrt[n]{\\lim_{x \\rightarrow a} f(x)} \\end{align} let's now see two special limits: \\begin{align} \\lim_{x \\rightarrow a} c = c \\end{align} \\begin{align} \\lim_{x \\rightarrow a} x = a \\end{align} if we now let \\(f(x) = x\\) on the power law and use \\(\\lim_{x \\rightarrow a} x = a\\), then: \\begin{align} \\lim_{x \\rightarrow a} x^n = [\\lim_{x \\rightarrow a} x]^n = a^n \\end{align} if we now let \\(f(x) = x\\) on the root law and use \\(\\lim_{x \\rightarrow a} x = a\\), then: \\begin{align} \\lim_{x \\rightarrow a} \\sqrt[n]{x} = \\sqrt[n]{\\lim_{x \\rightarrow a} x} = \\sqrt[n]{a} \\end{align} where \\(n\\) is a positive integer. also, if \\(n\\) is even then we assume that \\(a > 0\\). evaluating limits by direct substitution the limit laws prove that direct substitution can be used to obtain the value of a limit for polynomial and rational functions. if \\(f\\) is a polynomial or a rational function and \\(a\\) is in the domain of \\(f\\), then: \\begin{align} \\lim_{x \\rightarrow a} f(x) = f(a) \\end{align} functions that satisfy this property are said to be continuous at \\(a\\). however, not all limits can be evaluated initially by direct substitution. limit equality if \\(f(x) = g(x)\\) when \\(x \\neq a\\), then \\(\\lim_{x \\rightarrow a} f(x) = \\lim_{x \\rightarrow a} g(a)\\) using one-sided limits we say: \\begin{align} \\lim_{x \\rightarrow a} f(x) = l \\end{align} if and only if \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = l = \\lim_{x \\rightarrow a^{+}} f(x) \\end{align} the squeeze theorem the following two theorems describe how the limits of functions are related when the values of one function are greater than (or equal to) those of another. if \\(f(x) \\leq g(x)\\) when \\(x\\) is near \\(a\\) (except possibly at \\(a\\)) and the limits of \\(f\\) and \\(g\\) both exist as \\(x\\) approaches \\(a\\), then: \\begin{align} \\lim_{x \\rightarrow a} f(x) \\leq \\lim_{x \\rightarrow a} g(x) \\end{align} the squeeze theorem states: if \\(f(x) \\leq g(x) \\leq h(x)\\) when \\(x\\) is near \\(a\\) (except possibly at \\(a\\)) and: \\begin{align} \\lim_{x \\rightarrow a} f(x) = \\lim_{x \\rightarrow a} h(x) = l \\end{align} then: \\begin{align} \\lim_{x \\rightarrow a} g(x) = l \\end{align} this theorem is illustrated on figure 7. it says that if \\(g(x)\\) is squeezed between \\(f(x)\\) and \\(h(x)\\) near \\(a\\), and if \\(f\\) and \\(h\\) have the same limit \\(l\\) at \\(a\\), then \\(g\\) is forced to have the same limit \\(l\\) at \\(a\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_04.html",
    "title": "The Precise Definition of a Limit",
    "body": " index search search back the precise definition of a limit contents the precise definition of a limit one-sided limits precise definition of left-hand limit precise definition of right-hand limit limit laws proof of the sum law infinite limits the intuitive definition is inadequate for some purpose because of it vagueness. the precise definition of a limit as motivation consier the following function: \\begin{align} f(x) = \\begin{cases} 2x - 1 & \\text{ if } x \\neq 3 \\\\ 6 & \\text{ if } x = 3 \\\\ \\end{cases} \\end{align} think about how close to \\(3\\) does \\(x\\) have to be so that \\(f(x)\\) differs from \\(5\\) less than \\(0.1\\)? we know that the distance from \\(x\\) to \\(3\\) is \\(|x - 3|\\) and the distance from \\(f(x)\\) to \\(5\\) is \\(|f(x) - 5|\\), so out problem becomes: \\begin{align} | f(x) - 5 | < 0.1 \\text{ if } 0 < | x - 3 | < \\delta \\end{align} notice that if: \\begin{align} 0 < |x - 3| < \\frac{0.1}{2} = 0.5 \\end{align} then: \\begin{align} |f(x) - 5| = |(2x - 1) - 5| = |2x - 6| = 2|x - 3| < 2(0.05) = 0.1 \\end{align} that is: \\begin{align} |f(x) - 5| < 0.1 \\text { if } 0 < |x - 3| < 0.05 \\end{align} thus an answer to the proposed problem is given by \\(\\delta = 0.05\\). however, if we change the limit \\(0.1\\) to \\(0.01\\) by using the same method we find that \\(\\delta = 0.005\\), such that: \\begin{align} |f(x) - 5| < 0.01 \\text { if } 0 < |x - 3| < 0.005 \\end{align} and similarly: \\begin{align} |f(x) - 5| < 0.001 \\text { if } 0 < |x - 3| < 0.0005 \\end{align} these values: \\(0.1, 0.01\\) and \\(0.001\\) are what we call error tolerances. but to actually compute the limit we must be able to find a \\(\\delta\\) for any arbitrarily small positive number \\(\\epsilon\\): \\begin{align} |f(x) - 5| < \\epsilon \\text { if } 0 < |x - 3| < \\delta = \\frac{\\epsilon}{2} \\end{align} this is the precise way of saying that \\(f(x)\\) is close to \\(5\\) when \\(x\\) is close to \\(3\\) because the previous statement says that we can make the values of \\(f(x)\\) within an arbitrary distance \\(\\epsilon\\) from \\(5\\) by restricting \\(x\\) to be within a distance \\(\\frac{\\epsilon}{2}\\) from \\(3\\) (see figure 1). the precise definition of a limit is as follows: let \\(f\\) be a function defined on some open interval that contains the number \\(a\\), except possible at \\(a\\) itself. then we say that the limit of \\(f(x)\\) as \\(x\\) approaches \\(a\\) is \\(l\\) and we write: \\begin{align} \\lim_{x \\rightarrow a} f(x) = l \\end{align} if for every number \\(\\epsilon > 0\\) there is a number \\(\\delta > 0\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta \\text{ then } |f(x) - l| < \\epsilon \\end{align} we can reformulate this definition in terms of intervals: \\begin{align} \\lim_{x \\rightarrow a} f(x) = l \\end{align} means that for every \\(\\epsilon > 0\\) we can find \\(\\delta > 0\\) such that if \\(x \\in (a - \\delta, a + \\delta)\\) and \\(x \\neq a\\), then \\(f(x) \\in (l - \\epsilon, l + \\epsilon)\\) (see figure 3). also we can interpret the definition geometrically in terms of the graph of a function. if \\(\\epsilon > 0\\) is given we draw horizontal lines \\(y = l + \\epsilon\\) and \\(y = l - \\epsilon\\) as well as the graph of \\(f\\) (see figure 4). if \\(\\lim_{x \\rightarrow a} f(x) = l\\) then we can find a number \\(\\delta > 0\\) such that if \\(x \\in (a - \\delta, a + \\delta)\\) and \\(x \\neq a\\), then the curve \\(y = f(x)\\) lies between the lines \\(y = l - \\epsilon\\) and \\(y = l + \\epsilon\\) (see figure 5). once \\(\\delta\\) has been found, then any smaller \\(\\delta\\) will also work. note that this process must work for every positive number \\(\\epsilon\\), no matter how small (see figure 6). one-sided limits precise definition of left-hand limit \\begin{align} \\lim_{x \\rightarrow a^{-}} f(x) = l \\end{align} if for every number \\(\\epsilon > 0\\) there is a number \\(\\delta > 0\\) such that: \\begin{align} \\text{ if } a - \\delta < x < a \\text{ then } |f(x) - l| < \\epsilon \\end{align} that is, \\(x\\) is restricted to lie in the left half of the interval \\((a - \\delta, a + \\delta)\\). precise definition of right-hand limit \\begin{align} \\lim_{x \\rightarrow a^{+}} f(x) = l \\end{align} if for every number \\(\\epsilon > 0\\) there is a number \\(\\delta > 0\\) such that: \\begin{align} \\text{ if } a < x < a + \\delta \\text{ then } |f(x) - l| < \\epsilon \\end{align} that is, \\(x\\) is restricted to lie in the right half of the interval \\((a - \\delta, a + \\delta)\\). limit laws up until now we have used the precise definition of a limit to compute the limit of a given function. however if we were given a more complicated funtion a proof would require a great deal of ingenuity. fortunately we can use the limit laws, which can be proved using the definition of a limit. so the limits of complicated function can be found rigorously from the limit laws without resorting to the definition directly. proof of the sum law the sum law states that if the limits: \\begin{align} \\lim_{x \\rightarrow a} f(x) \\end{align} \\begin{align} \\lim_{x \\rightarrow a} g(x) \\end{align} exist. then: \\begin{align} \\lim_{x \\rightarrow a} [f(x) + g(x)] = \\lim_{x \\rightarrow a} f(x) + \\lim_{x \\rightarrow a} g(x) \\end{align} proof let \\(\\epsilon > 0\\) be given, we must find \\(\\delta > 0\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta \\text{ then } |f(x) + g(x) - (l + m)| < \\epsilon \\end{align} by the means of the triangle inequality: \\begin{align} | f(x) + g(x) - (l + m) | = | (f(x) - l) + (g(x) - m) | \\leq | f(x) - l | + | g(x) - m | \\end{align} we make \\(|f(x) + g(x) - (l + m)|\\) less that \\(\\epsilon\\) by letting: \\begin{align} | f(x) - l | < \\frac{\\epsilon}{2} \\end{align} and \\begin{align} | g(x) - m | < \\frac{\\epsilon}{2} \\end{align} since \\(\\frac{\\epsilon}{2} > 0\\) and \\(\\lim_{x \\rightarrow a} f(x) = l\\) there exists a number \\(\\delta_1 > 0\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta_1 \\text{ then } |f(x) - l| < \\frac{\\epsilon}{2} \\end{align} similarly, since \\(\\frac{\\epsilon}{2} > 0\\) and \\(\\lim_{x \\rightarrow a} g(x) = m\\) there exists a number \\(\\delta_2 > 0\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta_2 \\text{ then } |g(x) - m| < \\frac{\\epsilon}{2} \\end{align} let \\(\\delta = \\min\\{\\delta_1, \\delta_2\\}\\), notice: \\begin{align} \\text{ if } 0 < |x - a| < \\delta \\text{ then } 0 < |x - a| < \\delta_1 \\text{ and } 0 < |x - a| < \\delta_2 \\end{align} and also: \\begin{align} | f(x) - l | < \\frac{\\epsilon}{2} \\text{ and } | g(x) - m | < \\frac{\\epsilon}{2} \\end{align} and, therefore by the triangle inequality we showed before: \\begin{align} |f(x) + g(x) - (l + m)| \\leq | f(x) - l | + | g(x) - m | < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon \\end{align} thus, by the definition of a limit: \\begin{align} \\lim_{x \\rightarrow a} [f(x) + g(x)] = l + m \\end{align} infinite limits let \\(f\\) be a function defined on some open interval that contains the number \\(a\\), except possibly at \\(a\\) itself. then: \\begin{align} \\lim_{x \\rightarrow a} f(x) = \\infty \\end{align} means that for every postive number \\(m\\) there is a positive number \\(\\delta\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta \\text{ then } f(x) > m \\end{align} this says that the values of \\(f(x)\\) ca be made arbitrarily large (larger than any given number \\(m\\)) by requiring \\(x\\) to be close enough to \\(a\\) (see figure 10). let \\(f\\) be a function defined on some open interval that contains the number \\(a\\), except possibly at \\(a\\) itself. then: \\begin{align} \\lim_{x \\rightarrow a} f(x) = -\\infty \\end{align} means that for every postive number \\(n\\) there is a positive number \\(\\delta\\) such that: \\begin{align} \\text{ if } 0 < |x - a| < \\delta \\text{ then } f(x) < n \\end{align} see figure 11. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_05.html",
    "title": "Continuity",
    "body": " index search search back continuity contents continuity of a function properties of continuous functions continuity of polynomial and rational functions continuous functions continuity of composite functions the intermediate value theorem continuity of a function a function \\(f\\) is continuous at a number \\(a\\) if: \\begin{align} \\lim_{x\\rightarrow a} f(x) = f(a) \\end{align} this definition implicitly requires: \\(f(a)\\) is defined \\(\\lim_{x\\rightarrow a} f(x)\\) exists \\(\\lim_{x\\rightarrow a} f(x) = f(a)\\) this definition says that \\(f\\) is continuous at \\(a\\) if \\(f(x)\\) approaches \\(f(a)\\) as \\(x\\) approaches \\(a\\). and we say that \\(f\\) is discontinuous at \\(a\\) if \\(f\\) is not continuous at \\(a\\). we distinguish three cases of discontinuity: removable discontinuity infinite discontinuity jump discontinuity a function \\(f\\) continuous from the right at a number \\(a\\) if: \\begin{align} \\lim_{x\\rightarrow a^{+}} f(x) = f(a) \\end{align} a function \\(f\\) continuous from the left at a number \\(a\\) if: \\begin{align} \\lim_{x\\rightarrow a^{-}} f(x) = f(a) \\end{align} a function \\(f\\) is continuous on an interval if it is continuous at every number on the interval. properties of continuous functions if \\(f\\) and \\(g\\) are continuous at a number \\(a\\) and \\(c\\) is constant, then the following functions are also continuous at \\(a\\): \\(f + g\\) \\(f - g\\) \\(cf\\) \\(f\\cdot g\\) \\(\\frac{f}{g}, g(a) \\neq 0\\) proof since \\(f\\) and \\(g\\) are continuous at \\(a\\) we have that: \\begin{align} \\lim_{x \\rightarrow a} f(x) = f(a) \\end{align} and \\begin{align} \\lim_{x \\rightarrow a} g(x) = g(a) \\end{align} by the properties of limits: \\begin{align} \\lim_{x \\rightarrow a} [f(x) + g(x)] = [\\lim_{x \\rightarrow a} f(x)] + [\\lim_{x \\rightarrow a} g(x)] \\end{align} \\begin{align} = f(a) + g(a) = (f + g)(a) \\end{align} and thus, \\(f + g\\) is continuous at \\(a\\). continuity of polynomial and rational functions any polynomial functions is continuous on \\(\\mathbb{r}\\). proof a polynomial is a function of the form: \\begin{align} p(x) = c_nx^n + c_{n-1}x^{n-1} + \\cdots + c_1x + c_0 \\end{align} where \\(c_i \\in \\mathbb{r}\\), we know that: \\begin{align} \\lim_{x \\rightarrow a} c_0 = c_0 \\end{align} and \\begin{align} \\lim_{x \\rightarrow a} x^m = a^m, m = 1, 2, \\cdots, n \\end{align} therefore: \\begin{align} \\lim_{x \\rightarrow a} p(x) = \\lim_{x \\rightarrow a} c_nx^n + c_{n-1}x^{n-1} + \\cdots + c_1x + c_0 \\end{align} by the properties of limits: \\begin{align} \\lim_{x \\rightarrow a} p(x) = \\sum_{i = 0}^n \\lim_{x \\rightarrow a} c_i x^{i} \\end{align} \\begin{align} = \\left(\\sum_{i = 1}^n \\lim_{x \\rightarrow a} c_i x^{i}\\right) + \\lim_{x \\rightarrow} c_0 \\end{align} \\begin{align} = \\left(\\sum_{i = 1}^n c_i a^i\\right) + c_0 \\end{align} \\begin{align} = p(a) \\end{align} thus \\(p\\) is continuous at \\(a\\), where \\(a \\in \\mathbb{r}\\). any ration function is continuous whenever it is defined, that is, it is continuous on its domain. proof a rational function is a function of the form: \\begin{align} f(x) = \\frac{p(x)}{q(x)} \\end{align} where \\(p\\) and \\(q\\) are polynomials. the domain of \\(f\\) is \\(d = \\{x \\in \\mathbb{r} | q(x) \\neq 0\\}\\). so we now, by the previous proof, that \\(p\\) and \\(q\\) are continuous everywhere. thus by the properties of continuous functions \\(f\\) is continuous at every number in \\(d\\) (its domain). continuous functions the following types of functions are continuous on their domains: polynomial functions rational functions root functions trigonometric functions inverse trigonometric functions exponential functions logarithmic functions note that the inverse of a continuous one-to-one function is also continuous. our geometric intuitions makes it seem plausible: we know that the graph of \\(f^{-1}\\) is obtained by reflecting the graph of \\(f\\) on the line \\(y = x\\). so if the graph of \\(f\\) has no break in it (the function is continuous), then the graph of \\(f^{-1}\\) also has no break in it. continuity of composite functions if \\(f\\) is continuous at \\(b\\) and \\(\\lim_{x \\rightarrow a} g(x) = b\\), then \\(\\lim_{x \\rightarrow a} f(g(x)) = b\\), that is: \\begin{align} \\lim_{x \\rightarrow a} f(g(x)) = f\\left(\\lim_{x \\rightarrow a} g(x)\\right) \\end{align} if \\(g\\) is continuous at \\(a\\) and \\(f\\) is continuous at \\(g(a)\\), then the composite function \\(f \\circ g\\), given by \\((f \\circ g)(x) = f(g(x))\\) is continuous at \\(a\\) proof since \\(g\\) is continuous at \\(a\\) we have that: \\begin{align} \\lim_{x \\rightarrow a} g(x) = g(a) \\end{align} since \\(f\\) is continuous at \\(b = g(a)\\) we have that: \\begin{align} \\lim_{x \\rightarrow a} f(g(x)) = f(b) = f(g(a)) \\end{align} thus \\(f \\circ g\\) is continuous at \\(a\\). the intermediate value theorem suppose that \\(f\\) is continuous at the closed interval \\([a, b]\\) and let \\(n\\) be any number between \\(f(a)\\) and \\(f(b)\\), where \\(f(a) \\neq f(b)\\). there there exists a number \\(c \\in (a, b)\\), such that \\(f(c) = n\\). the intermediate value theorem states that a continuous functions takes on every intermediate value between the function values \\(f(a)\\) and \\(f(b)\\) (see figure 8). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M01/01_intro.html",
    "title": "IntroducciÃ³n a Big Data",
    "body": " index search search back introducciÃ³n a big data contents origen e historia diferencias entre big data machine learning e inteligencia artificial las cinco v del big data desaf os del big data aplicaciones y casos de uso beneficios del big data el big data son conjuntos de datos cuya principal caracterÃ­stica es que su tamaÃ±o es tal que no podemos utilizar el software de procesamiento convencional. estos conjuntos de datos pueden ser estructurados, semiestructurados o no estructurados. origen e historia los orÃ­genes de los datos masivos se remontan a los aÃ±os 60 y 70, cuando estÃ¡bamos apenas comenzando con los primeros centros de datos y las bases de datos relacionales. en el 2005, la gente se dio cuenta de la cantidad de datos que estÃ¡bamos generando en plataformas online. fue entonces cuando saliÃ³ a la luz hadoop, un sistema diseÃ±ado para guardar y analizar grandes cantidades de datos. tambiÃ©n en esa Ã©poca, las bases de datos nosql (no solo sql) hicieron sus primeras apariciones. los frameworks de cÃ³digo abierto como hadoop (y mÃ¡s recientemente, spark) provocaron que manejar y guardar esta cantidad de datos fuese mÃ¡s sencillo y barato. desde entonces la cantidad de datos sigue creciendo, con la diferencia de que ahora no sÃ³lo somos nosotros mismos los que generamos los datos si no que tambiÃ©n las mÃ¡quinas toman parte. con la llegada del internet de las cosas (iot), hay cada vez mÃ¡s objetos conectados a la red recopilando datos sobre cÃ³mo usamos las cosas y el rendimiento de los productos. y no podemos olvidar el auge del aprendizaje automÃ¡tico, que estÃ¡ generando aÃºn mÃ¡s datos. la magia del cloud computing ha ampliado las posibilidades del big data. en la nube, los desarrolladores pueden escalar cosas de forma Ã¡gil y rÃ¡pida. ademÃ¡s, las bases de datos de grÃ¡ficos son cada vez mÃ¡s populares e importantes, mostrando toneladas de datos de forma rÃ¡pida y completa. diferencias entre big data, machine learning e inteligencia artificial el machine learning estÃ¡ dentro de la inteligencia artificial, sin embargo, no toda inteligencia artifical es machine learning. de la misma forma podemos ver cÃ³mo el concepto de big data no estÃ¡ dentro de la inteligencia artificial, sino que tiene otras caracterÃ­sticas. aquella parte donde confluyen machine learning y big data recibe el nombre de big data analytics, un concepto relacionado con el dato puesto al servicio del entrenamiento de modelos con el fin de analizar patrones en los datos, predecir comportamientos, etc. big data analytics proporciona los datos que la ia y el ml necesitan para aprender y mejorar. la ia y el ml pueden utilizarse para analizar grandes conjuntos de datos y encontrar patrones y conocimiento oculto. este conocimiento puede utilizarse para mejorar los procesos de negocio, desarrollar nuevos productos y servicios y tomar mejores decisiones. por ejemplo: anÃ¡lisis de clientes: para comprender mejor sus necesidades y preferencias. detecciÃ³n de fraude: al analizar las transacciones, se pueden extraer diferentes mÃ©tricas y detectar asÃ­ actividades fraudulentas. desarrollo de nuevos productos: podemos identificar las distintas necesidades de los clientes y crear nuevos productos que las satisfagan. las cinco \"v\" del big data el concepto de las v's del big data ha evolucionado con el tiempo, comenzando con 3 v's: volumen, velocidad y variedad y ampliÃ¡ndose a 5 con valor y veracidad y, en algunos casos, hasta 7 v's: variabilidad y visualizaciÃ³n. volumen: se refiere a la enorme cantidad de datos generados y recopilados. variedad: hace referencia a la diversidad de tipos de datos (datos estructurados y datos no estructurados). velocidad: se refiere a la velocidad a la que se generan, procesan y analizan los datos. veracidad: relacionada con la calidad y la confiabilidad de los datos. valor: capacidad para convertir los datos en valor significativo para la toma de decisiones y la obtenciÃ³n de informaciÃ³n. desafÃ­os del big data infraestructuras para los datos: la gestiÃ³n y el almacenamiento de los datos pueden ser costosos y complicados. es necesario crear infraestructuras robustas y escalables. velocidad de procesamiento: el anÃ¡lisis en tiempo real y la capacidad de procesar datos rÃ¡pidamente son fundamentales en muchos casos. veracidad de los datos: los datos deben proceder de fuentes confiables que aseguren una toma de decisiones correcta. privacidad y seguridad: con la creciente adquisiciÃ³n de los datos la privacidad de los usuarios y la seguridad de la informaciÃ³n pueden estar amenazadas. esto deriva en la implantaciÃ³n de leyes y normas que regulen la privacidad y seguridad de los datos. elevados costos: implementar y mantener infraestructuras y tecnologÃ­as de big data puede conllevar una importante inversiÃ³n. profesionales especializados: existe una demanda creciente de profesionales con habilidades en el manejo, limpieza y anÃ¡lisis de datos masivos. la escaseza de los mismos puede suponer un desafÃ­o. interoperabilidad: la integraciÃ³n de diferentes sistemas y tecnologÃ­as puede resultar difÃ­cil y ser un verdadero desafÃ­o especialmente cuando se trabaja con conjuntos de datos provenientes de diversas fuentes y en diferentes formatos. gobernanza: gestionar y gobernar los datos de forma eficaz es un reto importante para las empresas que gestionan conjuntos masivos de datos. aplicaciones y casos de uso salud: poder mejorar la atenciÃ³n al paciente, desarrollar nuevos medicamentos y tratamientos, y prevenir enfermedades con antelaciÃ³n. finanzas: la detecciÃ³n del fraude, gestionar los riesgos de la concesiÃ³n de crÃ©ditos y blanqueo de capitales y tomar decisiones de inversiÃ³n. marketing: comprender el comportamiento de los clientes, personalizar las recomendaciones y mejorar las campaÃ±as de marketing. transporte: la optimizaciÃ³n de rutas, mejorar la seguridad y reducir el trÃ¡fico. gobierno: las campaÃ±as electorales, creaciÃ³n de planes de fidelizaciÃ³n polÃ­tico-social. beneficios del big data toma de decisiones: las empresas pueden tomar mejores decisiones al proporcionarles mÃ¡s informaciÃ³n mediante la explotaciÃ³n de los datos. mejora de la eficiencia: la eficiencia de los procesos de automatizaciÃ³n pueden llevar a mejorar la productividad y reducir costos. innovaciÃ³n: para la innovaciÃ³n de las estrategias o en la creaciÃ³n de nuevos productos el big data puede proporcionar nuevas ideas y perspectivas. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M01/02_hadoop.html",
    "title": "El NÃºcleo de Hadoop",
    "body": " index search search back el nÃºcleo de hadoop contents fundamentos y herramientas del ecosistema de hadoop la evoluci n de hadoop caracter sticas de hadoop c digo abierto distribuido escalable tolerante a fallos estructura de hadoop sistema de ficheros distribuidos qu es hadoop distributed file system hdfs sistema distribuido escalabilidad horizontal estructura jer rquica arquitectura y componentes namenode datanode escritura en hdfs lectura en hdfs comandos de hdfs listar ficheros leer y escribir ficheros cargar y descargar ficheros gesti n de ficheros gesti n de permisos gesti n de administraci n yarn gesti n de recursos por qu surge yarn arquitectura y componentes resource manager node manager application master secuencia de trabajo en hadoop yarn federaci n en hadoop yarn fundamentos y herramientas del ecosistema de hadoop la naturaleza de los datos masivos hace que la utilizaciÃ³n de sistemas de almacenamiento y procesamiento tradicionales, como las bases de datos relaciones, sea inviable. el ecosistema de hadoop es un entorno software que permite alamcenar, procesar y analizar este tipo de datos de forma eficiente. la evoluciÃ³n de hadoop a principio de los 2000 e inspirado por las innovaciones en sistemas distribuidos de google (google file system y el modelo de programaciÃ³n mapreduce), doug cutting desarrolla una alternativa en cÃ³digo abierto a estas tecnologÃ­as. inicialmente se crearon dos partes: hadoop distributed file system (hdfs): una manera de almacenar grandes cantidades de datos. mapreduce: un sistema para procesar estes datos distribuidos. tras el Ã©xito de hadoop surgieron otros proyectos como hive, pig y sparkk que ampliaron lo que se podÃ­a hacer con hadoop. en la actualidad hadoop es un estÃ¡ndar en el big data y es utilizado en muchas industrias diferentes. caracterÃ­sticas de hadoop cÃ³digo abierto el cÃ³digo fuente de hadoop se encuentra disposible pÃºblicamente, lo cual fomenta innovaciÃ³n ya que distintos desarrolladores pueden trabajar en mejorar el software. distribuido se trata de una plataforma de computaciÃ³n distribuida que ha sido diseÃ±ada para poder manejar grandes volÃºmenes de datos en mÃºltiples nodos de un clÃºster de servidores. de tal manera que hadoop puede distribuir la carga de trabajo y los datos entre varios nodos. esto permite procesamiento en paralelo y escalable. escalable la escalabilidad de hadoop se puede entender en dos dimensiones: escalabilidad horizontal: se aÃ±aden mÃ¡s nodos al clÃºster para aumentar la capacidad de almacenamiento y de procesamiento cuando la demanda asÃ­ lo requiere. escalabilidad vertical: se puede aumentar la capacidad de cpu, memoria y almacenamiento de nodos individuales para majera tareas mÃ¡s grandes o complejas. tolerante a fallos se ha diseÃ±ado hadoop de tal manera que se mantiene la disponibilidad del servicio y la integridad de los datos incluso cuando ocurren fallos en el hardware o en el software del sistema. esta tolerancia se obtiene mediante: la replicaciÃ³n de datos. la autorecuperaciÃ³n. la monitorizaciÃ³n constante del estado del clÃºster. estructura de hadoop en 2006 se ceden los dos componentes que formaban hadoop a apache software foundation. en 2012 se publica la versiÃ³n 1.0 y seguidamente en 2013 se publica la versiÃ³n 2.0 en la cual se introduce yarn como gestor de recursos y se desacoplan los mÃ³dulos hdf y mapreduce. finalmente en 2017 se publica la versiÃ³n 3.0. actualmente hadoop se conforma de los siguientes componentes principales: hadoop distributed system (hdf): sistema distribuido de almacenamiento primario de datos. este divide los datos en bloques y los distribuye en varios nodos del clÃºster para garantizar la tolerancia a fallos y la escalabilidad. mapreduce: proporciona un modelo de programaciÃ³n y procesamiento distribuido para realizar operaciones en conjuntos de datos distribuidos almacenados en hdf. este divide las tareas en dos fases: map: los datos se procesan y se filtran en tareas mÃ¡s pequeÃ±as que se ejecutan de forma paralela en diferentes nodos del clÃºster. reduce: los resultados intermedios de las tareas de combinan y se procesan para producir el resultado final. yet another resource negotiator (yarn): es el administrados e recursos del clÃºster. se encarga de gestionar recursos de procesamiento (cpu, memoria). tambiÃ©n permite la programaciÃ³n dinÃ¡mica de tareas y la adminitraciÃ³n eficiente de recursos. sistema de ficheros distribuidos Â¿quÃ© es hadoop distributed file system (hdfs)? hadoop crea una capa de abstracciÃ³n en forma de sistema de ficheros Ãºnico. tal que se encarga de almacenar los datos en varios nodos manteniendo sus metadatos. sistema distribuido en este sistema de ficheros los ficheros de dividen en bloques de un mismo tamaÃ±o (normalmente 128mb, pero es configurable) y se distribuyen en los nodos del clÃºster. cada uno de los bloques se replicarÃ¡ en varios nodos para que el sistema sea tolerante a fallos. para este tipo de sistemas no es Ã³ptimo que los ficheros tengan un tamaÃ±o pequeÃ±o. por ejemplo, si tenemos un fichero de 800mb, lo dividirÃ­amos en siete bloques. seis serÃ¡n de 128mb y el Ãºltimo serÃ¡ de 32mb escalabilidad horizontal la escalabilidad horizontal se implementa aumentando el espacio por medio de discos duros o aÃ±adiendo mÃ¡s nodos al sistema. este tipo de escalabilidad es mÃ¡s sencilla que la escalabilidad vertical ya que consiste en aÃ±adir hardware bÃ¡sico. estructura jerÃ¡rquica decimos que hdfs es jerÃ¡rquico, ya que existe una jerarquÃ­a de directorios y ficheros. arquitectura y componentes sigue una arquitectura maestro-esclavo con dos componentes principales: namenodes y datanodes. namenode almacena los metadatos de hdfs, incluyendo informaciÃ³n acerca de la estructura de directorios, los permisos en los archivos y la ubicaciÃ³n fÃ­sica de los bloques de datos. en un clÃºster hdfs sÃ³lo hay un namenode, y su disponibilidad es crucial, si este no es accesible este supone un error crÃ­tico en el sistema. por ello, a partir de hadoop 2 es posible configurar un namenode secundario en espera, o mÃ¡s de uno a partir de hadoop 3. estes nodos toman el control si se detecta algÃºn fallo en el nodo primario. es importante que los nodos esten sincronizados y que tengan los mismos metadatos que el nodo primario. datanode este tipo de nodos son los encargados de almadenar los datos y de recuperar los bloques de datos que conforman los archivos asÃ­ como de gestionar el almacenamiento del nodo. ademÃ¡s deben informar periÃ³dicamente del estado de los bloques al namenode. la pÃ©rdida de un datanode no afecta significativamente a la integridad del sistema, pero la pÃ©rdida de muchos podrÃ­a afectar a la disposibilidad y a la redundancia de los datos. escritura en hdfs cliente solicita la escritura al namenode namenode asigna bloques consultando su tabla de metadatos para determinar quÃ© datanodes deben contener los bloques del archivo. devuelve esta lista de datanodes al cliente. escritura de datos en datanodes por parte del cliente. confirmaciÃ³n al namenode por parte de los datanodes. el namenode actualiza su tabla de metadatos con la nueva informaciÃ³n acerta de la ubicaciÃ³n de los bloques de datos del archivo. lectura en hdfs cliente solicita la lectura de un archivo al namenode. el namenode consulta su tabla de metadatos y devuelve la ubicaciÃ³n del archivo al cliente. el cliente solicita los bloques de datos a los datanodes donde estÃ¡n almacenados y estes devuelven los bloques. el cliente reconstruye el archivo a partir de los bloques. el namenode actualiza su tabla de metadatos con nueva informaciÃ³n acerca de la ubicaciÃ³n de los bloques de datos del archivo. comandos de hdfs listar ficheros comando descripciÃ³n hdfs dfs -ls / lista todos los ficheros y directorios para el path / hdfs dfs -ls -h / lista los ficheros con su tamaÃ±o en formato legible hdfs dfs -ls -r / lista todos los ficheros y directorios recursivamente (con subdirectorios) hdfs dfs -ls /file* lista todos los ficheros que cumplen el patrÃ³n (ficheros que comienzan con 'file') hadoop fs -stat Â«type:%fÂ» / imprime estadÃ­sticas del fichero o directorio en el formato indicado leer y escribir ficheros comando descripciÃ³n hdfs dfs -text /app.log imprime el fichero en modo texto por la terminal hdfs dfs -cat /app.log muestra el contenido del fichero en la salida estÃ¡ndar hdfs dfs -appendtofile /home/file1 /file2 aÃ±ade el contenido del fichero local 'file1' al fichero en hdfs âfile2â cargar y descargar ficheros comando descripciÃ³n hdfs dfs -put /home/file1 /hadoop copia el fichero 'file1' del sistema de ficheros local a hdfs en la carpeta \"hadoop\" hdfs dfs -put -f /home/file1 /hadoop copia el fichero 'file1' del sistema de ficheros local a hdfs y lo sobreescribe en el caso de que ya exista hdfs dfs -put -l /home/file1 /hadoop copia el fichero 'file1' del sistema de ficheros local a hdfs. fuerza replicaciÃ³n 1 y permite al datanode persistir los datos de forma perezosa. hdfs dfs -put -p /home/file1 /hadoop copia el fichero 'file1' del sistema de ficheros local a hdfs. mantiene los tiempos de acceso, de modificaciÃ³n y propietario original hdfs dfs -get /file1 /home/ copia el fichero 'file1' de hdfs al sistema de ficheros local hdfs dfs -copytolocal /file1 /home/ copia el fichero 'file1' de hdfs al sistema de ficheros local (igual que el anterior) hdfs dfs -movefromlocal /home/file1 /hadoop copia el fichero 'file1' del sistema de ficheros local a hdfs y luego lo borra del sist. ficheros local gestiÃ³n de ficheros comando descripciÃ³n hdfs dfs -cp /hadoop/file1 /hadoop1 copia el fichero al directorio destino en hdfs hdfs dfs -cp -p /hadoop/file1 /hadoop1 copia el fichero al directorio destino en hdfs conservando tiempos de acceso y de modificaciÃ³n, propietario y modo hdfs dfs -rm /hadoop/file1 elimina el fichero 'file1' de hdfs y lo envÃ­a a la papelera hdfs dfs -rm -r /hadoop elimina el directorio y su contenido en hdfs hdfs dfs -rm -r /hadoop elimina el directorio y su contenido en hdfs hdfs dfs -rmr /hadoop elimina el directorio y su contenido en hdfs hdfs dfs -rm -skiptrash /file1 elimina el fichero sin dejarlo en la papelera hdfs dfs -mkdir /hadoop2 crea un directorio en hdfs hdfs dfs -touchz /hadoop3 crea un fichero en hdfs con tamaÃ±o 0 hadoop fs -getmerge -nl /file1 /file2 /output concatena los ficheros file1 y file2 en el fichero destino /output gestiÃ³n de permisos comando descripciÃ³n hdfs dfs -checksum /hadoop/file1 muestra la informaciÃ³n checksum del fichero hdfs dfs -chmod 775 /hadoop/file1 cambia los permisos del fichero en hdfs hdfs dfs -chmod -r 755 /hadoop cambia los permisos de los ficheros recursivamente hdfs dfs -chown hadoop:hadoop /file1 cambia el propietario y el grupo del fichero hdfs dfs -chown -r hadoop:hadoop /file1 cambia el propietario y el grupo recursivamente hdfs dfs -chgrp hadoop /file1 cambia el grupo del fichero gestiÃ³n de administraciÃ³n comando descripciÃ³n hdfs dfs -df /hadoop muestra la capacidad y el espacio libre y usado del sistema de ficheros hdfs dfs -df -h /hadoop muestra la capacidad y el espacio libre y usado del sistema de ficheros en formato legible hadoop version muestra la versiÃ³n de hadoop hdfs fsck / comprueba el estado de salud del sistema de ficheros hdfs dfsadmin -safemode leave deshabilita el modo seguro del namenode hdfs namenode -format formatea el namenode hadoop fs -test -e filename si el path existe en hdfs, devuelve 0 hadoop fs -setrep -w 3 /file1 cambia el factor de replicaciÃ³n de un fichero a 3. si se indica un directorio, cambia el factor de replicaciÃ³n de todos los ficheros que contiene yarn: gestiÃ³n de recursos por quÃ© surge yarn? yarn se encarga de administrar y asignar recursos a las diversas aplicaciones que se ejecutan en el clÃºster de hadooop. algunos de los objetivos principales de yarn son la escalabilidad, la eficiencia y la capacidad de ejecutar mÃºltiples frameworks de procesamiento de datos no sÃ³lo mapreduce. en la verisÃ³n 1, mapreduce contaba con un administrador de recursos y un planificador interno. yarn permite separar la gestiÃ³n de recursos de la lÃ³gica de programaciÃ³n. anteriormente se presentaban muchas limitaciones al no admitir apliaciones que siguiesen el modelo mapreduce (como el anÃ¡lisis de grafos) y era necesario transladar los datos a otras plataformas. a partir de hadoop 2 la utilizaciÃ³n de otros frameworks (giraph para anÃ¡lisis de datos, storm para anÃ¡lisis de datos en tiempo real, etc) es transparente. esto permite una mejor utilizaciÃ³n de los recursos del clÃºster y una mayor flexibilidad en la ejecuciÃ³n de aplicaciones. arquitectura y componentes yarn consta de tres componentes principales: resource manager, node manager y application master. resource manager se trata del maestro del clÃºster y se encarga de aceptar las solicitudes de recursos de las aplicaciones, negociar los recursos con los nodos y asignar recursos a la aplicaciones mediante un planificador. cabe destacar que este planificador no ofrece garantÃ­as de ejecuciÃ³n de la aplicaciones ni las monitoriza. existe un segundo componente denominado gestor de aplicaciones que se encarga de aceptar las peticiones de trabajos, negocial el contenedor para la ejecutaciÃ³n de la aplicaciÃ³n y proporcionar, en caso de error, el reinicio de los trabajos. node manager se ejecutan en cada nodo y son los encargados de administrar los recursos locales (cpu, almacenamiento, memoria) y reportar su disponibilidad al resource manager. tambiÃ©n gestiona el lanzamiento y la supervisiÃ³n de los contenedores que ejecutan las tareas de las aplicaciones y mapean las variables de entorno necesarias, descargan las dependencias y los servicios necesarios para crar los procesos. application master se crea y ejecuta para cada aplicaciÃ³n dentro de un contenedor. es el responsable de negociar los recursos con el resouce manager y de supervisar el progreso de la aplicaciÃ³n. secuencia de trabajo en hadoop yarn el cliente envÃ­a una aplicaciÃ³n yarn al resource manager el resource manager reserva recursos en contenedores para la ejecuciÃ³n de la aplicaciÃ³n el application manager se registra con el resource manager y solicita los recursos necesarios para la aplicaciÃ³n el application manager notifica al node manager la ejecuciÃ³n de los contenedores asignados la aplicaciÃ³n yarn se ejecuta en le contenedor correspondiente el application master supervisa la ejecuciÃ³n y reporta el estado al resource manager y al application manager al finalizar la ejecuciÃ³n el application manager notifica al resource manager federaciÃ³n en hadoop yarn la federaciÃ³n en hadoop yarn pemite la conexiÃ³n de mÃºltiples clÃºsteres yarn para formar un clÃºster lÃ³gico Ãºnico. de esta manera podemos lograr una gestiÃ³n centralizada de recursos lo cual simplifica su administraciÃ³n. una de las ventajas clave es el escalamiento horizontal sin lÃ­mites dados por el tamaÃ±o del clÃºster. por lo tanto, se facilita la ejecuciÃ³n de trabajos de gran embergadura y distribuidos. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M01/03_mapreduce.html",
    "title": "ProgramaciÃ³n MapReduce",
    "body": " index search search back programaciÃ³n mapreduce contents el paradigma de programaci n mapreduce hadoop streaming qu es mapreduce fases de map reduce hadoop streaming ejemplo pr ctico de mapreduce optimizaci n combiners y partitioner programaci n mapreduce con lenguajes de alto nivel hive introducci n a apache hive arquitectura de apache hive cliente cat logo centralizado metastore driver de gesti n hiveql parser optimizador y planificador de consultas motor de ejecuci n almacenamiento de datos estructura interna tipos de datos estructura de datos tablas particiones buckets el paradigma de programaciÃ³n mapreduce: hadoop streaming Â¿quÃ© es mapreduce? el flujo de procesamiento de mapreduce se divide en dos etapas: map y reduce las cuales se ejecutan de manera distribuida en distintos nodos de procesamiento mientras que un proceso central denominado job tracker controla su ejecuciÃ³n. fases de map reduce map: los datos de entrada se subdividen en bloques independientes que son procesados de forma paralela por los mappers. normalmente hadoop intenta ejecutar los mappers en los nodos donde se encuentran los datos sobre los que va a trabajar (data locality). esto reduce los tiempos de acceso a los datos y la necesidad de transferencia de datos entre nodos. shuffle & sort: los datos intermedios de los mappers se ordenan. este proceso comienza mientras las tareas map estÃ¡n en progreso, a medida que van terminando. no es necesario esperar a que todas terminen. reduce: comienza cuando todos los datos han sido ordenados en shuffle & sort. recibe como entrada estes datos ordenados estructurados como pares clave valor agrupados por clave. cada reducer toma los valores asociados a una clave y los agrega/procesa y devuelve el resultado como salida que se puede almacenar en hdfs o en otro sistema. generalmente se concatenan varias fases de mapper-reducer para completar una tarea. hadoop streaming hadoop streaming se trata de una utilidad que permite ejecutar aplicaciones en hadoop escritas en cualquier lenguaje, a pesar de que hadoop este escrito en java. este proporciona un interfaz de entrada y de salida (stdin/stdout) que es consumida por las aplicaciones, tal que existe una pasarela de datos que actÃºa como comunicaciÃ³n entre hadoop y la aplicaciÃ³n. en la imagen anterior se muestra el flujo de un trabajo mapreduce. se leen los datos de entrada y se produce una lista de pares clave-valor. se proporcionan los datos en este formato a un mapper externo a travÃ©s del canal estÃ¡ndar de entrada. se devuelve el resultado de los mapper en el mismo formato a travÃ©s del canal estÃ¡ndar de salida. se proporcionan los datos procesados por los mapper al reducer externo a travÃ©s de la entrada estÃ¡ndar. se devuelve el resultado del reducer a travÃ©s de stdout. finalmente se almacena el resultado del reducer, bien en hdfs o en otro sistema externo. como podemos ver en todas las fases del proceso de mantiene siempre la estructura de pares clave-valor. ejemplo prÃ¡ctico de mapreduce para ejemplificar mapreduce planteamos el siguiente problema: queremos obtener la frecuencia de cada palabra en un documento. map: dividimos el proceso en dos mappers (idealmente cada proceso mapper se deberÃ­a de ejecutar en el nodo en el cual se encuentra el bloque de datos). estes se encargan de obtener la frecuencia de las palabras en la \"porciÃ³n de texto\" sobre el que trabajan. shuffle & sort: se ordenan y se agrupan los valores por la clave, donde la clave en nuestro caso es la palabra y el valor su frecuencia. reduce: se ejecuta un reducer por cada clave presente en el resultado, que agrupa (en nuestro caso suma) los valores bajo cada clave, es decir las frecuencias asociadas a la palabra. optimizaciÃ³n: combiners y partitioner mapreduce cuenta con dos componentes para optimizar el procesamiento denominados combiners y partitioners. combiners: agrupan los resultados de cada mapper antes de llegar a la frase de shuffle and sort con el objetivo de reducir el tamaÃ±o de los datos que se le proporcionan al reducer. partitioners: se utilizan para determinar cÃ³mo se distribuyen los datos a cada reducer. por defecto hadoop distribuye la carga de trabajo de manera uniforme entre los reducers, pero se puede personalizar utilizando los partitioners. programaciÃ³n mapreduce con lenguajes de alto nivel: hive introducciÃ³n a apache hive hive fue creado por facebook en el aÃ±o 2007. se trata de una abstracciÃ³n sobre hadoop, que permite utilizar su lenguaje de consulta, hiveql, para generar de manera automÃ¡tica programas mapreduce. arquitectura de apache hive cliente existen varias maneras de interactuar con hive: interfaz web su interfaz de lÃ­nea de comandos (cli), beeline hiveserver: un servidor thrift de hive que permite acceder a travÃ©s de jdbc, odbc o thrift api. catÃ¡logo centralizado: metastore hive almacena los metadatos (esquemas de las tablas, tipos de datos, etc) en un catÃ¡logo centralizado denominado metastore, que se trata de una base de datos relacional. existen tres tipos de configuraciones: embedded metastore: se integra el cÃ³digo en el mismo proceso que el programa hive y la base de datos. generalmente utilizado en entornos de prueba. local metastore: se ejecuta de manera local pero en un proceso distinto. remote metastore: se configura en remote, tal que se desliga el metastore del resto de procesos. generalmente se utiliza en entornos de producciÃ³n. driver de gestiÃ³n es el encargado de gestionar una consulta hiveql durante todo su ciclo de vida. hiveql parser se encarga de transformar las consultas en hiveql a programas mapreduce. optimizador y planificador de consultas optimizador de consultas: reorganiza la consulta para poder optimizarla mediante tÃ©cnicas como el filtrado anticipado. planificador de consultas: decide cÃ³mo y cuÃ¡ndo se ejecutan las consultas. motor de ejecuciÃ³n se encarga de ejecutar la tarea de consulta de forma distribuida y paralela en los nodos del cluster de hadoop. almacenamiento de datos los datos deben estar almacenados en hdfs y generalmente se almacenan en un formato aceptado por bases de datos relacionalales (p.ej. csv) o formatos que se integran con hive como parquet o avro. estructura interna hive impone sobre hdfs una estructura propia que llamaremos esquema o schema, la cual es almacenada en su propia base de datos (metastore). tipos de datos hive soporta tipos de datos primitivos: string, int, boolean y otros tipos de datos como date, asÃ­ como tipos de datos compuestos: array: son grupos de elementos del mismo tipo. map: define elementos clave-valor: {1: \"flor, 2: \"hoja\", 3: \"perro\", 4: \"gato} struct: permite crear objetos: {\"animal\": \"perro\", \"tipo\": \"mamÃ­fero\", \"numero_patas\": 4} estructura de datos tablas las tablas estÃ¡n compuestas por filas y columnas y tienen esquemas bien definidos, pero tienen una estructura lÃ³gica que se corresponde con una carpeta en hdfs. existen dos tipos de tablas: tabla interna (o gestionada): son tablas gestionadas por hive. cuando se crea una tabla interna automÃ¡ticamente se crea un carpeta en hdfs y hive controla tanto la tabla como sus datos. si esta tabla se borra tambiÃ©n se borra la carpeta en hdfs. -- creaciÃ³n tabla interna create table libros ( id int, titulo string, autor string, categoria string); ) row format delimited fields terminated by ',' stored as textfile; tabla externa: se trata de una tabla populada con datos ya existentes en hdfs. su estructura se define en la metastore y si borramos la tabla no se borra su contenido en hdfs. esta opciÃ³n es Ãºtil cuando compartimos datos entre distintas herramientas. -- creaciÃ³n tabla externa create external table libros_externa ( id int, titulo string, autor string, categoria string); ) row format delimited fields terminated by ',' stored as textfile location '/curso-bigdata/externa/libros'; en la tabla anterior se indica la ubicaciÃ³n de los datos utilizando location. el tÃ©rmino stored as indica el formato del fichero en el que residen los datos. particiones las particiones son subdivisiones en una tabla hive que permite la optimizaciÃ³n de las consultas eliminando valores irrelevantes. las subdivisiones se realizan en base a los valores de las columnas, por ejemplo si tenemos una variable nominal se podrÃ­a particionar la tabla segÃºn las distintas categorÃ­as. existen dos tipos de particiones: particiones estÃ¡ticas: son definidas por parte del usuario pero no cambian en el tiempo. particiones dinÃ¡micas: hive se ocupa de crear automÃ¡ticamente las particiones basÃ¡ndose en los valores de la columna especificada. -- creaciÃ³n tabla con particiones create table libros_particion ( id int, titulo string, autor string, ) partitioned by (categoria string); -- insertar datos insert into table libros_particion partition (categoria=fantÃ¡stico) values (1, 'harry potter y la piedra filosofal', 'j.k. rowling'); (2, 'harry potter y la cÃ¡mara secreta', 'j.k. rowling'); -- etc buckets los buckets son otra tÃ©cnica de divisiÃ³n de datos, estes estÃ¡n identificados mediante un par 'clave, valor' que distribuye los datos segÃºn una funciÃ³n hash. el objetivo principal para utilizar buckets es optimizar las consultas, sobre todo cuando estas son de tipo join o uniÃ³n. podemos realizar los buckets bien de forma dinÃ¡mica o estÃ¡tica. la diferencia radica en la elecciÃ³n de la columna que especifiquemos en la clÃ¡usula clustered by. -- creaciÃ³n tabla con buckets create table prestamos ( id int, libro_id int, fecha_prestamo string, socio_id int, ) clustered by (libro_id) into 5 buckets; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M01/04_data.html",
    "title": "InyecciÃ³n/ExtracciÃ³n y SerializaciÃ³n/DeserializaciÃ³n de los Datos",
    "body": " index search search back inyecciÃ³n/extracciÃ³n y serializaciÃ³n/deserializaciÃ³n de los datos contents apache flume las fuentes de datos batch vs streaming herramientas para la inyecci n y extracci n de datos apache scoop apache flume ventajas desventajas arquitectura de apache flume agentes eventos flujo de datos apache avro serializaci n deserializaci n la evoluci n de los formatos estructura la evoluci n de los esquemas ventajas ejemplo de esquema apache parquet formato columnar ventajas apache parquet estructura de un fichero parquet apache flume las fuentes de datos las empresas buscan tener todos sus datos centralizados, sean de la naturaleza que sean. tal que existirÃ¡ una Ãºnica fuente de verdad o single source of truth que permite la consistencia de los datos. bajo esta premisa nacen los conceptos de data lake y data warehouse. en un data warehouse los datos deberÃ¡n adaptarse a una estructura definida antes de poder ser guardados. se utilizan los procesos etl de extracciÃ³n, transformaciÃ³n y carga para adaptar los datos y su formato a la estructura definida antes de volcarlos. en un data lake los datos se guardan tal cual se reciben, pero pueden ser transformados antes de su extracciÃ³n. este utiliza procesos elt en los que primero se realiza la carga y cuando queramos extraer informaciÃ³n Ãºtil de dichos datos, realizaremos una transformaciÃ³n en caso de ser necesario. batch vs streaming el procesamiento batch se da cuando los datos a procesar se encuentran en un almacÃ©n de datos estÃ¡tico y estes son finitos, tal que se escogen y procesan por lotes. el procesamiento streaming es el procesamiento sobre datos que fluyen a travÃ©s de un sistema, conforme se van aÃ±adiendo al mismo. estes datos no son finitos y la toma de decisiones sobre ellos se hace en tiempo real. a continuaciÃ³n mostramos las diferencias entre los dos tipos de procesamiento: Â  procesamiento batch procesamiento streaming hardware los recursos deben ser capaces de procesar y almacenar grandes conjuntos de datos los datos tienen menor tamaÃ±o, por lo que los requisitos computacionales y el almacenamiento puede ser menor latencia la latencia puede ser de minutos, horas o dÃ­as. la latencia debe ser en segundos o milisegundos. tamaÃ±o del conjunto de datos grandes lotes de datos. un paquete de datos o varios de ellos, siempre de tamaÃ±o menor. anÃ¡lisis cÃ¡lculo complejo y anÃ¡lisis en un marco temporal mÃ¡s amplio. informes o cÃ¡lculos simples sobre los datos. herramientas para la inyecciÃ³n y extracciÃ³n de datos a continuaciÃ³n comentaremos las herramientas que nos permiten la ingesta y extracciÃ³n de los datos de forma masiva. apache scoop estÃ¡ pensado para la transferencia de datos desde un almacÃ©n estructurado a otro y utiliza procesamiento por lotes. sin embargo, desde junio de 2021 el proyecto sqoop dejÃ³ de tener continuidad. apache flume flume es un software para la ingesta de datos masivos en streaming. fue presentado por cloudera en el aÃ±o 2010 y posteriormente se incorporÃ³ bajo licencia apache como open source a la fundaciÃ³n apache. flume estÃ¡ basado en el flujo de datos en streaming de eventos sencillos y permite la lectura y escritura de mÃºltiples fuentes de datos. ademÃ¡s de ello, flume tiene mecanismos que aseguran la fiabilidad y confiabilidad de los datos. ventajas puede manejar grandes volÃºmenes de datos eficientemente distribuyendo la carga entre mÃºltiples agentes. presenta una gran flexibilidad, ya que nos permite recoger datos de diversas fuentes sin atender a su formato. se integra perfectamente con el ecosistema hadoop. presenta tolerancia a fallos. desventajas puede resultar difÃ­cil configurar los parÃ¡metros adecuados para los agentes. esto puede derivar en un fenÃ³meno conocido como backpressure que ocurre cuando el volumen de datos entrantes supera a la cantidad de datos que pueden ser consumidos por flume dando lugar a pÃ©rdida de eventos y por lo tanto de informaciÃ³n, para evitar esto hay que configurar adecuadamente a los agentes. estÃ¡ estrechamente ligado a la ingesta de datos en hadoop, para la ingesta en otro tipo de sistemas podemos utilizar herramientas como kafka. no ofrece herramientas para el monitoreo y diagnÃ³stico de errores de forma clara para el usuario. arquitectura de apache flume agentes se trata de un conjunto de componentes independientes que dirigen los eventos desde la entrada a la salida. ademÃ¡s los agentes pueder recibir y enviarse datos entre sÃ­. un agente estÃ¡ compuesto de tres componentes fundamentales: source: es el punto de entrada de datos de un agente. cada source es configurada para leer datos desde un lugar o ubicaciÃ³n especÃ­fica y enviarlos al channel del agente. channels: es el lugar temporal donde los datos llegan desde el source y se procesan o no, dependiendo del caso de uso, para transmitirlos al destino final (sink). sinks: son los encargados de leer los datos de los canales y enviarlos al siguiente componente del sistema, que serÃ¡ u otro agente, o el destino final. si los datos son consumidos por los sinks se eliminan de los canales. eventos se trata de una unidad de \"dato\". el dato es extraÃ­do por la fuente, enviado y procesado por el canal y consumido por el sink o sumidero. un evento se compone de dos partes: cabecera: registra informaciÃ³n de metadata mediante pares clave-valor datos: son almacenados en forma de array en el cuerpo de un evento. flujo de datos el flujo de datos describe el recorrido de los eventos desde el comienzo hasta el destino final. apache avro serializaciÃ³n/deserializaciÃ³n la serializaciÃ³n de un objeto consiste en la conversiÃ³n de dicho objeto a un formato que puede ser transmitido y almacenado de forma eficiente (p.ej. binario o json). la conversiÃ³n inversa se denomina deserializaciÃ³n. este proceso se lleva a cabo por los siguientes motivos: almacenamiento: generalmente queremos almacenar los datos de una forma eficiente (minimizando su tamaÃ±o). transmisiÃ³n: en sistemas distribuidos es necesario el paso de datos entre sistemas, por lo tanto necesitamos que los datos esten en un formato que optmice su envÃ­o y asegure la reconstrucciÃ³n de los datos. intercambio de datos: la serializaciÃ³n y deserializaciÃ³n permite intercambiar informaciÃ³n manteniendo la integridad de los datos, independientemente del lenguaje de programaciÃ³n. la evoluciÃ³n de los formatos apache avro pertenece a la fundaciÃ³n apache y se integra perfectamente con el ecosistema hadoop. avro es un formato que contiene tanto los datos como el esquema que deben de seguir los datos. en comparaciÃ³n con json, este no fuerza al uso de un esquema de los datos y su tamaÃ±o aumenta cuando existen claves repetidas. avro specifica dos formas de llevar a cabo la serializaciÃ³n: binaria: resulta en ficheros mÃ¡s pequeÃ±os. json: mejor para depurar y para comunicaciÃ³n en entornos web. avro specifica un orden de datos estÃ¡ndar. estructura un fichero estÃ¡ conformado por dos componentes: cabecera: contiene metadatos acerca del esquema de los datos entre otras cosas. uno o mÃ¡s bloques que pueden contener metadatos o datos. en todo fichero hay al menos un bloque de metadatos (ver block 1 en la siguiente imagen). avro sigue un formato basado en filas, tal que se agupan los datos en grupos de filas. la evoluciÃ³n de los esquemas avro se puede adaptar a la evoluciÃ³n de los esquemas. en concreto tenemos dos tipos de esquema: esquema de escritura: se utiliza en la serializaciÃ³n. esquema de lectura: se utiliza en la deserializaciÃ³n. existen los siguientes tipos de compatibilidad: compatibilidad hacia delante: un nuevo esquema puede leer datos escritos con un esquema anterior. compatibilidad hacia atrÃ¡s: un esquema antiguo puede leer datos escritos con un nuevo esquema. compatibilidad completa: combina la compatibilidad hacia delante y hacia atrÃ¡s. para asegurar la compatibilidad de los esquema tenemos diversas reglas: al aÃ±adir campos estes siempre deben de tener un valor por defecto. sÃ³lo se pueden eliminar campo si estes no son necesarios para las aplicaciones que leen/escriben el esquema. ventajas la compresiÃ³n de los datos es automÃ¡tica estÃ¡ totalmente tipado los ficheros contienen tanto los datos como la definiciÃ³n del esquema los datos pueden ser procesado por casi cualquier lenguaje permite la evoluciÃ³n sencilla de la definiciÃ³n del esquema de los datos permite el paso de datos entre sistemas escritos en distintos lenguajes ejemplo de esquema vamos a ver cÃ³mo definir un esquema para los siguientes datos, uno estÃ¡ almacenado como un archivo csv, mientras que el otro estÃ¡ definido como un fichero json. name, email, age ana,ana@avro.com,34 juan,juan@avro.com,28 alvaro,alvaro@avro.com,35 maria,maria@avro.com,25 luis,luis@avro.com,30 [ {\"name\": \"david\", \"email\": \"david@avro.com\", \"age\": 28}, {\"name\": \"pablo\", \"email\": \"pablo@avro.com\", \"age\": 31}, {\"name\": \"ines\", \"email\": \"ines@avro.com\", \"age\": 34} ] definimos un esquema que define un objeto, en concreto un usuario. por lo tanto, el esquema serÃ¡ de tipo record. los esquemas se escriben en formato json, y hay que indicar por cada campo el nombre de Ã©ste y el tipo de datos asociado. { \"type\": \"record\", \"name\": \"user\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"email\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, ] } apache parquet formato columnar los formatos que habÃ­amos estado tratando guardan los datos en filas (csv, avro, json), los formatos de archivos columnares guardan los datos en columnas. lo formatos mÃ¡s populares son apace parquet y apache ocr. por ejemplo, dada la siguiente tabla: la siguiente imagen nos ilustra las diferencia entre guardarla en formato fila y en formato columna: ventajas el formato columnar presenta ventajas a la hora de llevar a cabo agrupaciones en grandes conjuntos de datos. si utilizamos el formato basado en filas deberemos de recorrer toda la tabla, sin embargo con el formato columnar sÃ³lo es necesario leer el campo por el cual se lleva a cabo la agrupaciÃ³n. por el mismo motivo este tipo de formato puede suponer menos costes (p.ej. si estÃ¡n alojados en el cloud). generalmente no sÃ³lo se cobra por el almacenamiento si no tambiÃ©n por la consulta y el uso de los datos. tal que si sÃ³lo queremos consultar una columna, con un formato basado en filas de nuevo deberemos de consultar la fila completa, mientras que el formato columnar nos permite sÃ³lo obtener el campo consultado. apache parquet parquet es un tipo de fichero en formato columnar diseÃ±ado para realizar consultas eficientes sobre las columnas. ademÃ¡s permite estructuras complejas de datos anidados y ofrece esquemas de compresiÃ³n muy eficientes. por ello es popular en campos como el big data, ya que a parte de su eficiencia reduce costes. parquet estÃ¡ sostenido y mantenido en la fundaciÃ³n apache y, por tanto, estÃ¡ disponible para cualquier proyecto bajo licencia apache. estructura de un fichero parquet los ficheros parquet tienen una estructura jerarquizada: encabezado, metadatos y grupos de filas. encabezado: contiene informaciÃ³n necesaria para la lectura del fichero. magic number se trata de un nÃºmero al final y al principio del fichero que asegura que estÃ¡ completo y no corrupto. metadatos: contienen informaciÃ³n sobre los datos; el esquema de las columnas, tipos de datos, algunas estadÃ­sticas y otros datos relevantes. grupos de filas: contienen los datos organizados por columnas. cada grupo de filas tiene un subconjunto de la tabla, donde las columnas de cada grupo de filas se dividen en pÃ¡ginas, que son las unidades mÃ¡s pequeÃ±as de almacenamiento. la subdivisiÃ³n en pÃ¡ginas tiene como fin mejorar la eficiencia ya que admiten la compresiÃ³n individual. podemos encontrar pÃ¡ginas de datos, Ã­ndices o de diccionario. esquema: define la estructura de los datos y los tipos de datos de cada columna. en caso de que los datos estÃ©n anidados tambiÃ©n define la jerarquÃ­a de estos. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M02/01_intro.html",
    "title": "Introduction To Apache Spark",
    "body": " index search search back introduction to apache spark contents key features of apache spark history and evolution architecture of apache spark core concepts and apis core concepts apis running spark applications and ecosystem conclusion apache spark is a unified analytics engine designed for large-scale data processing. it provides a consistent set of apis for handling a wide range of data processing tasks, including batch processing, sql queries, streaming computation, and machine learning. spark is essentially a computing engine that focuses on processing data from various storage systems such as cloud storage, distributed file systems, key-value stores, and message buses key features of apache spark unified platform: spark offers a single platform for diverse data processing tasks, enabling developers to use the same engine and apis for different types of data analysis, like processing data in batches, running sql queries, handling real-time data streams, and even machine learning. rich libraries: spark includes a collection of built-in and external libraries, such as spark sql for structured data processing, mllib for machine learning, spark streaming and structured streaming for stream processing, and graphx for graph analytics. designed for big data: spark addresses the challenges of processing massive datasets, driven by changes in hardware towards multi-core processors and the explosion in data volume and storage capabilities. history and evolution origins: spark originated in 2009 at uc berkeley's amplab as a research project aimed at overcoming the limitations of hadoop mapreduce for iterative algorithms. early focus: initial releases concentrated on batch applications and interactive data science using scala and shark (a sql engine). expansion: spark adopted a \"standard library\" approach, expanding with libraries like mllib, spark streaming, and graphx. open source development: in 2013, spark became part of the apache software foundation, leading to active development and widespread adoption. recent developments: recent releases have focused on refining structured apis such as dataframes and datasets for enhanced optimisation architecture of apache spark cluster management: spark uses a cluster manager, such as standalone, yarn, or mesos, to manage cluster resources. application structure: a spark application consists of a driver process and multiple executor processes. driver process: the driver runs the main() function, maintains application information, interacts with the user, and schedules tasks across executors. executor processes: executors execute spark code on data partitions distributed across the cluster. spark supports multiple language apis, including python, java, scala, r, and sql [5]. developers can write applications in their preferred language. core concepts and apis core concepts unified analytics engine: spark provides a consistent set of apis for various data processing tasks, including batch processing, sql queries, streaming computation, and machine learning. computing engine focus: spark primarily processes data loaded from diverse storage systems, rather than functioning as permanent storage itself. parallel processing: spark leverages parallel processing to efficiently analyse massive datasets, addressing the challenges posed by the shift to multi-core processors and the explosion of data. lazy evaluation: spark optimises execution plans before processing, delaying action until triggered. apis sparksession: the entry point for interacting with spark functionality, managing the spark application and providing access to various apis. dataframes: the primary structured api representing tabular data in rows and columns partitions: data is divided into partitions for parallel processing across executors transformations: operations that define modifications to a dataframe without altering the original data, including narrow transformations that operate within partitions and wide transformations that involve shuffling data across partitions. actions: actions trigger the execution of transformations and return results datasets: a type-safe version of the structured api in java and scala, enabling compile-time checks for data types resilient distributed datasets (rdds): spark's lower-level api, providing more control over data partitioning and manipulation but less commonly used in modern spark applications structured streaming: a high-level api that allows applying batch-like operations to streaming data mllib: spark's machine learning library, offering algorithms for classification, regression, clustering, and deep learning running spark applications and ecosystem spark-submit: a command-line tool for submitting spark applications to a cluster, allowing for resource specification, execution parameters, and command-line arguments. spark ecosystem and packages: a wide range of third-party packages expands spark's functionality and integrates with various systems. conclusion spark provides a powerful and versatile platform for tackling big data challenges across various domains. its unified engine, rich libraries, and intuitive apis empower developers to efficiently process, analyse, and extract insights from large datasets. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M02/02_prog_spark.html",
    "title": "Programming Spark Applications",
    "body": " index search search back programming spark applications contents chapter 3 building and running a spark application building spark jobs with maven chapter 4 creating a sparksession object understanding the sparksession object working with datasets dataframes and rdds building a sparksession object chapter 5 loading and saving data in spark chapter 6 manipulating your rdd chapter 9 foundations of datasets dataframes the proverbial workhorse for data scientists chapter 3: building and running a spark application spark applications can be built and run in different ways: interactive mode with spark shell: this method is suitable for quick prototyping and interactive data exploration. spark provides a shell interface for scala, python, and r, allowing users to execute commands and get immediate feedback. this interactive mode is excellent for learning spark apis, testing code snippets, and performing ad-hoc data analysis. ide for application development: for developing more complex applications, integrated development environments (ides) like eclipse and intellij are popular choices. these ides provide features like code completion, debugging, and project management, making it easier to develop, test, and deploy spark applications. building spark jobs with maven building spark jobs is slightly more intricate than building standard applications. this complexity arises because spark applications often need to be executed on a cluster of machines, requiring spark dependencies to be available on all nodes. there are two primary methods for building spark jobs: building with maven: maven is a widely used build tool in the java ecosystem, and it is officially recommended for packaging spark applications. maven simplifies the build process by managing dependencies, compiling code, and packaging it into a jar file. developers can include spark dependencies through maven central, a public repository for java libraries. maven can also package spark and its dependencies into a single executable jar file, making it easier to deploy and run on a cluster. building with other build systems: while maven is the recommended build tool, spark supports building a \"fat jar\" file that contains all its dependencies. this fat jar can be used with other build systems like sbt, gradle, or even custom build scripts. the process usually involves building the spark assembly jar using sbt and then including it in the build path of the other build system. this approach allows developers to use their preferred build tools while still ensuring that all necessary dependencies are included. steps to build a spark job with maven: create a new directory and generate the maven template. the example shows building a java spark job: mkdir example-java-build/; cd example-java-build mvn archetype:generate \\ -darchetypegroupid=org.apache.maven.archetypes \\ -dgroupid=spark.examples \\ -dartifactid=javawordcount \\ -dfilter=org.apache.maven.archetypes:maven-archetype-quickstart cp ../examples/src/main/java/spark/examples/javawordcount.java \\ javawordcount/src/main/java/spark/examples/javawordcount.java update maven pom.xml to include spark version and jdk version information. add the following code between the <project> tags: <dependencies> <dependency> <groupid>junit</groupid> <artifactid>junit</artifactid> <version>4.11</version> <scope>test</scope> </dependency> <dependency> <groupid>org.spark-project</groupid> <artifactid>spark-core_2.11</artifactid> <version>2.0.0</version> </dependency> </dependencies> <build> <plugins> <plugin> <groupid>org.apache.maven.plugins</groupid> <artifactid>maven-compiler-plugin</artifactid> <configuration> <source>1.7</source> <target>1.7</target> </configuration> </plugin> </plugins> </build> build the jar file: mvn package run the spark job: spark_home=\"../\" spark_examples_jar=\"./target/javawordcount-1.0-snapshot.jar\" java -cp ./target/javawordcount-1.0-snapshot.jar:../../core/target/spark-core-assembly-1.5.2.jar spark.examples.javawordcount local[1] ../../readme chapter 4: creating a sparksession object understanding the sparksession object the sparksession object acts as the primary entry point for interacting with spark functionalities. introduced in spark 2.0.0, it represents a connection to a spark cluster, which can be either local for development and testing or remote for distributed processing on a cluster of machines. the sparksession provides a unified interface for various spark components and operations: unified entry point: before sparksession, developers had to interact with multiple context objects like sparkcontext, sqlcontext, and hivecontext for different functionalities. sparksession encapsulates these contexts, providing a single entry point for all spark operations, simplifying the development process. dataset and dataframe creation: sparksession enables the creation of datasets and dataframes, which are high-level abstractions for representing structured data in spark. datasets are type-safe, providing compile-time type checking, while dataframes offer a schema-based view of the data. sql execution: sparksession facilitates the execution of sql queries against data in spark. it allows users to register dataframes as temporary views and then run sql queries against those views, providing a familiar way to interact with data. rdd access: while datasets and dataframes are the preferred abstractions in spark 2.0.0 and later, sparksession still provides access to the underlying rdds (resilient distributed datasets). developers can obtain the sparkcontext from the sparksession to work with rdds when necessary. working with datasets, dataframes, and rdds spark provides different abstractions for representing and manipulating data: datasets and dataframes: these are high-level, schema-based abstractions introduced in spark 2.0.0. datasets provide type safety and compile-time checking, while dataframes are untyped but offer a schema-based view of the data. both datasets and dataframes offer a rich api for data manipulation, including filtering, sorting, grouping, aggregation, and joining operations. they are built on top of rdds and leverage catalyst, spark's query optimizer, to optimize execution plans for better performance. rdds: rdds are the fundamental data structure in spark, representing an immutable, partitioned collection of data distributed across the cluster. rdds provide low-level control over data and operations, allowing developers to implement custom data processing logic. they are useful for complex computations that cannot be efficiently expressed using dataset or dataframe apis. rdds follow lazy evaluation, meaning that transformations on rdds are not executed immediately but are computed only when an action requiring the results is called. the choice of abstraction depends on the specific use case and the level of control required. datasets and dataframes are generally preferred for most data manipulation tasks due to their higher-level api, performance optimizations, and ease of use. rdds are suitable for situations demanding low-level control or when dealing with unstructured data. building a sparksession object scala and python: val sparksession = new sparksession.builder.master(master_path).appname(\"application name\").config(\"optional configuration parameters\").getorcreate() it's recommended to read values from the environment with reasonable defaults for flexibility in changing environments. spark-shell/pyspark automatically creates the sparksession object and assigns it to the spark variable. access the sparkcontext object using spark.sparkcontext. chapter 5: loading and saving data in spark spark offers flexible mechanisms for loading and saving data from various sources and formats: loading data into rdds: data can be loaded into rdds from various sources, including local collections, text files, csv files, sequence files, and external databases like hbase. sparkcontext provides functions like parallelize(), textfile(), sequencefile(), and newapihadooprdd() to load data into rdds. saving data from rdds: rdds can be saved to different formats like text files, sequence files, and object files. functions like saveastextfile(), saveasobjectfile(), and saveassequencefile() are used to save rdd data. loading and saving data with datasets and dataframes: datasets and dataframes provide more streamlined and efficient methods for data loading and saving. sparksession's read api supports reading data from various formats like csv, json, parquet, avro, and jdbc. similarly, the write api allows saving data to different formats. chapter 6: manipulating your rdd spark offers a rich set of operations for manipulating data in rdds, datasets, and dataframes: transformations: transformations are operations that create new rdds, datasets, or dataframes from existing ones without changing the original data. common transformations include map(), filter(), flatmap(), reducebykey(), groupbykey(), and sortbykey(). actions: actions are operations that trigger computations on rdds, datasets, or dataframes and return results to the driver program. examples of actions include count(), collect(), reduce(), take(), and saveastextfile(). shared states and accumulators: while distributed computation in spark generally discourages shared states, accumulators provide a safe mechanism for aggregating values from different partitions across the cluster. accumulators are write-only variables that can be used to count events or sum values from different parts of the data. broadcast variables: broadcast variables enable efficient sharing of read-only data across the cluster. instead of sending the data to every task, spark broadcasts the data once to each executor node, making it available to all tasks running on that node. example: parsing csv files with error handling (scala) import org.apache.spark.sparkconf import org.apache.spark.sparkcontext import org.apache.spark.sparkfiles import org.apache.spark.api.java.javasparkcontext import au.com.bytecode.opencsv.csvreader import java.io.stringreader object loadcsvwithcountersexample { def main(args: array[string]) { val sc = new sparkcontext(\"local\", \"chapter 6\") println(s\"running spark version ${sc.version}\") val invalidlinecounter = sc.accumulator(0) val invalidnumericlinecounter = sc.accumulator(0) val infile = sc.textfile(\"/volumes/sdxc-01/fdps-vii/data/line_of_numbers.csv\") val splitlines = infile.flatmap(line => { try { val reader = new csvreader(new stringreader(line)) some(reader.readnext()) } catch { case _ => { invalidlinecounter += 1 none } } }) val numericdata = splitlines.flatmap(line => { try { some(line.map(_.todouble)) } catch { case _ => { invalidnumericlinecounter += 1 none } } }) val summeddata = numericdata.map(row => row.sum) println(summeddata.collect().mkstring(\",\")) println(\"errors: \" + invalidlinecounter + \",\" + invalidnumericlinecounter) } } this example demonstrates the use of accumulators to count invalid lines and lines with invalid numeric data. it also utilises flatmap() to handle parsing errors and filter out invalid lines. example: word frequency analysis (python) from pyspark.context import sparkcontext from pyspark.conf import sparkconf from operator import add print(\"running spark version %s\" % (sc.version)) conf = sparkconf() print(conf.todebugstring()) # read and process barack obama's speeches lines = sc.textfile(\"sotu/2009-2014-bo.txt\") word_count_bo = lines.flatmap(lambda x: x.split(' ')). \\ map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \\ reducebykey(add) word_count_bo.count() # 6658 without lower, 6299 with lower, rstrip, lstrip 4835 # read and process abraham lincoln's speeches lines = sc.textfile(\"sotu/1861-1864-al.txt\") word_count_al = lines.flatmap(lambda x: x.split(' ')). \\ map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \\ reducebykey(add) word_count_al.count() # sort word counts for barack obama's speeches word_count_bo_1 = word_count_bo.sortby(lambda x: x[1], ascending=false) # print top 10 most frequent words for x in word_count_bo_1.take(10): print(x) # filter out common words common_words = [...] word_count_bo_clean = word_count_bo_1.filter(lambda x: x not in common_words) word_count_al_clean = word_count_al.filter(lambda x: x not in common_words) # find words spoken by obama but not lincoln for x in word_count_bo_clean.subtractbykey(word_count_al_clean).sortby(lambda x: x[1], ascending=false).take(15): print(x) this example illustrates reading text files, splitting into words, calculating word frequencies, sorting, filtering, and comparing word usage between two sets of speeches. chapter 9: foundations of datasets/dataframes â the proverbial workhorse for data scientists spark supports multiple programming languages, including scala, java, python, and r. this multilingual capability allows developers to use the language they are most comfortable with for different parts of a spark application. this approach, known as polyglot programming, offers several advantages: leveraging existing skills: developers can use their existing language skills to work with spark without having to learn a new language. using specialized libraries: different languages have different strengths and specialized libraries. polyglot programming allows developers to use the most suitable language and libraries for specific tasks within a spark application. code reusability: code written in one language can often be reused or adapted for use in other languages, promoting code sharing and reducing development time. spark encourages polyglot programming by providing consistent apis across different languages, making it easy to switch between languages and integrate code written in different languages. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_06.html",
    "title": "Limits at Infinity. Horizontal Asymptotes",
    "body": " index search search back limits at infinity. horizontal asymptotes contents intuitive definition of a limit at infinity horizontal asymptote evaluating limits at infinity infinite limits at infinity precise definition of a limit at infinity precise definition of an infinite limit at infinity intuitive definition of a limit at infinity let \\(f\\) be a function defined on some interval \\((a, \\infty)\\), then: \\begin{align} \\lim_{x\\rightarrow \\infty} f(x) = l \\end{align} means that the values of \\(f(x)\\) can be made arbitrarily close to \\(l\\) by requiring \\(x\\) to be sufficiently large. geometric illustration of this defintion are shown in figure \\(2\\). let \\(f\\) be a function defined on some interval \\((-\\infty, a)\\), then: \\begin{align} \\lim_{x\\rightarrow -\\infty} f(x) = l \\end{align} means that the values of \\(f(x)\\) can be made arbitrarily close to \\(l\\) by requiring \\(x\\) to be sufficiently large negative. this definition is illustrated in figure \\(3\\). horizontal asymptote the line \\(y = l\\) is called a horizontal asymptote of the curve \\(y = f(x)\\) if either: \\begin{align} \\lim_{x \\rightarrow \\infty} f(x) = l \\end{align} or \\begin{align} \\lim_{x \\rightarrow -\\infty} f(x) = l \\end{align} evaluating limits at infinity most of the limit laws also hold for limits at infinity, with the exception of laws \\(10\\) and \\(11\\). let's see the following theorem on the limit at infinity of a rational function: if \\(r > 0\\) is a rational number, then: \\begin{align} \\lim_{x \\rightarrow \\infty} \\frac{1}{x^r} = 0 \\end{align} if \\(r > 0\\) is a rational number such that \\(x^r\\) is defined for all \\(x\\), then: \\begin{align} \\lim_{x \\rightarrow -\\infty} \\frac{1}{x^r} = 0 \\end{align} infinite limits at infinity the notation: \\begin{align} \\lim_{x \\rightarrow \\infty} f(x) = \\infty \\end{align} means that the values of \\(f(x)\\) become large as \\(x\\) becomes large. similarly: \\begin{align} \\lim_{x \\rightarrow -\\infty} f(x) = \\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow \\infty} f(x) = -\\infty \\end{align} \\begin{align} \\lim_{x \\rightarrow -\\infty} f(x) = -\\infty \\end{align} in general, the limit laws cannot be applied to infinite limits, because \\(\\infty\\) is not a number. precise definition of a limit at infinity let \\(f\\) be a function defined on some interval \\((a, \\infty)\\), then: \\begin{align} \\lim_{x \\rightarrow \\infty} f(x) = l \\end{align} means that for every \\(\\epsilon > 0\\) there is a corresponding number \\(n\\) such that: \\begin{align} \\text{ if } x > n \\text{ then } |f(x) - l| < \\epsilon \\end{align} in words, this says that the values of \\(f(x)\\) can be mace arbitrarily cloe to \\(l\\) by requiring \\(x\\) to be sufficiently lare. graphically it says that by keeping \\(x\\) large enough we can make the graph of \\(f\\) lie between the given horizontal lines \\(y = l - \\epsilon\\) and \\(y = l + \\epsilon\\) as in figure 14. let \\(f\\) be a function defined on some interval \\((-\\infty, a)\\), then: \\begin{align} \\lim_{x \\rightarrow -\\infty} f(x) = l \\end{align} means that for every \\(\\epsilon > 0\\) there is a corresponding number \\(n\\) such that: \\begin{align} \\text{ if } x < n \\text{ then } |f(x) - l| < \\epsilon \\end{align} this definition is illustrated on figure 16. precise definition of an infinite limit at infinity let \\(f\\) be a function defined on some interval \\((a, \\infty)\\), then: \\begin{align} \\lim_{x \\rightarrow \\infty} f(x) = \\infty \\end{align} means that for every positive number \\(m\\) there is a corresponding positive number \\(n\\) such that: \\begin{align} \\text{ if } x > n \\text{ then } f(x) > m \\end{align} this definition is illustrated on figure 19. similar definitions apply when the symbol \\(\\infty\\) is replaced by \\(-\\infty\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M02/03_lib_spark.html",
    "title": "LibrerÃ­as/Componentes de Spark",
    "body": " index search search back librerÃ­as/componentes de spark contents spark sql spark sql architecture spark sql evolution spark sql programming example workflow important points code examples in spark sql machine learning with spark ml pipelines spark for machine learning ml pipelines addressing the data pipeline in ml structure of spark ml apis code examples from spark machine learning basic statistics with spark datasets linear regression with data transformation and model evaluation classification with decision tree data transformation and model evaluation clustering with k-means data transformation and model evaluation recommendation with als data transformation and model evaluation graphx introduction to graph processing graph processing systems challenges of graph processing spark graphx graphx architecture graphx computational model building graphs with graphx graphx api landscape structural apis community detection and analysis graphx algorithms graph parallel computation apis partition strategies case study alphago tweet analytics code examples in the graphx source building a simple graph defining vertices and edges creating rdds and the graph structural apis extracting subgraphs community detection pagerank calculation aggregate messages api finding the oldest follower finding the oldest followee calculating in-degree and out-degree alphago tweet analytics loading data and creating a dataframe mapping data to vertices and edges creating the graph and running algorithms spark sql spark sql is an important feature in the spark ecosystem that allows integration with different data sources and other subsystems, such as visualization. spark sql is not meant to replace sql databases, but rather to complement spark's data wrangling and input capabilities by providing a versatile query interface for spark data. this ability to scale complex data operations is only valuable if the results can be used flexibly, which is what spark sql achieves. spark sql architecture spark sql's architecture is layered, with each layer performing specific functions. the bottom layer is the data access layer, which works with multiple formats and typically utilizes a distributed filesystem such as hdfs. the computation layer leverages the distributed processing power of the spark engine, including its streaming capabilities, and typically operates on rdds (resilient distributed datasets). the dataset/dataframe layer provides the api for interacting with the data. spark sql sits on top of this layer, providing data access for various applications, dashboards, and bi tools. this architecture allows spark to leverage the vast knowledge base of sql among data professionals and use it to query spark data. spark sql evolution prior to spark 2.0, schemardd was at the heart of spark sql. it essentially attached a schema to an rdd, enabling sql queries to be run on rdds. however, with spark 2.0, datasets became the primary way to work with data. datasets offer the advantages of both rdds and strong typing, providing a more robust and efficient way to handle data. in languages like python and r, which lack compile-time type checking, datasets and dataframes are merged and referred to as dataframes. spark sql programming spark 2.0 introduced sparksession, which replaced sqlcontext, hivecontext, and other components. the sparksession instance has a versatile read method capable of handling various data formats like csv, parquet, json, and jdbc. this method allows you to specify format-related options such as headers and delimiters. to use spark sql, you first need to create a dataset by reading data from a source and informing spark about its structure and types. you can then apply sql statements to query the data. to create a view that can be queried using sql, you can use the createorreplacetempview method. you can then use sql statements to filter, join, and aggregate data within these views. example workflow a typical spark sql workflow involves: defining a case class to represent the data structure. reading the data file using sparksession.read, specifying options like header and inferschema. creating a dataset with the case class as its element type. creating a temporary view using createorreplacetempview for sql access. running sql queries on the view using spark.sql. displaying and analyzing the results using methods like show, head, and orderby important points spark 2.0 simplified spark sql by introducing datasets and sparksession. you can start the spark shell with the -deprecation flag to receive messages about deprecated methods. the read method can infer schema automatically using the inferschema option. use createorreplacetempview to avoid the temptablealreadyexists exception. spark sql enables complex queries involving multiple tables and various operations like filtering, joining, and aggregation. code examples in spark sql example 1: loading a csv file into a dataset case class employee(employeeid : string, lastname : string, firstname : string, title : string, birthdate : string, hiredate : string, city : string, state : string, zip : string, country : string, reportsto : string) // ... ... val filepath = \"/users/ksankar/fdps-v3/\" println(s\"running spark version ${sc.version}\") // val employees = spark.read.option(\"header\",\"true\"). csv(filepath + \"data/nw-employees.csv\").as[employee] println(\"employees has \"+employees.count()+\" rows\") employees.show(5) employees.head() this code snippet first defines a case class called employee representing the structure of the employee data. then, it sets a filepath variable pointing to the directory containing the data files. the code then uses the spark.read.csv method to read the csv file into a dataset called employees. the option(\"header\", \"true\") tells spark that the first row of the csv file contains column headers. the .as[employee] part specifies that the dataset should be composed of employee objects. finally, the code prints the number of rows in the dataset, displays the first \\(5\\) rows using show(5), and retrieves the first row using head(). example 2: creating a view and running sql queries employees.createorreplacetempview(\"employeestable\") var result = spark.sql(\"select * from employeestable\") result.show(5) result.head(3) // employees.explain(true) result = spark.sql(\"select * from employeestable where state = 'wa'\") result.show(5) result.head(3) // result.explain(true) this code creates a temporary view called \"employeestable\" from the employees dataset using createorreplacetempview. this view enables you to query the dataset using sql statements. the first spark.sql statement selects all columns from the \"employeestable\" view. the second query filters the results to include only employees from the state of washington (where state = 'wa'). both queries use show(5) to display the first \\(5\\) rows of the result and head(3) to retrieve the first \\(3\\) rows. example 3: handling multiple tables and joins // ... ... val orders = spark.read.option(\"header\",\"true\"). option(\"inferschema\",\"true\"). csv(filepath + \"data/nw-orders.csv\").as[order] println(\"orders has \"+orders.count()+\" rows\") orders.show(5) orders.head() orders.dtypes // ... ... // // now the interesting part // result = spark.sql(\"select orderdetailstable.orderid, shipcountry, unitprice, qty, discount from orderstable inner join orderdetailstable on orderstable.orderid = orderdetailstable.orderid\") result.show(10) result.head(3) // // sales by country // result = spark.sql(\"select shipcountry, sum(orderdetailstable.unitprice * qty * discount) as productsales from orderstable inner join orderdetailstable on orderstable.orderid = orderdetailstable.orderid group by shipcountry\") result.count() result.show(10) result.head(3) result.orderby($\"productsales\".desc).show(10) // top 10 by sales this example demonstrates loading the \"orders\" table, creating a view, and then performing joins and aggregations. it first reads the \"nw-orders.csv\" file into an orders dataset. notably, it uses the option(\"inferschema\", \"true\") option, which tells spark to automatically infer the schema for the data. this eliminates the need to define a case class beforehand. the code then executes two sql queries. the first query performs an inner join between the \"orderstable\" and \"orderdetailstable\" based on the common \"orderid\" column and selects specific columns from the joined result. the second query calculates total sales (sum(orderdetailstable.unitprice * qty * discount)) per country, groups the results by \"shipcountry\", and orders the final output by \"productsales\" in descending order to show the top \\(10\\) countries by sales. machine learning with spark ml pipelines this section provides a summary of spark's capabilities for machine learning (ml), focusing on ml pipelines and the transition from mllib to ml apis. it covers various ml algorithms, data transformation techniques, and the concept of pipelines for streamlined ml workflows. spark for machine learning spark is attractive for ml due to its ability to handle massive computations. spark 2.0.0 onwards, spark is considered a leading platform for building ml algorithms and applications. spark's ml capabilities are primarily accessed through the org.apache.spark.ml package for scala and java, and pyspark.ml for python. spark supports a wide array of ml algorithms, including basic statistics, linear regression, classification, clustering, recommendation systems, dimensionality reduction, feature extraction, and more. ml pipelines: addressing the data pipeline in ml before spark 1.6.0, the mllib apis operated on rdds, but they lacked support for the data pipelines inherent in ml. with the introduction of dataframes and datasets, mllib evolved into the ml pipeline framework, offering more capabilities and addressing the entire ml workflow. mllib apis are now in maintenance mode and will eventually be deprecated. while you should use ml apis going forward, some functionalities might require using mllib and converting the output rdd to a dataframe for further processing with ml apis. a typical ml process involves several steps: data acquisition: obtain data from internal or external sources, ensuring anonymity and removal of personally identifiable information (pii). data transformation: convert raw data into a usable format, for example, transforming a csv file into a dataframe. feature extraction: extract relevant features from the data, such as separating text into words or normalizing them. data splitting: divide the data into training and testing sets, using appropriate strategies based on data characteristics like time series or class imbalance. model training: fit the training data to different ml models. tune hyperparameters for optimal performance. select the best-performing model for the specific problem. model evaluation: assess the model's performance using the test data. model deployment: implement the trained model in a production environment for real-time predictions. ml pipelines in spark address all stages of this workflow. structure of spark ml apis spark ml apis have a specific structure that can be challenging to navigate initially. familiarity with this structure is key to effectively utilizing spark for ml tasks. the source material provides a diagram to illustrate this, recommending a deeper understanding of the pipeline concept to enhance proficiency in using spark ml classes. code examples from spark machine learning basic statistics with spark datasets this code snippet shows how to load car mileage data from a csv file, compute basic statistics using spark datasets, and calculate the correlation and covariance between specific variables: val spark = sparksession.builder .master(\"local\") .appname(\"chapter 11\") .config(\"spark.logconf\",\"true\") .config(\"spark.loglevel\",\"error\") .getorcreate() println(s\"running spark version ${spark.version}\") val filepath = \"/users/ksankar/fdps-v3/\" val cars = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\") .csv(filepath + \"data/car-data/car-milage.csv\") println(\"cars has \"+cars.count()+\" rows\") cars.show(5) cars.printschema() // computing statistics cars.describe(\"mpg\",\"hp\",\"weight\",\"automatic\").show() var cor = cars.stat.corr(\"hp\",\"weight\") println(\"hp to weight : correlation = %2.4f\".format(cor)) var cov = cars.stat.cov(\"hp\",\"weight\") println(\"hp to weight : covariance = %2.4f\".format(cov)) cor = cars.stat.corr(\"raratio\",\"width\") println(\"rear axle ratio to width : correlation = %2.4f\".format(cor)) cov = cars.stat.cov(\"raratio\",\"width\") println(\"rear axle ratio to width : covariance = %2.4f\".format(cov)) sparksession creation: the code starts by creating a sparksession, which is the entry point for spark applications. data loading: the spark.read.csv() method loads data from the specified csv file into a dataframe named cars. the option(\"header\",\"true\") indicates that the first row contains column headers, and option(\"inferschema\",\"true\") instructs spark to automatically infer the data types for each column. basic statistics: the describe() method computes summary statistics like count, mean, standard deviation, min, and max for the specified columns (\"mpg\", \"hp\", \"weight\", \"automatic\"). correlation and covariance: the stat.corr() and stat.cov() methods are used to calculate the correlation and covariance between pairs of variables (\"hp\" and \"weight\", \"raratio\" and \"width\"). linear regression with data transformation and model evaluation this code example demonstrates a linear regression model using spark ml pipelines. it includes data transformation, feature extraction, splitting data into training and testing sets, fitting the model, making predictions, and evaluating the model: // data transformation and feature extraction val cars1 = cars.na.drop() val assembler = new vectorassembler() assembler.setinputcols(array(\"displacement\", \"hp\", \"torque\", \"cratio\", \"raratio\", \"carbbarrells\", \"noofspeed\", \"length\", \"width\", \"weight\", \"automatic\")) assembler.setoutputcol(\"features\") val cars2 = assembler.transform(cars1) cars2.show(40) // data split val train = cars2.filter(cars1(\"weight\") <= 4000) val test = cars2.filter(cars1(\"weight\") > 4000) test.show() println(\"train = \"+train.count()+\" test = \"+test.count()) // linear regression model val alglr = new linearregression() alglr.setmaxiter(100) alglr.setregparam(0.3) alglr.setelasticnetparam(0.8) alglr.setlabelcol(\"mpg\") val mdllr = alglr.fit(train) println(s\"coefficients: ${mdllr.coefficients} intercept: ${mdllr.intercept}\") val trsummary = mdllr.summary println(s\"numiterations: ${trsummary.totaliterations}\") println(s\"iteration summary history: ${trsummary.objectivehistory.tolist}\") trsummary.residuals.show() println(s\"rmse: ${trsummary.rootmeansquarederror}\") println(s\"r2: ${trsummary.r2}\") // predictions val predictions = mdllr.transform(test) predictions.show() // model evaluation val evaluator = new regressionevaluator() evaluator.setlabelcol(\"mpg\") val rmse = evaluator.evaluate(predictions) println(\"root mean squared error = \"+\"%6.3f\".format(rmse)) val mse = evaluator.evaluate(predictions) println(\"mean squared error = \"+\"%6.3f\".format(mse)) data preprocessing: cars.na.drop() removes rows with missing values (na). vectorassembler combines multiple input columns into a single vector column named \"features\", which is required for many ml algorithms. data splitting: the data is split into training and test sets based on the \"weight\" column. linear regression: a linearregression object is created and configured with parameters like maximum iterations (setmaxiter), regularization parameter (setregparam), and elastic net parameter (setelasticnetparam). the model is trained using the fit() method on the training data. model coefficients, intercept, and training summary are printed. predictions: predictions are made on the test data using the transform() method, adding a \"predictions\" column to the resulting dataframe. model evaluation: a regressionevaluator is used to calculate rmse and mse. the evaluate() method calculates the metrics based on the \"mpg\" label column and the predicted values. classification with decision tree, data transformation and model evaluation this example demonstrates a classification task using a decision tree algorithm. it involves data loading, transformation, feature extraction, model training, prediction, and evaluation: // loading data val filepath = \"/users/ksankar/fdps-v3/\" val passengers = spark.read.option(\"header\",\"true\").option(\"inferschema\",\"true\") .csv(filepath + \"data/titanic3_02.csv\") println(\"passengers has \"+passengers.count()+\" rows\") passengers.show(5) passengers.printschema() // data transformation and feature extraction val passengers1 = passengers.select( passengers(\"pclass\"), passengers(\"survived\").cast(doubletype).as(\"survived\"), passengers(\"gender\"), passengers(\"age\"), passengers(\"sibsp\"), passengers(\"parch\"), passengers(\"fare\") ) passengers1.show(5) val indexer = new stringindexer() indexer.setinputcol(\"gender\") indexer.setoutputcol(\"gendercat\") val passengers2 = indexer.fit(passengers1).transform(passengers1) passengers2.show(5) val passengers3 = passengers2.na.drop() println(\"orig = \"+passengers2.count()+\" final = \"+ passengers3.count() + \" dropped = \"+ (passengers2.count() - passengers3.count())) val assembler = new vectorassembler() assembler.setinputcols(array(\"pclass\", \"gendercat\", \"age\", \"sibsp\", \"parch\", \"fare\")) assembler.setoutputcol(\"features\") val passengers4 = assembler.transform(passengers3) passengers4.show(5) // data split val array(train, test) = passengers4.randomsplit(array(0.9, 0.1)) println(\"train = \"+train.count()+\" test = \"+test.count()) // decision tree model val algtree = new decisiontreeclassifier() algtree.setlabelcol(\"survived\") algtree.setimpurity(\"gini\") algtree.setmaxbins(32) algtree.setmaxdepth(5) val mdltree = algtree.fit(train) println(\"the tree has %d nodes.\".format(mdltree.numnodes)) println(mdltree.todebugstring) println(mdltree.tostring) println(mdltree.featureimportances) // predictions val predictions = mdltree.transform(test) predictions.show(5) // model evaluation val evaluator = new multiclassclassificationevaluator() evaluator.setlabelcol(\"survived\") evaluator.setmetricname(\"accuracy\") val accuracy = evaluator.evaluate(predictions) println(\"test accuracy = %.2f%%\".format(accuracy*100)) data loading: the titanic passenger data is loaded from a csv file. data transformation and feature extraction: relevant columns are selected. stringindexer converts the categorical \"gender\" column into a numerical \"gendercat\" column. rows with missing values are dropped. vectorassembler combines selected features into a \"features\" vector column. data splitting: the data is split into training and test sets using randomsplit(). decision tree model: a decisiontreeclassifier is created and configured with parameters like label column, impurity measure (\"gini\"), maximum bins, and maximum depth. the model is trained using the fit() method. model details like the number of nodes, tree structure, and feature importances are printed. predictions: predictions are made on the test data. model evaluation: a multiclassclassificationevaluator is used to calculate the accuracy of the model. clustering with k-means, data transformation and model evaluation this example illustrates k-means clustering: // loading data // ... (code for loading data, similar to previous examples) // data transformation and feature extraction val assembler = new vectorassembler() assembler.setinputcols(array(\"x\", \"y\")) assembler.setoutputcol(\"features\") val data1 = assembler.transform(data) data1.show(5) // clustering model (k=2) var algkmeans = new kmeans().setk(2) var mdlkmeans = algkmeans.fit(data1) // predictions var predictions = mdlkmeans.transform(data1) predictions.show(3) predictions.write.mode(\"overwrite\").option(\"header\",\"true\").csv(filepath + \"data/cluster-2k.csv\") // model evaluation and interpretation (k=2) var wssse = mdlkmeans.computecost(data1) println(s\"within set sum of squared errors (k=2) = %.3f\".format(wssse)) println(\"cluster centers (k=2) : \" + mdlkmeans.clustercenters.mkstring(\"<\", \",\", \">\")) println(\"cluster sizes (k=2) : \" + mdlkmeans.summary.clustersizes.mkstring(\"<\", \",\", \">\")) // clustering model (k=4) algkmeans = new kmeans().setk(4) mdlkmeans = algkmeans.fit(data1) // model evaluation and interpretation (k=4) wssse = mdlkmeans.computecost(data1) println(s\"within set sum of squared errors (k=4) = %.3f\".format(wssse)) println(\"cluster centers (k=4) : \" + mdlkmeans.clustercenters.mkstring(\"<\", \",\", \">\")) println(\"cluster sizes (k=4) : \" + mdlkmeans.summary.clustersizes.mkstring(\"<\", \",\", \">\")) predictions = mdlkmeans.transform(data1) predictions.show(30) predictions.write.mode(\"overwrite\").option(\"header\",\"true\").csv(filepath + \"data/cluster-4k.csv\") data loading: data with two dimensions (x and y) is loaded. data transformation: vectorassembler creates a \"features\" vector column. clustering model: a kmeans object is created and the number of clusters (\\(k\\)) is set. the model is trained using the fit() method. predictions: cluster assignments for each data point are predicted. model evaluation: the computecost() method calculates the within set sum of squared errors (wsse), which is a measure of cluster cohesion. cluster centers and cluster sizes are printed. running with different k: the code runs the clustering with k=2 and k=4, comparing the wsse and cluster characteristics to illustrate the effect of choosing different values for k. recommendation with als, data transformation and model evaluation this example demonstrates a recommendation system using the alternating least squares (als) algorithm // loading data // ... (code for loading data from text files, using rdds and dataframes) // data transformation and feature extraction // ... (code for transforming data using rdds and dataframes) // data splitting val array(train, test) = ratings3.randomsplit(array(0.8, 0.2)) println(\"train = \"+train.count()+\" test = \"+test.count()) // recommendation model val algals = new als() algals.setitemcol(\"product\") algals.setrank(12) algals.setregparam(0.1) algals.setmaxiter(20) val mdlreco = algals.fit(train) // predicting using the model val predictions = mdlreco.transform(test) predictions.show(5) predictions.printschema() // model evaluation and interpretation val pred = predictions.na.drop() println(\"orig = \"+predictions.count()+\" final = \"+ pred.count() + \" dropped = \"+ (predictions.count() - pred.count())) val evaluator = new regressionevaluator() evaluator.setlabelcol(\"rating\") var rmse = evaluator.evaluate(pred) println(\"root mean squared error = \"+\"%.3f\".format(rmse)) var mse = evaluator.evaluate(pred) println(\"mean squared error = \"+\"%.3f\".format(mse)) mse = pred.rdd.map(r => rowsqdiff(r)).reduce(_+_) / predictions.count().todouble println(\"mean squared error (calculated) = \"+\"%.3f\".format(mse)) data loading: movielens data is loaded from text files using rdds and then converted to dataframes. data transformation: the data is transformed to a suitable format for the recommendation algorithm. data splitting: the data is split into training and test sets. recommendation model: an als object is created and configured with parameters like rank, regularization parameter, and maximum iterations. the model is trained on the training data. predictions: predictions are made on the test data. model evaluation: rows with nan predictions are dropped to address the cold start problem. rmse and mse are calculated using a regressionevaluator. mse is also calculated manually for demonstration purposes. graphx this summary provides an introduction to graph processing and the spark graphx framework. introduction to graph processing graph processing involves analysing and manipulating graph structures, which consist of vertices (nodes) connected by edges. this field has long been crucial in industries like logistics, transportation, and social networking, with applications ranging from route optimisation to social network analysis. the importance of graph processing has surged with the rise of the internet, social media, and large datasets. applications now include analysing research collaborations, understanding social behaviour in animal populations, and investigating financial networks like the panama papers. graph processing systems there are two main categories of graph-based systems: graph processing systems excel at executing complex algorithms on large graph datasets. examples include spark graphx, pregel bsp, and graphlab. graph databases, like allegrograph, titan, neo4j, and rdf stores, are designed for efficient graph-based queries. organisations with extensive graph-based applications often employ both a graph database and a graph processing system as part of a larger data processing workflow challenges of graph processing traditional relational database systems struggle with complex graph algorithms due to their iterative and recursive nature, which often span the entire graph. partitioning data across multiple systems, common in database systems, is suboptimal for graph algorithms, particularly for \"long-tail\" graphs with many sparsely connected nodes. frameworks like mapreduce, based on data parallelism and disk-based partitioning, also face challenges in efficiently representing and processing graphs, especially when dealing with the numerous edge cuts inherent in long-tail graphs. spark graphx spark graphx addresses these challenges by offering graph parallelism over data parallelism and utilising spark's data-distributed rdd mechanism. this approach combines the strengths of both data and graph parallelism, enabling efficient processing of complex graph algorithms on large datasets. graphx provides various partitioning and storage schemes to optimise performance and allows for tuning based on specific application requirements and data characteristics. while graphx excels at computation, the new graphframes api integrates dataframes with graphs to facilitate powerful graph queries. graphx architecture graphx is built on top of spark and leverages its distributed processing capabilities, algorithms, and versioned computation graph. some machine learning algorithms within spark also utilise graphx apis. graphx offers a rich computational model, built-in algorithms, and apis for developing custom algorithms. it provides functionalities for: graph creation structure queries attribute transformers structure transformers connection mining primitives graphx computational model graphx uses a property graph model, where: vertices are connected by edges. both vertices and edges can have arbitrary objects as properties, accessible to the apis. it is a directed multigraph, meaning edges have direction, and multiple edges can exist between vertices. this model supports various graph types, including bipartite and tripartite graphs. each vertex consists of a unique id (64-bit integer) and a property object, while edges comprise source and destination vertex ids and a property object. building graphs with graphx there are four ways to create a graph in graphx: loading an edge list file using graphloader.edgelistfile(...). loading edge tuples from rdds using fromedgetuples(). creating a graph from a list of edges using fromedges(). creating a graph using edge and vertex rdds. the last method offers flexibility, especially when manipulating user-defined objects for vertices and edges graphx api landscape the graphx apis are organized into different categories: objects: edge, edgerdd, and others reside under org.apache.spark.graphx. graph object: contains apis like triplets, persist, subgraph, etc.. graph algorithms: separated under the graphops object to distinguish algorithms from graph implementation. analytic functions: functions like svd++, shortestpath, and others are located under lib structural apis graphx provides structural apis for analysing graph structure, such as: numedges and numvertices for getting the number of edges and vertices. triplets for accessing edge and connected vertex information together. indegrees and outdegrees for retrieving incoming and outgoing edge counts for vertices. subgraph for extracting subgraphs based on edge and vertex property predicates. community detection and analysis graphx offers algorithms for exploring network connections and communities, with applications in areas like fraud detection and security. some key algorithms include: trianglecount for identifying and counting triangles within the graph, useful for spam detection and community ranking. connectedcomponents for finding groups of vertices connected by paths. stronglyconnectedcomponents for identifying communities with bidirectional connections between all members graphx algorithms graphx includes various built-in algorithms for graph analysis: algorithm type graphx method/exmaple graph-parallel computation aggregatemessages(), pregel() pagerank pagerank(), staticpagerank(), personalizedpagerank() shortest paths and svd++ shortestpaths(), svd++ label propagation (lpa) labelpropagation() pagerank is a prominent algorithm for ranking the importance of vertices, with variations for static iterations, dynamic convergence, and personalized ranking based on a specified vertex. graph parallel computation apis graphx provides two primary apis for implementing custom graph algorithms: aggregatemessages(): a versatile api for aggregating information from neighbouring edges and vertices, operating similarly to a mapreduce paradigm on the graph. pregel(): a more general api that encompasses aggregatemessages() and offers greater flexibility in algorithm design. partition strategies efficient partitioning of large graphs is crucial for performance in distributed processing. graphx offers different partition strategies to address the challenges of long-tail graphs and minimise communication overhead: edge cut: partitions vertices across machines, with communication cost proportional to the number of edges cut. vertex cut: partitions edges, potentially duplicating vertices, with cost proportional to the number of machines spanned by each vertex. graphx defaults to a vertex cut strategy to mitigate hotspot issues caused by uneven distribution of connections. it offers four main strategies: randomvertexcut, canonicalrandomvertexcut, edgepartition1d, and edgepartition2d. case study: alphago tweet analytics the source provides a case study applying graphx to analyse a retweet network of tweets related to the alphago project. it outlines a data pipeline for collecting, processing, and analysing tweets to understand user rankings, locations, time zones, and follower-followee relationships. the case study demonstrates the process of modelling the retweet network as a graph, defining vertices as users, edges as retweets, and creating objects to store user and tweet attributes. it then showcases using graphx to create the graph from the processed tweet data and apply algorithms like pagerank to analyse user influence within the retweet network. code examples in the graphx source building a simple graph the source demonstrates building a graph representing a \"giraffe graph\" with two strongly connected groups (cliques) linked by a weak connection. defining vertices and edges the first step involves defining the vertices and edges, along with their associated properties: case class person(name:string,age:int) val defaultperson = person(\"na\",0) val vertexlist = list( (1l, person(\"alice\", 18)), (2l, person(\"bernie\", 17)), (3l, person(\"cruz\", 15)), (4l, person(\"donald\", 12)), (5l, person(\"ed\", 15)), (6l, person(\"fran\", 10)), (7l, person(\"genghis\",854)) ) val edgelist = list( edge(1l, 2l, 5), edge(1l, 3l, 1), edge(3l, 2l, 5), edge(2l, 4l, 12), edge(4l, 5l, 4), edge(5l, 6l, 2), edge(6l, 7l, 2), edge(7l, 4l, 5), edge(6l, 4l, 4) ) case class person(name:string,age:int): defines a case class to represent a person with attributes for name and age. this will be used as the vertex property. defaultperson: creates an instance of person with default values, used for vertices not explicitly defined in vertexlist but present in edgelist. vertexlist: a list of tuples, where each tuple represents a vertex. the first element of the tuple is the vertex id (a long integer), and the second is a person object. edgelist: a list of edge objects, where each edge represents a connection between two vertices. the edge constructor takes the source vertex id, destination vertex id, and an integer representing \"betweenness centrality\" as arguments. creating rdds and the graph next, the code creates rdds from the vertex and edge lists and constructs the graph: val vertexrdd = sc.parallelize(vertexlist) val edgerdd = sc.parallelize(edgelist) val graph = graph(vertexrdd, edgerdd,defaultperson) sc.parallelize(...): the parallelize method of the sparkcontext (sc) creates an rdd from the provided list. graph(vertexrdd, edgerdd, defaultperson): this constructs the graph using the vertex rdd, edge rdd, and the defaultperson object for handling missing vertices. structural apis the source presents examples using structural apis to query the graph: graph.numedges graph.numvertices val vertices = graph.vertices vertices.collect.foreach(println) val edges = graph.edges edges.collect.foreach(println) val triplets = graph.triplets triplets.take(3) triplets.map(t=>t.tostring).collect().foreach(println) numedges and numvertices: these methods return the number of edges and vertices in the graph, respectively. vertices and edges: these properties provide access to the rdds containing the vertices and edges of the graph. collect: this action retrieves all elements of an rdd to the driver program. foreach(println): iterates through the collected elements and prints each one. triplets: this property returns an rdd of edgetriplet objects, each representing an edge along with its source and destination vertices and their properties. take(3): retrieves the first three elements of the rdd. map(t => t.tostring): transforms each edgetriplet into a string representation. extracting subgraphs the source demonstrates extracting subgraphs based on edge and vertex properties: val indeg = graph.indegrees indeg.collect() val outdeg = graph.outdegrees outdeg.collect() val alldeg = graph.degrees alldeg.collect() val g1 = graph.subgraph(epred = (edge) => edge.attr > 4) g1.triplets.collect.foreach(println) val g2 = graph.subgraph(vpred = (id, person) => person.age > 21) g2.triplets.collect.foreach(println) indegrees, outdegrees, degrees: these methods calculate the incoming, outgoing, and total degrees for each vertex, respectively. subgraph(epred = ..., vpred = ...): this method creates a subgraph by applying predicates to filter edges and vertices. epred is a function that takes an edge object and returns a boolean indicating whether to include the edge. vpred is a function that takes a vertex id and its corresponding property object and returns a boolean indicating whether to include the vertex. community detection the source shows examples of using graphx algorithms for community detection: val cc = graph.connectedcomponents() cc.triplets.collect graph.connectedcomponents.vertices.map(_.swap).groupbykey.map(_._2).collect cc.vertices.map(_._2).collect.distinct.size cc.vertices.groupby(_._2).map(p=>(p._1,p._2.size)).sortby(x=>x._2,false).collect() val ccs = graph.stronglyconnectedcomponents(10) ccs.triplets.collect ccs.vertices.map(_.swap).groupbykey.map(_._2).collect ccs.vertices.map(_._2).collect.distinct.size val tricounts = graph.trianglecount() val trianglecounts = tricounts.vertices.collect connectedcomponents(): this algorithm finds groups of vertices connected by paths, returning a new graph where each vertex is assigned the id of its connected component. stronglyconnectedcomponents(10): this algorithm identifies strongly connected components, where bidirectional paths exist between all members. the argument 10 specifies the maximum number of iterations. trianglecount(): this algorithm counts the number of triangles each vertex participates in. these code snippets showcase various operations on the resulting graphs, such as collecting triplets, grouping vertices, and counting connected components. pagerank calculation val ranks = graph.pagerank(0.1).vertices ranks.collect().foreach(println) val topvertices = ranks.sortby(_._2,false).collect.foreach(println) pagerank(0.1): this algorithm calculates the pagerank for each vertex in the graph. the argument 0.1 specifies the damping factor. sortby(_._2, false): sorts the pagerank results in descending order based on the pagerank value (_._2). aggregate messages api the source provides examples using the aggregatemessages() api to perform computations on the graph: finding the oldest follower val oldestfollower = graph.aggregatemessages[int]( edgecontext => edgecontext.sendtodst(edgecontext.srcattr.age), (x,y) => math.max(x,y) ) oldestfollower.collect() aggregatemessages[int](...): this api aggregates values from neighbouring vertices and edges. it takes two functions as arguments: sendmsg: a function that operates on each edge context (edgecontext) and sends messages to either the source or destination vertex. in this case, it sends the age of the source vertex (srcattr.age) to the destination vertex using sendtodst. mergemsg: a function that combines messages received at each vertex. here, it uses math.max to determine the oldest age among the received messages. finding the oldest followee val oldestfollowee = graph.aggregatemessages[int]( edgecontext => edgecontext.sendtosrc(edgecontext.dstattr.age), (x,y) => math.max(x,y) ) oldestfollowee.collect() this example is similar to the previous one but uses sendtosrc to send the age of the destination vertex (dstattr.age) to the source vertex. calculating in-degree and out-degree var idegree = graph.aggregatemessages[int]( edgecontext => edgecontext.sendtodst(1), (x,y) => x+y ) idegree.collect() graph.indegrees.collect() val odegree = graph.aggregatemessages[int]( edgecontext => edgecontext.sendtosrc(1), (x,y) => x+y ) odegree.collect() graph.outdegrees.collect() these examples demonstrate implementing the indegrees and outdegrees functionality using the aggregatemessages() api. they send a message of 1 to either the destination (for in-degree) or source (for out-degree) vertex for each edge and then sum the messages received at each vertex using x + y. alphago tweet analytics we present a case study using graphx to analyse a retweet network of tweets related to the alphago project. the provided code snippets focus on data loading and transformation to create the graph: loading data and creating a dataframe import org.apache.spark.sparkcontext import org.apache.spark.sparkconf import org.apache.spark.graphx._ println(new java.io.file( \".\" ).getcanonicalpath) println(s\"running spark version ${sc.version}\") val df = sqlcontext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").option(\"inferschema\", \"true\").option(\"delimiter\",\"|\").load(\"file:/users/ksankar/fdps-v3/data/retweetnetwork-small.psv\") df.show(5) df.count() case class user(name:string, location:string, tz : string, fr:int,fol:int) case class tweet(id:string,count:int) val graphdata = df.rdd println(\"--- the graph data ---\") graphdata.take(2).foreach(println) import statements: import necessary classes from spark and graphx. print statements: print the current directory and spark version. sqlcontext.read...: reads the data from a pipe-separated value (psv) file into a dataframe using the spark-csv package. case class user(...) and case class tweet(...): define case classes to represent user and tweet data. graphdata = df.rdd: extracts the underlying rdd from the dataframe. mapping data to vertices and edges val vert1 = graphdata.map(row => (row(3).tostring.tolong,user(row(4).tostring,row(5).tostring,row(6).tostring,row(7).tostring.toint,row(8).tostring.toint))) println(\"--- vertices-1 ---\") vert1.count() vert1.take(3).foreach(println) val vert2 = graphdata.map(row => (row(9).tostring.tolong,user(row(10).tostring,row(11).tostring,row(12).tostring,row(13).tostring.toint,row(14).tostring.toint))) println(\"--- vertices-2 ---\") vert2.count() vert2.take(3).foreach(println) val vertx = vert1.++(vert2) println(\"--- vertices-combined ---\") vertx.count() val edgx = graphdata.map(row => (edge(row(3).tostring.tolong,row(9).tostring.tolong,tweet(row(0).tostring,row(1).tostring.toint)))) println(\"--- edges ---\") edgx.take(3).foreach(println) vert1 and vert2: these variables use map transformations on the graphdata rdd to extract user data from different columns of the data and create rdds of vertices. vertx = vert1.++(vert2): combines the two vertex rdds. edgx: uses a map transformation to extract tweet data and create an rdd of edges. creating the graph and running algorithms val rtgraph = graph(vertx,edgx) val ranks = rtgraph.pagerank(0.1).vertices println(\"--- page rank ---\") ranks.take(2) println(\"--- top users ---\") val topusers = ranks.sortby(_._2,false).take(3).foreach(println) val topuserswnames = ranks.join(rtgraph.vertices).sortby(_._2._1,false).take(3).foreach(println) println(\"--- how big ? ---\") rtgraph.vertices.count rtgraph.edges.count println(\"--- how many retweets ? ---\") val ideg = rtgraph.indegrees val odeg = rtgraph.outdegrees ideg.sortby(_._2,false).take(3).foreach(println) odeg.sortby(_._2,false).take(3).foreach(println) println(\"--- max retweets ---\") val toprt = ideg.join(rtgraph.vertices).sortby(_._2._1,false).take(3).foreach(println) val toprt1 = odeg.join(rtgraph.vertices).sortby(_._2._1,false).take(3).foreach(println) rtgraph = graph(vertx, edgx): constructs the graph from the vertex and edge rdds. pagerank(0.1): calculates pagerank. sortby(...): sorts results. join(...): joins rdds to combine data. indegrees and outdegrees: calculate in-degree and out-degree values. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M02/04_monitorization.html",
    "title": "ConfiguraciÃ³n, monitorizaciÃ³n y optimizaciÃ³n de Spark",
    "body": " index search search back configuraciÃ³n, monitorizaciÃ³n y optimizaciÃ³n de spark contents monitoring spark applications what to monitor monitoring the driver and executors monitoring queries and tasks spark logs spark ui spark rest api spark ui history server debugging and spark first aid code examples optimising spark application performance indirect performance enhancements design choices cluster configurations data at rest shuffle configurations memory pressure and garbage collection direct performance enhancements parallelism improved filtering repartitioning and coalescing custom partitioning user-defined functions udfs temporary data storage caching joins aggregations broadcast variables code examples code example 1 registering classes for kryo serialisation code example 2 caching a dataframe monitoring spark applications this document explains how to monitor spark applications using logs and the spark ui. it covers the different components involved in a spark application and what to monitor to ensure its smooth execution. what to monitor when monitoring a spark application, it's essential to monitor the following: processes: monitor processes running your application at the level of cpu usage, memory usage etc. query execution: keep track of jobs, tasks and other aspects of query execution. monitoring the driver and executors importance: the driver holds the application's state and executors run individual jobs. it's crucial to monitor both to ensure their stability. metrics system: spark provides a configurable metrics system based on the dropwizard metrics library to monitor driver and executor states. configuration: the metrics system can be configured using a configuration file located at $spark_home/conf/metrics.properties. output sinks: the metrics can be output to various sinks, including cluster monitoring solutions like ganglia. monitoring queries and tasks granular monitoring: spark allows you to monitor individual queries, jobs, stages, and tasks. performance tuning: this granular information helps you with performance tuning and debugging. spark logs detailed monitoring: spark logs offer a detailed way to monitor applications, highlighting strange events or errors that might cause job failures. integrated logging: if you use the application template provided in this book, your application logs will appear alongside spark's logs. this makes it easy to correlate the two. changing log level: you can change spark's log level to adjust the detail of the logs. log location: logs are either printed to standard error in local mode or saved to files by your cluster manager when running spark on a cluster. benefits of log collection: collecting logs helps you debug issues and can be referenced in the future if an application crashes. spark ui visual monitoring: the spark ui provides a visual interface for monitoring running applications and viewing metrics about your spark workload. accessibility: each spark context launches a web ui, by default on port 4040, accessible via your web browser. multiple applications will launch web uis on increasing port numbers (4041, 4042...). ui tabs: the ui includes tabs for jobs, stages, storage, environment, sql, and executors, providing information on the corresponding aspects of your spark application. example: this document walks through an example using the sql tab to trace a query execution, providing a visual representation of the job, stages and tasks. the example shows how to: navigate to the sql tab in the spark ui after running a sql query. interpret aggregate statistics about the query, such as submission time, duration, and number of jobs. understand the directed acyclic graph (dag) of spark stages, where each blue box represents a stage of spark tasks, forming a job. examine each stage to understand its function. analyse the job's execution in the jobs tab, breaking down stages and tasks. click individual stages to view detailed information about their execution. review the summary metrics section, which provides statistics about various metrics. examine per-executor details to identify any struggling executors. access and understand the more detailed metrics by clicking \"show additional metrics.\" other tabs: storage: shows information about cached rdds/dataframes on the cluster, helpful for seeing if data has been evicted from the cache. environment: shows information about the runtime environment, including scala, java, and configured spark properties. configuration: you can configure the spark ui using network configurations and behaviour settings. refer to the relevant table on spark ui configurations in the spark documentation. spark rest api programmatic access: the spark rest api offers programmatic access to spark's status and metrics. location: the rest api is available at http://localhost:4040/api/v1. purpose: the rest api enables the building of visualisations and monitoring tools on top of spark. data: it exposes similar information to the web ui, except for sql-related information. use: the api is valuable for building custom reporting solutions. for a list of api endpoints, consult the relevant table on rest api endpoints in the spark documentation. spark ui history server post-execution access: the spark ui history server provides access to the spark ui and rest api even after an application ends or crashes. requirement: the application must be configured to save an event log using spark.eventlog.enabled and spark.eventlog.dir settings. usage: once events are logged, you can run the history server as a standalone application to reconstruct the web ui. some cluster managers and cloud services configure logging automatically and run a history server by default. additional configurations: you can further configure the history server, details of which can be found in the spark history server configurations table in the spark documentation. debugging and spark first aid the source outlines various issues and their possible solutions that you might encounter when working with spark, such as: spark jobs not starting: this section explains potential reasons why your spark jobs may not be starting and offers possible solutions. these include verifying network configurations, resource allocation, and cluster setup. errors before execution: this part focuses on debugging errors that occur even before your spark job starts execution. the source suggests scrutinising your code for errors, checking network connectivity, and troubleshooting library or classpath issues. errors during execution: the document addresses issues arising during the execution of a spark job. it recommends checking for data consistency, schema correctness, and logic errors in your code. slow tasks or stragglers: this section focuses on identifying and addressing slow tasks, often termed \"stragglers.\" the source attributes these to uneven data distribution, skewed keys, or hardware problems. it suggests solutions like repartitioning data, increasing memory allocation, and identifying problematic executors. slow aggregations: this section focuses on slow aggregations, recommending solutions such as increasing partitions, executor memory, and optimising data handling, specifically related to null values. slow joins: similar to slow aggregations, this section deals with slow join operations. it suggests exploring different join types, optimising join order, and using broadcast joins when possible. slow reads and writes: this part addresses slow input/output (i/o) operations, particularly with network file systems. it suggests enabling speculation to mitigate transient issues, ensuring adequate network bandwidth, and utilising locality-aware scheduling. driver outofmemoryerror or driver unresponsive: this section explains the critical issue of driver failure due to insufficient memory. it suggests avoiding collecting large datasets to the driver, controlling broadcast join sizes, and optimising memory usage. executor outofmemoryerror or executor unresponsive: this section deals with executor failures due to memory issues. it recommends increasing executor memory, optimising data partitioning and null value handling, and using java monitoring tools to identify problematic objects. unexpected nulls in results: this part focuses on unexpected null values, recommending validating data formats, using accumulators to count parsing errors, and ensuring that transformations result in valid query plans. no space left on disk errors: this section addresses disk space issues, suggesting increasing storage capacity, repartitioning data to avoid skew, and managing log and shuffle files. serialization errors: this part explains serialization errors, typically encountered with custom logic using udfs or rdds. it suggests ensuring that all required data and code can be serialized and properly registering classes when using kryo serialization. code examples the source provides one code example to demonstrate how to use the spark ui for monitoring and debugging. here is the code snippet and an explanation: # in python spark.read\\ .option(\"header\", \"true\")\\ .csv(\"/data/retail-data/all/online-retail-dataset.csv\")\\ .repartition(2)\\ .selectexpr(\"instr(description, 'glass') >= 1 as is_glass\")\\ .groupby(\"is_glass\")\\ .count()\\ .collect() this code snippet performs a series of operations on a csv file using pyspark, spark's python api: spark.read.option(\"header\", \"true\").csv(...): this line reads a csv file located at /data/retail-data/all/online-retail-dataset.csv, specifying that the file has a header row. .repartition(2): the data is repartitioned into two partitions. this action is explicitly taken to demonstrate how the number of partitions affects task distribution in the spark ui. .selectexpr(\"instr(description, 'glass') >= 1 as is_glass\"): this line adds a new column named is_glass. it uses the instr function to check if the description column contains the word \"glass\". if the word is found, the is_glass column is set to true; otherwise, it's set to false. .groupby(\"is_glass\").count(): the data is grouped by the is_glass column, and the count for each group is calculated. .collect(): this action collects the results of the count operation to the driver node. optimising spark application performance this note summarises ways to improve the performance of spark applications based on the source document. it covers both indirect enhancements and direct performance enhancements. indirect performance enhancements these are enhancements you can apply to improve spark jobs generally, rather than focusing on specific jobs or stages. design choices the choices you make when designing your applications can significantly impact performance. here are a few things to consider: choice of language: spark's structured apis perform similarly across languages like scala, java, python, and r. however, if custom transformations are needed, using scala or java is recommended for udfs as they offer better performance and type safety compared to python or r. using dataframes/sql/datasets: these offer better performance compared to rdds due to spark's sql engine optimisations. if you have to use udfs, scala or java will perform better than python or r. object serialisation in rdds: kryo serialisation is recommended over java serialisation as it's more compact and efficient. register the classes you want to serialise with kryo cluster configurations optimising cluster configurations can yield significant performance gains. monitoring machine performance is crucial, especially in shared cluster environments. consider the following: resource allocation: dynamic allocation allows applications to adjust resources based on workload, enabling efficient resource sharing in a cluster. scheduling: using scheduler pools for parallel job execution and dynamic allocation or setting max-executor-cores can optimise resource usage. data at rest efficient data storage is key to fast data reads. opting for structured binary formats and utilising features like data partitioning can optimise read performance. here are some points to consider: file format: use structured, binary formats like apache parquet as they are faster to parse than formats like csv. splittable file types: use splittable file types like gzip, bzip2, or lz4 (especially when compressed) for parallel data reads. avoid non-splittable formats like zip or tar. table partitioning: partition data based on frequently used filter keys like date or customer id to improve query performance by reducing data reads. bucketing: bucketing \"pre-partitions\" data based on potential joins or aggregations, enhancing performance and stability by ensuring consistent data distribution. number of files: aim for a balance between the number and size of files. too many small files increase overhead while fewer large files limit parallelism. an ideal size for files is a few tens of megabytes. data locality: spark can schedule tasks close to data blocks if the storage system supports locality hints (like hdfs), reducing network overhead. statistics collection: collecting statistics on tables (both table-level and column-level) allows the cost-based optimizer to make informed decisions for operations like joins and aggregations. shuffle configurations the external shuffle service can improve performance by allowing nodes to read shuffle data from remote machines even when those machines are busy. consider tuning shuffle configurations like the number of concurrent connections per executor. using kryo serialisation for rdd-based jobs and optimising the number of shuffle partitions can also enhance performance. memory pressure and garbage collection excessive garbage collection can hinder performance. strategies for mitigating this include: using structured apis: structured apis reduce memory pressure by avoiding the creation of jvm objects. monitoring garbage collection: track garbage collection frequency and duration by adding jvm options like -verbose:gc -xx:+printgcdetails -xx:+printgctimestamps to the spark.executor.extrajavaoptions configuration. garbage collection tuning: tune garbage collection based on insights from the gathered statistics. the spark documentation provides detailed guidance on tuning parameters. direct performance enhancements these enhancements focus on specific spark jobs or stages and may require individual inspection and optimisation. parallelism increasing parallelism is a key strategy for speeding up stages that process substantial amounts of data. aim for at least 2-3 tasks per cpu core. adjust the spark.default.parallelism and spark.sql.shuffle.partitions properties accordingly. improved filtering filtering data as early as possible in the data processing pipeline significantly reduces the amount of data processed, improving performance. utilise push-down predicates to filter data at the source level. leveraging partitioning and bucketing also facilitates efficient filtering. repartitioning and coalescing repartitioning, which incurs a shuffle, can improve data balancing across the cluster. however, coalescing, which merges partitions on the same node, is a more efficient option when reducing the number of partitions as it avoids a shuffle. repartitioning can be beneficial before joins or caching. custom partitioning while rarely necessary, custom partitioning at the rdd level provides granular control over data organisation across the cluster, potentially optimising performance and stability. user-defined functions (udfs) minimising the use of udfs is recommended as they introduce overhead by requiring data representation as jvm objects. prioritise the use of structured apis for efficient transformations. explore options like vectorized udfs for python, which process data in batches using pandas data frames. temporary data storage (caching) caching stores frequently accessed datasets in memory or on disk, reducing the need to recompute them. however, consider the cost of serialisation, deserialization, and storage when deciding whether to cache. caching is particularly beneficial for iterative operations or when reusing datasets multiple times. spark offers various storage levels for caching, each with different performance characteristics. refer to table 19-1 in the source for details on the available storage levels. joins understanding the different types of joins and their execution mechanisms is crucial for optimisation. prefer equi-joins as they are easily optimised by spark. strategically filtering data and ordering joins can also improve performance. use broadcast join hints to guide spark's query planning. minimise the use of cartesian joins or full outer joins as they can often be replaced with more efficient filtering-style joins. aggregations optimise aggregations by filtering data beforehand and ensuring sufficient parallelism. when using rdds, choose aggregation methods like reducebykey over groupbykey for improved speed and stability. broadcast variables broadcast variables can improve performance by distributing read-only copies of large datasets used across multiple udf calls to all nodes, reducing data transfer overhead. this technique is beneficial for datasets like lookup tables or machine learning models. code examples the provided source document offers two code examples that focus on: registering classes for kryo serialisation: this snippet demonstrates how to configure spark to use kryo for object serialisation in rdd transformations, which can be more efficient than the default java serialisation. caching a dataframe: this example illustrates how to use the cache() method to store a dataframe in memory for faster access in subsequent operations, showcasing a simple performance optimisation technique. code example 1: registering classes for kryo serialisation conf.registerkryoclasses(array(classof[myclass1], classof[myclass2])) this code snippet, presented in the context of optimising object serialisation in rdds, demonstrates how to register specific classes (myclass1 and myclass2) with the kryo serialiser. kryo is a more efficient serialisation library compared to java's default serialisation mechanism, offering both compactness and speed advantages. to enable kryo serialisation, you need to set the spark.serializer configuration property to org.apache.spark.serializer.kryoserializer. after enabling kryo, you can register the classes you'll be working with using the registerkryoclasses method on the sparkconf object. by registering these classes, you instruct kryo to handle their serialisation and deserialization, potentially leading to improved performance in rdd transformations that involve these custom data types. code example 2: caching a dataframe # in python # original loading code that does *not* cache dataframe df1 = spark.read.format(\"csv\")\\ .option(\"inferschema\", \"true\")\\ .option(\"header\", \"true\")\\ .load(\"/data/flight-data/csv/2015-summary.csv\") df2 = df1.groupby(\"dest_country_name\").count().collect() df3 = df1.groupby(\"origin_country_name\").count().collect() df4 = df1.groupby(\"count\").count().collect() df1.cache() df1.count() # in python df2 = df1.groupby(\"dest_country_name\").count().collect() df3 = df1.groupby(\"origin_country_name\").count().collect() df4 = df1.groupby(\"count\").count().collect() this code example demonstrates the use of caching to optimise the repeated use of a dataframe. initial dataframe loading: the first code block loads a dataframe (df1) from a csv file. three subsequent dataframes (df2, df3, and df4) are derived from df1, each performing a groupby and count operation. without caching, these operations would repeatedly read and parse the original csv file, leading to redundant work. caching the dataframe: the second code block introduces the cache() method applied to df1. this instructs spark to store the dataframe in memory after its first computation. the count() action is used to trigger the caching process as caching is a lazy operation in spark. benefit of caching: the final code block re-executes the groupby and count operations for df2, df3, and df4. due to caching, these operations now access the dataframe from memory, significantly reducing processing time by avoiding the repeated file reads and parsing. this example highlights how caching can substantially improve performance in scenarios where a dataset is reused multiple times. the persist() method, which provides more control over storage levels (memory, disk, or both), is also mentioned as an alternative to cache(). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M03/01.html",
    "title": "IntroducciÃ³n a las arquitecturas de procesamiento de streams: Lambda y Kappa",
    "body": " index search search back introducciÃ³n a las arquitecturas de procesamiento de streams: lambda y kappa contents what are data streams challenges with data streams key elements for analysing data streams working with data streams data windows architectures for handling data streams in today's world, we are surrounded by a constant flow of data. much of this data is created automatically and used between machines. this data is important because it can teach us things. but it is different from the data we are used to working with. this type of data is called a data stream. what are data streams? a data stream is a series of data that comes into a system over time. it's like a river of information that never stops flowing. here are some important things to remember about data streams: order is important: the order the data comes in matters because it can show how the data is related. think about the temperature recorded every hour. the order helps us see how the temperature changes over the day. it never ends: data streams can go on forever, so we can't store all the information. imagine trying to keep all the tweets ever sent! we only see a part: at any given time, we can only see a small part of the data stream. it's like looking at a small section of a river. speed changes: the speed at which data arrives can change. sometimes it's a trickle, and other times it's a flood! challenges with data streams because data streams are different from traditional data, they pose unique challenges: traditional data mining techniques assume we have all the data at once. with data streams, we only have a part of the data at any time. the way data is spread out (its distribution) can change over time. this is called concept drift and means that a model we built yesterday might not work well today. we may not get feedback on our models right away. this makes it harder to know if our models are working correctly. key elements for analysing data streams to work with data streams, we need special tools: memory: this acts like a temporary storage space to hold incoming data until it can be processed. algorithms: special algorithms are needed to learn from the data and make decisions. these algorithms need to be fast and able to adapt to changing data. change monitoring: we need ways to watch for concept drift, which is when the patterns in the data change over time. performance evaluation: traditional methods for evaluating models don't work well with data streams. new methods are needed to see how well our models are performing. working with data streams: data windows one important technique for handling data streams is called windowing. since we can't store all the data, windows allow us to focus on the most recent data: landmarks: we can define windows based on specific events. think about analysing data between each time a sensor is reset. sliding windows: these windows keep a fixed amount of the most recent data. as new data arrives, old data is dropped. example: imagine you are analysing tweets about a football match. a sliding window might keep only the last 10 minutes of tweets, allowing you to see what people are saying right now. architectures for handling data streams there are two main ways to build systems for processing data streams: lambda architecture: this approach uses two paths: one for real-time processing (online) and one for batch processing (offline). it's like having a team that gives quick updates and another team that does a more detailed analysis later. kappa architecture: this approach uses only real-time processing. it's like having one team that can handle everything quickly. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M03/02.html",
    "title": "Componentes tecnolÃ³gicos de adquisiciÃ³n y transmisiÃ³n/distribuciÃ³n de eventos: Kafka",
    "body": " index search search back componentes tecnolÃ³gicos de adquisiciÃ³n y transmisiÃ³n/distribuciÃ³n de eventos: kafka contents introduction apache kafka architecture of kafka reading and writing in kafka kafka command line interface cli conclusion this summary explains apache kafka, a platform for handling large streams of data, based on the provided source text. introduction in large-scale data processing, data needs to be transmitted efficiently from its source to the processing system. traditional methods rely on direct connections between devices, which is not scalable. message queues and pub/sub systems offer improved scalability. message queues involve a single consumer receiving and processing each message. this approach is common in microservices and suitable for bulk task processing. if one consumer fails, another can take over the message. pub/sub systems use a central node called a broker to manage message queues, called topics. all consumers subscribed to a topic receive copies of the messages. this is useful for distributing data to multiple systems and is fault-tolerant, allowing consumers to recover missed messages after a failure. however, traditional pub/sub systems can face performance issues and limited storage capacity in massive data environments. this is where apache kafka comes in. apache kafka apache kafka is a streaming platform that uses the pub/sub model for sending messages and monitoring events. it is designed to handle large data streams with high performance and low latency. kafka offers persistent data storage for a user-defined duration and even includes a processing engine (kafka streams) for data transformation before it reaches the consumers. architecture of kafka kafka's architecture is distributed and fault-tolerant, thanks to its high data replication. a kafka cluster consists of multiple brokers, each typically located on a different server. these brokers store data and can manage multiple topics. each topic can be distributed across multiple brokers, further enhancing fault tolerance. topics are divided into partitions to improve fault tolerance and throughput. a partition is essentially a data stream, acting as the fundamental data structure within kafka. it can be viewed as a log file where data is appended. sequential writing and reading of data in partitions improve performance. each data entry in a partition has a unique identifier called an offset, which is helpful for resuming reading from a specific point. partitions offer scalability, allowing the size of a topic to exceed the capacity of a single machine. they increase throughput by enabling parallel data serving to multiple consumers. additionally, partitions provide redundancy because multiple copies of the same partition (called replicas) are stored on different brokers. if one broker fails, the partition can be recovered from another broker. it's crucial to note that while the order of data arrival is guaranteed within a partition, it is not guaranteed between different partitions. replication is a core feature of kafka's architecture. a replica is a copy of a partition and plays a vital role in fault tolerance. the replication factor determines the number of copies made for each partition. a designated replica acts as the leader, responsible for receiving and sending data to consumers. the remaining replicas, called followers, synchronise with the leader asynchronously using zookeeper. zookeeper is another key component in the architecture, managing service discovery and leader election for kafka brokers. it informs kafka about changes in the cluster's topology, ensuring each node knows about new brokers, broker failures, topic additions or removals, and other events. this provides a synchronised view of the kafka cluster's configuration. reading and writing in kafka producers send events or data to kafka, which are distributed among the different partitions. each piece of data goes to a single partition, ensuring that the order of arrival is maintained only within those partitions. write operations are append-only, meaning data is sequentially added to the end of the partition on disk. consumers can choose the offset from which they want to read data. kafka doesn't keep track of which messages have been read, which simplifies the system but makes complex delivery logic more challenging. consumers are typically organised into groups, ensuring each consumer reads from a different partition and enhancing scalability. kafka command line interface (cli) the kafka cli provides a way to interact with kafka from the command line. it is used for tasks such as initialising zookeeper and brokers, creating and managing topics, publishing data to topics, and consuming data from them. conclusion kafka is a powerful platform designed to handle high-volume data streams in a distributed and fault-tolerant manner. understanding its architecture, features, and command-line interface is crucial for effectively utilising kafka in data processing pipelines. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M03/03.html",
    "title": "Procesamiento de streams: Apache Spark Streaming",
    "body": " index search search back procesamiento de streams: apache spark streaming contents what is spark streaming how spark streaming works key features using spark streaming additional features this summary will help you understand spark streaming, a system for processing large streams of data in real time. it's based on apache spark, a powerful engine for handling big data. what is spark streaming? spark streaming is a component of apache spark designed for real-time data processing. it takes continuous data streams and processes them in small batches called micro-batches. this approach is based on defining data windows that collect data from the stream and are then processed. one of the key benefits of spark streaming is that it extends the familiar spark api, meaning the syntax is almost identical. this allows developers to work with streaming data using the same tools and concepts they use for batch processing. spark streaming is versatile in terms of data sources and outputs. it can ingest data from various sources like kafka, flume, twitter, and network sockets. similarly, it can write processed data to various destinations like hdfs, databases, and dashboards. how spark streaming works the basic workflow of spark streaming involves: creating micro-batches: the incoming data stream is divided into micro-batches. the default is timestamp-based windows without overlap, and you specify the window size in seconds. processing as rdds: each micro-batch is treated as an rdd (resilient distributed dataset), the fundamental data structure in spark. you can apply the same actions and transformations used in regular spark operations. managing with dstreams: the sequence of micro-batches is stored in a dstream (discretized stream), which provides additional functionality specific to stream processing. essentially, a dstream represents a continuous stream of rdds. key features high-level abstraction: spark streaming hides the complexities of stream processing from the user, simplifying development. code reusability: since micro-batches are processed as rdds, you can reuse existing spark code, including sparksql and mllib libraries. micro-batch approach: this offers advantages like high throughput (processing more instances per unit of time) but comes with increased latency as the minimum processing time is limited by the batch window size. architecture fit: spark streaming is highly suited for lambda architectures where spark handles offline processing, and spark streaming manages online processing. however, it might not be ideal for kappa architectures that aim for purely stream-based processing. using spark streaming create a streaming context: this is done using the spark context and specifies the duration of each micro-batch. define data input: spark streaming supports basic inputs like files, sockets, and rdd queues. it also has advanced input options using libraries for sources like kafka, flume, and kinesis. custom inputs can also be created using ad-hoc connectors. apply transformations: dstreams support various transformations similar to rdds like map, flatmap, filter, reduce, and count. these allow you to manipulate and process data within each micro-batch. define data output: output can be directed to the standard output using dstream.pprint(), saved to external storage with dstream.saveastextfiles(), or processed using custom functions applied to each rdd via dstream.foreachrdd(). start and manage processing: the ssc.start() command initiates data processing without blocking the program. to keep the script running until the stream processing is finished, use ssc.awaittermination(). additional features sliding windows: spark streaming provides the window() function to create sliding windows across multiple rdds. this allows you to analyse data over a larger time frame while still processing in micro-batches. stateful operations: for operations that need to keep track of previous states, spark streaming offers the updatestatebykey() function. this is useful for tasks like accumulating counts or maintaining averages. checkpointing: when using stateful operations, it's essential to activate checkpointing. this periodically backs up the state and metadata to fault-tolerant storage (like hdfs) to ensure recovery in case of failures. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_07.html",
    "title": "Derivatives and Rates of Change",
    "body": " index search search back derivatives and rates of change contents tangents velocities derivatives rates of change tangents the tangent line to the curve \\(y = f(x)\\) at the point \\(p(a, f(a))\\) is the line through \\(p\\) with slope: \\begin{align} m = \\lim_{x \\rightarrow a} \\frac{f(x) - f(a)}{x - a} \\end{align} provided that this limit exists (see figure 1). we sometimes refer to the slope of the tangent line to a curve at a point as the slope of the curve at the point. if we zoom in far enough toward the point, the curve looks almost like a straight line (see figure 2). there is another expression for the slope of a tangent line. if \\(h = x - a\\), then \\(x = a + h\\), and so the slope of the secant line \\(pq\\) becomes: \\begin{align} m_{pq} = \\frac{f(a + h) - f(a)}{h} \\end{align} see figure 3. notice that as \\(x\\) approaches \\(a\\), then \\(h\\) approaches \\(0\\). therefor3 the definition of the slope of the tangent line becomes: \\begin{align} m = \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a)}{h} \\end{align} velocities suppose an object moves along a straight line following the equation \\(s = f(t)\\), where \\(s\\) is the displacement of the object from the origin at time \\(t\\). the function \\(f\\) that describes the motion is called position function. in the time interval from \\(t = a\\) to \\(t = a + h\\) the change is position is \\(f(a + h) - f(a)\\) (see figure 5). the average velocity over this time interval is: \\begin{align} \\text{average velocity} = \\frac{\\text{displacement}}{\\text{time}} = \\frac{f(a + h) - f(a)}{h} \\end{align} now suppose we compute the average velocities over shorter and shorter time intervals, that is, we let \\(h\\) approach \\(0\\). we define the velocity or instantaneous velocity \\(v(a)\\) at time \\(t = a\\) to be the limit of these average velocities: \\begin{align} v(a) = \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a)}{h} \\end{align} provided that this limit exists. derivatives limits of the form: \\begin{align} \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a)}{h} \\end{align} arise whenever we calculate a rate of change in any of the sciences or engineering. the derivative of a function \\(f\\) at a number \\(a\\), denoted by \\(f'(a)\\) is: \\begin{align} f'(a) = \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a)}{h} \\end{align} if this limit exists. letting \\(x = a + h \\leftrightarrow h = x - a\\), then an equivalent way of this definition is: \\begin{align} f'(a) = \\lim_{x \\rightarrow a} \\frac{f(x) - f(a)}{x - a} \\end{align} we defined the tangent line to the curve \\(y = f(x)\\) at \\(p(a, f(a))\\) to be the line that passes through \\(p\\) and has slope \\(m\\). by the previous definition this slope \\(m\\) is the same as the derivative \\(f'(a)\\). if we use the point-slope form of the equation of a line, we can write an equation of the tangent line to the curve \\(y = f(x)\\) at the point \\((a, f(a))\\): \\begin{align} y - f(a) = f'(a)(x - a) \\end{align} rates of change suppose that \\(y\\) is a quantity that depends on another quantity \\(x\\), we write \\(y = f(x)\\). if \\(x\\) changes to \\(x_1\\) to \\(x_2\\), then the change in \\(x\\) is: \\begin{align} \\delta x = x_2 - x_1 \\end{align} and the corresponding change in \\(y\\) is: \\begin{align} \\delta y = f(x_2) - f(x_1) \\end{align} the difference quotient is given by: \\begin{align} \\frac{\\delta y}{\\delta x} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1} \\end{align} and is called the average rate of change of \\(y\\) with respect to \\(x\\) over the interval \\([x_1, x_2]\\). this can be interpreted as the slope of the secand line \\(pq\\) in figure 8. by analogy with velocity, if we consider the average rate of change over smaller and smaller intervals, letting \\(\\delta x\\) approach \\(0\\). the limit of these average rates of change is called the (instantaneous) rate of change of \\(y\\) with respect to \\(x\\) at \\(x = x_1\\). this can be interpreted as the slope of the tangent to the curve \\(y = f(x)\\) at \\(p(x_1, f(x_1))\\): \\begin{align} \\lim_{\\delta x \\rightarrow 0} \\frac{\\delta y}{\\delta x} = \\lim_{x_2 \\rightarrow x_1} \\frac{f(x_2) - f(x_1)}{x_2 - x_1} \\end{align} we recognize this limit as the derivative \\(f'(x_1)\\) so the derivative of \\(f'(x_1)\\) is the instantaneous rate of change of \\(y = f(x)\\) with respect to \\(x\\) when \\(x = x_1\\). this means that when the derivative is large the curve is steep (as at the point \\(p\\) in figure 9), therefore the \\(y\\)-values change rapidly. however, when the derivative is small, the curve is relatively flat (as at point \\(q\\)) and the \\(y\\)-values change slowly. then \\(f'(a)\\) is the velocity of a particle at time \\(t = a\\) and its speed is the absolute value of the velocity, \\(|f'(a)|\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Calculus/CET/02_08.html",
    "title": "The Derivative as a Function",
    "body": " index search search back the derivative as a function contents other notations theorems how can a function fail to be differentiable higher derivatives if we replace a in equation 1 by a variable \\(x\\), we obtain: \\begin{align} f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x)}{h} \\end{align} so we can regard \\(f'\\) as a new function, called the derivative of \\(f\\), which can be interpreted geometrically as the slope of the tangent line to the graph of \\(f\\) at the point \\((x, f(x))\\). this function is called the derivative of \\(f\\) because it has been derived from \\(f\\) by the limiting operation above. the domain of \\(f'\\) is the set \\(\\{x | f'(x) \\text{exists}\\}\\) and may be smaller than the domain of \\(f\\). other notations some common alternative notations for th derive are as follows: \\begin{align} f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{fx} = \\frac{d}{dx} f(x) = df(x) = d_x f(x) \\end{align} the symbols \\(d\\) and \\(\\frac{d}{dx}\\) are called differentiation operators because they indicate the operation of differentiation. the symbol \\(\\frac{dy}{dx}\\) was introduced by leibniz as a synonim for \\(f'(x)\\). we can rewrite the definition of the derivative in leibniz notation in the form: \\begin{align} \\frac{dy}{dx} = \\lim_{\\delta x \\rightarrow 0} \\frac{\\delta y}{\\delta x} \\end{align} to indicate the value of a derivative \\(\\frac{dy}{dx}\\) in leibniz notation at a specific number \\(a\\), we use the notation: \\begin{align} \\left.\\frac{dy}{dx}\\right|_{x=a} \\end{align} theorems a function \\(f\\) is differentiable at \\(a\\) if \\(f'(a)\\) exists. it is differentiable on an open interval \\((a, b)\\) if it is differentiable at every number in the interval. if \\(f\\) is differentiable \\(a\\), then \\(f\\) is continuous at \\(a\\). proof: we assume that \\(f\\) is differentiable at \\(a\\), so we have to prove that \\(f\\) is continuous at \\(a\\), that is we have to show: \\begin{align} \\lim_{x \\rightarrow a} f(x) = f(a) \\end{align} we will do this by showing that the difference \\(f(x) - f(a)\\) approaches \\(0\\). then, multiplying and dividing \\(f(x) - f(a)\\) by \\(x - a\\) \\begin{align} f(x) - f(a) = \\frac{f(x) - f(a)}{x - a} (x - a) \\end{align} by the limit laws: \\begin{align} \\lim_{x \\rightarrow a} [f(x) - f(a)] = \\lim_{x \\rightarrow a} \\frac{f(x) - f(a)}{x - a} (x - a) \\end{align} \\begin{align} = \\lim_{x \\rightarrow a} \\frac{f(x) - f(a)}{x - a} \\cdot \\lim_{x \\rightarrow a} (x - a) \\end{align} because we know that \\(f\\) is differentiable at \\(a\\), then: \\begin{align} f'(a) = \\lim_{x \\rightarrow a} \\frac{f(x) - f(a)}{x - a} \\end{align} exists, such that: \\begin{align} = f'(a) \\cdot (a - a)= f'(a) \\cdot 0 = 0 \\end{align} now we use this result to prove that \\(f\\) is continuous: \\begin{align} \\lim_{x \\rightarrow a} f(x) = \\lim_{x \\rightarrow a} [f(a) + (f(x) - f(a))] \\end{align} \\begin{align} = \\lim_{x \\rightarrow a} f(a) + \\lim_{x \\rightarrow a} [f(x) - f(a)] \\end{align} from our previous lemma we know that \\(\\lim_{x \\rightarrow a} [f(x) - f(a)] = 0\\). then: \\begin{align} = \\lim_{x \\rightarrow a} f(a) + 0 = f(a) \\end{align} therefore \\(f\\) is continuous at \\(a\\). note that the converse of this theorem is false, that is, there are functions that are continuous but not differentiable. how can a function fail to be differentiable we consider three scenarios: if the graph of a function \\(f\\) has a corner or a kink in it, then the graph of \\(f\\) has no tangent at this point and \\(f\\) is not differentiable there. by the contrapositive of \"if \\(f\\) is differentiable at \\(a\\), then \\(f\\) is continuous at \\(a\\)\" we know that if \\(f\\) is not continuous at \\(a\\) then \\(f\\) is not differentiable at \\(a\\). if the curve given by \\(f\\) has a vertical tangent line when \\(x = a\\), \\(f\\) is continuous at \\(a\\) and \\(\\lim_{x \\rightarrow a} |f'(x)| = \\infty\\) then \\(f\\) is not differentiable at \\(a\\). figure 7 illustrates the three possibilities: higher derivatives if \\(f\\) is a differentiable function, then its derivative \\(f'\\) is also a function, so \\(f'\\) may have a derivative of its own, denoted by \\((f')' = f''\\), called the second derivative of \\(f\\). using leibniz notation: \\begin{align} \\frac{d}{dx} \\left(\\frac{dy}{dx}\\right) = \\frac{d^2y}{dx^2} \\end{align} in general, we can interpret a second derivative as a rate of change of a rate of change. the most familiar example of this is acceleration. if \\(s(t)\\) is the position function, we know that its first derivative represents the velocity \\(v(t)\\): \\begin{align} v(t) = s'(t) = \\frac{ds}{dt} \\end{align} the instantaneous rate of change of velocity with respect to time is called the acceleration \\(a(t)\\). thus the acceleration function is the derivative of the velocity function, that is the second derivative of the position function: \\begin{align} a(t) = v'(t) = s''(t) \\end{align} or in leibniz notation: \\begin{align} a = \\frac{dv}{dt} = \\frac{d^2s}{dt^2} \\end{align} the third derivative \\(f'''\\) is the derivative of the second derivative \\(f''' = (f'')'\\). it can be interpreted as the slope of the curve \\(y = f''(x)\\) or as the rante of change of \\(f''(x)\\). alternative notations are: \\begin{align} y''' = f'''(x) = \\frac{d}{dx} \\left( \\frac{d^2y}{dx^2} \\right) = \\frac{d^3y}{dx^3} \\end{align} we can also interpret the third derivative physically. given the position function \\(s(t)\\) its third derivative is the derivative of the acceleration function and is called the jerk: \\begin{align} j = \\frac{da}{dt} = \\frac{d^3s}{dt^3} \\end{align} it represents the rate of change of acceleration. it is named like so because a large jerk means a sudden change in acceleration, which causes an abrupt movement. in general, the \\(n\\)th derivative of \\(f\\) is denoted by \\(f^{(n)}\\) and is obtained from \\(f\\) by differentiating \\(n\\) times. if \\(y = f(x)\\), we write: \\begin{align} y^{(n)} = f^{(n)}(x) = \\frac{d^n y}{dx^n} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Master/3C_1C/ICPDM/M04/01.html",
    "title": "Proveedores de soluciones: AWS",
    "body": " index search search back proveedores de soluciones: aws contents casos de uso casos de uso i consultas en un data lake de amazon s3 casos de uso ii an lisis de datos de registros en su almac n de datos casos de uso iii canalizaciones de etl determinadas por eventos amazon web services amazon emr spot instances amazon kinesis tipos data lake: repositorio de informaciÃ³n donde se almacena informaciÃ³n de muchos tipos (datos estructurados, no estructurados, multimedia, etc.). vemos en la figura superior que varios servicios acceden al data lake. por un lado tenemos los servicios centrados en la analÃ­tica y la definiciÃ³n de modelo a travÃ©s de aprendizaje automÃ¡tico. por otra parte tenemos el moviemiento o consumo/generaciÃ³n de datos en tiempo real. casos de uso casos de uso i: consultas en un data lake de amazon s3 en este caso en concreto, tenemos que se alamcenan los datos \"en crudo\" en un bucket dentro de amazon s3. podemos utilizar las herramientas de aws glue que nos permiten estructurar y catalogar estes datos (ver la figura inferior). a partir del catÃ¡logo generado por aws glue podemos utilizar distintas herramientas como amazon athena, amazon redshift o amazon emr (basado en el esquema mapreduce) para analizar y hacer consultas sobre los datos. finalmente, amazon posee otra herramienta: amazon quicksigth, que lleva a cabo reportes de manera automÃ¡tica sobre los datos. casos de uso ii: anÃ¡lisis de datos de registros en su almacÃ©n de datos para este otro caso, tenemos como mayor diferencia que en lugar de utilizar como datos de entrada sÃ³lo un bucket en aws s3, podemos utilizar distintas fuentes de datos como amazon redshift, amazon rds o una base de datos relacional cualquiera ejecutÃ¡ndose dentro de amazon ec2 (servicio de mÃ¡quinas virtuales). a partir de estas fuentes de datos utilizamos aws glue para catalogar los datos y para aplicar transformaciones si es necesario (aws glue etl). los pasos siguientes son los mismos que para el caso anterior. casos de uso iii: canalizaciones de etl determinadas por eventos para este caso, lo que se ilustra es que podemos definir funciones/script aws lambda que son accionados por eventos (p.ej. inserciÃ³n de nuevos datos). estas funciones lambda pueden hacer que se cataloguen los nuevos datos, o pueden ejecutar un script etl y almacenar el resultado en aws s3 o amazon redshift. amazon web services en la siguiente imagen se ilustra la secuencia de fases por las que pasan, o pueden pasar, los datos en aws indicando las herramientas que podemos utilizar en cada una de las fases. recolecciÃ³n y almacenamiento de datos: los datos pueden ser directamente almacenados mayormente en aws s3. tambiÃ©n tenemos el servicio kinesis que se basa en streaming. procesamiento de eventos: el procesamiento de eventos puede conllevar el definir funciones lambda en amazon aws que accionen otras operaciones como vimos anteriormenete. tambiÃ©n se puede aplicar emr sobre los eventos, entre otros. procesamiento de los datos: como hemos visto en los casos de uso anteriores podemos utilizar aws glue para procesar (catalogar) los datos almacenados. tambiÃ©n podemos utilizar emr que se trata de un servicio basado en mapreduce y tambiÃ©n tenemos la aplicabilidad de algoritmos de aprendizaje automÃ¡tico sobre los datos. por otro lado podemos simplemente almacenar los datos en el data lake por defecto en aws que es amazon redshift. anÃ¡lisis de los datos: fuera del alcance de esta asignatura. en esta asignatura nos centraremos en el procesamiento de datos utilizando emr. amazon emr este servicio se ejecuta en un clÃºster spark-hadoop, por lo tanto podremos utilizar tanto hadoop como spark dentro del mismo. en la siguiente figura se ilustra cÃ³mo utilizar esta herramienta. subimos nuestros datos a un bucket en aws s3. creamos un clÃºster emr, configurado de la manera que se crea precisa. una vez creado podemos utilizar herramientas de monitorizaciÃ³n para supervisar el clÃºster. spot instances amazon emr al final es una instancia de amazon ec2 preconfigurada que simplifica considerablemente el proceso de configuraciÃ³n/instalaciÃ³n del clÃºster para poder utilizar hadoop o spark. las spot instances lo que nos permiten es definir estas mÃ¡quinas virtuales de tal manera que sÃ³lo se ejecuten cuando es preciso, en lugar de estar ejecutÃ¡ndose de forma continua (ver la documentaciÃ³n para mÃ¡s informaciÃ³n) amazon kinesis se trata de un servicio que trabaja con datos en streaming que ofrece alta eficiencia es escalable y autogestionado. como es de esperar estÃ¡ integrado con amazon erm y con otras soluciones de almacenamiento como son amazon s3, redshift, dynamodb, etc. tipos amazon kinesis video streams: los datos de entrada son datos multimedia, como son los vÃ­deos. una vez procesados con kinesis estes pueden ser consumidos por otras herramientas de ia como son amazon rekognition video, amazon sagemaker, entre otros. amazon kinesis data streams: trabaja sobre datos tanto estructurados como no estructurados, que, tras ser procesados pueden utilizarse como entrada para otras herramientas como amazon kinesis data analytics, spark on emr, aws lambda, entre otros. amazon kinesis data firehose: se trata de la evoluciÃ³n de la herramienta anterior que simplifica el procesamiento. realmente se puede entender como un distribuidor de informaciÃ³n que pude ser posteriormente almacenada en s3, redshift o utilizada en herramientas como elasticsearch service. amazon kinesis data analytics: no se trata de otro tipo per se. se trata de una herramienta que permite llevar a cabo anÃ¡lisis sobre los datos en tiempo real. como podemos ver en la figura inferior, en la primera fase utilizamos los servicios de ingesta de datos en streaming de kinesis (amazon kinesis data firehose, amazon kinesis data streams) que envÃ­an los datos a amazon kinesis data analytics para llevar a cabo el anÃ¡lisis. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  }
]