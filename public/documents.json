[
  {
    "id": "https://albamr09.github.io/src/Notes/Rices/arch_install.html",
    "title": "Arch Linux Installation",
    "body": " index search search back arch linux installation set up set the keyboard layout: $ loadkeys es augment the size of the iso image: $ mount -o remount,size=1g /run/archiso/cowspace download git: $ pacman -syy && pacman -s git configure git to store the credentials: $ git config --global credential.helper store clone the repository: git clone https://github.com/albamr09/archinstaller.git now, you are good to go to start the installation process. install place yourself inside the root of the repository: cd archinstaller check out the configuration file, in case some values do not make sense to you: cat install_scripts/config.sh if you are satisfied with the configuration, simply execute: cd install_scripts && ./install.sh this will cause the installation to begin. it is mostly automatic, but sometimes you will have to enter a password here and there. so do not just let it execute by itself, because there are timeouts that will cause the installation to hault with an error. once this finished, reboot your computer. when the computer is up and running again, you will be met with a very minimal login interface. log in with you user, and execute the following: cd /install_scripts && ./post_install this script prompts you to connect to a wifi access point. it also sets up some needed services (like lightdm!) and removes all the installation files used from you system so it is nice an clean. well, now your arch linux is ready to go! $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/SVM/Optimal Margin Classifier.html",
    "title": "Optimal Margin Classifier",
    "body": " index search search back optimal margin classifier we use this classifier to categorize datasets that are perfectly separable, that is to say, we use it over data that is linearly separable. this classifier will help us find the green line we saw in the geometric margin. what the optimal margin classifier does is choose the parameters \\(w, b\\) that maximize \\(\\gamma\\) one way to solve this optimization problem is: \\begin{align} \\underset{\\gamma, w, b}{\\max} \\gamma \\end{align} subject to \\begin{align} \\frac{y^{(i)}(w^tx + b)}{||w||} \\geq \\gamma \\end{align} this will cause the maximization of the geometric margin with respect to the training set. the restriction means that we want to maximize \\(\\gamma\\) while having every example have a geometric margin of at least \\(\\gamma\\). because this is a non-convex problem, we will transform it. given \\(\\gamma = \\frac{\\hat{\\gamma}}{||w||}\\), then \\(\\gamma \\cdot ||w|| = \\hat{\\gamma}\\), and so if we multiply in the subject both sides by \\(||w||\\): \\begin{align} \\frac{y^{(i)}(w^tx + b)}{||w||} \\cdot ||w|| \\geq \\gamma \\cdot ||w|| \\leftrightarrow y^{(i)}(w^tx + b) \\geq \\hat{\\gamma} \\end{align} and the optimization problem can be re-written as: \\begin{align} \\underset{\\hat{\\gamma}, w, b}{\\max} \\frac{\\hat{\\gamma}}{||w||} \\end{align} subject to \\begin{align} y^{(i)}(w^tx + b) \\geq \\hat{\\gamma} \\end{align} however, we are still stuck with a non-convex objective \\(\\frac{\\hat{\\gamma}}{||w||}\\). because, as we've said previously scaling the functional margin (changing the magnitude of \\(w^tx + b\\)) does not change the decision boundary itself, we will add an scaling constraint that the functional margin of \\(w, b\\) with respect to the training set must be 1: \\(\\hat{\\gamma} = 1\\) observe, now, that maximizing \\(\\frac{\\hat{\\gamma}}{||w||} = \\frac{1}{||w||}\\) is like minimizing \\(||w||^2\\), we re-write the optimization problem as follows: \\begin{align} \\underset{w, b}{\\min} ||w||^2 \\end{align} subject to \\begin{align} y^{(i)}(w^tx + b) \\geq 1 \\end{align} we will revise once more the optimization problem for the optimal margin classifier. first, we have to suppose two facts: by the representer theorem we can assume that \\(w\\) can be expressed as a linear combination of \\(x\\), that is: \\begin{align} w = \\sum_{i=1}^m \\alpha_i x^{(i)} \\end{align} let's review this claim with logistic regression. we know that we apply stochastic gradient descent (we update \\(\\theta\\) for every example, instead of summing all the examples) on \\(\\theta\\) as follows: \\begin{align} \\theta = \\theta - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} \\end{align} which means that in every interation we are updating \\(\\theta\\) by adding or substracting a factor \\(\\alpha_i\\) multiplied by \\(x^{(i)}\\). therefore we can show by mathematical induction that if we start with \\(\\theta_0 = c\\), where \\(c\\) is a constant and go on adding and substracting \\(ax^{(i)}\\), where \\(a= \\alpha (h_\\theta(x^{(i)}) - y^{(i)})\\), then \\(w\\) can be expressed as a linear combination of \\(x\\). you can also derive the gradient descent expression in our optimization problem, and show that in this case \\(w\\) is also a linear combination of \\(x\\). we can rewrite \\(w\\) as follows: \\begin{align} w = \\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)} \\end{align} given any decision boundary, the vector \\(w\\) is always orthogonal to the decision boundary: now, the optimization problem becomes (note \\(w^2 = w^tw\\)): \\begin{align} \\underset{w, b}{min} \\frac{1}{2}||w||^2 = \\underset{w, b}{min} \\frac{1}{2} (\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)})^t(\\sum_{j=1}^m \\alpha_j y^{(j)} x^{(j)}) = \\end{align} \\begin{align} \\underset{w, b}{min} \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)}y^{(j)}(x^{(i)})^tx^{(j)} \\end{align} we now denote the inner product of \\((x^{(i)})^t x^{(j)}\\) as \\(\\langle x^{(i)}, x^{(j)} \\rangle\\), so: \\begin{align} \\underset{w, b}{min} \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i \\alpha_j y^{(i)}y^{(j)} \\langle x^{(i)}, x^{(j)} \\rangle \\end{align} and the restriction of the optimization becomes: \\begin{align} y^{(i)}(w^tx^{(i)} + b) \\geq 1 \\rightarrow y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)}x^{(j)})^tx^{(i)} + b) \\geq 1 \\rightarrow \\end{align} \\begin{align} y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)}(x^{(j)})^tx^{(i)}) + b) \\geq 1 \\rightarrow y^{(i)}((\\sum_{j=1}^m \\alpha_j y^{(j)} \\langle x^{(j)}, x^{(i)} \\rangle) + b) \\geq 1 \\end{align} applying convex optimization theory you can simplify this optimization problem further to: \\begin{align} \\underset{\\alpha}{max} \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, x^{(j)}\\rangle \\end{align} subject to \\begin{align} \\alpha_i \\geq 0 \\end{align} \\begin{align} \\sum_{i=1} y^{(i)}\\alpha_i = 0, i=1, \\cdots,m \\end{align} train the classifier to train the svm we have to solve the optimization problem for \\(\\alpha\\) classify an example to predict an example \\(x\\): \\begin{align} h_{w,b} = g(w^tx + b) = g\\left(\\left(\\sum_{i=1}^m \\alpha_i y^{(i)}x^{(i)}\\right)^tx + b\\right) = g\\left(\\left(\\sum_{i=1}^m \\alpha_i y^{(i)} \\langle x^{(i)}, x^{(j)} \\rangle\\right) + b\\right) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/SVM/Functional Margin.html",
    "title": "Functional Margin",
    "body": " index search search back functional margin intuition the functional margin describes how accurately do we classify an example. for example, for binary classification, given an example x: \\begin{align} h_\\theta(x) = g(\\theta x) = \\begin{cases} \\text{ predict } 1 & \\text{ if } \\theta^t x \\geq 0, \\text{ that is } h_\\theta(x)=g(\\theta x) \\geq 0.5\\\\ \\text{ predict } 0 & \\text{ otherwise } \\\\ \\end{cases} \\end{align} let's distinguish between the two cases when classifying an example \\(x^{(i)}\\): (1) if \\(y^{(i)} = 1\\), then we want \\(h_\\theta(x) = g(\\theta x) \\approx 1\\), which means we want \\(\\theta \\cdot x >> 0\\). (2) if \\(y^{(i)} = 0\\), then we want \\(h_\\theta(x) = g(\\theta x) \\approx 0\\), which means we want \\(\\theta \\cdot x << 0\\). as we can see in the following graph, the bigger \\(z = \\theta x\\) the closer \\(g(z)\\) is to one and vice versa. formal definition the functional margin of the hyperplane defined by \\((w, b)\\) with respect to the example \\((x^{(i)}, y^{(i)})\\) is defined as: \\begin{align} \\hat{\\gamma}^{(i)} = y^{(i)}(w^tx^{(i)}+b) \\end{align} so, if we modify slightly the two statements above and use the new notation for svms: if \\(y^{(i)} = 1\\), then we want \\(w^t \\cdot x + b >> 0\\). if \\(y^{(i)} = 0\\), then we want \\(w^t \\cdot x + b << 0\\). the combination of these two declarations yields the definition of the functional margin. why?, well: when \\(y^{(i)}\\) is positive, we want to have \\(w^tx^{(i)} + b >> 0\\) by (1), so \\(\\hat{\\gamma}^{(i)}\\) will be large, because both values are positive when \\(y^{(i)}\\) is negative, we want to have \\(w^tx^{(i)} + b << 0\\) by (2), so \\(\\hat{\\gamma}^{(i)}\\) will be large, because both values are negative so, given an example \\(x^{(i)}\\), if \\(\\hat{\\gamma}^{(i)} > 0\\) that means either \\(y^{(i)} = 1\\) and \\(w^tx + b > 0\\) or \\(y^{(i)} = -1\\) and \\(w^tx + b < 0\\) which shows that the classification is correct. evaluation to evaluate the functional margin with respect to the training set we make use of the worst case notion: \\begin{align} \\hat{\\gamma} = \\underset{i}{\\min} \\hat{\\gamma}^{(i)} \\end{align} that is, we evaluate how well we are doing in the worst example. normalizing the functional margin note that the functional margin is very easy to cheat (to increase its value with any meaningful change to the decision boundary). given our definition for \\(g\\): \\begin{align} g = \\begin{cases} 1, & \\text{ if } z \\geq 0 \\\\ -1, & \\text{ otherwise } \\end{cases} \\end{align} it follows that \\(h_{w,b}(x^{(i)}) = g(2w^tx^{(i)} + 2b) = g(w^tx^{(i)} + b)\\), because what matters is the sign, not the magnitude. however, if you scale \\(w\\) and \\(b\\) by a factor of \\(n\\) where \\(n\\) is a positive number then \\(\\gamma \\) increases because: \\begin{align} \\hat{\\gamma}^{(i)} = (w^tx + b) \\end{align} so, \\begin{align} n \\cdot \\hat{\\gamma}^{(i)} = n \\cdot (w^tx + b) \\end{align} where, \\begin{align} \\hat{\\gamma}^{(i)} < n \\cdot \\hat{\\gamma}^{(i)} \\end{align} one way to avoid this is to normalize the length of the parameters, that is either: add a constraint where \\(||w|| = 1\\) or set \\((w, b)\\) to be \\((\\frac{w}{||w||}, \\frac{b}{||b||})\\) in both cases we are re-scaling the parameters. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/SVM/Geometric Margin.html",
    "title": "Geometric Margin",
    "body": " index search search back geometric margin intuition first of all, let's assume we have a dataset that is linearly separable like: here we have two examples of two decision boundaries that do classify correctly all of the samples. however the red one looks worse than the green one. that is because for the red one there are some examples that are very close to the boundary compared to the rest. whereas for the green one there is a bigger separation. so, first we define a line by the equation \\(w^tx + b = 0\\), therefore: every example \\(x\\) that lies to the left of the line satisfies \\(w^tx + b < 0\\) and every example \\(x\\) that lies to the right of the line satisfies \\(w^tx + b > 0\\) furthermore the geometric margin with respect to a single example \\((x^{(i)}, y^{(i)})\\) is the euclidean distance between the point \\((x^{(i)}, y^{(i)})\\) and the line we have defined as \\(w^tx + b = 0\\). euclidean distance to the decision boundary the decision boundary corresponding to (w, b) is shown, along with the vector w. note that w is orthogonal (at 90º) to the separating hyperplane. consider the point at \\(a\\), which represents the example \\(x^{(i)}\\) with \\(y^{(i)} = 1\\). its distance to the decision boundary, denoted by \\(\\gamma^{(i)}\\), is given by the line segment \\(ab\\). how do we find \\(\\gamma^{(i)}\\): we know \\(\\frac{w}{||w||}\\) is a unit length vector pointing to the same direction as \\(w\\). also \\(a = x^{(i)}\\) we also know that the vector between points \\(a\\) and \\(b\\) is defined like \\(a - b\\), in this scenario, \\(a - b = \\gamma^{(i)}\\frac{w}{||w||}\\), where \\(\\gamma^{(i)}\\) is the length of the vector and \\(\\frac{w}{||w||}\\) is the direction of the vector. thus if we solve for \\(b\\), \\(b = x^{(i)} - \\gamma^{(i)}\\frac{w}{||w||}\\) furthermore, \\(b\\) lies on the decision boundary, therefore: \\begin{align} w^t(b) + b = 0 \\rightarrow w^t\\left(x^{(i)} - \\gamma^{(i)}\\frac{w}{||w||}\\right) + b = 0 \\end{align} solving for \\(y^{(i)}\\) yields: \\begin{align} \\gamma^{(i)} = \\frac{w^tx^{(i)} + b}{||w||} = \\left(\\frac{w}{||w||}\\right)^tx(i) + \\frac{b}{||w||} \\end{align} formal definition the geometric margin of the hyperplane \\((w, b)\\) with respect to \\((x^{(i)}, y^{(i)})\\) is defined as: \\begin{align} \\gamma^{(i)} = \\frac{w^t x^{(i)} + b}{||w||} \\end{align} this is the definition for a positive example (\\(y^{(i)} = 1\\)), and measures the euclidean distance from the decision boundary to the example \\((x^{(i)}, y^{(i)})\\). if we generalize, as to compute the geometric margin for both positive and negative examples: \\begin{align} \\gamma^{(i)} = \\frac{y^{(i)} (w^t x^{(i)} + b)}{||w||} \\end{align} evaluation to evaluate the geometric margin with respect to the training set we make use of the worst case notion: \\begin{align} \\gamma = \\underset{i}{\\min} \\gamma^{(i)} \\end{align} that is, we evaluate how well we are doing in the worst example. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/SVM/SVM.html",
    "title": "SVM",
    "body": " index search search back svm notation functional margin geometric margin functional and geometric margin optimal margin classifier svm kernels the kernel trick applying kernels validity of kernels generality of the kernel trick l1-norm soft margin svm outliers optimization kernel examples the support vector machine allows you to find potential non-linear decision boundaries: svm provides an algorithm that: maps a vector of features to a vector of features of a much higher dimension (manually picking the new features is difficult, that is why we automate it with these types of algorithms) \\begin{align} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1\\cdot x_2 \\\\ \\vdots \\end{bmatrix} \\end{align} applies a linear classifier over the high dimensional features (note: if you apply logistic regression to high dimensional vectors then it can learn non-linear decision boundaries) notation labels: \\(y^{(i)} \\in \\{-1, +1\\}\\) now the hypothesis outputs a \\(1\\) or a \\(-1\\), which means: \\begin{align} g(z) = \\begin{cases} 1, & \\text{ if } z \\geq 0 \\\\ 0, & \\text{ otherwise } \\\\ \\end{cases} \\end{align} that is, now instead of a smooth transition of probabilities from zero to one, we have a hard transition between \\(1\\) and \\(-1\\). weights: now the weights \\(\\theta \\in \\mathbb{r}^{(n+1)}\\), where \\(\\theta_0 = 1\\) are divided into: \\(w \\in \\mathbb{r}^{(n)}\\) and \\(b \\in \\mathbb{r}\\). thus we drop the convention of assigning \\(x_0 = 1\\). also now the hypothesis function is defined as: \\(h_{w,b}(x) = g(w^tx + b) = g((\\sum_{i=1}^n w_i x) + b)\\) functional margin functional margin geometric margin geometric margin relationship between functional margin and geometric margin as you may have picked up we can stablish an equality between both margins: \\begin{align} \\gamma^{(i)} = \\frac{\\hat{\\gamma}^{(i)}}{||w||} \\end{align} optimal margin classifier optimal margin classifier svm kernels kernel trick to apply kernels first we will lay out the kernel trick: write the algorithm in terms of the inner products of the training examples \\(\\langle x^{(i)}, x^{(j)} \\rangle=(\\langle x, z \\rangle)\\) let there be a mapping \\(x \\rightarrow \\phi(x)\\), where \\(\\phi(x)\\) is a high dimensional feature vector. find a way to compute \\(k(x, z) = \\phi(x)^t\\phi(z)\\), even if \\(x, z\\) are very high dimensional features vectors (which would be very computationally expensive). where \\(k(x, z)\\) is denoted as the kernel function replace \\(\\langle x, z \\rangle\\) with \\(k(x, z)\\) applying kernels given \\(x, z \\in \\mathbb{r}^n\\), where: \\begin{align} x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\end{align} we define the mapping \\(\\phi(x) \\in \\mathbb{r}^{n^2}\\) as follows: \\begin{align} \\phi(x) = \\begin{bmatrix} x_ix_i \\\\ \\end{bmatrix} \\end{align} \\(\\forall i, j\\) with \\(1 \\leq i,j \\leq n\\) so we have \\begin{align} k(x, z) = \\phi(x)^t \\phi(z) = \\sum_{i=1}^{n^2} \\phi(x)_i \\phi(z)_i = \\sum_{i=1}^n \\sum_{j=1}^n (x_ix_j) (z_iz_j) \\end{align} which would take \\(o(n^2)\\) time to compute. but, observe that: \\begin{align} (x^tz)^2 = (x^tz)^t(x^tz) = \\sum_{i=1}^n\\sum_{j=1}^n (x_iz_i)(x_jz_j) = \\sum_{i=1}^n\\sum_{j=1}^n (x_ix_j)(z_iz_j) \\end{align} whick takes \\(o(n)\\) time to compute. so we conclude that the kernel can be defined as \\(k(x, z) = (x^tz)^n\\) given \\(x, z \\in \\mathbb{r}^n\\) \\(k(x, z) = (x^tz + c)^2\\) where the mapping function \\(\\phi\\) is defined as: given \\begin{align} x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\phi(x) = \\begin{bmatrix} x_1x_1 \\\\ x_1x_2 \\\\ x_2x_1 \\\\ x_2x_2 \\\\ \\sqrt{2c}x_1 \\\\ \\sqrt{2c}x_2 \\\\ \\end{bmatrix} \\end{align} given \\(x, z \\in \\mathbb{r}^n\\) \\(k(x, z) = (x^tz+ c)^d\\) where \\(\\phi(x)\\) contains the \\(\\binom{n+d}{d}\\) combinations of monomials of degree d. (note: a monomial of degree 3 could be \\(x_1x_2x_3\\) or \\(x_1x_2^2\\), etc) validity of kernels to test is a kernel is valid we use mercer's theorem that says: k is a valid kernel function (i.e. \\(\\exists \\phi\\) such that \\(k(x, z) = \\phi(x)^t\\phi(z)\\)) if and only if for any \\(d\\) points \\(\\{x^{(1)}, \\cdots , x^{(d)}\\}\\) the corresponding kernel matrix \\(k\\) is positive semi-definite, that is \\(k \\geq 0\\) we are going to prove the first part of this theorem: given examples \\(\\{x^{(1)}, \\cdots , x^{(d)}\\}\\), let \\(k \\in \\mathbb{r}^{d\\times d}\\), be the kernel matrix, such that \\begin{align} k_{ij} = k(x^{(i)}, x^{(j)}) \\end{align} then, if \\(k\\) is a valid kernel: \\begin{align} z^tkz = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t k_{ij} z_j = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t \\phi(x^{(i)})^t \\phi(x^{(j)}) z_j = \\end{align} we expand \\(\\phi(x^{(i)})^t \\phi(x^{(j)})\\) as follows: \\begin{align} = \\sum_{i=1}^d \\sum_{j=1}^d z_i^t \\left[\\sum_{k=1}^d (\\phi(x^{(i)}))_k (\\phi(x^{(j)}))_k\\right] z_j = \\end{align} now, if we rearrange the sums: \\begin{align} = \\sum_{k=1}^d \\left[\\sum_{i=1}^d z_i (\\phi(x^{(i)}))_k\\right]^2 \\end{align} so, because the power of two of any real number is a positive number, and the sum of positive numbers is positive we derive: \\begin{align} \\sum_{k=1}^d \\left[\\sum_{i=1}^d z_i (\\phi(x^{(i)}))_k\\right]^2 \\geq 0 \\end{align} which means that \\(k \\geq 0\\), hence \\(k\\) is a positive, semi-definite matrix generality of the kernel trick the kernel trick can be applied to more algorithms, not only in svm. because, if you have any algorithm written in terms of \\(\\langle x^{(i)}, x^{(j)} \\rangle\\), you can apply the kernel trick to it. some of the algorithms that can be re-written like this are: lineal regression logistic regression gdm pca etc. l1-norm soft margin svm it may be the case where you map your data to a very high dimensional space, but it is still not linearly separable, or the decision boundary becomes too complex: in order to avoid this we will use a modification of the basic algorithm called l1-norm soft margin svm. with this new algorithm the optimization problem becomes \\begin{align} \\underset{w,b,\\xi_i}{min} \\frac{1}{2}||w||^2 + c \\sum_{i=1}^m \\xi_i \\end{align} subject to \\begin{align} y^{(i)}(w^tx^{(i)} + b) \\geq 1 - \\xi_i \\end{align} \\begin{align} \\xi_i \\geq 0, i = 1, \\cdots, m \\end{align} note that if \\(x^{(i)}\\) is classified correctly then \\(y^{(i)}(w^tx^{(i)} + b) \\geq 0\\) and therefore satisfies \\(y^{(i)}(w^tx^{(i)} + b) \\geq 1 - \\xi_i\\), because \\(\\xi_i \\geq 0\\) before the modification, the restriction forced the functional margin to be at least 1, however after the modification, because \\(\\xi_i\\) is positive we relax the restriction. also, we do not want \\(\\xi_i\\) to be too big, that is why it is added to the optimization objective as a cost. graphical representation with the addition of \\(\\xi_i\\) we are allowing some examples to have a functional margin less than 1, by setting \\(\\xi_i \\geq 0\\). for example look at the example \\(x^{(i)}\\) which has \\(\\xi_i = 0.5\\) outliers this relaxation on the restriction upong the geometric margin also avoids the following problem. if you have a lot of data that is linearly separable, but you have one outlier the optimal margin classifier allows for the decision boundary to be drastically changed because its optimization is based on the word performing example (which would be the outlier in this case). thus: however, the l1-norm soft margin svm allows for this example to be classified incorrectly of be close to the decision boundary without changing the boundary which makes it more robuts to outliers. optimization picking up the optimal margin classifier optimization problem, after applying the insight derived from the representer theorem, we have that the only addition needed to implement this algorithm is: \\begin{align} \\underset{\\alpha}{max} \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m\\sum_{j=1}^m y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, x^{(j)}\\rangle \\end{align} subject to \\begin{align} \\sum_{i=1} y^{(i)}\\alpha_i = 0 \\end{align} \\begin{align} 0 \\leq \\alpha_i \\leq c, i = 1, \\cdots , m \\end{align} the parameter \\(c\\) is a parameter your choose and it determines the level of strictness you want your model to have about some examples being misclassified. kernel examples the gaussian kernel: \\(k(x, z) = \\exp\\left(\\frac{||x-z||^2}{2\\sigma}\\right)\\) linear kernel: \\(k(x, z) = \\phi(x)^t\\phi(z)\\), where \\(\\phi(x) = x\\) polynomial kernel: \\(k(x, z) = (x^tz)^d\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Grandes Datasets.html",
    "title": "Grandes Datasets",
    "body": " index search search back grandes datasets cuando los conjuntos de datos son muy grandes los algoritmos son computacionalmente más caros: varianza elevada: se obtiene mejor rendimiento con más ejemplos. sesgo/bias elevado: se obtiene mejor rendimiento con más características. stochastic gradient descent el algoritmo de stochastic gradient descent es el siguiente: reordenar aleatoriamente el conjunto de datos para cada ejemplo \\(i\\) y cada característica \\(j\\): \\(\\theta_j = \\theta_j - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) es decir, se ajusta \\(\\theta\\) para cada ejemplo, en lugar de hacer el cálculo sobre todo el conjunto de datos cada iteración es más rápida no converge como batch gradient descent, llega a una aproximación. mini batch gradient descent esta técnica lo que hace el utilizar \\(b\\) ejemplos para calcular el gradiente: para cada \\(b\\) ejemplos y cada característica \\(j\\): \\(\\theta_j = \\theta_j - \\alpha \\frac{1}{b}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) permite vectorización $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Gradient Checking.html",
    "title": "Gradient Checking",
    "body": " index search search back gradient checking consiste en la estimación numérica de los gradientes, tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta} \\approx \\frac{j(\\theta - \\epsilon) - j(\\theta + \\epsilon)}{2 \\cdot \\epsilon} \\end{align} \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_j} \\approx \\frac{j(\\theta_0, ..., \\theta_j - \\epsilon, ..., \\theta_n) - j(\\theta_0, ..., \\theta_j + \\epsilon, ..., \\theta_n)}{2 \\cdot \\epsilon} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Ceiling Analysis.html",
    "title": "Ceiling Analysis",
    "body": " index search search back ceiling analysis supongamos que tenemos un pipeline que conforma todo nuestro sistema de aprendizaje automático y está formado por: obtención de la imagen detección de texto segmentación de caracteres reconocimiento de caracteres lo que hacemos es determinar una o varias métricas de evaluación, por ejemplo nosotros utilizaremos la precisión. entonces, ahora creamos una tabla indicando el valor de métrica para cada parte del sistema así como para el sitema completo: componente precisión detección del texto 82% segmentación de caracteres 90% reconocimiento de caracteres 100% total 72% a partir de esta tabla podemos comprobar que mejorar la detección en el texto y la segmentación de caracteres mejora el rendimiento del modelo. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Partes del algoritmo en python.html",
    "title": "Algoritmo",
    "body": " index search search back algoritmo dado un conjunto de entrenamiento \\(x\\), donde \\(x\\) es una matriz \\((n + 1) \\times m\\) con \\(m\\) ejemplos: propagación hacia adelante def feed_forward(self, theta=none, capa=none, test=false): if theta is none: # si no se introduce theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if test: # si se indica utilizar x_test a = self.x_test n, m = self.x_test.shape # guardar dimensiones de test else: a = self.x # la primera entrada es x n, m = self.x.shape # guardar dimensiones de train if capa is not none: # si se ha indicado una capa if capa <= len(theta) and capa >= 0: # chequeamos que la capa esta dentro de los limites for i in range(capa): # recorremos las capas a = self.sigmoid(theta[i], a) # calculamos la salida de la capa a = np.concatenate((np.matrix(np.ones(m)), a)) # añadimos una fila de unos return a else: print(\"el número de capa no es válido\") # mensaje de error else: for elemento in theta: a = self.sigmoid(elemento, a) # calculamos la salida de la capa actual a = np.concatenate((np.matrix(np.ones(m)), a)) # añadimos una fila de unos h = a[1:, :] # eliminamos los 1 en la última capa return h calculo del coste en la última capa def calculo_coste(self, theta=none, unrolled=false): if theta is none: # si no se introduce theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if unrolled: # si theta se ha flatten en un vector de una dimension theta = self._roll_theta(theta) # crear lista con matriz theta de capa capa h = self.feed_forward(theta) # obtener la salida para todos los ejemplo coste = -np.sum(np.diagonal(self.y_hot_enc.t.dot(np.log(h)) + (1 - self.y_hot_enc.t).dot(np.log(1 - h))))/self.m # calcular el error con la matriz codificada de y if self.reg: # si se ha indicado que se aplica regularizacion reg_parcial = 0 # inicializamos la variable temporal for elemento in theta: # para capa reg_parcial += np.sum(np.power(elemento[:, 1:], 2)) # no sumar el término independiente en cada nodo: primera fila reg_result = self.reg_par/(2*self.m)*(reg_parcial) # calcular la regularizacion coste = coste + reg_result return coste actualizar los pesos con propagación hacia atrás: def back_propagation(self, theta=none, unrolled=false, unroll=false): if theta is none: # si no se ha indicado ningun theta como argumento theta = self.theta # inicializar theta con el almacenado en el objeto if unrolled: theta = self._roll_theta(theta) # creamos una lista del array delta = [] # inicializamos las lista temporal que contendra el delta de cada nodo delta_sum = [] # inicializamos la lista temporal que contendra el sumatorio delta gradientes = [] # inicializamos la lista que contendrá los gradientes de cada capa h = self.feed_forward(theta=theta) # calculamos el valor del la salida para empezar a propagar hacia atras delta_next = h - self.y_hot_enc # calculamos el primer delta: el de la ultima capa delta.append(delta_next) # lo añadimos a la lista temporal indice = self.numero_capas - 1 # el indice indica hasta que capa calcular la salida for elemento in reversed(theta[1:]): # recorremos las capas de atras hacia adelante h = self.feed_forward(theta=theta, capa=indice) # calculamos la salida de la capa actual delta_aux = np.multiply(elemento.t.dot(delta_next), self.sigmoid_gradient(elemento, h)) # aplicamos la formula del gradiente delta_next = delta_aux[1:, :] # no cogemos el elemento independiente delta.append(delta_next) # lo añadimos a la lista de delta indice -= 1 # actualizamos el indice delta.reverse() # damos la vuelta a la lista for indice in range(len(delta)): h = self.feed_forward(theta=theta, capa=indice) # obtenemos la salida de cada capa delta_sum.append(delta[indice].dot(h.t)) # añadimos (delta * a) a la lista de delta_mayuscula -> sumatorio for indice in range(len(delta_sum)): gradiente = (1/self.m) * delta_sum[indice] # calculamos el grandiente: delta_mayuscula / m if self.reg: gradiente[1:, :] += (self.reg_par/self.m) * theta[indice][1:, :] # si se indica regularizacion aplicarla: no regularizan primer elemento gradientes.append(gradiente) # lo añadimos a la lista coste = self.calculo_coste(theta=theta) if unroll: # si se ha indicado que se quiere hacer flatten a un vector de una dimension return coste, self._unroll_theta(gradientes) else: return coste, gradientes $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Ejemplo Cálculo Función de Coste.html",
    "title": "Ejemplo Cálculo Función de Coste",
    "body": " index search search back ejemplo cálculo función de coste utilizamos como ejemplo la figura de multiclasificación donde tenemos que \\(c=3\\), y la hipótesis tiene los valores: \\begin{align} h_\\theta(x_1) = \\begin{bmatrix} 0.02 \\\\ 0.1 \\\\ 0.88 \\\\ \\end{bmatrix} \\end{align} y la salida real para el ejemplo \\(x_1\\) tiene los valores: \\begin{align} y_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\end{bmatrix} \\end{align} entonces la función de coste se calcularía como (observa que esto es sólo para un ejemplo, por lo que obviamos el primer sumatorio): \\begin{align} j(\\theta) = - \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))] \\end{align} \\begin{align} j(\\theta) = - \\{[(y_{11}\\cdot\\log(h_\\theta(x_1)_{1})) + (1-y_{11})\\cdot\\log(1-h_\\theta(x_1)_{1})] + \\end{align} \\begin{align} + [(y_{21}\\cdot\\log(h_\\theta(x_1)_{2})) + (1-y_{21})\\cdot\\log(1-h_\\theta(x_1)_{2})] + \\end{align} \\begin{align} + [(y_{31}\\cdot\\log(h_\\theta(x_1)_{3})) + (1-y_{31})\\cdot\\log(1-h_\\theta(x_1)_{3})]\\} \\end{align} sustituimos los valores de cada vector: \\begin{align} j(\\theta) = - \\{ [(0\\cdot\\log(0.02)) + (1-0)\\cdot\\log(1-0.02)] + \\end{align} \\begin{align} + [(0\\cdot\\log(0.1)) + (1-0)\\cdot\\log(1-0.1)] + \\end{align} \\begin{align} + [(1\\cdot\\log(0.88)) + (1-1)\\cdot\\log(1-0.88)] \\} = \\end{align} calculamos los valores: \\begin{align} j(\\theta) = - (\\log(0.98) + \\log(0.9) + \\log(0.88)) \\end{align} \\begin{align} j(\\theta) = - (-0.009 - 0.046 -0.056) = - (-0.111) = 0.111 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Derivadas capas intermedias.html",
    "title": "Derivadas capas intermedias",
    "body": " index search search back derivada capas intermedias donde \\(q\\) denota la capa, con \\(1 \\leq q \\leq (k-1)\\). pues lo que tenemos que hacer es, de nuevo, aplicar la regla de la cadena, entre el peso \\(\\theta_{it}^{(q)}\\) (peso \\(t\\) del nodo \\(i\\) de la capa \\(q\\)) y todo nodo \\(a_{lj}^{(q+1)}\\)(es decir para el nodo \\(l\\) en la capa \\(q+1\\) para el ejemplo \\(j\\)). \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\frac{j(\\theta)}{\\delta a^{(q+1)}_{1j}}\\frac{\\delta a_{1j}^{(q+1)}}{\\delta \\theta_{it}^{(q)}} + \\cdots + \\frac{j(\\theta)}{\\delta a^{(q+1)}_{(s_(q+1))j}}\\frac{\\delta a_{(s_(q+1))j}^{(q+1)}}{\\delta \\theta_{it}^{(q)}} \\] donde \\(s_{(q+1)}\\) es el número de nodos en la capa \\(q+1\\). para cada término \\(l\\) de la suma, debemos volver a aplicar la regla de la cadena, tal que: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\] es decir: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{l=1}^{s_{(q+1)}} \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\] cabe destacar que \\(\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} = \\frac{\\delta g(z_{lj}^{(q)})}{\\delta z_{lj}^{(q)}} \\frac{\\delta z_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}}\\) (explicado en derivada de la función del coste). entonces, si generalizamos para todos los ejemplos, \\(m\\): \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{j=1}^m\\sum_{l=1}^{s_{(q+1)}} \\frac{j(\\theta)}{\\delta a^{(q+1)}_{lj}}\\frac{\\delta a_{lj}^{(q+1)}}{\\delta a_{lj}^{(q)}}\\frac{\\delta a_{lj}^{(q)}}{\\delta \\theta_{it}^{(q)}} \\] $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/index.html",
    "title": "Anexo",
    "body": " index search search back anexo clasificación múltiple función de coste retropropagación algoritmo clasificación múltiple para crear una red neuronal que permita trabajar con \\(c\\) clases lo que hacemos es hacer que la red neuronal tenga \\(c\\) nodos en su capa de salida. esto se ilustra en la siguiente imagen: de tal manera que ahora, cada salida \\(y_j\\) será un vector columna \\(c\\times1\\), donde existe un valor por cada categoría, al igual que la hipótesis para el ejemplo \\(j\\), \\(h_\\theta(x_j)\\), es un vector columna \\(c\\times1\\). como podemos ver, los valores de \\(y_j\\) indican claramente a qué clase pertenece el ejemplo \\(j\\) (clase 3), mientras que la hipótesis \\(h_\\theta(x_j)\\) ofrece, para cada clase (columna) la probabilidad de que el ejemplo \\(j\\) pertenezca a esa clase. función de coste notación como ya hemos visto en función del número de clases la salida tendrá distinta forma: clasificación binaria: para cada ejemplo \\(j\\), \\(y_j \\in \\{0, 1\\}\\), \\(h_\\theta(x_j) \\in \\mathbb{r}\\) clasificación múltiple: para cada ejemplo \\(j\\), \\(y \\in \\mathbb{r}^c\\), \\(h_\\theta(x_j) \\in \\mathbb{r}^c\\), donde \\(c\\) es el número de clases sea \\(k\\) el número de capas y \\(s_i\\) el número de nodos en la capa \\(i\\). sea \\(y=(y_{ij})\\) una matriz \\(c\\times m\\), donde \\(m\\) es el número de ejemplos y cada \\(y_{j}\\) es el vector columna \\(c\\times1\\) de salida para el ejemplo \\(j\\). definimos la función de coste como sigue: \\begin{align} j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))]\\right\\} \\end{align} el primer sumatorio que va de 1 a \\(m\\) se encarga de calcular el coste para cada ejemplo \\(j\\). mientras que el segundo sumatorio, que va de 1 a \\(c\\), se encarga de calcular el coste para cada nodo de salida. esta función se aplica sobre los \\(k\\) nodos en la capa de salida. ejemplo cálculo función de coste regularización definimos la función de coste introduciendo regularización como sigue: \\begin{align} j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m \\sum_{i=1}^c [y_{ij}\\cdot \\log(h_\\theta(x_j)_i)] + [(1-y_{ij})\\cdot \\log(1-(h_\\theta(x_j)_i))]\\right\\} + \\frac{\\lambda}{2m} \\sum_{q=1}^k \\sum_{i=1}^{s_q}\\sum_{j=1}^{s_{q+1}} (\\theta_{ji}^{(q)})^2 \\end{align} antes de nada, recordar que \\(s_q\\) denota el número de nodos en la capa \\(q\\). entonces, el primer término de la función es igual que cuando no se aplicaba regularización. expliquemos el segundo término. la regularización, en este caso, consiste en sumar todos los pesos de la red neuronal, por lo tanto: por cada capa \\(q\\), con \\(1 \\leq q \\leq k\\), sumamos todos los elementos de la matriz de pesos \\(\\theta^{q}\\), que como sabemos tiene dimensiones \\(s_{q} \\times s_{q-1}\\) dada la matriz \\(\\theta^{(q)}\\) recorremos cada columna \\(i\\), con \\(1 \\leq i \\leq s_{q-1}\\) recorremos cada elemento \\(j\\) de la columna \\(i\\), con \\(1 \\leq j \\leq s_{q}\\) sumamos al total cada elemento de la matriz \\(\\theta^{(q)}_{ji}\\) una vez se han sumado todas las matrices de pesos obtenemos un escalar, que multiplicamos por \\(\\frac{\\lambda}{2m}\\) múltiple ejemplos la salida de cada capa \\(q\\) es una matriz \\(s_q \\times m\\), donde \\(s_q\\) denota el número de nodos en la capa \\(q\\) y \\(m\\) denota el número de ejemplos. como vimos en nuestras figuras, donde se presentaban los cálculos sólo para un ejemplo, en cada capa \\(q\\) podemos mapear la salida de los \\(s_q\\) nodos a un vector columna \\(s_q \\times 1\\). si generalizamos esto a \\(m\\) ejemplos tenemos que la salida de cada capa es una matriz \\(s_q \\times m\\). esto se ilustra en la siguiente imagen: retropropagación vamos, ahora a explicar cómo se aplica la retropropagación. lo primero que debemos tener en cuenta es que este proceso se basa en la misma idea de optimización que la regresión lineal y la regresión logística, es decir, lo que queremos hacer es minimizar el coste, \\(j(\\theta)\\) sea \\(c\\) el número de nodos en la última capa, \\(\\theta_{it}\\) el peso \\(t\\) del nodo \\(i\\) de la última capa \\(k\\), \\(a_{ij}^{(k)}\\) la salida del nodo \\(i\\) para el ejemplo \\(j\\) en la capa \\(k\\): calculamos el gradiente de la última capa \\(k\\) como: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta j(\\theta)}{\\delta a_{1j}^{(k)}}\\frac{\\delta a_{1j}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) calculamos el gradiente en capas intermedias utilizando la regla de la cadena como: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} = \\sum_{i=1}^{s_{(q+1)}} \\frac{\\delta j(\\theta)}{\\delta a_{ij}^{(q+1)}}\\frac{\\delta a_{ij}^{(q+1)}}{\\delta a_{ij}^{(q)}}\\frac{\\delta a_{ij}^{(q)}}{\\delta \\theta_{it}^{(q)}}\\) normalmente en las capas intermedias, \\(q\\), nos referimos al término \\(\\frac{\\delta j(\\theta)}{\\delta a_{ij}^{(q+1)}}\\) como \\(\\delta^{(q+1)}_{ij}\\). explicación de la retropropagación derivada de la función de coste a continuación explicamos cómo derivar la función de coste (paso 1). derivada de la función de coste capas intermedias veamos, ahora, cómo llevar a cabo el paso 2: ¿cómo calculamos el gradiente (o lo que contribuye el peso \\(it\\) en el error) para los pesos de las capas intermedias?, es decir, cómo calculamos: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(q)}} \\] derivadas capas intermedias ejemplo de retropropagación ejemplo de retropropagación algoritmo partes del algoritmo en python notación época: iteración en el entrenamiento pesos (\\(w_j^k, b_j^k\\)): se inicializan de forma aletoria (evitar simetría) y con valores bajos. criterios de finalización \\(j\\) o gradiente de \\(j\\) inferior a un umbral número máximo de épocas velocidad de apredizaje \\(\\mu\\) intermedia: evita lentitud en las oscilaciones caída en mínimos locales que pueden tener \\(j\\) elevado. es por ello que se ejecuta varias veces el entrenamiento y se selecciona aquel que obtenga mejor resultado. actualización de pesos patrón a patrón en lugar de tras computar el error sobre todo el dataset. puede evitar mínimos locales y converge antes. función de activación sigmoide para clasificación o linear para regresión pseudocódigo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Explicación de la retropropagación.html",
    "title": "Explicación de la retropropagación",
    "body": " index search search back explicación de la retropropagación descenso gradiente derivada del error gradiente capa de salida retropropagación del gradiente gradiente acumulado derivadas de las funciones de activación componentes: \\(w_j^k\\): peso de la neurona \\(j=1...i_k\\) en la capa \\(k=1...h\\) \\(a_{ij}^k=(w_j^k)^th_i^{k-1}+b_j^k\\): \\(i=1...n\\) (patrón), \\(k=1...h\\)(capa), \\(j=1...i_k\\) (neurona de la capa \\(k\\)). \\(h_i^{k-1}\\): salida de la capa \\(k-1\\) con \\(i_{k-1}\\) valores (uno por cada neurona \\(j=1...i_{k-1}\\) para cada patrón \\(x_i\\). \\(h_i^k\\): salida de la capa \\(k\\) para cada patrón \\(x_i\\): \\(h_{ij}=f(a_{ij}^k)\\) con \\(j=1...i_k\\) \\(y_{ij}\\): salida verdadera de la neurona de salida \\(j\\) y el patrón \\(x_i\\). descenso gradiente \\begin{align} \\delta w_j^k=-\\mu \\frac{\\delta j}{\\delta w_j^k} \\end{align} \\begin{align} \\delta b_j^k=-\\mu \\frac{\\delta j}{\\delta b_j^k} \\end{align} para \\(k=1...h\\). de tal forma que se actualizan los pesos \\(w_j^k\\) y el offset \\(b_j^k\\) de la capa \\(k\\) y de la neurona \\(j\\). tenemos que la capa de salida está compuesta de \\(i_h\\) neuronas que se recorren con el índice \\(j\\). accedemos a la salida verdadera del ejemplo \\(i\\) para la neurona \\(j\\) (\\(y_{ij}\\)) y restamos la salida predicha \\(h_{ij}^h\\) que hace referencia a la salida de la función de activación de la capa \\(h\\) para la neurona \\(j\\) y el ejemplo \\(i\\). la diferencia se eleva al cuadrado para obtener mse. también se puede vectorizar restando los vectores \\(y_i\\) y \\(h_i^h \\in \\mathbb{r}^j\\). de esta manera obtenemos el error para el patrón \\(x_i\\): \\(j_i\\). derivada del error aplicamos la regla de la cadena sobre \\(j_i\\), ya que este depende de \\(a_{ij}^k\\): \\begin{align} \\frac{\\delta j_i}{\\delta w_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k} \\end{align} \\begin{align} \\frac{\\delta j_i}{\\delta b_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k} \\end{align} definimos: \\begin{align} \\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k} \\end{align} este indica el gradiente de la capa siguiente, para evitar tener que calcularlo. \\begin{align} \\frac{\\delta a_{ij}^k}{\\delta w_j^k} = h_i^{k-1} \\end{align} \\begin{align} \\frac{\\delta a_{ij}^k}{\\delta b_j^k} = 1 \\end{align} debido a que el valor de \\(a_{ij}^k\\) es la combinación lineal de la entradas y los pesos, donde las entradas son las salidas de la capa anterior (\\(k-1\\)), es decir \\(h_i^{k-1}\\), de tal manera que: \\begin{align} a_{ij}^k = (w_j^k)^th_i^{k-1}+b_j^k \\end{align} por lo que la derivada en función de \\(w_j^k\\) se corresponde con \\(h_i^{k-1}\\) y la derivada en función de \\(b_j^k\\) es 1. gradiente si sustituimos \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\) y \\(\\frac{\\delta a_{ij}^k}{\\delta w_j^k} = h_i^{k-1}\\) en \\(\\frac{\\delta j_i}{\\delta w_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k}\\) obtenemos: \\begin{align} \\delta w_j^k=-\\mu \\frac{\\delta j}{\\delta w_j^k} \\end{align} \\begin{align} \\delta w_j^k=-\\mu \\sum_{i=1}^n \\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta w_j^k} = -\\mu \\sum_{i=1}^n\\delta_{ij}^kh_i^{k-1} \\end{align} hacemos los mismo para el offset sustituyendo \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\) y \\(\\frac{\\delta a_{ij}^k}{\\delta b_j^k} = 1\\) en \\(\\frac{\\delta j_i}{\\delta b_j^k}=\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k}\\) obtenemos: \\begin{align} \\delta b_j^k=-\\mu \\frac{\\delta j}{\\delta b_j^k} \\end{align} \\begin{align} \\delta b_j^k= -\\mu \\sum_{i=1}^n\\frac{\\delta j_i}{\\delta a_{ij}^k}\\frac{\\delta a_{ij}^k}{\\delta b_j^k}=-\\mu \\sum_{i=1}^n\\delta_{ij}^k \\end{align} capa de salida calculamos \\(\\delta_{ij}^k\\) en la capa de salida (\\(k=h\\)), cuyo valor se va a propagar hacia las capas anteriores. lo que vamos a calcular es \\(\\delta_{ij}^k \\equiv \\frac{\\delta j_i}{\\delta a_{ij}^k}\\). tenemos que la función de coste para el patrón \\(i\\), \\(j_i\\) viene definida por: \\begin{align} j_i=\\frac{1}{2}\\sum_{j=1}^{i_h}(y_{ij}-h_{ij}^h)^2=\\frac{|y_i-h_i^h|^2}{2} \\end{align} además el valor de \\(a_{ij}^k\\), que es la combinación lineal de las entradas (salidas de las neuronas capa anterior, \\(k-1\\)) y los pesos junto con el offset: \\begin{align} a_{ij}^k=(w_j^k)^th_i^{k-1}+b_j^k \\end{align} por lo tanto en la capa final: \\begin{align} \\frac{\\delta j_i}{\\delta a_{ij}^h}=\\frac{1}{2}\\frac{\\delta (y_{ij}-h_{ij}^h)^2}{\\delta (y_{ij}-h_{ij}^h)}\\frac{\\delta (y_{ij}-h_{ij}^h)}{\\delta a_{ij}^h} \\end{align} donde: \\begin{align} \\frac{\\delta (y_{ij}-h_{ij}^h)^2}{\\delta (y_{ij}-h_{ij}^h)}=2(y_{ij}-h_{ij}^h) \\end{align} \\begin{align} \\frac{\\delta (y_{ij}-h_{ij}^h)}{\\delta a_{ij}^h}=\\frac{\\delta y_{ij}}{\\delta a_{ij}^h}-\\frac{\\delta h_{ij}^h}{\\delta a_{ij}^h}=0-f'(a_{ij}^h) \\end{align} ya que sabemos que \\(h_{ij}^h=f(a_{ij}^h)\\), por lo que: \\begin{align} \\frac{\\delta h_{ij}^h}{\\delta a_{ij}^h}=\\frac{\\delta f(a_{ij}^h)}{\\delta a_{ij}^h}=f'(a_{ij}^h) \\end{align} una vez desarrollado todo esto sustiuimos los resultados en \\(\\frac{\\delta j_i}{\\delta a_{ij}^h}\\): \\begin{align} \\frac{\\delta j_i}{\\delta a_{ij}^h}=\\frac{1}{2}2(y_{ij}-h_{ij}^h)(-f'(a_{ij}^h))=(y_{ij}-h_{ij}^h)f'(a_{ij}^h) \\end{align} de tal forma que: \\begin{align} \\delta_{ij}^h=\\frac{\\delta j_i}{\\delta a_{ij}^h}=(y_{ij}-h_{ij}^h)f'(a_{ij}^h)=\\epsilon_{ij}^hf'(a_{ij}^h) \\end{align} donde se define \\(\\epsilon_{ij}^h\\) como: \\begin{align} \\epsilon_{ij}^h=y_{ij}-h_{ij}^h \\end{align} finalmente obtenemos que el antigradiente en la última capa \\(h\\) viene dado por: \\begin{align} \\delta w_j^h=-\\mu \\sum_{i=1}^n\\delta_{ij}^hh_i^{h-1}=-\\mu\\sum_{i=1}^n\\epsilon_{ij}^hf'(a_{ij}^h)h_i^{h-1} \\end{align} \\begin{align} \\delta b_j^h=-\\mu \\sum_{i=1}^n\\delta_{ij}^h=-\\mu\\sum_{i=1}^n\\epsilon_{ij}^hf'(a_{ij}^h) \\end{align} retropropagación del gradiente para las capas anteriores a la capa de salida (\\(k<h\\)): \\begin{align} \\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\frac{\\delta j_i}{\\delta a_{il}^{k+1}}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k} \\end{align} en este caso se utiliza la regla de la cadena para obtener \\(\\delta_{ij}^k\\) de modo que se tienen en cuenta todas las combinaciones del gradiente acumulado \\(\\delta a_{il}^{k+1}\\) con la neurona actual (\\(\\delta a_{ij}^k\\)) donde \\(l=1...i_{k+1}\\), es decir se tienen encuenta todas las neuronas de la capa siguiente. con grafos, la regla de la cadena se puede interpretar como todos los caminos posibles desde la capa de salida hasta la neurona \\(j\\) en la capa \\(k\\). cada camino une cada neurona \\(l\\) de la capa siguiente: \\(\\delta_{il}^{k+1}\\) (el cual ya tiene el gradiente acumulado de las capas siguientes) con una neurona \\(j\\) de la capa actual: \\(\\delta a_{ij}^k\\) de la siguiente forma: \\(\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\). además se suman todas las combinación posibles: \\(\\sum_{l=1}^{i_{k+1}}\\). esto nos permite utilizar el gradiente acumulado calculado en la capa siguiente que se propaga hacia atrás en la red neuronal, lo que evita tener que calcular \\(\\frac{\\delta j_i}{\\delta a_{ij}^k}\\). por otro lado tenemos: \\begin{align} a_{il}^{k+1}=(w_l^{k+1})^th_i^{k}+b_l^{k+1} \\end{align} que es el cálculo de la neurona \\(l\\) de la capa siguiente, por lo que utiliza como entradas las salidas de la neurona de esta capa \\(h_i^{k}\\). esta es la versión vectorizada del cálculo, si lo expresamos como sumatorio: \\begin{align} a_{il}^{k+1}=\\sum_{m=1}^{i_k}w_{lm}^{k+1}h_{im}^{k}+b_{lm}^{k+1}=\\sum_{m=1}^{i_k}w_{lm}^{k+1}f(a_{im}^{k})+b_{lm}^{k+1} \\end{align} de tal forma que se multiplican los \\(i_k\\) pesos de la capa siguiente (\\(w_{lm}^{k+1}\\)) con las \\(i_k\\) salidas de la capa actual (\\(h_{im}^{k}\\)) y sumamos los offset (\\(b_{lm}^{k+1}\\)). además sabemos que \\(h_{im}^{k}=f(a_{im}^{k})\\). por lo tanto: \\begin{align} \\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=\\sum_{m=1}^{i_k}(\\frac{\\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\\delta a_{ij}^k}+\\frac{\\delta b_{lm}^{k+1}}{\\delta a_{ij}^k}) \\end{align} la primera derivada tiene la siguiente forma: \\begin{align} \\frac{\\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\\delta a_{ij}^k}=w_{lm}^{k+1}\\frac{\\delta f(a_{im}^{k})}{\\delta a_{ij}^k} \\end{align} \\begin{align} \\frac{\\delta f(a_{im}^{k})}{\\delta a_{ij}^k} =\\begin{cases} f'(a^k_{im})=f'(a^k_{ij}) & m=j\\\\ 0 & m \\ne j \\end{cases} \\end{align} por lo que podemos eliminar el sumatorio sobre \\(m\\) y la derivada sobre el offset ya que su valor es nulo: \\begin{align} \\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}=w_{lj}^{k+1}f'(a^k_{ij}) + 0 \\end{align} gradiente acumulado si volvemos a \\(\\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\). sustituimos \\(\\frac{\\delta a_{il}^{k+1}}{\\delta a_{ij}^k}\\) obteniendo: \\begin{align} \\delta_{ij}^k=\\frac{\\delta j_i}{\\delta a_{ij}^k}=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1}f'(a^k_{ij}) \\end{align} podemos extraer \\(f'(a^k_{ij})\\) ya que esta no depende de \\(l\\): \\begin{align} \\delta_{ij}^k=f'(a^k_{ij})\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1} \\end{align} si definimos: \\begin{align} \\epsilon_{ij}^k=\\sum_{l=1}^{i_{k+1}}\\delta_{il}^{k+1}w_{lj}^{k+1} \\end{align} tenemos que: \\begin{align} \\delta_{ij}^k=f'(a^k_{ij})\\epsilon_{ij}^k \\end{align} derivadas de las funciones de activación la derivada de la función sigmoide: \\begin{align} f'(t)=af(t)(1-f(t)) \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Ejemplo de retropropagación.html",
    "title": "Ejemplo de retropropagación",
    "body": " index search search back ejemplo de retropropagación por ejemplo, supongamos que tenemos una red con tres capas, entonces \\(k=3\\), dado un ejemplo \\(x_j\\). en este caso tenemos que capa 3 la derivada en la última capa, para el único vector de pesos \\(\\theta^{(3)}_1\\) que tiene \\(n\\) elementos (features o características), es: \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_{1t}^{(3)}}\\), para cada \\(t\\), \\(0 \\leq t \\leq n\\) como: \\begin{align} j(\\theta) = e^{(3)}(a_1^{(3)}) = e^{(3)}(g(z_1^{(3)})) = e^{(3)}(g(\\theta^{(3)}\\cdot a^{(2)})) \\end{align} donde denotamos la función que calcula el error entre lo predicho y la salida real como \\(e\\), y \\(g\\) es la función de activación. entonces, aplicamos la regla de la cadena para cada elemento \\(t\\) en el vector de pesos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{1t}^{(3)}} = \\frac{\\delta j(\\theta)}{\\delta a_1^{(3)}}\\frac{\\delta a_1^{(3)}}{\\delta z_1^{(3)}}\\frac{\\delta z_1^{(3)}}{\\delta \\theta_{1t}^{(3)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{1}^{(3)}} = \\frac{\\delta j(\\theta)}{\\delta a_1^{(3)}}\\frac{\\delta a_1^{(3)}}{\\delta z_1^{(3)}}\\frac{\\delta z_1^{(3)}}{\\delta \\theta_{1}^{(3)}} \\end{align} capa 2 si ahora queremos obtener la derivada para uno de los vectores de pesos en la capa \\(2\\), volvemos a aplicar la regla de la cadena. tenemos ahora que desestructurar la función de coste todavía más, hasta obtener la expresión que incluye las salidas de la capa \\(1\\), \\(a^{(1)}\\). \\begin{align} j(\\theta) = e^{(3)}(g(\\theta^{(3)}\\cdot a^{(2)})) = e^{(3)}(g(\\theta^{(3)}\\cdot g(z^{(2)}))) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot a^{(1)}))) \\end{align} sea \\(\\delta^{(3)}_{1j}\\): \\begin{align} \\delta^{(3)}_{1j} = \\frac{\\delta j(\\theta)}{\\delta a_{1j}^{(3)}}\\frac{\\delta a_{1j}^{(3)}}{\\delta z_{1j}^{(3)}} \\end{align} entonces, aplicamos la regla de la cadena para cada nodo \\(i\\) de la capa \\(2\\) y para cada elemento \\(t\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(2)}} = \\sum_{l=1}^{s_{(3)}} \\delta_{lj}^{(3)}\\frac{\\delta z_{lj}^{(3)}}{\\delta a_{lj}^{(2)}}\\frac{\\delta a_{lj}^{(2)}}{\\delta z_{lj}^{(2)}}\\frac{\\delta z_{lj}^{(2)}}{\\delta \\theta_{it}^{(2)}} = \\delta_{1j}^{(3)}\\frac{\\delta z_{1j}^{(3)}}{\\delta a_{1j}^{(2)}}\\frac{\\delta a_{1j}^{(2)}}{\\delta z_{1j}^{(2)}}\\frac{\\delta z_{1j}^{(2)}}{\\delta \\theta_{it}^{(2)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{i}^{(2)}} = \\delta_{j}^{(3)}\\frac{\\delta z^{(3)}}{\\delta a_{j}^{(2)}}\\frac{\\delta a_{j}^{(2)}}{\\delta z_{j}^{(2)}}\\frac{\\delta z_{j}^{(2)}}{\\delta \\theta_{i}^{(2)}} \\end{align} capa 1 para la capa \\(1\\), volvemos a expandir la función de coste para ver cómo aplicar la regla de la cadena: \\begin{align} j(\\theta) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot a^{(1)}))) = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot g(z^{(1)})))) = \\end{align} \\begin{align} = e^{(3)}(g(\\theta^{(3)}\\cdot g(\\theta^{(2)} \\cdot g(\\theta^{(1)} x_j)))) \\end{align} para simplificar la notación: sea, para cada nodo \\(l\\) de la capa \\(2\\) \\begin{align} \\delta^{(2)}_{lj} = \\delta_{1j}^{(3)}\\frac{\\delta z_1^{(3)}}{\\delta a_{lj}^{(2)}}\\frac{\\delta a_{lj}^{(2)}}{\\delta z_{lj}^{(2)}} \\end{align} aplicamos la regla de la cadena, tal que para cada nodo \\(l\\) de la capa \\(2\\): \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(1)}} = \\sum_{l=1}^{s_{(2)}} \\delta_{lj}^{(2)}\\frac{\\delta z_{lj}^{(2)}}{\\delta a_{lj}^{(1)}}\\frac{\\delta a_{lj}^{(1)}}{\\delta z_{lj}^{(1)}}\\frac{\\delta z_{lj}^{(1)}}{\\delta \\theta_{it}^{(1)}} \\end{align} si vectorizamos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{i}^{(1)}} = \\delta_{j}^{(2)}\\frac{\\delta z_{j}^{(2)}}{\\delta a_{j}^{(1)}}\\frac{\\delta a_{j}^{(1)}}{\\delta z_{j}^{(1)}}\\frac{\\delta z_{j}^{(1)}}{\\delta \\theta_{i}^{(1)}} \\end{align} el procedimiento se ilustra en la siguiente figura: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Anexo/Derivada de la función de coste.html",
    "title": "Derivada de la función de coste",
    "body": " index search search back derivada de la función de coste sabemos que la función de coste: \\[%align j(\\theta) = - \\frac{1}{m} \\left\\{ \\sum_{j=1}^m\\sum_{i=1}^c (y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\] donde \\(\\theta_{it}^{(k)}\\) es el peso que conecta el nodo \\(i\\) de la capa \\(k\\) con el nodo \\(t\\) de la capa \\((k-1)\\), es decir, es el elemento en la fila \\(i\\) columna \\(t\\) de la matriz de pesos de la capa \\(k\\), \\(\\theta^{(k)}\\). por la regla de la cadena, separamos la derivada de la función del coste en función de los pesos en dos términos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\sum_{j=1}^m \\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} capa de salida procedemos a calcular la derivada: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left(- \\frac{1}{m}\\right) \\left\\{ \\sum_{j=1}^m\\sum_{i=1}^c (y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\] sacamos el término constante de la derivada y aplicamos la propiedad: \"la derivada de una suma equivale a la suma de las derivadas\" \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left\\{(y_{ij}\\cdot \\log(h_\\theta(x_j)_i)) + [(1-y_{ij})\\log(1-h_\\theta(x_j)_i)]\\right\\} \\] sea \\(h_\\theta(x_j) = a^{(k)}_j\\), es decir la salida de la última capa para el ejemplo \\(j\\). \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\left\\{(y_{ij}\\cdot \\log(a^{(k)}_{ij})) + [(1-y_{ij})\\log(1-a^{(k)}_{ij})]\\right\\} \\] sacaremos el término \\(y_{ij}\\) de la derivada y juntemos todas las expresiones: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\log(a^{(k)}_{ij}) \\right) + (1-y_{ij}) \\left(\\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\log(1-a^{(k)}_{ij})\\right)\\right\\} \\] aplicamos la regla de la cadena sobre el logaritmo: \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\right) + (1-y_{ij}) \\left(\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\frac{\\delta (1-a^{(k)}_{ij})}{\\delta \\theta_{it}^{(k)}} \\right)\\right\\} \\] como sabemos: \\(\\frac{\\delta (1)}{\\delta \\theta_{it}^{(k)}} = 0\\), entonces \\(\\frac{\\delta(1-a_{ij}^{(k)})}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta (1)}{\\delta \\theta_{it}^{(k)}} - \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = 0 + (-1) \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) entonces \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\left\\{y_{ij} \\left(\\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\right) + (1-y_{ij}) \\left((-1)\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\frac{\\delta a^{(k)}_{ij}}{\\delta \\theta_{it}^{(k)}} \\right)\\right\\} \\] sacamos \\(\\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\) como factor común y aplicamos el \\((-1)\\): \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\left\\{y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - \\left((1-y_{ij})\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\right)\\right\\} \\] sustituimos \\(\\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} = \\left\\{y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - \\left((1-y_{ij})\\frac{\\delta \\log(1-a^{(k)}_{ij})}{\\delta (1-a^{(k)}_{ij})} \\right)\\right\\}\\) \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\] si resolvemos las derivadas de los logaritmos obtenemos: \\[%align \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} = y_{ij} \\frac{\\delta \\log(a_{ij}^{(k)})}{\\delta a_{ij}^{(k)}} - (1-y_{ij})\\frac{\\delta \\log(1-a_{ij}^{(k)})}{\\delta (1-a_{ij}^{(k)})} \\] nos centraremos ahora en la derivada que nos falta \\(\\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}}\\): sabemos que, vectorizando la operación, \\(a^{(k)}_j = g(z^{(k)}_j)\\), donde \\(g\\) es la función de activación (en este caso sigmoide). además: \\[%align z^{(k)}_j = \\theta^{k} \\cdot a^{(k-1)}_j \\] por lo tanto, para cada nodo \\(i\\) en la última capa \\(k\\): \\[%align z^{(k)}_{ij} = \\sum_{l=1}^{s_{(k-1)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} \\] donde \\(s_{(k-1)}\\) es el número de nodos en la capa \\(k-1\\). entonces, aplicamos de nuevo la regla de la cadena: \\[%align \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{(k)}} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\] resolvemos la derivada para el segundo término: \\[%align \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\sum_{l=1}^{s_{(k-1)}} \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} \\] tal que: \\[%align \\frac{\\delta}{\\delta \\theta_{it}^{(k)}} \\theta^{(k)}_{il} \\cdot a^{(k-1)}_{lj} = \\begin{cases} a_{lj}^{(k-1)}, & t = l \\\\ 0, & t \\neq l \\\\ \\end{cases} \\] por lo tanto, como sólo hay un \\(l\\) con \\(l = t\\) donde \\(1 \\leq l \\leq s_{(k-1)}\\), entonces: \\[%align \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = a_{lj}^{(k-1)} = a_{tj}^{(k-1)} \\] juntamos ambos términos de la derivada inicial, con \\(\\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{k}} = \\sigma'(z_{ij}^{(k)})\\) \\[%align \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\frac{\\delta g(z_{ij}^{(k)})}{\\delta z_{ij}^{(k)}} \\frac{\\delta z_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} = \\sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)} \\] vamos a resumir lo que tenemos hasta ahora. por la regla de la cadena, separamos la derivada de la función del coste en función de los pesos en dos términos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\sum_{j=1}^m \\sum_{i=1}^c \\frac{\\delta e^{(k)}}{\\delta a_{ij}^{(k)}} \\frac{\\delta a_{ij}^{(k)}}{\\delta \\theta_{it}^{(k)}} \\end{align} si sustituimos ambos términos, para la capa de salida \\(k\\): \\[%align \\frac{\\delta j(\\theta)}{\\delta \\theta_{it}^{(k)}} = \\left(- \\frac{1}{m}\\right) \\sum_{j=1}^m\\sum_{i=1}^c \\sigma'(z_{ij}^{(k)}) a_{tj}^{(k-1)}\\left\\{ \\frac{y_{ij}}{a_{ij}^{(k)}} - \\left(\\frac{(1-y_{ij})}{(1-a^{(k)}_{ij})} \\right)\\right\\} \\] $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/NeuralNetworks/Neural Networks.html",
    "title": "Neural Networks",
    "body": " index search search back neural networks architecture algorithm output layer forward propagation parameters outputs graphical representation optimization problem loss function optimization back-propagation vectorization improving a neural network activation functions initialization techniques anexo architecture input: given any input \\(x\\) the first thing we do is flatten it. for example if \\(x\\) is a rgb image of \\(64 \\times 64\\), then \\(x \\in \\mathbb{r}^{64 \\times 64 \\times 3}\\) (for each of the \\(64 \\times 64\\) pixels we have three color channels: red, green, blue), is flattened into a vector in \\(\\mathbb{r}^{(64*64*3) \\times 1}\\) neuron: is an operation that has two parts: linear part: we denote the linear part like \\(z^{[i]}\\), where \\(i\\) is the current layer. activation part layer: a layer is a compound of neurons that are not connected with each other. algorithm the principal steps of the algorithm are: initialize the weights \\(w\\) and biases \\(b\\) randomly find the optimal \\(w, b\\) use the optimized \\(w, b\\) to predict the output by using the formula \\(\\hat{y} = \\sigma(wx +b)\\) output layer sigmoid the output layer will be different depending on the problem we are tackling. for example if we want to discriminate between 3 classes then the output layer could be as follows: so now the output is a vector \\(\\hat{y} \\in \\mathbb{r}^{c \\times 1}\\) where \\(c\\) is the number of classes. softmax the previous classifier allows for outputting multiples classes in the result, that is we can obtain a predicted output of the form \\(\\hat{y} = \\begin{bmatrix} 1 \\\\1 \\\\ 0 \\end{bmatrix}\\). what if we want to add a constraint such that only one class can be predicted. then we use the softmax function as the activation function on the output layer: thus, instead of a probability for each class what we obtain is a probability distribution for all the classes. relu on linear regression we do not want the activation function to be linear, because then the whole network becomes a very large linear regression. instead we use as an activation function the relu function (rectified linear unit), whose output is zero if the input value is negative and linear otherwise. loss function the loss function when using the sigmoid function on the output layer is as follows: \\begin{align} \\mathcal{l} = - \\frac{1}{q} \\sum_{k=1}^q [y^{(k)} \\log(\\hat{y}^{(k)}) + (1- y^{(k)})\\log(1-\\hat{y}^{(k)})] \\end{align} where \\(\\hat{y}^{(k)}\\) are the predicted values and \\(q\\) is the total number of neurons on the output layer. however, if we use the softmax function as the activation function on the last layer we have to use a different derivative because this function does depend on the outputs of the other neurons. thus, we make use of a function called cross entropy loss: \\begin{align} \\mathcal{l}_{ce} = - \\sum_{k=1}^q y^{(k)} \\log(\\hat{y}^{(k)}) \\end{align} for linear regression we use as the loss function the l1-norm or the l2-norm. the latter is defined as follows: \\begin{align} \\mathcal{l} = || \\hat{y} - y ||_2^2 \\end{align} forward propagation the forward propagation equations are the following: \\begin{align} z^{[i]} = w^{[i]} a^{[i-1]} + b^{[i]} \\tag{1} \\end{align} where \\(i\\) is the layer with \\(i \\geq 1\\), and the first layer equals the input matrix, that is \\(a^{[0]} = x\\). by applying the activation function over \\((1)\\): \\begin{align} a^{[i]} = g(z^{[i]}) \\end{align} where \\(g\\) is the activation function (e.g \\(\\sigma(z^{[i]})\\)). now, what are the shapes of these matrices? \\(z^{[i]} \\in \\mathbb{r}^{s_i \\times m}\\) \\(a^{[i]} \\in \\mathbb{r}^{s_i \\times m}\\) where \\(s_i\\) is the number of neurons on the ith layer and \\(m\\) is the number of examples. note that the shape of the final layer changes depending on the task. so if \\(k\\) is the number of layers: in linear regression: \\(\\hat{y} = a^{[k]} \\in \\mathbb{r}^{1 \\times m}\\) in multi-class classification: \\(\\hat{y} = a^{[k]} \\in \\mathbb{r}^{c \\times m}\\), where \\(c\\) is the number of classes. also the shape of the weights are \\(w[i] \\in \\mathbb{r}^{s_i \\times s_{i-1}}\\), that is, this matrix is compatible with the outputs of the previous layer (\\(a^{[i-1]} \\in \\mathbb{r}^{s_{i-1} \\times m}\\)) and the linear part of the next layer (\\(z^{[i]} \\in \\mathbb{r}^{s^i \\times m}\\)). however, the bias are \\(b^{[i]} \\in \\mathbb{r}^{s^i \\times 1}\\), therefore we cannot perform an element wise summation because the shape of \\((w^{[i]} a^{[i-1]}) \\in \\mathbb{r}^{s_i \\times m}\\) and \\(b^{[i]}\\) are not compatible. to avoid this problem we apply a technique called broadcasting to \\(b\\), such that we replicate \\(b^{[i]}\\) \\(m\\) times: \\begin{align} \\hat{b}^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ b^{[i]} & b^{[i]} & \\cdots & b^{[i]} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} to sum up, the shapes of the data and the parameters on each layer \\(i\\) are: parameters \\begin{align} \\hat{b}^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ b^{[i]} & b^{[i]} & \\cdots & b^{[i]} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} \\begin{align} w^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ w^{[i](1)} & w^{[i](2)} & \\cdots & w^{[i](s_{i-1})} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times s_{i-1}} \\end{align} outputs note that for each example \\(j\\) on layer \\(i\\) \\(z^{[i](j)} = (w^{[i]} a^{[i-1](j)} + \\hat{b}^{[i]})\\), then: \\begin{align} z^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ z^{[i](1)} & z^{[i](2)} & \\cdots & z^{[i](m)} \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} \\begin{align} a^{[i]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ g(z^{[i](1)}) & g(z^{[i](2)}) & \\cdots & g(z^{[i](m)}) \\\\ | & | & \\cdots & | \\\\ \\end{bmatrix} \\in \\mathbb{r}^{s_i \\times m} \\end{align} graphical representation now we present a small example of how forward propagation works on neural networks: optimization problem what we want to do is find the parameters \\(w^{[i]}, b^{[i]}\\) for each layer \\(i\\) that minimize the cost. loss function so first of all we define a cost function for the objective \\(\\mathcal{l}(\\hat{y}, y)\\), where \\(\\hat{y}\\) is the predicted output and \\(y\\) is the real output. the cost function will depend on the type of problem (classification, regression). optimization the we optimize the loss function we defined by using backward propagation. for each layer \\(l=1, \\cdots, k\\), where \\(k\\) is the number of layers, we apply batch gradient descent (not mandatory, but here it is convenient as we can vectorize the derivatives) as follows: \\begin{align} w^{[l]} = w^{[l]} - \\alpha \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[l]}} \\end{align} \\begin{align} b^{[l]} = b^{[l]} - \\alpha \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta b^{[l]}} \\end{align} back-propagation to compute the derivatives of the cost function with respect to \\(w^{[l]}\\) and \\(b^{[l]}\\) we use the chain rule. output layer suppose we have \\(k\\) layers, then we start by calculating \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}}\\) and \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta b^{[k]}}\\), that is, the derivatives on the last layer. thus, to update \\(w^{[k]}\\) (we apply the same logic for \\(b^{[k]}\\)): \\begin{align} \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}(\\hat{y^{(i)}}, y^{(i)})}{\\delta w^{[k]}} = \\end{align} because \\(\\hat{y^{(i)}} = (a^{[k]})^{(i)}\\): \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta w^{[k]}} \\end{align} we apply the chain rule on the derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta w^{[k]}} \\end{align} because \\((a^{[k]})^{(i)} = g((z^{[k]})^{(i)})\\), where \\(g\\) is the activation function: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta w^{[k]}} \\end{align} we apply the chain rule on the last derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k]}} \\end{align} hidden layers what about the previous layer \\(k-1\\)? \\begin{align} \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k-1]}} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}(\\hat{y^{(i)}}, y^{(i)})}{\\delta w^{[k-1]}} = \\end{align} because \\(\\hat{y^{(i)}} = (a^{[k]})^{(i)}\\): \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((a^{[k]})^{(i)} = g((z^{[k]})^{(i)})\\), where \\(g\\) is the activation function: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the last derivative, therefore: \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta g((z^{[k]})^{(i)})}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta (z^{[k]})^{(i)}} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} as you can see the first two derivatives are the same as the derivatives on the layer \\(k\\), let's denote \\((\\delta^{[k]})^{(i)} = \\frac{\\delta \\mathcal{l}((a^{[k]})^{(i)}, y^{(i)})}{\\delta (a^{[k]})^{(i)}} \\frac{\\delta (a^{[k]})^{(i)}}{\\delta (z^{[k]})^{(i)}}\\) the accumulated gradient on layer \\(k\\) for example \\(i\\), then: \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((z^{[k]})^{(i)} = w^{[k]} (a^{[k-1]})^{(i)} + b^{[k]}\\): \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (w^{[k]} (a^{[k-1]})^{(i)} + b^{[k]})}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta (a^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} because \\((a^{[k-1]})^{(i)} = g((z^{[k-1]})^{(i)})\\) \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta g((z^{[k-1]})^{(i)})}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} we apply the chain rule on the last derivative, hence: \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta g((z^{[k-1]})^{(i)})}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} \\begin{align} = \\sum_{i=1}^m (\\delta^{[k]})^{(i)} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta (a^{[k-1]})^{(i)}} \\frac{\\delta (a^{[k-1]})^{(i)}}{\\delta (z^{[k-1]})^{(i)}} \\frac{\\delta (z^{[k-1]})^{(i)}}{\\delta w^{[k-1]}} \\end{align} vectorization output layer accumulated gradient for layer \\(k\\): \\(\\delta_w^{[k]} = \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta a^{[k]}} \\frac{\\delta a^{[k]}}{\\delta z^{[k]}}\\) gradient for layer \\(k\\): \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k]}} = \\delta_w^{[k]} \\frac{\\delta z^{[k]}}{\\delta w^{[k]}}\\) hidden layer accumulated gradient for layer \\(k-1\\): \\(\\delta_w^{[k-1]} = \\delta_w^{[k]} \\frac{\\delta z^{[k]}}{\\delta a^{[k-1]}} \\frac{\\delta a^{[k-1]}}{\\delta z^{[k-1]}}\\) gradient for layer \\(k-1\\): \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w^{[k-1]}} = \\delta_w^{[k-1]} \\frac{\\delta z^{[k-1]}}{\\delta w^{[k-1]}}\\) graphical representation on the following image we show how to obtain the gradient of the first element of the first layer's first neuron's weights \\(w^{[1]}_{11}\\) on the first layer: improving a neural network activation functions why do we need activation functions? well, suppose you have the following network where the activation function is the identity function. that is \\(a^{[i]} = g(z^{[i]}) = z^{[i]}\\): then: \\begin{align} \\hat{y} = a^{[3]} = z^{[3]} = w^{[3]} a^{[2]} + b^{[3]} = w^{[3]} z^{[2]} + b^{[3]} = w^{[3]} (w^{[2]} a^{[1]} + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} (w^{[2]} z^{[1]} + b^{[2]}) + b^{[3]} = w^{[3]} (w^{[2]} (w^{[1]} x + b^{[1]}) + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} (w^{[2]} w^{[1]} x + w^{[2]} b^{[1]} + b^{[2]}) + b^{[3]} \\end{align} \\begin{align} = w^{[3]} w^{[2]} w^{[1]} x + w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]} \\end{align} if \\begin{align} w = w^{[3]} w^{[2]} w^{[1]} \\end{align} \\begin{align} b = w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]} \\end{align} then: \\begin{align} \\hat{y} = wx + b \\end{align} as you can see if we do not use activation functions, it does not mater how deep your network is, it is going to be equivalent to a linear regression. depending on the task at hand we use different activation functions: sigmoid: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\), it maps \\(z \\in (-\\infty, \\infty)\\) to \\((0, 1)\\) it is good for classification works well when the values are in the linear region of the function however when the values are on the extremes the gradient (slope) is very small, therefore it ends up vanishing in the network. relu: \\(relu(z) = \\begin{cases}0 & z \\leq 0 \\\\ 1 & z > 0\\end{cases}\\) tanh: \\(tanh(z) = \\frac{e^z - e^{-z}}{(e^z + e^{-z})}\\) initialization techniques usually we normalize the input to avoid having saturated activation functions. to normalize: \\begin{align} x^{(i)}_j = \\frac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\end{align} for every example \\(i\\) and feature \\(j\\). where: \\(\\mu_j\\) is the mean of the \\(j\\) feature, thus: \\(\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}_j\\) \\(\\sigma_j^2\\) is the variance of the \\(j\\) feature, thus: \\(\\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}_j - \\mu_j)^2\\) vanishing/exploding gradients suppose you have the following network, where the activation function is the identity function and \\(b=0\\). then \\(\\hat{y} = w^{[l]} a^{[l-1]} = w^{[l]} w^{[l-1]} a^{[l-2]} = \\cdots = w^{[l]} w^{[l-1]} \\cdots w^{[1]} x\\) therefore, if: \\begin{align} w^{[l]} = \\begin{bmatrix} 1.5 & 0 \\\\ 0 & 1.5 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\hat{y} = \\begin{bmatrix} 1.5^l & 0 \\\\ 0 & 1.5^l \\\\ \\end{bmatrix} \\end{align} which means we end up with an exploding gradient. the inverse happens when: \\begin{align} w^{[l]} = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\\\ \\end{bmatrix} \\end{align} then: \\begin{align} \\hat{y} = \\begin{bmatrix} 0.5^l & 0 \\\\ 0 & 0.5^l \\\\ \\end{bmatrix} \\end{align} which results in a vanishing gradient. to avoid this somewhat, we need to initialize the weights properly. what we want is for the weights to be very close to one to avoid the exploding/diminishing problem. intuition given a single neuron: then \\(a = g(z)\\) and \\(z = w_1 x_1 + \\cdots + w_n x_n\\). we can see that \\(z\\) will increase if \\(n\\) increases, therefore we would want \\(w_i\\) to be as small as \\(n\\) is large, that is: \\begin{align} w_i = \\frac{1}{n} \\end{align} initialization techniques if we want the value of \\(w^{[l]}\\) to be proportional to the number of inputs coming from the layer \\(l\\) (\\(n^{[l-1]}\\)). it works very well for sigmoid activation: w[k] = np.random.randn(shape)*np.sqrt(1/n[l-1]) for relu: w[k] = np.random.randn(shape)*np.sqrt(2/n[l-1]) xavier initialization (used with tanh): \\(w^{[l]} \\sim \\sqrt{\\frac{1}{n^{[l-1]}}}\\) her initialization: \\(w^{[l]} \\sim \\sqrt{\\frac{2}{n^{[l]} + n^{[l-1]}}}\\) also you need to initialize the weights randomly, else you will run into the symmetry problem, where all neurons learn the same thing (that is they update very similarly). optimization mini batch gradient descent mini batch gradient descent is a trade off between batch gradient descent and stochastic gradient descent. also, because mini batch gradient descent is an approximation it introduces some noise on the loss function: however mini batch gradient descent is more used because batch gradient descent can be very computationally expensive. momentum algorithm this algorithm combines gradient descent and momentum. suppose you have the following contour plot, where the horizontal axis is much more extended that the vertical axis. by default on gradient descent the gradient of the loss will be orthogonal to the contour at the given point, as we can see: however, what we would like, so it would converge faster, is to make it move more horizontally than vertically. in order to do that we use a technique called momentum. it takes intro account past updates to find the right way to go. if you take an average of past updates, then: vertical axis: it practically cancels itself because it oscillates a lot horizontal axis: its value it's maintained because the past and present gradients go in the same direction to update the weights we apply the following equation: \\begin{align} \\upsilon = \\beta \\upsilon + (1 - \\beta) \\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w} \\end{align} where: \\(\\upsilon\\): stores past updates \\(\\frac{\\delta \\mathcal{l}(\\hat{y}, y)}{\\delta w}\\): stores the current update we average with \\(\\beta\\) and \\((1 - \\beta)\\) finally we update the weights: \\begin{align} w = w - \\alpha \\upsilon \\end{align} anexo for more info about cost function and how to derive them: anexo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Ecuación Normal.html",
    "title": "Ecuación Normal",
    "body": " index search search back ecuación normal descripción de los datos hipótesis funcion de coste minimización del coste anotaciones descripción de los datos \\(x = (x_{ij})\\) una matriz \\(m \\times (n + 1)\\) donde cada \\(x_{i}\\) es un vector fila \\(1 \\times (n+1)\\), que incluye los valores de todas las características para el ejemplo \\(i\\). \\begin{align} x = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_i \\\\ \\vdots \\\\ x_m \\\\ \\end{bmatrix} \\end{align} cabe destacar que \\(x_{i0} = 1\\), es el término independiente. \\(\\theta = (\\theta_i)\\) es un vector columna \\((n+1)\\times 1\\) donde cada \\(\\theta_i\\) es el peso de la característica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector columna \\(m\\times 1\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 \\\\ \\cdots \\\\ y_m\\end{bmatrix} \\end{align} hipótesis dado un conjunto de \\(m\\) datos, es decir matriz \\(x\\), de dimensiones \\(m \\times (n+1)\\), la hipótesis se define como: \\begin{align} h_\\theta(x) = x \\cdot \\theta = \\begin{bmatrix} x_1 \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{1i} \\\\ \\vdots \\\\ x_j \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{ji} \\\\ \\vdots \\\\ x_m \\cdot \\theta = \\sum_{i=0}^{n+1} \\theta_i x_{mi} \\\\ \\end{bmatrix} \\end{align} observa que ahora \\(x\\) y \\(\\theta\\) están colocados de forma inversa a como lo hacíamos en la regresión lineal y la regresión logística. esto es debido a que hemos transpuesto las matrices \\(x\\) y \\(\\theta\\), con respecto a como las habíamos definido en las secciones anteriores. el cálculo es el mismo. funcion de coste se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) \\end{align} la expresión \\((x\\theta - y)^t(x\\theta - y)\\) es equivalente a \\((h_\\theta(x) - y)^2\\), que se utilizaba en la función de coste de la regresión lineal. regularización con regularización, se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) + \\frac{1}{2m} \\lambda \\theta^t\\theta \\end{align} minimización del coste con la ecuación normal, en lugar de actualizar el vector de pesos \\(\\theta\\) de forma iterativa, lo que hacemos es igualar la derivada del coste en base a los pesos a cero utilizando derivación matricial: \\[%align \\delta_\\theta j(\\theta) = \\begin{bmatrix} \\frac{\\delta j(\\theta)}{\\delta \\theta_0} \\\\ \\vdots \\\\ \\frac{\\delta j(\\theta)}{\\delta \\theta_i} \\\\ \\vdots \\\\ \\frac{\\delta j(\\theta)}{\\delta \\theta_n} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ \\end{bmatrix} = \\overrightarrow{0} \\] a continuación exponemos cómo se calcula la derivada: sustituímos la función de coste: \\[%align \\delta_\\theta j(\\theta) = \\delta_\\theta \\frac{1}{2m}(x \\theta - y)^t (x \\theta - y) \\] aplicamos la propiedad \\((a + b)^t = a^t + b^t\\) \\[%align \\delta_\\theta j(\\theta) = \\delta_\\theta \\frac{1}{2m}((x\\theta)^t - y^t) (x \\theta - y) \\] sacamos el factor constante de la derivada y realizamos la multiplicación: \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - (x\\theta)^ty -y^tx\\theta + y^ty \\] aplicamos la propiedad \\(ab = b^ta^t\\), tal que \\(y^t(x\\theta) = (x\\theta)^t((y)^t)^t = (x\\theta)^ty\\) \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - (x\\theta)^ty - (x\\theta)^ty + y^ty \\] agrupamos términos compatibles: \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - 2(x\\theta)^ty + y^ty \\] como \\(\\delta_\\theta y^ty=0\\): \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{2m} \\delta_\\theta (x\\theta)^t(x\\theta) - 2(x\\theta)^ty \\] finalmente calculamos la derivada matricial: \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{2m} 2x^t(x\\theta) - 2x^ty = \\frac{1}{m} x^t(x\\theta) - x^ty \\] ahora igualamos la expresión obtenida a cero: \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{m} [x^tx\\theta - x^ty] = 0 \\] multiplicamos por \\(m\\) en ambos lados de la ecuación: \\[%align x^tx\\theta - x^ty = 0 \\] sumamos \\(x^ty\\) en ambos lados de la ecuación: \\[%align x^tx\\theta - x^ty + x^ty= x^ty \\] \\[%align x^tx\\theta = x^ty \\] multiplicamos por \\((x^tx)^{-1}\\) por la izquierda en ambos lados de la ecuación: \\[%align (x^tx)^{-1}x^tx\\theta = (x^tx)^{-1}x^ty \\] \\[%align i\\theta = (x^tx)^{-1}x^ty \\] \\[%align \\theta = (x^tx)^{-1}x^ty \\] de tal manera que ahora hemos calculado el vector de pesos óptimo que minimiza el coste. regularización con regularización debemos derivar la función que coste que incluye el parámetro de regularización: \\begin{align} j(\\theta) = \\frac{1}{2m} (x\\theta - y)^t(x\\theta - y) + \\frac{1}{2m} \\lambda \\theta^t\\theta \\end{align} el primer término ya lo hemos derivado, por lo tanto procedemos a derivar el segundo término: \\[%align \\delta_\\theta \\frac{1}{2m} \\lambda \\theta^t\\theta \\] sacamos el factor constante \\(\\frac{\\lambda}{2m}\\) fuera de la derivada \\[%align \\frac{\\lambda}{2m} \\delta_\\theta [\\theta^t\\theta] \\] llevamos a cabo la derivada matricial: \\[%align \\frac{\\lambda}{2m} 2 \\theta = \\frac{\\lambda}{m} \\theta \\] juntamos las derivadas de ambos términos: \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{m} [x^tx\\theta - x^ty] + \\frac{\\lambda}{m} \\theta \\] sacamos \\(\\frac{1}{m}\\) como factor común e igualamos a cero \\[%align \\delta_\\theta j(\\theta) = \\frac{1}{m} (x^tx\\theta - x^ty + \\lambda\\theta) = 0 \\] multiplicamos por \\(m\\) en ambos lados de la ecuación: \\[%align x^tx\\theta - x^ty + \\lambda\\theta = 0 \\] sumamos \\(x^ty\\) en ambos lados de la ecuación: \\[%align x^tx\\theta - x^ty + x^ty + \\lambda\\theta = x^ty \\] \\[%align x^tx\\theta + \\lambda\\theta = x^ty \\] sacamos \\(\\theta\\) como factor común \\[%align (x^tx + \\lambda i)\\theta = x^ty \\] donde \\(i\\) es la matriz identidad e dimensiones \\((n+1) \\times (n+1)\\). multiplicamos \\((x^tx + \\lambda i)^{-1}\\) por la izquierda en ambos lados de la ecuación: \\[%align (x^tx + \\lambda i)^{-1}(x^tx + \\lambda i)\\theta = (x^tx + \\lambda i)^{-1}x^ty \\] \\[%align i\\theta = (x^tx + \\lambda i)^{-1}x^ty \\] \\[%align \\theta = (x^tx + \\lambda i)^{-1}x^ty \\] de tal forma que hemos calculado el \\(\\theta\\) óptimo que minimiza el coste, utilizando regularización. anotaciones no se debe utilizar la ecuación normal cuando el número de ejemplos \\(m\\) es muy grande, ya que es rendimiento del algoritmo es malo hay que tener cuidado con si las matrices son inversibles si \\(m \\leq n\\), entonces las matrices no son invertibles. si \\(\\lambda > 0\\), entonces aseguramos la inversibilidad de las matrices. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/index.html",
    "title": "Machine Learning Stanford Coursera",
    "body": " index search search back machine learning stanford coursera apredizaje supervisado regresión lineal regresión logística ecuación normal neural networks gradient checking inicialización aleatoria evaluación de modelos svm aprendizaje no supervisado en el aprendizaje no supervisado, los ejemplos de entrenamiento no tienen etiquetas (\\(y\\)). se utilizan para buscar correlación y patrones en los ejemplos de entrenamiento. clustering dimensionality reduction expectation-maximization algorithms sistemas de recomendación grandes datasets aprendizaje online map reduce datos artificiales ceiling analysis $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Aprendizaje Online.html",
    "title": "Aprendizaje Online",
    "body": " index search search back aprendizaje online en estos tipos de problemas se generan datos de forma continua, tal que para cada nuevo dato: obtenemos \\((x, y)\\) actualizamos \\(\\theta\\) utilizando el nuevo ejemplo: \\(\\theta_j = \\theta_j - \\alpha (h_\\theta(x) - y)x_j\\) ejemplo aprender a buscar. supongamos que lo queremos aprender es aquellos resultados que le interesen más al usuario. si tenemos los siguientes datos: \\(x\\): características del producto \\(y\\): si el usuario hace click entonces, lo que queremos aprender es \\(p(y= 1|x;\\theta)\\), tal que por ejemplo enseñemos los 10 productos cuya probabilidad es mayor. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Datos Artificiales.html",
    "title": "Datos Artificiales",
    "body": " index search search back datos artificiales cómo podemos generar datos? manualmente modificando los datos de entrada (añadir ruido en sonido, distorsionar imagen, etc) no obstante, debemos evitar añadir ruido aleatorio, ya que esto no ayuda a extraer características significativas del conjunto de datos. estos métodos se suelen utilizar si el modelos tiene un sesgo bajo y se produce underfitting, (por lo que hace falta añadir características). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Inicialización aleatoria.html",
    "title": "Inicialización aleatoria",
    "body": " index search search back inicialización aleatoria cuando creamos una red neuronal, si inicializamos todos los pesos \\(\\theta\\) a cero, entonces todos los nodos serán iguales. por ello se inicializa \\(\\theta\\) con valores aleatorios dentro de un rango \\([- \\epsilon, \\epsilon]\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/DimensionalityReduction/Dimensionality Reduction.html",
    "title": "Dimensionality Reduction",
    "body": " index search search back dimensionality reduction standardize data pca find underlying space represent the subspace algorithm layout performing eigen decomposition ica intuition solution given examples \\(\\{x^{(i)}\\}_{i=1}^n\\) where \\(x^{(i)} \\in \\mathbb{r}^d\\), we want to find out if our data lives is a low dimensional space. look at the next example: we can see that the two features are correlated, and we can project the points onto a line, reducing the space from two dimensions to one. so it might be the case that some features are highly correlated, and so de d-dimensional space can be as a k-dimensional space where \\(0 < k < d\\): \\begin{align} \\begin{bmatrix} x_{11} & \\cdots & x_{1d} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{nd} \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} x_{11} & \\cdots & x_{1k} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{nk} \\\\ \\end{bmatrix} \\end{align} standardize data a lot of the times the units of each feature in the data make the values in one column (feature) be much bigger than the values in another column. thus, the first step is to standardize your data: center data have it have variance equal to one so we transform our data as follows: \\begin{align} x_j^{(i)} = \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j} \\end{align} where: \\(u_j\\) is the mean of the feature \\(j\\) over the \\(n\\) examples, such that \\(u_j = \\frac{1}{n}\\sum_{i=1}^nx^{(i)}_j\\) \\(\\sigma_j\\) is the standard deviation of the feature \\(j\\) over the \\(n\\) examples, where \\(\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n(x^{(i)}_j - \\mu_j)^2\\) pca find underlying space to reduce the dimensionality of our data we first define a subspace and then we project each point onto the subspace. this projection is the closes point in the subspace to the point we are trying to project, this has as a consequence that the \"line\" connecting the point to its projection is always perpendicular to the subspace: the goal is to choose the subspace that maximizes the variance of the projected points, to retain the maximum possible variance of the data. as you can see if we choose the blue line as the subspace the variance is much bigger that if we choose the red line: represent the subspace let us suppose the subspace is defined by a basis vector \\(u \\in \\mathbb{r}^d\\) where \\(u\\) is a unit vector, then projection of \\(\\overrightarrow{x^{(i)}}\\) on to the space spanned by \\(u\\) will be: \\begin{align} proj(u)\\overrightarrow{x^{(i)}} \\end{align} where \\(proj(u)\\) is the projection matrix and \\(x^{(i)} \\in \\mathbb{r}^d\\). so, because \\(proj(u) = \\frac{uu^t}{u^tu}\\), then the projected point is defined as: \\begin{align} proj(u)\\overrightarrow{x^{(i)}} = \\frac{uu^t}{u^tu} \\overrightarrow{x^{(i)}} = ((x^{(i)})^tu)u \\end{align} where \\(((x^{(i)})^tu)\\) is an scalar. so, now our goal is to find a \\(u\\) that maximizes the variance across the \\(n\\) examples. that is, we want to maximize the sum of the square of the norms of the projected points: more formally: \\begin{align} u = \\underset{u}{\\arg \\max} \\frac{1}{n}\\sum_{i=1}^n ||proj(u)x^{(i)}||^2 = \\frac{1}{n}\\sum_{i=1}^n ||((x^{(i)})^tu)u||^2 \\end{align} because the norm of a unit vector multiplied by a scalar is just the square of the scalar (for \\(3 \\cdot \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}\\): \\(||\\begin{pmatrix} 3 & 0 & 0 \\end{pmatrix}|| = (\\sqrt{3^2 + 0^2 + 0^2})^2 = 3^2\\)): \\begin{align} u = \\underset{u}{\\arg \\max} \\frac{1}{n}\\sum_{i=1}^n ((x^{(i)})^tu)^2 = \\frac{1}{n}\\sum_{i=1}^n ((x^{(i)})^tu)^t((x^{(i)})^tu) = \\frac{1}{n}\\sum_{i=1}^n u^t x^{(i)} (x^{(i)})^t u \\end{align} because \\(u, u^t\\) are a common factor in the sum: \\begin{align} u = \\underset{u}{\\arg \\max} \\left[u^t \\left(\\frac{1}{n}\\sum_{i=1}^n x^{(i)} (x^{(i)})^t \\right) u\\right] \\end{align} now, we know that given the optimization problem of the form \\(\\underset{u}{\\arg \\max} \\left[u^t a u\\right]\\), the solution \\(u\\) is the eigenvector corresponding to the largest eigenvalue of \\(a\\). in this scenario, \\(a = \\left(\\frac{1}{n}\\sum_{i=1}^n x^{(i)} (x^{(i)})^t \\right)\\), which equals the sample covariance matrix, which is defined as: \\begin{align} \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - \\mu)^t (x^{(i)} - \\mu) = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)} - 0)^t (x^{(i)} - 0) = \\frac{1}{n} \\sum_{i=1}^n (x^{(i)})^t x^{(i)} \\end{align} note that, because our data is now centered after standardizing it, the mean \\(\\mu\\) equals zero. hence, we want to calculate the eigenvectors of the sample covariance matrix of x. mind you, we have derived this solution for a space defined by only one vector \\(u\\). however given basis vectors \\((u_1, \\cdots, u_k)\\) the optimization problem holds and the solution are the \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues of \\(a\\). algorithm layout the steps of pca are the following: calculate the sample covariance matrix as \\(x^tx\\) calculate the eigenvector and eigenvalues of \\(x^tx\\), such that: \\begin{align} \\begin{matrix} (\\lambda_1, u_1) \\\\ (\\lambda_2, u_2) \\\\ \\vdots \\\\ (\\lambda_d, u_d) \\\\ \\end{matrix} \\end{align} are the \\(d\\) eigenvectors (\\(u_i\\)) and eigenvalues (\\(\\lambda_i\\)). we assume the tuples are ordered in decreasing order with respect to the eigenvalues, such that if \\(i > j\\) then \\(\\lambda_i > \\lambda_j\\). find \\(k\\) such that we satisfy a confidence level with respect to the variance, i.e. suppose you want to preserve 95% of the variance of the original data then: \\begin{align} \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^d \\lambda_j} = 95\\% \\end{align} choose the \\(k\\) eigenvectors with the largest corresponding eigenvalues. performing eigen-decomposition first of all, let us present two properties regarding eigen-decompositions of a matrix \\(x\\): if \\(x\\) is a square matrix and symmetric then \\(x\\) has orthogonal eigenvectors and real eigenvalues. if \\(x\\) is also positive semi-definite then the eigenvalues are positive. in our case, the eigen-decomposition is done over \\(x^tx\\), therefore this matrix is guaranteed to be a square matrix, symmetric and positive semi-definite. then, performing the eigen decomposition of \\(x^tx\\) is equivalent to performing singular value decomposition (svd) over \\(x\\), such that the single values equal the square root of the eigenvalues. large datasets to perform pca on large datasets we use a technique called power iteration, which consists on: for \\(i=0\\), initialize \\(u^{(i)}\\) to random values other than zero set \\(i = i+1\\), and \\(u^{(i)} = (x^tx)u^{(i-1)}\\) re-scale \\(u^{(i)}\\) to have unit length such that: \\(u^{(i)}=\\frac{(x^tx)u^{(i-1)}}{||(x^tx)u^{(i-1)}||}\\) go to step 2. eventually it will converge to the largest eigenvector. rephrasing pca another way to describe the problem solved by pca, equivalent to the maximization variance perspective, is: find a subspace such that the projection of the points are as close to the original data as possible, that is minimize the sum of the distances between the projected points and the original points. ica this algorithm pretends to solve what is commonly known as the source separation problem, where we are given a dataset \\(x\\) that is a mixture of some source data \\(s\\). we then use these mixed sources \\(x\\) to construct a unmixing matrix \\(w\\) to recover the source \\(s\\). intuition imagine there are \\(d\\) speakers and \\(d\\) microphones randomly distributed in a room, such that: \\(s \\in \\mathbb{r}^d\\) is the representation of what a speaker says. so \\(s_j^{(i)}\\) is what the \\(j\\) speaker said in moment \\(i\\). \\(x \\in \\mathbb{r}^d\\) is the representation of what a microphone records. so \\(x_j^{(i)}\\) is what the \\(j\\) microphone recorded in moment \\(i\\). for example, given two speaker, what they say is represented as follows: meanwhile the recordings of the microphones are the following: we are only given \\(x\\), and the goal is to recover the original speech signal spoken by each speaker. we assume that \\(x\\) is a linear combination of what each speaker says, thus \\(x = as\\), where \\(a\\) is a quare matrix \\(d \\times d\\) and is called the mixing matrix. what we want to do is to compute the inverse of \\(a\\), \\(w\\) such that \\(w = a^{-1}\\), where \\(w\\) is called the unmixing matrix. then: \\begin{align} a^{-1}x = a^{-1}as \\rightarrow a^{-1}x = s \\rightarrow wx = s \\rightarrow s = wx \\end{align} solution to solve this problem we make the following assumptions: the number of sources \\(s\\) are equal to the number of \"examples\" in the mixed dataset \\(x\\) \\(x\\) is a linear combination of \\(s\\), such that \\(s = wx\\) \\(s_j\\) is independent of \\(s_k\\), whenever \\(j \\neq k\\). that is to say, each belongs to a different probability distribution, and are two independent random variables. each \\(s_j\\) is not gaussian. intuition suppose we are given a random variable \\(x\\) such that \\(x ~ unif [0,1]\\), then the probability density function is: let us define a new distribution as follows \\(y=2x\\), then the probability density function is: note, that the function is \"stretched\" as to always satisfy the condition that the integral of \\(pdf\\) must equal 1, which is the same as saying the area under the function is 1. so now, \\(p_y (y) = p_x(x) \\cdot \\frac{1}{2} = p_x(\\frac{y}{2})\\cdot\\frac{1}{2}\\), because \\(x = \\frac{y}{2}\\). but what happens in a higher dimension? that is, what happens when we multiply \\(x \\in \\mathbb{r}^d\\) by a mixing matrix \\(w \\in \\mathbb{r}^{d \\times d}\\). well, given \\(y \\in \\mathbb{r}^{d \\times d}\\), such that \\(y=wx\\), then to perform a change of random variable we use the jacobian: \\begin{align} p_y(y) = p_x(x)\\frac{1}{|w|} = p_x(w^{-1}y)\\frac{1}{|w|} \\end{align} where \\(|w|\\) is the determinant of \\(w\\). first of all we define the distribution of the mixed dataset as follows: \\begin{align} p_x(x) = \\prod_{j=1}^d p_s (s_j) |w| = \\prod_{j=1}^d p_s (w_j^tx) |w| \\end{align} note that \\(s_j=(w_j)^tx\\). we also assume that \\(p_s\\) is distributed according to a logistic distribution, thus: cumulative distribution function (cdf): \\(\\frac{1}{1+e^{-x}} = \\sigma(x)\\) probability density function (pdf): \\(\\sigma(x)\\sigma(1-x)\\) so, we obtain the likelihood of \\(w\\) as follows: \\begin{align} l(w) = \\sum_{i=1}^n \\left[\\left(\\sum_{j=1}^d \\log[\\sigma(x^{(i)})(1-\\sigma(x^{(i)}))]\\right) + \\log{|w|}\\right] \\end{align} where \\(w\\) is the parameter we are trying to obtain. so, to solve the optimization problem: we define the maximization of the likelihood as the objective we compute the derivative of \\(l(w)\\) and perform gradient descent, such that the update step is as follows: \\begin{align} w = w - \\alpha \\left\\{\\begin{bmatrix} (1- 2\\sigma(w_1^tx^{(i)})) \\\\ (1- 2\\sigma(w_2^tx^{(i)})) \\\\ \\vdots \\\\ (1- 2\\sigma(w_d^tx^{(i)})) \\\\ \\end{bmatrix} (x^{(i)})^t + (w^t)^{-1} \\right\\} \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Sistemas de Recomendación.html",
    "title": "Sistemas de Recomendación",
    "body": " index search search back sistemas de recomendación dados los parámetros: \\(n_\\mu\\): número de usuarios \\(n_m\\): número de ítems valorables \\(r(i,j)\\): marcador de si el ítem ha sido valorado, tal que: \\begin{align} r(i, j) = \\begin{cases} 1, & \\text{ si el usuario j ha valorado el ítem i} \\\\ 0, & \\text{ en cualquier otro caso} \\end{cases} \\end{align} \\(y^{(i, j)}\\): valoración del usuario \\(j\\) al ítem \\(i\\). el objetivo de un sistema de recomendación es predecir los valores de las valoraciones donde \\(r(i, j) \\neq 1\\) (es decir predecir las valoraciones de usuarios hacia ítems que no han valorado con anteioridad) content based recommendations cada ítem está definido por \\(n\\) características. para cada usuario \\(j\\), debemos obtener \\(\\theta^{(j)} \\in \\mathbb{r}^{n+1}\\), de tal manera que para predecir la valoración de \\(x^{(i)} \\rightarrow h_\\theta(x^{(i)}) = (\\theta^{(j)})^t x^{(i)}\\) función de coste sea \\(m^{(j)}\\) el número de ítems valorados por el usuario \\(j\\), entonces la función de coste se define como: \\begin{align} j(\\theta) = \\frac{1}{2m^{(j)}}\\sum_{j=1}^{n_\\mu} \\sum_{i; r(i, j) = 1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)})^2 + \\frac{\\lambda}{2m^{(j)}} \\sum_{k=1}^{n_\\mu} \\theta^{(j)}_k \\end{align} donde \\(\\theta = \\{\\theta_1, \\cdots, \\theta_{n_\\mu}\\}\\) es decir queremos minimizar la \"distancia\" entre lo predicho \\((\\theta^{(j)})^t x^{(i)}\\) para el ítem (que ha sido valorado, por lo tanto \\(r(i, j) = 1\\)) y el usuario \\(i\\) y la valoración real \\(y^{(i, j)}\\). descenso gradiente lo que queremos es minimizar el coste, por lo tanto, calculamos \\(\\frac{\\delta j(\\theta)}{\\delta \\theta_j}\\) para obtener el vector en dirección al mayor incremento en la función, seguidamente, utilizar su opuesto, obtenemos el vector que apunta a la dirección de menor incremento. es decir, aplicamos descenso gradiente como sigue: para \\(k = 0\\): \\begin{align} \\theta_k^{(j)} = \\theta_k^{(j)} - \\alpha \\left(\\sum_{i; r(i,j)=1} (\\theta^{(j)})^tx^{(i)} - y^{(i,j)}x^{(i)}_k \\right) \\end{align} para \\(k \\neq 0\\): \\begin{align} \\theta_k^{(j)} = \\theta_k^{(j)} - \\alpha \\left(\\sum_{i; r(i,j)=1} (\\theta^{(j)})^tx^{(i)} - y^{(i,j)}x^{(i)}_k + \\lambda \\theta_k^{(j)} \\right) \\end{align} collaborative filtering collaborative filtering consiste en calcular las características de cada usuario (ejemplo \\(x^{(i)}\\)) en función de los pesos \\(\\theta^{(j)}\\). una vez hecho esto se calculan los pesos óptimos que que minimizan la función de coste y volvemos a obtener las características de cada usuario en función de estes nuevos pesos. este proceso se describe más formalmente a continuación: problema de optimización el problema de optimización se describe como sigue: dados \\(\\theta^{(1)}, \\cdots, \\theta^{n_\\mu}\\): para un ejemplo \\(x^{(i)}\\) \\begin{align} \\underset{x^{(i)}}{\\min{}} \\frac{1}{2} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{k=1}^n (x_k^{(i)})^2 \\end{align} para todos los ejemplos del conjunto \\(x^{0}, \\cdots, x^{(n_m)}\\): \\begin{align} \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\min{}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2 \\end{align} es decir queremos minimizar la \"distancia\" entre lo predicho \\((\\theta^{(j)})^t x^{(i)}\\) para el ítem (que ha sido valorado, por lo tanto \\(r(i, j) = 1\\)) y el usuario \\(i\\) y la valoración real \\(y^{(i, j)}\\). además como queremos obtener los valores de \\(x\\) que minimizan el coste, los añadimos como coste a problema de optimización para evitar overfitting. algoritmo el algoritmo consta de los siguientes pasos: inicializar \\(x^{(1)}, \\cdots, x^{(m)}\\) y \\(\\theta^{(1)}, \\cdots, \\theta^{(n_\\mu)}\\) de forma aleatoria. calcular \\(x\\) a partir de \\(\\theta\\) calcular \\(\\theta\\) a partir de \\(x\\) volvemos al paso 2. es decir, queremos obtener \\(x\\) y \\(\\theta\\) que optimice el siguiente problema: \\begin{align} \\underset{x^{(1)}, \\cdots, x^{(n_m)}, \\theta^{(1)}, \\cdots, \\theta^{(n_\\mu)}}{\\min{}} \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j)=1} ((\\theta^{(j)})^tx^{(i)} - y^{(i,j)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{i=1}^{n_\\mu}\\sum_{k=1}^n (\\theta_k^{(i)})^2 \\end{align} observa que, como estamos optimizando tanto \\(\\theta\\) como \\(x\\), entonces los añadimos como coste a la función de optimización para evitar overfitting: \\(\\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2\\) \\(\\frac{\\lambda}{2} \\sum_{i=1}^{n_\\mu}\\sum_{k=1}^n (\\theta_k^{(i)})^2\\) para aplicar la optimización utilizamos descenso gradiente: primero en función de \\(x\\) y después en función de \\(\\theta\\): \\begin{align} x^{(i)}_k = x^{(i)}_k - \\alpha \\left( \\sum_{j:r(i, j)=1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)}) \\theta_k^{(j)} + \\lambda x^{(i)}_k\\right) \\end{align} \\begin{align} \\theta^{(j)}_k = \\theta^{(j)}_k - \\alpha \\left( \\sum_{i:r(i, j)=1} ((\\theta^{(j)})^t x^{(i)} - y^{(i, j)}) x_k^{(i)} + \\lambda \\theta^{(j)}_k\\right) \\end{align} buscar ítems relacionados si \\(||x^{(i)} - x^{(j)}\\)|| es un valor pequeño entonces los ítems \\(i\\) y \\(j\\) son similares. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Clustering.html",
    "title": "Clustering",
    "body": " index search search back clustering k-means escogemos e inicializamos \\(k\\) centroides que servirán para hacer de clústers. para cada ejemplo \\(j\\): asignamos el centroide más cercano una vez asignados todos los ejemplos, resituamos cada centroide en función de los ejemplos asignados al mismo. volvemos al paso 2. algoritmo entrada: número de clústers \\(k\\) conjunto de entrenamiento: \\(\\{x^{(0)}, \\cdots, x^{(m)}\\}\\), con \\(x^{(i)} \\in \\mathbb{r}^n\\) inicializamos los \\(k\\) centroides \\((\\mu_1, \\cdots, \\mu_k) \\in \\mathbb{r}^n\\) de forma aleatoria. repetimos: para cada ejemplo \\(x^{(j)}\\), \\(c^{(j)}\\) es el índice del centroide más cercano a \\(x^{(j)}\\): \\(\\underset{i}{min}||x^{(j)} - \\mu_i||\\) para cada clúster: \\(\\mu_i\\) es la media de los puntos \\(x^{(j)}\\) asignados al centroide \\(i\\): \\(\\mu_i = \\frac{1}{t} \\left[\\sum_{j=1}^t x^{(j)} \\text{ donde } c^{(j)} = i\\right]\\), donde \\(t\\) es el número de ejemplos asignados al centroide \\(i\\). si el centroide no tiene puntos, se elimina o se vuelve a inicializar de forma aleatoria. clústers no claramente separables cuando los datos contienen mucho ruido lo que se hace es resolver el siguiente problema de optimización: \\begin{align} \\underset{c^{(1)}, \\cdots, c^{(m)}, \\mu_1, \\cdots, \\mu_k}{min} j(c^{(1)}, \\cdots, c^{(m)}, \\mu_1, \\cdots, \\mu_k) \\end{align} donde la función de coste \\(j\\) se define como: \\begin{align} j(c^{(i)}, \\mu_i) = \\frac{1}{m} \\sum_{i=1}^m ||x^{(i)} - \\mu_{c^{(i)}}||^2 \\end{align} es decir, el coste es equivalente a la suma de la distancia entre el ejemplo \\(x^{(i)}\\) y su clúster asignado \\(\\mu_{c^{(i)}}\\), para cada ejemplo. el algoritmo de optimización lo que hace es: minimiza el coste con respecto a \\(c\\) minimiza el coste con respecto a \\(\\mu\\) inicialización aleatoria debemos escoger un número de centroides \\(k\\) menor que el número de ejemplos \\(m\\). inicializamos cada centroide equivalente a un ejemplo aleatorio del conjunto de entrenamiento: \\(\\mu_i = x^{(j)}\\) hay que tener en cuenta que, en función de la inicialización de los centroides, se pueden obtener distintos resultados en el problema de optimización, por ello lo que se hace es: aplicar el algoritmo muchas veces escoger el modelo que obtuvo menor coste este proceso es viable si el número de clústers es pequeño. parametrización de clústering una forma de escoger el número de clústers \\(k\\) es utilizando el método del codo: se aplica el modelo con un número distinto de clústers se evalúa con alguna métrica el rendimiento (coste) del modelo y se elige el ofrece una mayor mejora con respecto a un número de clústers menor $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Regresión Logística.html",
    "title": "Regresión Logística",
    "body": " index search search back regresión logística descripción de los datos hipótesis funcion de coste descenso gradiente descripción de los datos \\(x = (x_{ij})\\) una matriz \\(n \\times m\\) donde cada \\(x_{ij}\\) es la característica \\(i\\) del ejemplo \\(j\\), tal que \\begin{align} x = \\begin{bmatrix} x_{11} & \\cdots & x_{1m} \\\\ \\cdots & \\ddots & \\cdots \\\\ x_{n1} & \\cdots & x_{nm} \\\\ \\end{bmatrix} \\end{align} cada columna es un ejemplo en cada fila están los valores de una característica \\(\\theta = (\\theta_i)\\) es un vector fila \\(1\\times n\\) donde cada \\(\\theta_i\\) es el peso de la característica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_1 & \\cdots & \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector fila \\(1\\times m\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 & \\cdots & y_m\\end{bmatrix} \\end{align} donde cada salida \\(y_j\\), para un clasificador de dos clases sólo puede tener los valores \\(0\\) o \\(1\\). hipótesis para un valor \\(z\\), la función sigmoide \\(g\\) se define como: \\begin{align} g(z) = \\frac{e^z}{(1+e^z)} = \\frac{1}{(1 + e^{-z})} \\end{align} sea \\(g\\) la función sigmoide. dado un ejemplo, es decir un vector columna \\(x\\), de dimensiones \\(n \\times 1\\), la hipótesis se define como: \\begin{align} h_\\theta(x) = g\\left(\\sum_{i=1}^n \\theta_i \\cdot x_i\\right) = \\begin{cases} 0, & h_\\theta(x) < 0.5 \\\\ 1, & h_\\theta(x) \\geq 0.5 \\\\ \\end{cases} \\end{align} dado un conjunto de \\(m\\) datos, es decir matrix \\(x\\), de dimensiones \\(n \\times m\\), la hipótesis se define como: \\begin{align} h_\\theta(x) = \\theta\\cdot x = \\begin{bmatrix}g(\\sum_{i=1}^n \\theta_i \\cdot x_{i1}) & \\cdots & g(\\sum_{i=1}^n \\theta_i \\cdot x_{im})\\end{bmatrix} \\end{align} el resultado es un vector fila \\(1 \\times m\\), es decir como el vector de salidas \\(y\\) funcion de coste se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{m}\\sum_{j=1}^m \\text{coste}(h_\\theta(x_j)) \\end{align} donde \\(\\text{coste}\\) es una función definida como sigue: \\begin{align} \\text{coste}(h_\\theta(x_j)) = [-y_j \\log(h_\\theta(x_j))] - [(1-y_j)\\log(1-h_\\theta(x_j))] \\end{align} regularización con regularización, se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = -\\frac{1}{m}\\sum_{j=1}^m [y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))] + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} descenso gradiente para actualizar el vector de pesos \\(\\theta\\) aplicamos el descenso gradiente. para cada peso \\(\\theta_i\\): \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) \\end{align} la derivada del coste en función del peso \\(\\theta_i\\) se calcula como sigue: sustituimos la función de coste \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\left(-\\frac{1}{m}\\sum_{j=1}^m [y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} sacamos el factor constante \\(\\frac{1}{m}\\) y aplicamos la propiedad \"la derivada de una suma equivale a la suma de las derivadas\", tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\frac{\\delta}{\\delta \\theta_i} \\left([y_j \\log(h_\\theta(x_j))] + [(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{\\delta}{\\delta \\theta_i} [y_j \\log(h_\\theta(x_j))]\\right) + \\left(\\frac{\\delta}{\\delta \\theta_i}[(1-y_j)\\log(1-h_\\theta(x_j))]\\right) = \\end{align} sacamos los factores constantes \\(y_j\\) y \\(1-y_j\\) y aplicamos la regla de la cadena: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(y_j\\frac{\\delta\\log(h_\\theta(x_j))}{\\delta h_\\theta(x_j)} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) + \\left((1-y_j)\\frac{\\delta\\log(1-h_\\theta(x_j))}{\\delta (1- h_\\theta(x_j))}\\frac{\\delta (1- h_\\theta(x_j))}{\\delta \\theta_i}\\right) = \\end{align} tenemos que, para el último término: \\begin{align} \\frac{\\delta (1- h_\\theta(x_j))}{\\delta \\theta_i} = \\frac{\\delta(1)}{\\delta \\theta_i} - \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = - \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} \\end{align} por lo tanto, si sustituimos: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(y_j\\frac{\\delta\\log(h_\\theta(x_j))}{\\delta h_\\theta(x_j)} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) + \\left((1-y_j)\\frac{\\delta\\log(1-h_\\theta(x_j))}{\\delta (1- h_\\theta(x_j))}(-1)\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\right) = \\end{align} aplicamos la regla \\(\\frac{\\delta \\log(x)}{\\delta x} = \\frac{1}{x}\\), sacamos la expresión \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\) como factor común y hacemos negativo el segundo término: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{y_j}{h_\\theta(x_j)} \\right) - \\left(\\frac{(1 - y_j)}{1-h_\\theta(x_j)}\\right) \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\end{align} aplicamos operationes aritméticas: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{(y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j))}{h_\\theta(x_j)(1-h_\\theta(x_j))} \\right)\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\end{align} centrémonos ahora en \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\). para calcular esta derivada, primero expresamos la hipótesis utilizando la función sigmoide: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} g(\\theta x_j) \\end{align} aplicamos la regla de la cadena \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} \\frac{\\delta \\theta x_j}{\\delta \\theta_i} \\end{align} sabemos que la derivada del segundo término \\(\\frac{\\delta \\theta x_j}{\\delta \\theta_i}\\) equivale a \\(x_{ij}\\), por lo tanto, calcularemos sólo \\(\\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j}\\) sea: \\begin{align} g(\\theta x_j) = \\frac{1}{1 + e^{-\\theta x_j}} = (1 + e^{-\\theta x_j})^{-1} \\end{align} aplicamos la regla de la cadena \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = \\frac{\\delta(1 + e^{-\\theta x_j})^{-1}}{\\delta(1+e^{-\\theta x_j})}\\frac{\\delta(1+e^{-\\theta x_j})}{\\delta \\theta x_j} \\end{align} resolvemos la primera derivada aplicando las propiedades de las derivadas sobre los polinomios y volvemos a aplicar la propiedad de que la derivada de una suma equivale a la suma de las derivadas en el segundo término: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2}\\left[\\frac{\\delta (1)}{\\delta \\theta x_j} + \\frac{\\delta e^{-\\theta x_j}}{\\delta \\theta x_j} \\right] \\end{align} como \\(\\frac{\\delta (1)}{\\delta \\theta x_j}\\) equivale a cero: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2} \\frac{\\delta e^{-\\theta x_j}}{\\delta \\theta x_j} \\end{align} resolvemos la última derivada, sabiendo que \\(\\frac{\\delta e^x}{\\delta x} = e^x\\) \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = (-1)(1 + e^{-\\theta x_j})^{-2} (-1) e^{-\\theta x_j} = (1 + e^{-\\theta x_j})^{-2} e^{-\\theta x_j} = \\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} \\end{align} como \\(\\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} = \\left(\\frac{1}{1+e^{-\\theta x_j}}\\right)\\left(1 - \\frac{1}{1 + e^{-\\theta x_j}}\\right)\\), entonces: \\begin{align} \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} = \\frac{e^{-\\theta x_j}}{(1 + e^{-\\theta x_j})^2} = \\left(\\frac{1}{1+e^{-\\theta x_j}}\\right)\\left(1 - \\frac{1}{1 + e^{-\\theta x_j}}\\right) = h_\\theta(x_j) (1- h_\\theta(x_j)) \\end{align} ya que según la definición de la hipótesis \\(h_\\theta(x_j) = \\frac{1}{1 + e^{-\\theta x_j}}\\) por lo tanto, juntado los resultados, tenemos que: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta g(\\theta x_j)}{\\delta \\theta x_j} \\frac{\\delta \\theta x_j}{\\delta \\theta_i} = h_\\theta(x_j) (1- h_\\theta(x_j)) x_{ij} \\end{align} volvemos, entonces, a la derivada de la función de coste y sustituimos \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i}\\) \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left(\\frac{(y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j))}{h_\\theta(x_j)(1-h_\\theta(x_j))} \\right) h_\\theta(x_j) (1- h_\\theta(x_j)) x_{ij} = \\end{align} los términos \\(h_\\theta(x_j) (1- h_\\theta(x_j))\\) se cancelan tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m \\left((y_j)(1-h_\\theta(x_j)) - (1-y_j)(h_\\theta(x_j)) \\right) x_{ij} = \\end{align} aplicamos operaciones aritméticas: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m (y_j - y_jh_\\theta(x_j) - h_\\theta(x_j) + y_jh_\\theta(x_j)) x_{ij} \\end{align} el término \\(y_jh_\\theta(x_j)\\) se cancela, tal que: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = -\\frac{1}{m}\\sum_{j=1}^m (y_j - h_\\theta(x_j)) x_{ij} \\end{align} finalmente movemos el \\((-1)\\) dentro del sumatorio: \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{1}{m}\\sum_{j=1}^m (h_\\theta(x_j)-y_j) x_{ij} \\end{align} por lo tanto la función del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\right] \\end{align} observa que tiene la misma forma que para la regresión lineal, pero la hipótesis para la regresión logística está definida en términos de la función sigmoide regularización con regularización debemos derivar la función de coste que incluye el parámetro de regularización: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) + \\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2 \\end{align} el primer término ya lo hemos derivado, por lo tanto procedemos a derivar el segundo término: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} \\left(\\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2\\right) \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2 = \\begin{cases} 2 \\theta_k, & k = i \\\\ 0, & k \\neq i \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sólo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} 2\\theta_i = \\frac{\\lambda}{m} \\theta_i \\end{align} por lo tanto la función del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) + \\frac{\\lambda}{m}\\theta_i\\right] \\end{align} al igual que antes, observa que tiene la misma forma que para la regresión lineal, pero la hipótesis para la regresión logística está definida en términos de la función sigmoide $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/ExpectationMaximization/Expectation-Maximization Algorithms.html",
    "title": "Expectation Maximization",
    "body": " index search search back expectation maximization introduction gaussian mixture models em algorithm with gmm's e-step m-step optimal parameters of a gmm iterative process recap: anomaly detection generalized em algorithm jensen's inequality motivation e-step m-step putting everything together derive em for gmm e-step m-step introduction this technique is employed in density estimation problems and anomaly detection. such problems aim to represent data in a compact form using a statistical distribution, e.g., gaussian, beta, or gamma. you can think of those problems as a clustering task but from a probabilistic point of view. this is what makes the em algorithm a probabilistic generative model. thus, if we are given \\(n\\) samples, we model them with \\(p(x)\\), such that if \\(p(x) < \\epsilon\\), where \\(\\epsilon\\) is some threshold, then we detect an anomaly. however, you may expect that a single gaussian with its mean and variance cannot map thousands of instances in a dataset into a set of \\(k\\) clusters accurately, so we may assume that there are \\(k\\) distributions that describe the data, hence we use mixture models. for example, imagine you have the following dataset: it looks like the data comes from two different gaussian distributions: so to model this data we use a mixture of gaussian models. note that if we knew by which distribution each sample was generated, we would simply use mle, however we do not know this information, therefore we use the expectation maximization algorithm and we introduce the latent variable \\(z\\) in place of the predicted output \\(y\\) we had in supervised learning algorithms. to model the data, first of all, we suppose that there is a latent (hidden/unobserved) random variable \\(z\\), and \\(x^{(i)}, z^{(i)}\\) are distributed (by a joint distribution) like so \\begin{align} p(x^{(i)},z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)}) \\end{align} where \\(z^{(i)} \\sim multinomial(\\phi)\\), that is \\(z^{(i)}\\) is distributed according to a multinomial distribution. this distribution models for each \\(z^{(i)}\\) the probability of it being equal to \\(1, 2, ..., k\\), where \\(k\\) is the number of clusters. this will denote the probability of a point \\(x^{(i)}\\) being drawn from each of the distributions. and \\(p(x^{(i)}|z^{(i)}=j)\\) is the probability of \\(x^{(i)}\\) being generated by the cluster \\(j\\). where \\(x^{(i)}|z^{(i)} = j\\) is drawn from a normal distribution \\(\\mathcal{n}(\\mu_j, \\sigma_j)\\). gaussian mixture models to build a density estimator model, we cannot rely on a simple distribution. mixture models try to tackle this limitation by combining a set of distributions to create a convex space where we can search for the optimal parameters for such distributions using maximum likelihood estimation (mle). a mixture model is expressed by the following equations: \\begin{align} p(x^{(i)}) = \\sum_{j=1}^k \\phi^{(i)}_j p_j(x^{(i)}) \\tag{1} \\end{align} \\begin{align} 0 \\leq \\phi^{(i)}_j \\leq 1, \\sum_{j=1}^k \\phi^{(i)}_j = 1 \\end{align} where \\(k\\) is the number of mixture components (clusters), \\(\\phi^{(i)}_j\\)'s are the mixture weights, and \\(p_j(x^{(i)})\\)'s are members of a family of distributions (gaussian, poisson, bernoulli, etc). so for each example \\(x^{(i)}\\) and for each distribution \\(j\\), each weight \\(\\phi^{(i)}_j\\) is between 0 and 1, and the sum over \\(k\\) of the weights \\(\\phi_j^{(i)}\\) for every example \\(x^{(i)}\\) equals one. consequently, a gmm is a mixture model where the \\(p_j(x^{(i)})\\) is a finite combination of gaussian distributions. therefore, a gmm can be precisely defined by the following set of equations: \\begin{align} p(x^{(i)};\\theta) = \\sum_{j=1}^k \\phi^{(i)}_j \\mathcal{n}(x^{(i)};\\mu_j,\\,\\sigma_j) \\end{align} \\begin{align} 0 \\leq \\phi^{(i)}_j \\leq 1, \\sum_{j=1}^k \\phi^{(i)}_j = 1 \\end{align} where \\(\\theta\\) is the collection of all the parameters of the model (mixture weights, means, and covariance matrices): \\begin{align} \\theta = \\{\\phi_1, \\cdots, \\phi_k, \\mu_1, \\cdots, \\mu_k, \\sigma_1, \\cdots, \\sigma_k\\} \\end{align} for example, the following plot shows what a gmm derived from 3 mixture components looks like: as a consequence, for each data point, \\(x^{(i)}\\) (in red), we can compute the probability that it belongs to each component (\\(p(x^{(i)}|z^{(i)} = j)\\), where \\(j = 1, 2, 3\\))(make a “soft” assignment). this quantity is called “responsibility”. em algorithm with gmm's the expectation maximization algorithm is comprised of two steps: guess the value of the responsibilities \\(w^{(i)}_j\\), that represent the \"amount\" of each \\(x^{(i)}\\) that was generated from the distribution \\(j\\) (or the probability that the \\(j\\)th distribution generated the point \\(x^{(i)}\\)). compute the values of the parameters of the distributions: \\(\\theta = \\{\\phi, \\mu, \\sigma\\}\\) according to the \\(mle\\) (maximum likelihood estimation) with respect to the parameters. thus, we want to maximize \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\). e-step in this step, as we have said, we will compute the value of the responsibilities with the given parameters \\(\\phi, \\mu, \\sigma\\). so for each example \\(i\\) and each component (distribution) \\(j\\), the amount of \\(x^{(i)}\\) that is generated by the component \\(j\\) is given by: \\begin{align} w^{(i)}_j = p(z^{(i)} = j | x^{(i)}; \\phi_j, \\mu_j, \\sigma_j) \\end{align} by bayes' rule, we can rewrite the equation as follows: \\begin{align} w^{(i)}_j = \\frac{p(x^{(i)}|z^{(i)} = j)p(z^{(i)} = j)}{\\sum_{l=1}^k \\left[p(x^{(i)}|z^{(i)} = l)p(z^{(i)} = l)\\right]} \\end{align} note that the likelihood \\(p(x^{(i)}|z^{(i)} = j)\\) and each likelihood \\(p(x^{(i)}|z^{(i)} = l)\\) come from a gaussian distribution, therefore: \\begin{align} p(x^{(i)}|z^{(i)} = j) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\sigma_j|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}(x^{(i)} - \\mu_j)^t \\sigma_j^{-1} (x^{(i)} - \\mu_j)\\right) \\tag{2} \\end{align} to simplify notation we will denote \\(p(x^{(i)}|z^{(i)} = j)\\) as \\(\\mathcal{n}(\\mu_j, \\sigma_j)\\). on the other hand, the prior \\(p(z^{(i)} = j)\\) comes from a multinomial distribution, hence: \\begin{align} p(z^{(i)} = j) = \\phi_j \\tag{3} \\end{align} combining all the expressions: \\begin{align} w^{(i)}_j = \\frac{\\phi_j\\mathcal{n}(\\mu_j, \\sigma_j)}{\\sum_{l=1}^k \\left[\\phi_l\\mathcal{n}(\\mu_l, \\sigma_l)\\right]} \\tag{4} \\end{align} all that is left to do is plug all of the values into each equation \\((2)\\) and \\((3)\\) (this values are known, given the equations are written in terms of the distributions' parameters) and compute each \\(w^{(i)}_j\\) given \\((4)\\). m-step in this step what we do is maximize the log likelihood of the distributions' parameters \\(\\theta\\), that is we maximize \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\). but first, let us see how do we maximize the parameters in gmm. optimal parameters of a gmm we are going to show how to maximize the log likelihood of the parameters of a gaussian mixture model. the goal of the gmm is to represent the distribution of the data as accurately as possible using a linear combination of gaussian distributions. given a dataset \\(x\\) of \\(m\\) data points, we assume they are i.i.d (independent and identically distributed), therefore the maximum likelihood estimator over \\(x\\) can be expressed as the product of the individual likelihoods. to simplify the equations, we are going to directly apply the logarithm to the mle function: \\begin{align} \\log \\mathcal{l}(x|\\theta) = \\log p(x|\\theta) = \\log \\prod_{i=1}^m p(x^{(i)}|\\theta) = \\sum_{i=1}^m \\log p(x^{(i)}|\\theta) \\end{align} by \\((1)\\) we know that \\(p(x^{(i)}|\\theta)\\) is a linear combination of gaussian distributions, therefore: \\begin{align} \\log \\mathcal{l}(x|\\theta) = \\sum_{i=1}^n \\log \\sum_{j=1}^k \\phi_j^{(i)}\\mathcal{n}(x^{(i)}|\\mu_j, \\sigma_j) \\end{align} this equation is not tractable, so we won't get an analytical solution by just taking the its derivative with respect to \\(\\theta\\) and setting it to 0. the following set of equations outline how we would evaluate it: \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\mu_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\mu_j} = 0^t \\end{align} \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\sigma_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\sigma_j} = 0 \\end{align} \\begin{align} \\frac{\\delta \\mathcal{l}}{\\delta \\phi_j} \\sum_{i=1}^m \\frac{\\delta \\log p(x^{(i)}|\\theta)}{\\delta \\phi_j} = 0 \\end{align} observe that the computation of each parameter from \\(\\theta (\\mu, \\sigma, \\phi)\\) depends on the other parameters in a complex way. to solve those equations, we can use the strategy of optimizing some parameters while keeping the others fixed. going back to the expectation maximization algorithm, there is a way of updating the individual parameters of a gmm given prior (initialized at random) parameters \\(\\mu, \\sigma, \\phi\\). this approach works by updating some parameters while keeping the others fixed. so, by solving the derivatives presented above we derive the three following updating rules: \\begin{align} \\hat{\\mu}_j = \\frac{\\sum_{i=1}^m w^{(i)}_jx^{(i)}}{\\sum_{l=1}^m w^{(l)}_j} \\end{align} \\begin{align} \\hat{\\sigma}_j = \\frac{\\sum_{i=1}^m w^{(i)}_j (x^{(i)} - \\hat{\\mu}_j)(x^{(i)} - \\hat{\\mu}_j)^t}{\\sum_{l=1}^m w^{(l)}_j} \\end{align} \\begin{align} \\hat{\\phi}_j = \\frac{1}{m} \\sum_{i=1}^m w^{(i)}_j \\end{align} to simplify a bit the notation, if \\(n_j = \\sum_{l=1}^m w^{(i)}_l\\): \\begin{align} \\hat{\\mu}_j = \\frac{1}{n_j} \\sum_{i=1}^m w^{(i)}_jx^{(i)} \\end{align} \\begin{align} \\hat{\\sigma}_j = \\frac{1}{n_j}\\sum_{i=1}^m w^{(i)}_j (x^{(i)} - \\hat{\\mu}_j)(x^{(i)} - \\hat{\\mu}_j)^t \\end{align} \\begin{align} \\hat{\\phi}_j = \\frac{n_j}{m} \\end{align} note that the update of \\(\\mu, \\sigma, \\phi\\), all depend on the responsibilities (\\(w^{(i)}_j\\)), which by its turn, depends on \\(\\mu, \\sigma, \\phi\\). that’s why there's not a closed-form solution to equations. furthermore these equations do not aim to precisely maximize over \\(\\theta\\) the actual log likelihood. instead they maximize a proxy function of the log-likelihood over \\(\\theta\\), namely, the expected log-likelihood, which can be derived from the log-likelihood using jensen's inequality as follows: \\begin{align} \\hat{\\mathcal{l}}(x|\\theta) = \\sum_{i=1}^m\\sum_{j=1}^k w^{(i)}_j \\log \\left( \\frac{\\phi_j \\mathcal{n}(x^{(i)} | \\mu_j, \\sigma_j)}{w^{(i)}_j} \\right) \\tag{5} \\end{align} iterative process the process consists of an iterative process that alternates between two steps. the first step is to compute the responsibilities (e step) of each mixture component for each data point using the current parameters (\\(\\mu, \\sigma, \\phi\\)). the second step consists of updating the parameters (m step) in order to maximize the expected log-likelihood given by \\((5)\\) the e and m steps are repeated until there is no significant progress in the proxy function of the log-likelihood computed after the m step. recap: anomaly detection thus, when the parameters \\(\\theta\\) are optimized, we can compute \\(p(x) = \\sum_{j=1}^k p(x|z = j)\\) and if \\(p(x) < \\epsilon\\) you can flag \\(x\\) as an anomaly. generalized em algorithm jensen's inequality convex function we are going to show what jensen's inequality is about. so: let \\(f\\) be a convex function (e.g. \\(f''(x) > 0\\)) and let \\(x\\) be a random variable, then \\begin{align} f(e[x]) \\leq e[f(x)] \\end{align} where \\(e\\) is the expected value. further, if \\(f''(x) > 0\\) (we say f is strictly convex, that is f is not a straight line), then: \\begin{align} e[f(x)] = f(e[x]) \\leftrightarrow \\text{ x is a constant, more formally } x = e[x] \\text{ with probability 1} \\end{align} concave function we are going to apply the same arguments with a concave function. note that a concave function equals the negative of a convex function, thus: let \\(f\\) be a concave function (e.g. \\(f''(x) < 0\\)) and let \\(x\\) be a random variable, then \\begin{align} f(e[x]) \\geq e[f(x)] \\end{align} where \\(e\\) is the expected value. further, if \\(f''(x) < 0\\) (we say f is strictly concave), then: \\begin{align} e[f(x)] = f(e[x]) \\leftrightarrow \\text{ x is a constant, more formally } x = e[x] \\text{ with probability 1} \\end{align} some intuition given any convex function (the inverse also applies to concave functions), if we draw a chord between any two points, its middle point (that is the expected value of the function or \\(e[f(x)]\\)) is always above that the value of the expected value under the function (that is \\(f(e[x])\\)). graphically: motivation given a model for \\(p(x, z , \\theta)\\) where \\(\\theta\\) are the parameters of the model. we only observe \\(x = \\{x^{(1)}, \\cdots, x^{(m)}\\}\\). the goal is to obtain by the maximum likelihood estimation the value of \\(\\theta\\) that maximizes the log likelihood, defined as: \\begin{align} \\theta = \\underset{\\theta}{\\arg \\max{l(\\theta)} } = \\sum_{i=1}^m \\log (p(x^{(i)}; \\theta)) \\end{align} if we marginalize \\(z^{(i)}\\): \\begin{align} \\theta = \\underset{\\theta}{\\arg \\max{l(\\theta)} } = \\sum_{i=1}^m \\log \\sum_{z^{(i)}} (p(x^{(i)}, z^{(i)}; \\theta)) \\end{align} e-step in the e-step we construct a lower bound from a given theta: so, let's say \\(l(\\theta)\\) is the log likelihood. on the first iteration, the graph would be as follows: and on the second iteration: we iterate until there are no significant changes in the lower bound, that is the algorithm converges to a local optimum (it should be noted the optimum is local not absolute, and it depends on the initialization of the distributions' parameters). m-step now, in the m-step we maximize the log likelihood as follows: \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log p(x^{(i)}; \\theta) \\end{align} by marginalizing \\(z^{(i)}\\): \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\theta) \\end{align} we now introduce a probability distribution over \\(z^{(i)}\\) (thus \\(\\sum_{z^{(i)}}q(z^{(i)}) = 1\\)), and we multiply by \\(\\frac{q(z^{(i)})}{q(z^{(i)})}\\): \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log \\sum_{z^{(i)}} q(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\end{align} now, by the definition of expected value (given a sequence of real values \\(a_1, \\cdots, a_n\\) with probabilities \\(p_1, \\cdots, p_n\\), the expected value is defined as: \\(e = \\sum_{i=1}^n p_i a_i\\)), if \\(p_i = q(z^{(i)})\\) and \\(a_i = \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\) \\begin{align} \\underset{\\theta}{\\max{ }} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\end{align} if we apply the concave version of jensen's inequality we obtain a lower bound of the form: \\begin{align} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\geq \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\right] \\end{align} if \\(\\log (x) = f(x)\\), then this equation can be mapped to the inequality: \\begin{align} f(e[x]) \\geq e[f(x)] \\end{align} note that \\(log\\) is a concave function. if we \"unpack\" the expected value: \\begin{align} \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})} \\right] = \\sum_{i=1}^m \\sum_{z^{(i)}} \\log q(z^{(i)}) \\left[\\frac{p(x^{(i)}, z^{(i)}; \\theta)}{q(z^{(i)})}\\right] \\end{align} make log likelihood and lower bound equal on theta for each \\(\\theta\\) on the e-step you wan the value of \\(\\theta\\) under the lower bound function to be equal to \\(l(\\theta)\\), which is what guarantees that when you optimize the lower bound you optimize \\(l(\\theta)\\). so, for a given iteration the current value of \\(\\theta\\), denoted by \\(\\hat{\\theta}\\), we want: \\begin{align} \\sum_{i=1}^m \\log e_{z^{(i)}\\sim q} \\left[\\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})}\\right] = \\sum_{i=1}^m e_{z^{(i)} \\sim q} \\left[\\log \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})} \\right] \\end{align} remember, by the extension on jensen's inequality we know that \\(e[f(x)] = f(e[x])\\) if and only if \\(x\\) is a constant. in this case \\begin{align} x = \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{q(z^{(i)})} = constant \\end{align} for this to hold, we need \\(q(z^{(i)})\\) to be directly proportional to \\(p(x^{(i)}, z^{(i)}; \\hat{\\theta})\\) (so when one is bigger the other is bigger and vice versa, so the ratio between the two remains constant). so: \\begin{align} q(z^{(i)}) \\propto p(x^{(i)}, z^{(i)}; \\hat{\\theta}) \\end{align} because \\(\\sum_{z^{(i)}}q(z^{(i)}) = 1\\), a way to ensure this is to set each \\(q^{(i)} = p(x^{(i)}, z^{(i)}; \\hat{\\theta})\\) and then normalize it to make sure the sum of \\(q\\) over \\(z^{(i)}\\) equals one. hence: \\begin{align} q(z^{(i)}) = \\frac{p(x^{(i)}, z^{(i)}; \\hat{\\theta})}{\\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\hat{\\theta})} \\end{align} it turns out you can further derive this equation to be: \\begin{align} q(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\hat{\\theta}) \\end{align} putting everything together so, after everything we have seen, we can summarize the em generalized algorithm as follows: if \\(\\theta\\) is the value of the parameters in the current iteration: e-step: set \\begin{align} q_i(z^{(i)}) = p(z^{(i)}|x^{(i)}; \\theta) \\end{align} m-step: set \\begin{align} \\theta := \\underset{\\theta}{\\arg \\max} \\sum_{i=1}^m \\sum_{z^{(i)}} q_i(z^{(i)}) \\log \\left[\\frac{p(x^{(i)}, z^{(i)};\\theta)}{q_i(z^{(i)})}\\right] \\end{align} derive em for gmm from the generalized algorithm given a model described by: \\(p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)})\\) where \\(z^{(i)} \\sim multinomial(\\phi)\\) (which means \\(p(z^{(i)} = j) = \\phi_j\\)) and \\(x^{(i)} | z^{(i)} \\sim \\mathcal{n}(\\mu_j, \\sigma_j)\\) e-step on the e-step we compute: \\(q_i(z^{(i)}) p(z^{(i)} = j | x^{(i)}; \\phi, \\mu, \\sigma)\\) if we look at e-step from gmm's we can see that the expression above equals \\(w^{(i)}_j\\). m-step now on the m-step what we do is maximize the lower bound we have constructed in the e-step. for that we need to compute the value of the parameters \\(\\phi, \\mu, \\sigma\\) that maximize this function, that is: \\begin{align} \\underset{\\phi, \\mu, \\sigma}{\\max} \\sum_{i=1}^m \\sum_{z^{(i)}} q_i(z^{(i)}) \\log \\left( \\frac{p(x^{(i)}, z^{(i)}; \\phi, \\mu, \\sigma)}{q_i(z^{(i)})} \\right) = \\end{align} as we know \\(q_i(z^{(i)}) = w^{(i)}_j\\) and \\(p(x^{(i)}, z^{(i)}) = p(x^{(i)}|z^{(i)}) p(z^{(i)})\\), thus: \\begin{align} = \\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{p(x^{(i)}|z^{(i)} = j, \\mu_j, \\sigma_j) p(z^{(i)} = j)}{w^{(i)}_j} \\right) \\end{align} we also know that \\(p(z^{(i)} = j) = \\phi_j\\) and \\(x^{(i)} | z^{(i)} \\sim \\mathcal{n}(\\mu_j, \\sigma_j)\\), therefore: \\begin{align} = \\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{\\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) \\phi_j}{w^{(i)}_j} \\right) \\end{align} where: \\begin{align} \\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) = \\frac{1}{(2\\pi)^{1/2}|\\sigma_j|^{1/2}} \\exp \\left( -\\frac{1}{2}(x^{(i)} - \\mu_j)^t \\sigma_j^{-1}(x^{(i)} - \\mu_j)\\right) \\end{align} from now on we denote \\(\\sum_{i=1}^m \\sum_{j}^k w^{(i)}_j \\log \\left( \\frac{\\mathcal{n}(x^{(i)}; \\mu_j, \\sigma_j) \\phi_j}{w^{(i)}_j} \\right)\\) as \\(\\mathcal{l}(\\phi, \\mu, \\sigma)\\): to maximize this formula over \\(\\phi, \\mu\\) and \\(\\sigma\\) you have to compute the derivatie of the function with respect to each parameter, such that: \\(\\delta_{\\mu_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\), then: \\(\\mu_j = \\sum_{i}^m \\frac{w^{(i)}_j x^{(i)}_j}{w^{(i)}_j}\\) (same as in m-step in gmm's) \\(\\delta_{\\sigma_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\) and \\(\\delta_{\\phi_j} (\\mathcal{l}(\\phi, \\mu, \\sigma)) = 0\\) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Regresión Lineal.html",
    "title": "Regresión Lineal",
    "body": " index search search back regresión lineal descripción de los datos hipótesis funcion de coste descenso gradiente descripción de los datos \\(x = (x_{ij})\\) una matriz \\(n \\times m\\) donde cada \\(x_{ij}\\) es la característica \\(i\\) del ejemplo \\(j\\), tal que \\begin{align} x = \\begin{bmatrix} x_{11} & \\cdots & x_{1m} \\\\ \\cdots & \\ddots & \\cdots \\\\ x_{n1} & \\cdots & x_{nm} \\\\ \\end{bmatrix} \\end{align} cada columna es un ejemplo en cada fila están los valores de una característica \\(\\theta = (\\theta_i)\\) es un vector fila \\(1\\times n\\) donde cada \\(\\theta_i\\) es el peso de la característica \\(i\\), tal que: \\begin{align} \\theta = \\begin{bmatrix} \\theta_1 & \\cdots & \\theta_n\\end{bmatrix} \\end{align} \\(y = (y_j)\\) es un vector fila \\(1\\times m\\) donde cada \\(y_j\\) es la salida real para el ejemplo \\(j\\), tal que: \\begin{align} y = \\begin{bmatrix} y_1 & \\cdots & y_m\\end{bmatrix} \\end{align} hipótesis dado un ejemplo, es decir un vector columna \\(x\\), de dimensiones \\(n \\times 1\\), la hipótesis se define como: \\begin{align} h_\\theta(x) = \\sum_{i=1}^n \\theta_i \\cdot x_i \\end{align} dado un conjunto de \\(m\\) datos, es decir matriz \\(x\\), de dimensiones \\(n \\times m\\), la hipótesis se define como: \\begin{align} h_\\theta(x) = \\theta\\cdot x = \\begin{bmatrix}\\sum_{i=1}^n \\theta_i \\cdot x_{i1} & \\cdots & \\sum_{i=1}^n \\theta_i \\cdot x_{im}\\end{bmatrix} \\end{align} el resultado es un vector fila \\(1 \\times m\\), es decir como el vector de salidas \\(y\\) funcion de coste se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 \\end{align} esta función de coste se denomina mínimos cuadrados. regularización con regularización, se define la función de coste, \\(j(\\theta)\\) como: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} descenso gradiente para actualizar el vector de pesos \\(\\theta\\) aplicamos el descenso gradiente. para cada peso \\(\\theta_i\\): \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) \\end{align} la derivada del coste en función del peso \\(\\theta_i\\) se calcula como sigue: sustituimos la función de coste \\begin{align} \\frac{\\delta j(\\theta)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2\\right) = \\end{align} sacamos el factor constante de la derivada \\begin{align} = \\frac{1}{2m} \\frac{\\delta}{\\delta \\theta_i} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2\\right) \\end{align} aplicamos la propiedad que dice que la derivada de una suma equivale a la suma de las derivadas \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m \\frac{\\delta}{\\delta \\theta_i}(h_\\theta(x_j) - y_j)^2\\right) \\end{align} aplicamos la regla de la cadena \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m \\frac{\\delta(h_\\theta(x_j) - y_j)^2}{\\delta (h_\\theta(x_j) - y_j)} \\frac{\\delta (h_\\theta(x_j) - y_j)}{\\delta \\theta_i}\\right) \\end{align} aplicamos aritmética \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\left[\\frac{\\delta (h_\\theta(x_j))}{\\delta \\theta_i} - \\frac{\\delta (y_j)}{\\delta \\theta_i}\\right]\\right) \\end{align} como la derivada de \\(y_i\\) es función de \\(\\theta_i\\) es cero, procedemos a calcular la derivada de \\(h_\\theta(x_j)\\): \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = \\frac{\\delta}{\\delta \\theta_i} \\sum_{k=1}^n \\theta_k x_{kj} = \\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k x_{kj} \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k x_{kj} = \\begin{cases} x_{kj}, & k = i \\\\ 0, & k \\neq i \\\\ \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sólo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = x_{kj} \\end{align} volemos a la derivada del peso, con \\(\\frac{\\delta h_\\theta(x_j)}{\\delta \\theta_i} = x_{kj}\\) y \\(\\frac{\\delta y_j}{\\delta \\theta_i} = 0\\): \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\left[x_{ij} - 0\\right]\\right) \\end{align} \\begin{align} = \\frac{1}{2m} \\left(\\sum_{j=1}^m 2(h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\end{align} sacamos el factor constante 2 como factor común que se elimina con 1/2 \\begin{align} = \\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) \\end{align} finalmente, sustituimos todo en la función del gradiente: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right)\\right] \\end{align} regularización con regularización debemos derivar la función que coste que incluye el parámetro de regularización: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) + \\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2 \\end{align} el primer término ya lo hemos derivado, por lo tanto procedemos a derivar el segundo término: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} \\left(\\sum_{k=1}^n \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2\\right) \\end{align} donde \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\theta_k^2 = \\begin{cases} 2 \\theta_k, & k = i \\\\ 0, & k \\neq i \\end{cases} \\end{align} como para todo \\(k\\), con \\(1 \\leq k \\leq n\\) sólo hay un \\(k\\), tal que \\(k = i\\), entonces: \\begin{align} \\frac{\\delta}{\\delta \\theta_i} \\left(\\frac{\\lambda}{2m} \\sum_{k=1}^n \\theta_k^2\\right) = \\frac{\\lambda}{2m} 2\\theta_i = \\frac{\\lambda}{m} \\theta_i \\end{align} por lo tanto la función del descenso gradiente equivale a: \\begin{align} \\theta_i = \\theta_i - \\alpha \\left( \\frac{\\delta j(\\theta)}{\\delta \\theta_i}\\right) = \\theta_i - \\alpha \\left[\\frac{1}{m} \\left(\\sum_{j=1}^m (h_\\theta(x_j) - y_j) \\cdot x_{ij}\\right) + \\frac{\\lambda}{m}\\theta_i\\right] \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Evaluación de modelos.html",
    "title": "Evaluación de modelos",
    "body": " index search search back evaluación de modelos separación de datos entrenamiento en regresión lineal entrenamiento en regresión logística selección de modelos cross validation proceso de selección diagnóstico: sesgo vs varianza regresión lineal con regularización curva de aprendizaje debugging un algoritmo de aprendizaje medidas de evaluación balance entre precisión y recall separación de datos a la hora de entrenar un modelo, separamos los datos en dos conjuntos: conjunto de entrenamiento: \\(70\\%\\) - \\(80\\%\\) conjunto de test: \\(30\\%\\) - \\(20\\%\\) entrenamiento en regresión lineal el proceso de entrenamiento en la regresión lineal consiste en: entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \\(\\theta\\) minimizando el coste \\(j(\\theta)\\) calcular el coste sobre el conjunto de test \\(j_{test}(\\theta)\\) \\[%align j_{test}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m_{test}} (h_\\theta(x^{(i)}_{test}) - y^{(i)}_{test})^2 \\] entrenamiento en regresión logística el proceso de entrenamiento en la regresión logística consiste en: entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \\(\\theta\\) minimizando el coste \\(j(\\theta)\\) calcular el coste sobre el conjunto de test \\(j_{test}(\\theta)\\) \\[%align j_{test}(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m_{test}} \\left[y^{(i)}_{test} \\log(h_\\theta(x^{(i)}_{test})) + (1-y^{(i)}_{test})\\log(1-h_\\theta(x^{(i)}_{test})) \\right] \\] el error de clasificación en la regresión logística se define como sigue: \\begin{align} error(h_\\theta(x), y) = \\begin{cases} 1, & \\text{ si } h_\\theta(x) \\geq 0.5 \\rightarrow \\log(h_\\theta(x)) = 1 \\text{ e } y = 0 \\\\ 1, & \\text{ si } h_\\theta(x) < 0.5 \\rightarrow \\log(h_\\theta(x)) = 0 \\text{ e } y = 1 \\\\ 0, \\text{ en cualquier otro caso } \\\\ \\end{cases} \\end{align} selección de modelos supongamos que tenemos \\(n\\) modelos, tal que cada modelo es equivalente al anterior pero con una característica más en sus datos: modelo 1: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1\\) modelo 2: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2\\) modelo n: \\(h_\\theta(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + \\cdots + \\theta_n \\cdot x_n\\) para evaluar los modelos lo que hacemos es escoger el que menor coste obtenga sobre el conjunto de test, tras ser entrenado sobre el conjunto de entrenamiento. \\begin{align} \\begin{bmatrix} \\theta^{(1)} \\\\ \\theta^{(2)} \\\\ \\vdots \\\\ \\theta^{(n)} \\\\ \\end{bmatrix} \\rightarrow \\begin{bmatrix} j_{test}(\\theta^{(1)}) \\\\ j_{test}(\\theta^{(2)}) \\\\ \\vdots \\\\ j_{test}(\\theta^{(n)}) \\\\ \\end{bmatrix} \\end{align} sin embargo, se puede dar el problema de que el mejor simplemente produzca overfitting sobre el conjunto de test (lo cual es probable cuando el vector de pesos tiene dimensiones grandes). para solventar este problema: cross validation separaremos el conjunto de datos en tres conjuntos: conjunto de entrenamiento: \\(60\\%\\) conjunto de validación cruzada (cross validation): \\(20\\%\\) conunto de test: \\(20\\%\\) por lo tanto ahora la función de coste para cada conjunto tiene la forma: función de coste para el conjunto de entrenamiento: \\begin{align} j_{train}(\\theta) = \\frac{1}{2m_{train}} \\sum_{i=1}^{m_{train}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} función de coste para el conjunto de test: \\begin{align} j_{test}(\\theta) = \\frac{1}{2m_{test}} \\sum_{i=1}^{m_{test}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} función de coste para el conjunto de validación cruzada: \\begin{align} j_{cv}(\\theta) = \\frac{1}{2m_{cv}} \\sum_{i=1}^{m_{cv}} error(h_\\theta(x^{(i)}), y^{(i)}) \\end{align} proceso de selección entonces ahora para seleccionar un modelo lo que hacemos que para cada modelo \\(q\\): minimizamos \\(j_{train}(\\theta^{(q)})\\) para obtener los pesos \\(\\theta^{(q)}\\) óptimos. calculamos el coste sobre el conjunto de validación cruzada \\(j_{cv}(\\theta^{(q)})\\) una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validación cruzada y calculamos \\(j_{test}(\\theta^{(q)})\\) para evaluar la capacidad de generalización del modelo. diagnóstico: sesgo vs varianza underfitting: cuando se produce underfitting el coste de entrenamiento y el coste de validación tienen valores similares y ambos tiene valores bastante altos overfitting: cuando se produce overfitting el coste de entrenamiento es mucho menor que el coste de validación cruzada. regresión lineal con regularización también es importante observar cómo afecta el parámetro de regularización a nuestros modelos. por ejemplo, en la regresión linear, la función de coste tiene la forma: \\begin{align} j(\\theta) = \\frac{1}{2m}\\sum_{j=1}^m (h_\\theta(x_j) - y_j)^2 + \\frac{1}{2m} \\lambda \\sum_{i=1}^n \\theta_i^2 \\end{align} por lo tanto, el aumentar o reducir \\(\\lambda\\) es directamente proporcional al coste. si el parámetro de regularización \\(\\lambda\\) es muy grande entonces los pesos van a tender a ser muy pequeños (ya que el coste aumenta al aumentar el valor de \\(\\lambda\\)) si el parámetro de regularización \\(\\lambda\\) es muy pequeño entonces los pesos van a poder ser grandes (ya que el coste se reduce al reducir el valor de \\(\\lambda\\)) escoger el parámetro de regularización para escoger el parámetro de regularización seguimos el mismo proceso que para escoger el mejor modelo, para cada modelo \\(q\\): minimizamos \\(j_{train}(\\theta^{(q)})\\) para obtener los pesos \\(\\theta^{(q)}\\) óptimos. calculamos el coste sobre el conjunto de validación cruzada \\(j_{cv}(\\theta^{(q)})\\) una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validación cruzada y calculamos \\(j_{test}(\\theta^{(q)})\\) para evaluar la capacidad de generalización del modelo. curva de aprendizaje a continuación vamos a estudiar cómo afecta el tamaño del conjunto de datos \\(m\\), el sesgo y la varianza a nuestro modelo: cuanto mayor es el tamaño, más difícil es encontrar una hipótesis que se adapte (\\(j_{train}(\\theta)\\) es mayor), pero el modelo generaliza mejor (\\(j_{cv}(\\theta)\\) es menor) cuando el sesgo (bias) es grande, entonces se produce underfitting y las predicciones de nuestro modelo son malas: el error del modelo es elevado, tanto sobre el conjunto de entrenamiento como sobre el conjunto de validación cruzada tener más ejemplos ayuda al modelo cuando la varianza (variance) es grande, entonces se produce overfitting, tal que el error en el conjunto de validación cruzada es muy alto: el modelo se adapta al conjunto de datos, por lo que el error de entrenamiento es menor tener más muestras ayuda al modelo debugging un algoritmo de aprendizaje para arreglar el overfitting que se produce cuando la varianza es elevada: obtener más datos de entrenamiento utilizar menos características (reducir el grado del vector de pesos), pero tras un proceso de selección de aquellas más relevantes intentar aumentar el parámetro de regularización para arregar el underfitting que se produce cuando el sesgo es elevado: añadir más características añadir características polinómicas intentar reducir el parámetro de regularización en las redes neuronales: las redes pequeñas tienden a producir underfitting pero son menos costosas computacionalmente las redes grandes tienen más características, por lo tanto hay una mayor probabilidad de overfitting gestionar datos sesgados: hay que ser consciente que a veces, por ejemplo en problemas de clasificación, hay categorías que con más comunes que el resto medidas de evaluación la precisión y el recall son medidas de evaluación que se complementan:     resultado resultado     1 0 predicción 1 verdadero positivo (tp) falso positivo (fp) predicción 0 falso negativo (fn) verdadero negativo (tn) precisión = \\(\\frac{tp}{\\text{ # positivos predichos }} = \\frac{tp}{tp + fp}\\) recall = \\(\\frac{tp}{\\text{ # positivos reales }} = \\frac{tp}{tp + fn}\\) balance entre precisión y recall cuánto mayor es la precisión menor es el recall y viceversa. entonces si queremos un modelo más preciso: \\begin{align} \\begin{cases} \\text{predecir } 1, \\text{ si } h_\\theta(x) \\geq 0.7 \\\\ \\text{predecir } 0, \\text{ si } h_\\theta(x) < 0.7 \\\\ \\end{cases} \\end{align} entonces, la precisión es mayor ya que el número de \\(fp\\) es menor, pero el recall es menor, ya que el número de \\(fn\\) es mayor. lo mismo pasa si queremos evitar falsos negativos, entonces hacemos: \\begin{align} \\begin{cases} \\text{predecir } 1, \\text{ si } h_\\theta(x) \\geq 0.5 \\\\ \\text{predecir } 0, \\text{ si } h_\\theta(x) < 0.5 \\\\ \\end{cases} \\end{align} tal que se reduce el número de \\(fn\\), y se aumenta el recall, pero el número de \\(fp\\) es mayor, por lo que se reduce la precisión. entonces, para encontrar un punto de balance entre las dos medidas tenemos que seleccionar un valor límite, tal que hacemos la predicción en base a \\(h_\\theta(x) \\geq \\text{ limite }\\). para calibrar ese límite podemos utilizar dos métricas de evaluación: la media de ambas métricas: \\(\\frac{p + r}{2}\\), funciona mal cuando \\(p >> r\\) o \\(r >> p\\), ya que el valor va a ser alto, pero no se ha encontrado un equilibrio. la puntuación \\(f_1 = 2 \\cdot \\frac{p\\cdot r}{(p + r)}\\), tal que cuanto mayor sea esta puntuación mejor ahora \\begin{align} \\begin{cases} f_1 \\approx 0, && p >> r \\\\ f_1 \\approx 0, && r >> p \\\\ \\end{cases} \\end{align} para escoger el límite lo que se hace es calcular la puntuación \\(f_1\\) sobre el conjunto de validación cruzada, y se escoge aquel límite que ofrezca la mayor puntuación. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0001)_MLStanfordCoursera/Map Reduce.html",
    "title": "Map Reduce",
    "body": " index search search back map reduce map reduce nos permite paralelizar los algoritmos. por ejemplo, supongamos que: tenemos \\(m = 400\\) datos utilizamos batch gradient descent para resolver el problema de optimización tenemos un número de pc equivalente a 4 sea \\(i\\) el índice de un pc este entrena sobre \\(x^{(i)}, \\cdots, x^{(i+100)}\\) calculamos el coste parcial para este conjunto de datos como: \\(temp_j^{(k)} = \\sum_{i}^{i+100} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j\\) ahora, combinamos todos los pesos y aplicamos descenso gradiente: \\(\\theta_j = \\theta_j - \\alpha \\frac{1}{400} \\left( \\sum_{i}^k temp_j^{(i)}\\right)\\) este tipo de técnicas se utilizan si los algoritmos de entrenamiento pueden ser utilizamos como la suma de funciones, tanto el coste como el gradiente. también es aplicable a pcs con múltiples cores. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/Models/Motion Model.html",
    "title": "Motion Model",
    "body": " index search search back motion model introduction the motion of the system will always contain uncertainty, because it does not move perfectly with the command it is given. for example, suppose the robot follows the path illustrated in the next image: however the internal estimate of the system yields the following result: showing that it has a tendency to drift to the right. recursive bayes filter as you may recall from bayes filter our belief at time \\(t\\) was defined as follows: \\[ bel(x_t) = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] and we said we used our motion model to estimate the next state of the system \\(\\overline{x_t}\\): \\[ bel(\\overline{x}_t) = \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] what we are doing here is marginilizing \\(x_t\\) so that we know the probability of being at state \\(x_t\\) given previous states \\(x_{t-1}\\) and control command \\(u_t\\). our motion model specifies a posterior probability, that asks what is the probability of the state being \\(x_t\\) given we were at state \\(x_{t-1}\\) and carried the command \\(u_t\\). typical motion models odometry-based models: we use the measurements (odometry) of the robot about how it moved as a command velocity-based models: we simply tell the system to move at a given velocity. reasons for motion errors of wheeled robots some errors that cause wrong movement estimations are the following: for the three different causes there are physical factors that make our robot move differently that what we expect it to for the given control command. odometry motion model suppose a motion takes place, where the initial point is described as \\((\\overline{x}, \\overline{y}, \\overline{\\theta})\\) and the final point is \\((\\overline{x}', \\overline{y}', \\overline{\\theta}')\\) our odometry information is given by \\(u = (\\delta_{rot1}, \\delta_{trans}, \\delta_{rot2})\\), where \\(\\delta_trans\\) is the distance between the two points, \\(\\delta_{rot1}\\) is the rotation on the first point and \\(\\delta_{rot2}\\) is the rotation on the second point. all of them are defined as follows: \\[ \\delta_{trans} = \\sqrt{(\\overline{x}' - \\overline{x})^2 + (\\overline{y}' - \\overline{y})^2} \\] \\[ \\delta_{rot1} = atan2(\\overline{y}' - \\overline{y}, \\overline{x}' - \\overline{x}) - \\overline{\\theta} \\] \\[ \\delta_{rot2} = \\overline{\\theta}' - \\overline{\\theta} - \\delta_{rot1} \\] each part is illustrated in the following image: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/OccpancyGridMaps/Occpancy Grid Maps.html",
    "title": "Occupancy Grid Maps",
    "body": " index search search back occupancy grid maps introduction description of the mapping task grid maps assumptions occupancy probability joint distribution estimating a map from data static state binary bayes filter odds ratio log odds ration algorithm inverse sensor model for laser range finders introduction occupancy grid maps store information about the environment regarding which parts of the map are occupied and which are free. features vs volumetric maps feature map representations store where on the environment certain points or landmarks lay that the systems uses in order to estimate where it is. volumetric maps are most typically used to store free space. description of the mapping task the goal to obtain a map of a given environment is to compute the most likely map given the sensor data: \\[ m^* = \\arg \\max_m p(m|u_1,z_1, \\cdots, u_t,z_t) \\] however, we will simplify this problem by assuming we already know the poses for certain, thus we swap the control commands \\(u_t\\) for poses \\(x_t\\): \\[ m^* = \\arg \\max_m p(m|x_1,z_1, \\cdots, x_t,z_t) \\] grid maps grid maps discretize the environment by dividing it into a finite number of cells, which encode information about its occupation. that is, a cell is either free or occupied. grids are rigid structures, where cells are distributed uniformly along the grid and represent a definite space. generally we describe cells as pixels. assumptions for each cell, the area corresponding to the cell are completely free or occupied. every cell can be described with a binary random variable that models the occupancy: the world is static. the cells are independent of each other. which means: if i know the occupancy state of a given cell, it does not help me estimate the occupancy state of another. occupancy probability as we have said, each cell is a binary random variable that models the occupancy, that is: if we are certain a cell \\(m_i\\) is occupied: \\(p(m_i) = 1\\) if we are certain a cell \\(m_i\\) is free: \\(p(m_i) = 0\\) if we have no knowledge about the cell \\(m_i\\): \\(p(m_i) = 0.5\\) notation the probability of a cell \\(m_i\\) being occupied is expressed as follows: \\[ p(m_i = occ) = p_{occ}(m_i) = p(m_i) \\] the probability of it being free is given by: \\[ p(m_i = free) = p_{free}(m_i) = 1- p_{occ}(m_i) = p(\\neg m_i) \\] also, the shading in the map tells us how certain we are about \\(p(m_i)\\), that is the more intense the shade the higher the probability. joint distribution the map is described by a probability distribution defined as the joint belief of each cell in the map: \\[ p(m) = p(m_1, m_2, \\cdots, m_n) \\] to simplify this distribution we exploit one of the assumptions made before, that said cells were independent of each other, thus: \\[ p(m) = \\prod_i p(m_i) \\] estimating a map from data our goal is to estimate the map given the sensor data \\(z_{1:t}\\) and the poses \\(x_{1:t}\\), that is: \\[ p(m | z_{1:t}, x_{1:t}) = \\prod_i p(m_i|z_{1:t}, x_{1:t}) \\] in order to do this we use a variant of the bayes filter called binary bayes filter that is optimized for binary random variables (\\(m_i\\)). static state binary bayes filter so, for each cell in the environment \\(m_i\\) we compute: \\[ p(m_i|z_{1:t}, x_{1:t}) \\] we apply the bayes rule to swap \\(m_i\\) for \\(z_t\\), therefore: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, z_{1:t-1}, x_{1:t})p(m_i, z_{1:t-1}, x_{1:t})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we apply markov's assumption and assume independence between \\(z_t\\) and the previous observations \\(z_{1:t-1}\\) and poses \\(x_{1:t-1}\\), therefore: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, x_t)p(m_i, z_{1:t-1}, x_{1:t})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we also make use of the markov's assumption to discard future poses when we the most up to date observation is of \\(t-1\\): \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(z_t|m_i, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})}{p(z_t | z_{1:t-1}, x_{1:t})} \\] we apply bayes rule again over \\(p(z_t|m_i, x_t)\\) to swap \\(z_t\\) and \\(m_i\\) again: \\[ p(z_t|m_i, x_t) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t)}{p(m_i|x_t)} \\] we plug this into the previous expression: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i|x_t) p(z_t | z_{1:t-1}, x_{1:t})} \\] we assume that \\(p(mi|x_t) \\approx p(mi)\\), because knowing for certain the current pose tells us nothing about the state of the cell: \\[ p(m_i|z_{1:t}, x_{1:t}) = \\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i) p(z_t | z_{1:t-1}, x_{1:t})} \\] we compute this same derivation for the complement of \\(m_i\\): \\[ p(\\neg m_i|z_{1:t}, x_{1:t}) = \\frac{p(\\neg m_i|z_t, x_t) p(z_t|x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i) p(z_t | z_{1:t-1}, x_{1:t})} \\] what we are going to do is compute the ratio between these two expressions: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{\\frac{p(m_i|z_t, x_t) p(z_t|x_t) p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i) p(z_t | z_{1:t-1}, x_{1:t})}}{\\frac{p(\\neg m_i|z_t, x_t) p(z_t|x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i) p(z_t | z_{1:t-1}, x_{1:t})}} \\] now, all of the terms that do not depend on \\(m_i\\) can be discarded: \\(p(z_t|x_t)\\) and \\(p(z_t | z_{1:t-1}, x_{1:t})\\), then \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{\\frac{p(m_i|z_t, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i)}}{\\frac{p(\\neg m_i|z_t, x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i)}} \\] we reorganize the expression: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)p(m_i, z_{1:t-1}, x_{1:t-1})p(\\neg m_i)}{p(\\neg m_i|z_t, x_t) p(\\neg m_i, z_{1:t-1}, x_{1:t-1})p(m_i)} \\] \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{p(\\neg m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{p(\\neg m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{p(\\neg m_i, z_{1:t-1}, x_{1:t-1})} \\frac{p(\\neg m_i)}{ p(m_i)} \\] we express \\(\\neg m_i\\) in terms of \\(m_i\\): \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{ 1- p(m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{1-p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{1-p(m_i)}{ p(m_i)} \\] note that now we have three expressions: the first one uses the observation \\(z_t\\) the second one is a recursive term the third one describes our prior knowledge about the state of the cell without any other information about the environment. odds ratio what we do now is turn this ratio called odds ratio into the probability as follows: \\[ odds(x) = \\frac{p(x)}{1-p(x)} \\] we multiply by \\(1-p(x)\\) in both sides. \\[ odds(x)(1-p(x)) = p(x) \\] we expand the left hand side expression: \\[ odds(x)-odds(x)p(x) = p(x) \\] we add \\(odds(x)p(x)\\) to both sides: \\[ odds(x) = p(x) + odds(x)p(x) \\] we extract \\(p(x)\\) as a common factor on the right hand side: \\[ odds(x) = p(x) (1 + odds(x)) \\] we divide by \\((1 + odds(x))\\) on both sides: \\[ \\frac{odds(x)}{(1 + odds(x))} = p(x) \\] and finally: \\[ p(x) = \\frac{1}{\\left(1 + \\frac{1}{odds(x)} \\right)} \\] so by using: \\[ p(x) = [1 + odds(x)^{-1}]^{-1} \\] in our update rule: \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + (\\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})})^{-1}\\right]^{-1} \\] \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + \\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})}\\right]^{-1} \\] because: \\[ \\frac{p(m_i|z_{1:t}, x_{1:t})}{1-p(m_i|z_{1:t}, x_{1:t})} = \\frac{p(m_i|z_t, x_t)}{ 1- p(m_i|z_t, x_t)} \\frac{p(m_i, z_{1:t-1}, x_{1:t-1})}{1-p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{1-p(m_i)}{ p(m_i)} \\] the inverse equals: \\[ \\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})} = \\frac{ 1- p(m_i|z_t, x_t)}{p(m_i|z_t, x_t)} \\frac{1-p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{ p(m_i)}{1-p(m_i)} \\] therefore: \\[ p(m_i | z_{1:t}, x_{1:t}) = \\left[1 + \\frac{ 1- p(m_i|z_t, x_t)}{p(m_i|z_t, x_t)} \\frac{1-p(m_i, z_{1:t-1}, x_{1:t-1})}{p(m_i, z_{1:t-1}, x_{1:t-1})} \\frac{ p(m_i)}{1-p(m_i)}\\right]^{-1} \\] which means, we can obtain information about the state of the \\(m_i\\) grid cell given the observation and the positions. log odds notation to make the computation more efficient we are going to take the log of this expression. the notation will be the following: \\[ l(m_i | z_{1:t}, x_{1:t}) = \\log \\left(\\frac{1-p(m_i|z_{1:t}, x_{1:t})}{p(m_i|z_{1:t}, x_{1:t})}\\right) \\] note that we can map from the log space to the probability space and vice versa as follows: \\[ l(x) = \\log \\frac{p(x)}{1-p(x)} \\] \\[ p(x) = 1- \\frac{1}{1 + \\exp(l(x))} \\] given this facts, we can turn the aforementioned product into a sum, because the log of the product of two terms equal the sum of the log of each term. \\[ l(m_i|z_{1:t}, x_{1:t}) = l(m_i|z_t,x_t) + l(m_i|z_{1:t-1}, x_{1:t-1}) - l(m_i) \\] where: \\(l(m_i|z_t,x_t)\\) is the inverse sensor model, which contains information about what we sensed. \\(l(m_i|z_{1:t-1}, x_{1:t-1})\\) is the recursive term, that is the state of cell on the previous iteration. \\(l(m_i)\\) is the prior. in short: \\[ l_{t,i} = inv\\_sensor\\_model(m_i, x_t, z_t) + l_{t-1, i} - l_0 \\] algorithm as we can see in the algorithm what we do is, given an observation \\(z_t\\) we go through each cell, and if the cell is close to the area where the observation took place then we update the state of the cell taking into account the sensor information. else we just propagate the previous state into the current state: inverse sensor model for laser range finders on the following graph we show the way we update the occupancy probability of the cells. here, the x axis represent several cells and the y axis represents the occupancy probability. at cell \\(n\\) our scanner detects an obstacle at time \\(t\\), this corresponds to the observation \\(z_{t,n}\\). therefore: the probability of cells prior to cell \\(n\\) of being occupied is low, because we were able to shot a laser through them without encountering no obstacle the probability of the n-th cell of being occupied is high, because it is the place where we found the obstacle. the probability of cells after \\(n\\) of bain occupied is unknown because we cannot see after the laser. a similar idea could be applied to sonar range sensor, which measure an area instead of a line: however the graph is a bit different, we now take into account that the sensor might not be completely reliable, therefore when the sonar detects an obstacle at a given distance we spread the probability of being occupied over adjacent cells/distances. example the idea is to add sensor information to a current \"map\" to increase the certainty of the state of each cell: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/index.html",
    "title": "Online Training: Mobile Robotics",
    "body": " index search search back online training: mobile robotics source: online training: mobile robotics by cyrill stachniss bayes filter occpancy grid maps motion model observation model kalman filter extended kalman filter particle filter markov decision processes slam graph-based slam graph-based slam with landmarks $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/KalmanFilter/Extended Kalman Filter.html",
    "title": "Extended Kalman Filter",
    "body": " index search search back extended kalman filter introduction local linearization jacobian error under local linearization linearized motion model linearized observation model algorithm kalman gain localization example introduction what happens if we are dealing with non-linear dynamic systems, such that we do not use our linear models anymore: \\[ x_t = a_tx_{t-1} + b_tu_t + \\epsilon_t \\] \\[ z_t = c_t x_t + \\delta_t \\] but we introduce new functions that need not be linear: \\[ x_t = g(u_t, x_{t-1}) + \\epsilon_t \\] \\[ z_t = h(x_t) + \\delta_t \\] before, when we transformed our belief (a gaussian) with a linear transformation, something like the following happened: where the distribution of the upper left is the result of transforming the distribution of the bottom by applying the linear function on the upper right. however, if we try to do this same thing with a non-linear transformation, we could end up with something like this: so, the result of the transformation is clearly no a gaussian. which means, the kalman filter is not applicable anymore. to prevent this problem we are going to use local linearization. local linearization in order to perform local linearization what we do is approximate the non-linear functions \\(g\\) and \\(h\\) by the means of the taylor expansion. thus we re-define our non-linear functions as follows: the linearization for prediction step consists of linearizing around our previous state \\(x_{t-1} = (\\mu_{t-1}, \\sigma_{t-1})\\)and is described as follows: \\[ g(u_t, x_{t-1}) \\approx g(u_t, \\mu_{t-1}) + \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}(x_{t-1} - \\mu_{t-1}) \\] \\(g(u_t, \\mu_{t-1})\\) is the value of our non-linear model at the linearization point \\(\\mu_{t-1}\\), which corresponds to our previous belief. \\(g_t = \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}\\) is the slope of the local linearization at \\(x_{t-1}\\). this is a first partial derivative which constitutes a jacobian. \\((x_{t-1} - \\mu_{t-1})\\) tells us how far we are away from the linearization point \\(\\mu_{t-1}\\). for the correction step we linearize around our predicted state \\(\\overline{x}_t = (\\overline{\\mu}_t, \\overline{\\sigma}_t)\\): \\[ h(x_t) \\approx h(\\overline{\\mu}_t) + \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t} (x_t - \\overline{\\mu}_t) \\] \\(h(\\overline{\\mu}_t)\\) is the value of our non-linear observation model at the linearization point, which now is the predicted belief, that is the best estimate that i have. \\(h_t = \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t}\\) is the jacobian that equals the slope at the linearization point. \\((x_t - \\overline{\\mu}_t)\\) signifies how far away is the variable \\(x_t\\) to our linearization point \\(\\overline{\\mu}_t\\). jacobian given a function \\(f: \\mathbb{r}^n \\rightarrow \\mathbb{r}^m\\), such that given \\(x \\in \\mathbb{r}^n\\), \\(x \\mapsto f(x) \\in \\mathbb{r}^{m}\\). then the jacobian has the following shape: \\[ j = \\begin{bmatrix} \\frac{\\delta f_1}{\\delta x_1} & \\frac{\\delta f_1}{\\delta x_2} & \\cdots & \\frac{\\delta f_1}{\\delta x_n} \\\\ \\vdots & \\cdots & \\cdots & \\vdots \\\\ \\frac{\\delta f_m}{\\delta x_1} & \\frac{\\delta f_m}{\\delta x_2} & \\cdots & \\frac{\\delta f_m}{\\delta x_n} \\end{bmatrix} \\in \\mathbb{r}^{m \\times n} \\] and we can illustrate it graphically: as you can see, for points close to the linearization point, it constitutes a good approximation, but the further we move away the bigger the error is. so, let's revisit the transformation of our gaussian belief. remember we had, the following non-linear transformation: what we do now, is take the mean of our belief \\(\\mu_t\\) and approximate it locally with a linear function by using the taylor expansion as we have explained before. and then we transform our gaussian belief with this linear approximation (represented by the red line) which results in the following transformation: error under local linearization when we perform local linearization the error depends on to factors: the difference between the non-linear function and its linear approximation the uncertainty of our original gaussian distribution. because the larger the uncertainty, more probability mass will fall farther from our linearization point (the mean of that same gaussian distribution), and remember that the further we are from the linearization point the worse the approximation is, and thus the bigger the error is. linearized motion model we defined our linear motion model as follows: \\[ p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) \\] if our world is non-linear we substitute \\[ x_t = a_tx_{t-1} + b_tu_t + \\epsilon_t \\] for \\[ x_t = g(u_t, x_{t-1}) + \\epsilon_t \\] therefore the motion model is expressed as follows: \\[ p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - g(u_t, x_{t-1}))^tr^{-1}_t(x_t - g(u_t, x_{t-1}))) \\] finally we find a linear approximation, such that: \\[ g(u_t, x_{t-1}) \\approx g(u_t, \\mu_{t-1}) + \\frac{\\delta g(u_t, \\mu_{t-1})}{\\delta x_{t-1}}(x_{t-1} - \\mu_{t-1}) = g(u_t, \\mu_{t-1}) + g_t(x_{t-1} - \\mu_{t-1}) \\] and the linearized motion model becomes: \\[ p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t) ^{-\\frac{1}{2}} \\cdot \\] \\[ \\cdot \\exp(-\\frac{1}{2}(x_t - g(u_t, \\mu_{t-1}) - g_t(x_{t-1} - \\mu_{t-1}))^tr^{-1}_t \\cdot \\] \\[ \\cdot (x_t - g(u_t, \\mu_{t-1}) - g_t(x_{t-1} - \\mu_{t-1}))) \\] where \\(r^{-1}_t\\) describes the motion noise. linearized observation model we defined our linear observation model as follows: \\[ p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - c_tx_t)^tq^{-1}_t(z_t - c_tx_t)) \\] if our world is non-linear we substitute \\[ z_t = c_t x_t + \\delta_t \\] for \\[ z_t = h(x_t) + \\delta_t = h(\\overline{\\mu}_t) + \\delta_t \\] note that \\(x_t = \\overline{\\mu}_t\\) here refers to our best estimation up until now, that comes from the prediction step. therefore the observation model is expressed as follows: \\[ p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - h(\\overline{\\mu}_{t}))^tq^{-1}_t(z_t - h(\\overline{\\mu}_{t}))) \\] finally we find a linear approximation, such that: \\[ h(x_t) \\approx h(\\overline{\\mu}_t) + \\frac{\\delta h(\\overline{\\mu}_t)}{\\delta x_t} (x_t - \\overline{\\mu}_t) = h(\\overline{\\mu}_t) + h_t (x_t - \\overline{\\mu}_t) \\] and the linearized observation model becomes: \\[ p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\cdot \\] \\[ \\cdot \\exp(-\\frac{1}{2}(z_t - h(\\overline{\\mu}_t) - h_t (x_t - \\overline{\\mu}_t))^tq^{-1}_t \\cdot \\] \\[ \\cdot (z_t - h(\\overline{\\mu}_t) - h_t (x_t - \\overline{\\mu}_t))) \\] where \\(q^{-1}_t\\) describes the measurement noise. algorithm to take into account the linearized models, we have to make a few changes to the kalman filter algorithm: the first thing that changes is that we use our linearized moition model \\(g(u_t, \\mu_{t-1})\\) to obtain our predicted estate \\(\\overline{x}_t = (\\overline{\\mu}_t, \\overline{\\sigma}_t)\\) we use the jacobian \\(g_t\\) to transform our previous uncertainty \\(\\sigma_{t-1}\\), given the jacobian is the linear transformation that approximates the non-linear transformation we defined originally for our motion model. same thing goes for the correction step. we use the jacobian \\(h_t\\) to apply a linear transformation that allows us to map \\(\\overline{\\sigma}_t\\) from the state space to the observation space, and thus calculate the kalman gain taking into account the measurement noise. then we compute the corrected mean of the estimated state \\(x_t\\) by obtaining the weighted sum of the mean of the predicted state \\(\\overline{\\mu}_t\\) and the correction factor. this correction factor equals the difference between the actual measurement \\(z_t\\) and the mapping of the predicted state to the observation space given by our linearized function \\(h(\\overline{\\mu}_t)\\). this mapping equals the expected measurement given our state is \\(\\overline{\\mu}_t\\). we do the same thing for the uncertainty \\(\\sigma_t\\). kalman gain suppose you have a perfect sensor, that is we trust completely the values given by this sensor and thus we set the measurement noise to be equal to zero (\\(q_t = 0\\)). then, the kalman gain becomes: \\[ k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + q_t)^{-1} \\] \\[ k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + 0)^{-1} \\] \\[ k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t^t)^{-1} \\overline{\\sigma}_t^{-1} h_t^{-1} \\] \\[ k_t = \\overline{\\sigma}_t i \\overline{\\sigma}_t^{-1} h_t^{-1} \\] \\[ k_t = i h_t^{-1} = h_t^{-1} \\] so, when we perform the correction over the mean of our belief: \\[ \\mu_t = \\overline{\\mu}_t + k_t (z_t - h(\\overline{\\mu}_t)) \\] \\[ \\mu_t = \\overline{\\mu}_t + h_t^{-1} (z_t - h(\\overline{\\mu}_t)) \\] \\[ \\mu_t = \\overline{\\mu}_t + h_t^{-1} z_t - h_t^{-1}h(\\overline{\\mu}_t) \\] with \\(h_t^{-1}h(\\overline{\\mu}_t)\\) what we are doing is, first computing \\(h(\\overline{\\mu}_t)\\) to map \\(\\overline{\\mu}_t\\) to the observation space, and the undoing this mapping with \\(h_t^{-1}\\), which means: \\[ \\mu_t = \\overline{\\mu}_t + h_t^{-1} z_t - \\overline{\\mu}_t \\] \\[ \\mu_t = \\overline{\\mu}_t - \\overline{\\mu}_t + h_t^{-1} z_t \\] \\[ \\mu_t = h_t^{-1} z_t \\] where \\(h_t^{-1}\\) maps \\(z_t\\) from the observation space to the state space, and this means in this update we trust our observation completely, and therefore our estate equals the observation. on the contrary, suppose the sensor is very unreliable, and so the noise is set to be infinity. then the correction step is executed as follows: \\[ k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + q_t)^{-1} \\] \\[ k_t = \\overline{\\sigma}_t h_t^t \\cdot (h_t \\overline{\\sigma}_t h_t^t + \\infty)^{-1} \\] because we are dividing by infinity, \\(k_t = 0\\). so the mean of our belief is computed as follows: \\[ \\mu_t = \\overline{\\mu}_t + k_t (z_t - h(\\overline{\\mu}_t)) \\] \\[ \\mu_t = \\overline{\\mu}_t + 0 (z_t - h(\\overline{\\mu}_t)) \\] \\[ \\mu_t = \\overline{\\mu}_t \\] hence, if the measurement is too noisy, we only take into account our predicted state. localization example localization example using extended kalman filter (from 11') $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/KalmanFilter/Kalman Filter.html",
    "title": "Kalman Filter",
    "body": " index search search back kalman filter introduction properties of gaussian distributions linear model models linear motion model linear observation model gaussian world representing the belief algorithm assumptions the kalman filter requires the world to be gaussian, that is every probability distribution used must be gaussian. this filter also assumes linear models. introduction suppose your position on a 2d plane is given by the black dot: let's say you get your estimate as to where you are, inferred from a certain control command. for example if you are trying to move forward in the same direction you estimate your new position will be the cross: this is will be the prediction step. now suppose you get an observation about the distance to the nearest lighthouse: so now we can perform our new state estimate by combining by the means of a weighted sum our prediction along with the measurement: this weighted sum is performed trading off how certain you are about your prediction and how certain you are about your observation. properties of gaussian distributions in order to derive and prove some parts of the kalman filter we exploit the following properties: the product of two gaussian is a gaussian. a gaussian stays gaussian under linear transformations the marginal and conditional distribution of a gaussian is a gaussian linear model what does it mean when we say the kalman filter uses linear models? this means that both the motion model and the observation model can be expressed through a linear function, that is: \\[ f(x) = ax + b \\] one important property is that if a gaussian distribution is transformed through a linear function it stays gaussian. also, we introduce noise by using a zero mean gaussian distribution. models the motion model is defined as follows: \\[ \\overline{x}_t = a_t x_{t-1} + b_tu_t + \\epsilon_t \\] where \\(x_{t-1}\\) is the previous state estimate, \\(u_t\\) is the control command at time \\(t\\) and \\(\\epsilon_t\\) is gaussian noise. let's dive a little deeper: \\(a_t\\) is a matrix \\(n \\times n\\) (a mapping between the state space and the state space) which tells us how the state evolves from \\(t-1\\) to \\(t\\) without control commands or noise. we can use this matrix to encode information about velocity, acceleration, etc. \\(b_t\\) is a matrix \\(n \\times l\\) (a mapping between the control space and the state space) that describes how the control command \\(u_t\\) changes the state from \\(t_1\\) to \\(t\\). \\(\\epsilon_t\\) is a random variable that represents the motion noise with covariance \\(r_t\\). the observation model is defined as follows: \\[ z_t = c_t \\overline{x}_t + \\delta_t \\] where \\(\\overline{x}_t\\) is the estimated state and \\(\\delta_t\\) is gaussian noise. \\(c_t\\) is a matrix \\(k \\times n\\) which describes a mapping between the state \\(\\overline{x}_t\\) to an observation \\(z_t\\). \\(\\delta_t\\) is a random variable that represents the observation noise with covariance \\(q_t\\). linear motion model now that we have defined our linear models, we are going to show how to express the motion under a gaussian: \\[ p(x_t|x_{t-1}, u_t) = \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) \\] which means we are obtaining the value of a probability distribution that incorporates our linear model for the prediction: \\[ p(x_t|x_{t-1}, u_t) \\sim \\mathcal{n}(a_tx_{t-1} + b_tu_t, r_t) \\] linear observation model we will apply the same reasoning to obtain the observation model under a gaussian: \\[ p(z_t|x_t) = \\det(2\\pi q_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(z_t - c_t\\overline{x}_t)^tq^{-1}_t(z_t - c_t\\overline{x}_t)) \\] which means we are obtaining the difference between the observation \\(z_t\\) and what i expect to observe \\(\\overline{x}_{t}\\) (\\(z_t - c\\overline{x}_t\\)) while also taking the uncertainty into account \\(q_t^{-1}\\). we compute this incorporating our linear model for the correction step: \\[ p(z_t|x_t) \\sim \\mathcal{n}(c_t\\overline{x}_t, q_t) \\] gaussian world as we have said, we are assuming everything is gaussian. up until now we have described our models by using gaussian distributions, however we still have to make sure these are maintained when we are performing the prediction and the update. so, given the belief at time \\(t\\): if we suppose \\(\\overline{bel}(x_t)\\) is gaussian, then \\(bel(x_t)\\) is gaussian because the product of gaussian distribution is a gaussian distribution. therefore we need to show that \\(\\overline{bel}(x_t)\\) is also gaussian. that is: \\[ \\overline{bel}(x_t) = \\int p(x_t|u_t,x_{t-1})bel(x_{t-1})dx_{t-1} \\] we know, by its definition, that \\(p(x_t|u_t,x_{t-1})\\) is gaussian, and also we can prove by mathematical induction that \\(bel(xx_{t-1})\\) is gaussian. because if we start from a gaussian distributed belief and everything stays gaussian then the belief at time \\(t-1\\) will also be gaussian. noting that the convolution of two gaussian stays gaussian we conclude that \\(\\overline{bel}(x_t)\\) is gaussian and thus \\(bel(t)\\) is also gaussian. let's show however that the integral preserves the gaussian. note that we can express the predicted belief by using our linear models as follows: \\[ \\overline{bel}(x_t) = \\int p(x_t|u_t,x_{t-1}) bel(x_{t-1})dx_{t-1} \\] \\[ = \\int \\det(2\\pi r_t)^{-\\frac{1}{2}} \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t))bel(x_{t-1})dx_{t-1} \\] \\[ = \\eta \\int \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t))bel(x_{t-1})dx_{t-1} \\] \\[ = \\eta \\int \\exp(-\\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t)) exp(-\\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1})) \\] where \\(exp(-\\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1}))\\) is the expected value of the gaussian that describes our previous belief \\(bel(x_{t-1})\\). now, we combine both exponentials, given \\(exp(x) \\cdot exp(y) = exp(x + y)\\): \\[ \\overline{bel}(x_t) = \\eta \\int \\exp(-l_t)dx_{t-1} \\] given: \\[ l_{t} = \\frac{1}{2}(x_t - a_tx_{t-1} - b_tu_t)^tr^{-1}_t(x_t - a_tx_{t-1} - b_tu_t) \\] \\[ + \\frac{1}{2}(x_{t-1} - \\mu_{t-1})^t\\sigma_{t-1}^{-1}(x_{t-1} - \\mu_{t-1}) \\] we can split \\(l_t\\) up in a part that only depends on \\(x_t\\) and another part that depends on \\(x_t, x_{t-1}\\). such that: \\[ l_t = l_t(x_{t-1}, x_t) + l_t(x_t) \\] thus: \\[ \\overline{bel}(x_t) = \\eta \\int \\exp(-l_t(x_{t-1}, x_t) -l_t(x_t))dx_{t-1} \\] \\[ \\overline{bel}(x_t) = \\eta \\exp(-l_t(x_t)) \\int \\exp(-l_t(x_{t-1}, x_t) )dx_{t-1} \\] this way we have: \\(\\exp(-l_t(x_t))\\): gaussian distribution \\(\\int \\exp(-l_t(x_{t-1}, x_t) )dx_{t-1}\\): this is the marginalization of a gaussian of the variable \\(x_{t-1}\\), which happens to also be a gaussian. therefore we have shown that everything stays gaussian: representing the belief we have said that everything is gaussian, which includes our belief. this belief will be represented, like any other gaussian is, by its mean \\(\\mu\\) and variance \\(\\sigma\\). so our belief at time \\(t\\) would be represented by \\((\\mu_t, \\sigma_t)\\). algorithm the kalman filter algorithm is defined as follows: inputs: \\(\\mu_{t-1}\\): previous mean that describes our belief at time \\(t-1\\) \\(\\sigma_{t-1}\\): previous covariance that describes our uncertainty at time \\(t-1\\) \\(z_t\\): the observation at time \\(t\\). \\(u_t\\): the control command at time \\(t\\). the algorithm is, as usual, divided into a prediction step and a correction step: in the prediction step we estimate our next belief, described by a gaussian \\(\\overline{bel}(x_t) \\sim \\mathcal{n}(\\overline{\\mu_t}, \\overline{\\sigma_t})\\). first we compute our new estimated mean \\(\\overline{\\mu}_t\\) by multiplying our transformation function \\(a_t\\) by the previous mean \\(\\mu_{t-1}\\) which tells us how the state evolves generally without any motion added to it (i.e. velocity, acceleration, etc). to add the motion we add \\(b_tu_{t}\\). then we update our uncertainty. the estimate of the new covariance is derived from how a gaussian changes through a linear transformation, thus we compute \\(a_t \\sigma_{t-1} a_t^t\\). we also add additional noise that the motion adds to the new belief by adding \\(r_t\\). the we apply the correction step: what we mainly do is computing the weighted sum between two distributions first we obtain the weighting factor \\(k_t\\), also known as the kalman gain. this equals a ratio between the prediction and the observation. here we use \\(c_t^t\\) to map our uncertainty from the state space to the observation space. on the denominator we map our uncertainty onto the observation space and we also add the measurement noise \\(q_t\\) then we divide the two terms to obtain a factor that tells us if we trust more the prediction or the correction. then we modify our estimated mean \\(\\overline{\\mu}_{t}\\) with a weighted correction: we compute the error between what we observed and what we predicted \\(z_t - c_t \\overline{\\mu}_t\\) (again \\(c\\) to map to the observation space). then we change the estimated state by this error pondered by \\(k\\). we also update our uncertainty assumptions we can apply the kalman filter as long as the two following assumptions hold: everything is gaussian the motion and observation model are linear however, what if this is not the case? extended kalman filter $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/BayesFilter/Bayes Filter.html",
    "title": "Bayes Filter",
    "body": " index search search back bayes filter state estimation recursive state estimation recursive bayes filter intuition derivation prediction and correction step implementation popular filters model examples state estimation state estimation means we want to estimate the state of the system based on sensor measurements and control commands. thus, given observations \\(z\\) and control commands \\(u\\), estimate the current state \\(x\\) at time \\(t\\): \\[ p(x_t|z_{1:t}, u_{1:t}) \\] recursive state estimation recursive state estimation means we want to update our belief based on the observation that comes in reusing the previous distribution that we had. therefore, using the previous definition of the current state, we would introduce recursion by computing \\(x_t\\) based on the current measurement \\(z_t\\), the current control command \\(u_t\\) and the previous state \\(x_{t-1}\\). the latter is in itself also defined recursively. recursive bayes filter intuition we start with no knowledge of the environment, so our state is described by a uniform distribution, indicating we could be located at any point in space. after receiving a measurement \\(z\\), we update our belief. in this case we have sensed a door, and we know there are three doors in our map. therefore the probability of obtaining the measurement \\(z\\) given we are in front of that door is larger than in the other possible positions. so if we combine our previous belief with this measurement's probability distribution, our belief becomes: now we move forward, so we also have to shift our belief forward. note, however, that our movement is not exact, there is also a level of uncertainty, so we describe it by using a distribution. hence, when combining our previous belief with the probability distribution for the motion our certainty about our state decreases, and our belief becomes: we receive yet another measurement \\(z\\), again we have that \\(p(z|x)\\) is larger on the locations where there is a door, because this measurement has sensed a door. so if we combine this probability distribution for this measurement with our previous state we increase our certainty about our current state. therefor, our belief becomes: derivation the belief at time \\(t\\) is given by: \\[ bel(x_t) = p(x_t | z_{1:t}, u_{1:t}) \\] that is, where am i at moment \\(t\\), given all previous observations \\(z_{1:t}\\) and control commands \\(u_{1:t}\\). we now apply bayes rule, to swap \\(x_t\\) and \\(z_t\\) on the conditional probability: \\[ = \\eta \\cdot p(z_t | x_t, z_{1:t-1}, u_{1:t}) \\cdot p(x_t|z_{1:t-1}, u_{1:t}) \\] where \\(\\eta\\) is a normalization constant. now, let's pay attention to \\(p(z_t | x_t, z_{1:t-1}, u_{1:t})\\). by the markov assumption we are going to assume that the current state \\(x_t\\) and the previous observations and control commands are conditionally independent. that is, they do not give any information about the likelihood of the observation \\(z_t\\). thus, we drop them from the equation: \\[ = \\eta \\cdot p(z_t | x_t) \\cdot p(x_t|z_{1:t-1}, u_{1:t}) \\] for \\(p(x_t|z_{1:t-1}, u_{1:t})\\) we are going to use the law of total probability to add a new variable, so we integrate over this new variable. more concretely to add \\(x_{t-1}\\), which will allow us to introduce recursion to our expression: \\[ = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t}) dx_{t-1} \\] we could interpret this rewritten expression as: for each previous state \\(x_{t-1}\\) we multiply \\(p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t})\\), the probability of being in the new state \\(x_t\\) given the previous state \\(x_{t-1}\\), observations \\(z_{1:t-1}\\) and control commands \\(u_{1:t}\\) by \\(p(x_{t-1}|z_{1:t-1},u_{1:t})\\), the probability of being in the state \\(x_{t-1}\\) given the previous observations \\(z_{1:t-1}\\) and control commands \\(u_{1:t}\\) once again we apply the markov assumption over \\(p(x_t | x_{t-1}, z_{1:t-1}, u_{1:t})\\), because knowing where i am at moment \\(t-1\\), we assume the observations \\(z_{1:t-1}\\) do not add any information. however note the control command does indeed hold valuable information, as it tells us action last executed that moved us from \\(x_{t-1}\\) to \\(x_t\\). so we simplify the expression as follows: \\[ = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t}) dx_{t-1} \\] we now suppose that knowing what action or command is executed in the future does not tell us anything about the present. hence we ignore the latest control command \\(u_t\\), so the expression becomes: \\[ = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot p(x_{t-1}|z_{1:t-1},u_{1:t-1}) dx_{t-1} \\] note that we have finally derived a recursive expression for our belief, given: \\[ bel(x_{t-1}) = p(x_{t-1}|z_{1:t-1}, u_{1:t-1}) \\] we substitute this expression in the belief at time \\(t\\): \\[ = \\eta \\cdot p(z_t | x_t) \\cdot \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] prediction and correction step usually the bayes filter is broken up into: prediction step: estimates where the future state is based on the control command at time \\(t\\) and makes use of the motion model. motion model: \\(p(x_t | x_{t-1}, u_{1:t})\\) \\[ \\hat{bel}(x_t) = \\int p(x_t | x_{t-1}, u_{1:t}) \\cdot bel(x_{t-1}) dx_{t-1} \\] correction step: we get an observation that we use to correct potential mistakes we make in the prediction step. this correction is made using the observation or measurement model. observation model: \\(p(z_t | x_t)\\) \\[ bel(x_t) = \\eta \\cdot p(z_t | x_t) \\cdot \\hat{bel}(x_t) \\] implementation in order to implement a bayes filter we need to define certain things: specify the motion model specify the observation model specify the belief how do we move from one state to the next (i.e. linear model, non-linear model) popular filters kalman filters and efk use gaussians to represent the belief, motion model and observation model they use linear or linearized models particle filter the can use arbitrary models to represent state, motion model and observation model model examples motion model given a current state \\(x_t\\), the motion model could look like: in the first case, let's suppose a point represents the next state \\(x_{t+1}\\) after the control command is applied. if we execute our system \\(n\\) times, we get \\(n\\) estimations that are illustrated by the \\(n\\) points in the graph. they represent an approximation of the distribution that describes our predicted state (illustrated by the graph in the upper left corner). this distribution is our motion model at time \\(t+1\\). for the two middle graphs, we can deduce that our system is certain about the angle of motion, however it shows more uncertainty about the distance. finally in the last two graphs we see the opposite. the system knows how much we moved, that is the distance, but is uncertain about the angle of movement. measurement model suppose we have a sensor that tell us the distance between us and the closest obstacle in front of us. we know that this sensor is noisy, so to mimic that noise we can describe the measurement model as a normal distribution. in the previous image, the star represents the closest obstacle, and we use the gaussian distribution to describe how likely it is of obtaining a given measurement. observe that the further we move away from the actual obstacle the lower is the probability of that measurement taking place. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0003)_Online_Training_Mobile_Robotics/ParticleFilter/Particle Filter.html",
    "title": "Particle Filter",
    "body": " index search search back particle filter intro particle set particle generation gaussian sampling importance sampling principle characteristics algorithm summary monte carlo localization structure particle filter algorithm for localization example resampling techniques roulette wheel low variance resampling algorithm intro with particle filters we are not restricting ourselves with parametric probability distributions like we do for example with kalman filters where we use gaussian distributions. as usual, we suppose we are given a map, and instead of using one parametric from we use non-parametric samples as a hypothesis of where the system might be. so, we are going to leave behind gaussian distributions to describe the estimate: and we are going to model our estimate using an arbitrary distribution: it turns out that we can describe this kind of distribution using samples: as you can see, the amount of samples in the areas where the density of the probability distribution is higher is also higher and vice versa. basically we have samples distributed over a state space, so imagine each sample signifies a little bit of probability mass, so we only need to count how many samples fall into a certain region to asses the probability that the system is in that region. also, we can weight our samples, so the larger the weight the larger the probability mass associated with that sample (taking into account that the sum of the weight have to amount to one). the weighting of the samples allows us to use less samples to represent the same probability distribution: note that by using samples we are computing an approximation of the probability function. and we use this weighted samples to estimate our belief. some examples are: we use the particles to approximate the probability function, where the more particles fall into a region, the higher the probability of the region. particle set we represent the sample set or particle set as follows: \\[ \\mathcal{x} = \\{\\langle x^{[j]}, w^{[j]}\\rangle\\}_{j=1\\cdots j} \\] where: there are \\(j\\) samples \\(x^{[j]}\\) represents the hypothesis (i.e. the state of the system) \\(w^{[j]}\\) represents the normalized weight assigned to jth particle the sum or integration over the particles represent the posterior (i.e. the probability function): \\[ p(x) = \\sum_{j=1}^j w^{[j]} \\delta_{x^{[j]}} (x) \\] where \\(\\delta\\) is the dirac function. note that \\(\\int \\delta(x) dx = 1\\) particle generation gaussian sampling note that closed form sampling is only possible for a few distributions, for example: for a gaussian distribution to obtain an approximation from sampling, we would sample by summing \\(12\\) times a random (uniformly drawn) number \\(x \\in [-\\sigma, \\sigma]\\), where \\(\\sigma\\) represents the standard deviation, and divide the sum by \\(\\frac{1}{2}\\). then you would draw samples that are approximately close to a gaussian distribution. importance sampling principle but, how can we approximate for another probability distribution functions? it turns out we can do this by sampling from a different probability function that the actual probability function and then compensating for the mistakes that we have done by drawing from this \"mistaken\" probability function. to do this we apply the importance sampling principle which tells us: we can use a different distribution (proposal distribution) \\(\\pi\\) to generate samples from the target (real) distribution \\(f\\). we need to account for the differences between \\(\\pi\\) and \\(f\\) using a weight, given by \\(\\omega = \\frac{f(x)}{\\pi{x}}\\) we need to assert the following pre-condition: \\(f(x) > 0 \\rightarrow \\pi(x) > 0\\) you can see that the weights are larger where the difference between the proposal and the target function is bigger. observe on the right side of the graph that we have drawn a low number of samples because our proposal probability function tells us the us the density on that region is low. however the target function shows a high probability in that same region, so by computing the difference between the proposal and the target function we assign bigger weights to those few particles. characteristics it is a recursive bayes filter uses a non-parametric approach models the distribution using samples and so the model need not be linear. the prediction step consists of drawing samples from the proposal function (takes the motion into account) the correction step consists of weighting the samples by the ration between the target function and the proposal function (takes the observation into account) the more particles we use to approximate the probability function the better the estimate is. algorithm the algorithm is composed of the following three steps: (prediction step) sample the particles using the proposal distribution (this signifies: where could my system be?). because we can choose the proposal function, what we do in this step is sampling from the motion model: \\[ x_t^{[j]} \\sim proposal(x_t|\\cdots) \\] (correction step) compute the importance weights to compensate from the mistakes made by sampling from the proposal distribution. if we derive the following expression, we obtain that the weights are given by the observation model: \\[ w_t^{[j]} = \\frac{target(x_t^{j})}{proposal(x_t^{j})} \\] resampling: draw with replacement \\(j\\) samples \\(i\\) with probability \\(w_t^{[i]}\\). so now we have a resampled set of samples where we update the weights by dividing the by \\(1/j\\) so they are normalized. what we do is generate a new set of samples where we replace the weight by the frequency. we do this because we work with a finite number of samples, so it could be the case that some particles have a very low probability and thus contribute very little to approximating the probability function. so it is better to eliminate those samples and replace them with a sample that is located in an area with high probability. we start with a empty sample set for the prediction step \\(\\hat{\\mathcal{x}}_t\\) and for the correction step \\(\\mathcal{x}_t\\). (prediction step) for \\(j=1\\cdots j\\): sample particle \\(x^{[j]}_t\\) from the proposal distribution \\(\\pi(x_t)\\), this distribution can be defined by the user, and corresponds to the belief at time \\(t-1\\) and constrained to the control command at time \\(t\\), \\(u_t\\). compute the weight by obtaining the difference between the proposal distribution and the target distribution. this results in using the observation model save the pair \\(x_t^{[j]}\\), \\(w_t^{[j]}\\) to the prediction sample set \\(\\hat{\\mathcal{x}}_t\\). (correction step) for \\(j=1\\cdots j\\): draw a particle \\(x_t^{[j]}\\) with replacement from the prediction sample set with probability proportional to the weight of the sample \\(w_t^{[j]}\\). add the particle to the correction sample set \\(\\mathcal{x}_t\\) return the resampled sample set \\(\\mathcal{x}_t\\) summary what the particle filter does is: it takes each particle as a pose hypothesis that says \"this is where the system is at time \\(t\\)\". then it adds weight to each particle signifying how much that pose hypothesis conforms to the given observation, and tells us how likely the hypothesis is. if we do this with \\(n\\) particles what we obtain is a belief, that is, a set of possibilities of where we are which describe my probability distribution. monte carlo localization monte carlo localization refers to the estimation the location and orientation of the system using a particle filter. for example: with a particle filter, our belief shows where the robot is located by having a bigger density of particles right under where the robot is. another example is the following, where we start with all the particles scattered over the map that means the particles are sampled from a uniform distribution so every point in space is equally likely to be the location of the robot. once the robot drives around and obtains new measurements the probability mass concentrates on places where the robot is more likely to be in given the motion commands and the observations. eventually the system converges and you end up with a unimodal distribution that is similar to a gaussian distribution. structure each particle represents a pose hypothesis we represent the proposal probability function by drawing from the motion model. because we are sampling from the motion distribution what we do is increasing the uncertainty about the motion at time \\(t\\) and thus account for the noise present in each motion. \\[ x_t^{[j]} \\sim p(x_t|x_{t-1}, u_t) \\] we apply the correction via the observation model. so the weight of each particle is proportional to the likelihood of an observation \\(z_t\\) given i know where i am \\(x_t\\) and the map of the environment \\(m\\). this result is dependent of the choice made previously of sampling from the motion model. \\[ w_t^{[j]} = \\frac{target}{proposal} \\propto p(z_t|x_t,m) \\] particle filter algorithm for localization we modify slightly our particle filter algorithm to use it for localization: we sample from the motion model \\(p(x_t|u_t, x^{[j]}_{t-1})\\) instead from the generic proposal function \\(\\pi(x_t)\\) we compute the weights with \\(w_t^{[j]} = \\frac{target}{proposal} \\propto p(z_t|x_t,m)\\). example first we start with a uniform distribution, and we sample from that distribution, obtaining \\(j\\) particles distributed over the space with the same probability. then we obtain an observation, and in the weighting step we increase the weight of the samples with are more likely given the observation. in this case the samples in front of doors, while the rest of the particles get a lower weight. then we apply resampling to replace weight by frequencies (the probability mass of a particle is bigger if this particle has been resampled several times, which means it weight was bigger than the rest of the samples). in the following picture the resampling step is executed along with the motion step (so the probability function is offsetted): because the prediction/motion was already performed before, now we obtain another observation: when we obtain the weights, two things happen. first, and as before, the particles (pose hypothesis) more likely to be correct given the observation obtain a larger weight. second, because the is a bigger number of particles in front of the second door the density in this area is bigger than in the areas in front of the other doors. another resampling and prediction step: resampling techniques roulette wheel first we create a roulette wheel where each field represents a particle, and the bigger the weight associated with that particle the bigger the field is: the idea is that, we normalize the weights, and each time we draw a number between zero and one, which will \"point\" to a weight. however this method can lead to suboptimal solutions. suppose that for some reason each time we end up with uniform weight, so that no particle is more likely than any other. then, with the wheel roulette we will duplicate some particles and remove some others. however this does not make sense, because every particle had the same weight. thus we introduce the lower variance resampling. low variance resampling here, the idea is using \\(j\\) arrows instead of only one, where the arrows are at the same angular distance from each other. so in order to sample what we do is, we simply turn the arrows, and where all the arrows end up, that is the samples we choose. this solution is faster, with time complexity equal to \\(o(j)\\) compared to the wheel roulette's \\(o(j \\log j)\\), and resolves the suboptimal solution problem presented earlier. algorithm first we draw a random number between \\(0\\) and \\(\\frac{1}{j}\\) then we pick \\(j-1\\) particles by advancing in the array in steps of \\(\\frac{1}{j}\\) to efficiently implement this what we do is, in each element of the array we store the cummulative weight up until that point: so, we draw a random number between \\(0\\) and \\(\\frac{1}{j}\\), if that number is bigger than the weight accumulated up until weight \\(i\\), then we move to the next one, else if it is less we sample the particle \\(i\\). and then we advance \\(\\frac{1}{j}\\). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/ParticleFilters/Particle Filters.html",
    "title": "Particle Filters",
    "body": " index search search back particle filters properties introduction initial probability resampling motion algorithm resampling example properties state space belief efficiency accuracy continuous multimodal   approximate introduction so, in a first instance, given a floor plan the robot has to perform global localization. that is, it does not know where it is, and it has to find out based on sensor measurements: the robot has range sensors (blue lines), which use sonar sensors (basically sound) to obtain the distance between the robot and the obstacles around it. it uses this sensors to determine a posterior distribution that models its position at a given time. each particle (dot) is a discrete guess whether the robot might be, and it holds the following information: x coordinate, y coordinate, and heading direction. then, the comprise of multiple of these guesses make up the representation for the posterior of the robot's location. so, initially the robot is completely uncertain as to where it is, which derives into a uniform distribution as to where it may be, and thus the particles are scattered all over the floor plan. however as time passes, the particle filter makes them survive according to how consistent these particles are compared to the sensor measurements: in summary, the particles guess where the robot might be moving and then the filter makes them \"survive\" (it does not discard them) using survival of the fittest. this latter statement means that those particles that are more consistent with the measurements are more likely to survive. initial probability at the start the robot only has the map of the room and no other knowledge, therefore there is equal probability that the robot is at any position in the map. hence, we create a set of \\(n\\) particles modeled after a uniform distribution. which means each particle is as likely to be chosen as any other. resampling at first, we have \\(n\\) particles scattered all over the map and most of them are wrong. so now, we can start removing some of the wrong guesses using measurements of the environment. and we do this by resampling \\(n\\) particles. this translates into, we choose \\(n\\) particles that represent the place we believe the robot is in. so, now the filter can go through each of our particles and determine what the measurement would be if our robot was in the position indicated by the particle. in other words, each particle has assigned an importance weight \\(w\\) that determines how likely the measurement \\(z\\) is given a concrete particle \\(p_i\\), (\\(p(z|p_i)\\)). so, given a total of \\(n\\) particles: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1\\\\ p_2 & \\rightarrow w_2\\\\ \\vdots\\\\ p_n & \\rightarrow w_n\\\\ \\end{cases} \\end{align} let \\(w = \\sum_i w_i\\) be the sum of all the weights. we introduce a new variable \\(\\alpha\\) which represents the normalized weights: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1 & \\rightarrow \\alpha_1 = \\frac{w_1}{w}\\\\ p_2 & \\rightarrow w_2 & \\rightarrow \\alpha_2 = \\frac{w_2}{w}\\\\ \\vdots\\\\ p_n & \\rightarrow w_n & \\rightarrow \\alpha_n = \\frac{w_n}{w}\\\\ \\end{cases} \\end{align} hence, \\(\\sum_i \\alpha_i = 1.0\\). so we have now defined a new probability distribution that describes more clearly the position of our robot because it takes into account our measurement. so, it's time for the resampling. we have to choose \\(n\\) particles from the overall set, where each particle \\(p_i\\) is chosen with probability \\(\\alpha_i\\): note we allow replacement, so we can draw multiple copies of the same particle \\(p_i\\). so what will happen is, the higher \\(\\alpha_i\\) the more likely it is that particle \\(p_i\\) is chosen multiple times, meanwhile the lower \\(\\alpha_i\\) is the more likely it is that it will not be chosen, and therefore simply removed from the set of particles. as you can see in the previous image, we have drawn three times \\(p_2\\), probably because the associated \\(\\alpha_2\\) was larger than the rest of the \\(\\alpha\\). therefore, the particles with a low importance weight will survive with a much lower rate than the ones with a higher importance weight. motion we also have to take into account that our particles cannot be static, but have to move with our robot. so whichever motion is applied to the robot should be applied to every single particle. once the motion is applied we obtain a measurement and perform resampling to choose those particles that are more likely to describe the real position of the robot. note that the motion will most probably contain noise, so we do not want to propagate the motion equally to each particle. what we would want is to add some gaussian noise to the particles to represent somewhat this uncertainty about the motion. algorithm measurement updates we compute the posterior over state distribution: \\begin{align} p(x|z) \\propto p(z|x)p(x) \\end{align} here: \\(p(x)\\) is the distribution over the set of particles. \\(p(z|x)\\) is the distribution that models the importance weights. and by resampling we obtain \\(p(x|z)\\), because we draw with probability equal to the importance weight a given particle. motion updates we compute the posterior over distribution one step later (after movement): \\begin{align} p(x^t) = \\sum p(x^t|x)p(x) \\end{align} where: \\(p(x)\\) is the distribution over the set of particles. and then, we sample from the sum. that is we generate a random particle \\(x^t\\) by applying the motion model \\(p(x^t|x)\\) to the particles \\(p(x)\\). resampling example suppose we have the following data: \\begin{align} n \\begin{cases} p_1 & \\rightarrow w_1 = 0.6 & \\rightarrow \\alpha_1 = \\frac{w_1}{w} = \\frac{0.6}{6.0} = 0.1\\\\ p_2 & \\rightarrow w_2 = 1.2 & \\rightarrow \\alpha_2 = \\frac{w_2}{w} = \\frac{1.2}{6.0} = 0.2\\\\ p_3 & \\rightarrow w_3 = 2.4 & \\rightarrow \\alpha_3 = \\frac{w_3}{w} = \\frac{2.4}{6.0} = 0.4\\\\ p_4 & \\rightarrow w_4 = 0.6 & \\rightarrow \\alpha_4 = \\frac{w_4}{w} = \\frac{0.6}{6.0} = 0.1\\\\ p_5 & \\rightarrow w_2 = 1.2 & \\rightarrow \\alpha_5 = \\frac{w_5}{w} = \\frac{1.2}{6.0} = 0.2\\\\ \\end{cases} \\end{align} then, the probability of never sampling \\(p_3\\) is given by the multiplication rule of probability: on the first draw: \\begin{align} p(\\bar{p_3}) = p(p_1) + p(p_2) + p(p_4) + p(p_5) = 0.6 \\end{align} because we allow for resampling, on the second draw: \\begin{align} 0.6 \\cdot p(\\bar{p_3}) = 0.6 \\cdot (p(p_1) + p(p_2) + p(p_4) + p(p_5)) = 0.6^2 \\end{align} thus, on the fifth and final draw: \\begin{align} 0.6^4 \\cdot p(\\bar{p_3}) = 0.6^4 \\cdot (p(p_1) + p(p_2) + p(p_4) + p(p_5)) = 0.6^5 = 0.0777 \\end{align} however, the probabily of never drawing \\(p_1\\) equals: \\begin{align} p(\\bar{p_1}) = 0.9 ^ 5 = 0.59 \\end{align} therefore, the particles with a low importance weight will survive with a much lower rate than the ones with a higher importance weight. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/PIDControl/PIDControl.html",
    "title": "PID Control",
    "body": " index search search back pid control up until now we have created paths that connect dots with straight lines. however this tend to become unnefficient, as you can smooth them to reduce the distance and also to represent the motion of an object is a more realistic way. smoothing algorithm initilize \\(y_i\\) to the non-smooth locations \\(x_i\\): \\(y_i = x_i\\) optimize two criteria: first minimize the distance between the non-smooth point and the smooth point \\begin{align} \\min_i (x_i - y_i)^2 \\end{align} then minimize the distance between two consecutive smooth points \\begin{align} \\min_i (y_i - y_{i+1})^2 \\end{align} to optimize both, we include a parameter \\(\\alpha\\), to minimize the weigthed sum: \\begin{align} \\min (x_i - y_i)^2 + \\alpha (y_i - y_{i+1})^2 \\end{align} we optimize both, because they are in conflict with each other: if we only optimize the first one, we obtain the same path as the original non-smoothed path if we only optimize the second one, we obtain no path pid control if we have a car that has a steering angle \\(\\alpha\\), how would we go about defining this parameter? p controller we set this angle proportional by some factor of \\(\\tau\\) to the crosstrack error. where the crosstrack error refers to the lateral error between the vehicle and the reference trajectory. thus: \\begin{align} \\alpha = \\tau cte \\end{align} note that with this approach we will eventually overshoot when reaching for the reference trajectory. that is because the car it not oriented the same as the trajectory, therefore it needs to reposition once it reaches the trajectory: pd control here the steering angle does no only take into account the \\(cte\\), but it also uses the derivative of cte. the latter will compute how much we are reducing the error in each moment \\(t\\), and use this value to counter steer this angle (reduce the angle): \\begin{align} \\alpha = - \\tau_p cte - \\tau \\frac{\\delta}{\\delta t} cte \\end{align} where: \\begin{align} \\frac{\\delta}{\\delta t} cte = \\frac{cte_t - cte_{t-1}}{\\delta t} \\end{align} systematic bias in real life there is usually some noise when it comes to the angle of the wheels, and we refer to that as systematic bias. for example the wheels might be deviated a certain angle without us knowing. pid control because of this systematic bias, the error with respect to the reference trajectory is very large. therefore if we sum it over time we obtain larger and larger values. so, if we sum this cte error weighted by a factor \\(\\tau_i\\), we can correct this error by counter steering: \\begin{align} \\alpha = - \\tau_p cte - \\tau_d \\frac{\\delta}{\\delta t} cte - \\tau_i \\sum cte \\end{align} where \\(\\sum cte\\) equals the sum of the \\(cte\\) error overtime. note: \\(- \\tau_p cte\\): represents the proportional error \\(- \\tau_d \\frac{\\delta}{\\delta t}\\): represents the differential error \\(- \\tau_i \\sum cte\\): represents the integral error twiddle we use twiddle to optimize a set of parameters. in our case what we do is optimize, that is minimize, the average cte. so, given a parameters vector \\(p = [0, 0, 0]\\) and a vector of potential changes \\(dp = [1, 1, 1]\\) we: execute run() which computes the \"optimal\" steering angle and moves the robot accordingly. it also stores this motion as a trajectory. this function will return a \"goodness\" metric, that will signify the cte. so, after executing run() we get the best error so far. we modify p to make our error smaller, to make this modification we use twiddle. the algorithm is as follows: # compute initial error best_error = run(p) # while the sum of the potential changes is bigger than a tolerance parameter while sum(dp) < tolerance: # iterate over every parameter for i in range(len(p)): # update the parameter value by the value of the corresponding potential change p[i] += dp[i] # compute the new error for this change err = run(p) # does this better the previous error? err < best_error: # make the change bigger dp[i] *= 1.1 # if the error is worse else: # we try updating the parameter by subtracting (by two because we added before) p[i] -= 2*dp[i] err = run(p) # does this better the previous error? err < best_error: # make the change bigger dp[i] *= 1.1 # if substracting does not make the error better else: # we decrease the change dp[i] *= 0.9 basically twiddle decreases/increases the parameters first a little bit, and for each time we make the error better we augment the increase or decrease. and we stop when there are no major changes being made to the parameters, that is sum(dp) < tolerance. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/index.html",
    "title": "Artificial Intelligence Robotics",
    "body": " index search search back artificial intelligence robotics histogram localization kalman filters particle filters search pid control slam $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/Search/Search.html",
    "title": "Search",
    "body": " index search search back search the process of finding a path from a starting point to the goal location is called robot motion planning or simply planning. then given: map starting location goal location cost function the goal is to find the minimum cost path between the start and the goal. a* consider a square grid having many obstacles and we are given a starting cell and a target cell. we want to reach the target cell (if possible) from the starting cell as quickly as possible. what a* search algorithm does is that at each step it picks the node according to a value \\(f = g + h(x,y)\\), where \\(g\\) is the current cost and \\(h(x,y)\\) is the value of the heuristic function in cell \\((x,y)\\). that is h is the estimated movement cost to move from that given square on the grid to the final destination. this is often referred to as the heuristic, which is nothing but a kind of smart guess. algorithm 1. let openlist equal empty list of nodes 2. let closedlist equal empty list of nodes 3. put startnode on the openlist (leave it's f at zero) 4. while openlist is not empty 5. let currentnode equal the node with the least f value 6. remove currentnode from the openlist 7. add currentnode to the closedlist 8. if currentnode is the goal 9. you've found the exit! 10. let children of the currentnode equal the adjacent nodes 11. for each child in the children 12. if child is in the closedlist 13. continue to beginning of for loop 14. child.g = currentnode.g + distance b/w child and current 15. child.h = distance from child to end 16. child.f = child.g + child.h 17. if child.position is in the openlist's nodes positions 18. if child.g is higher than the openlist node's g 19. continue to beginning of for loop 20. add the child to the openlist dynamic programming given a grid and a goal position, dynammic programming gives you the optimal action for each cell. where the optimal action is to move to the direction that offers the lower distance to the goal. to compute this distance we calculate: \\begin{align} f(x,y) = g = min_{x',y'} f(x', y') + 1 \\end{align} that is, we obtain recursively the distance of each neighbour to the goal and we add one. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/Localization/Localization.html",
    "title": "Histogram Localization",
    "body": " index search search back histogram localization probability given by belief probability after sense exact motion inexact motion bayes rule motion using total probability summary localization algorithm probability given by belief suppose every place in the world is modeled after an uniform probability distribution, then every cell the robot has the same probability. probability after sense now suppose the robot is able to sense a color, and each cell has a different color assigned. let's assume the robot senses the color red, then the cells with this color assigned should have a higher probability. therefore we define two values, a hit value and a miss value. if the cell is red, then we multiply its probability by the hit value. if the cell is not red, then we multiply its probability by the miss value. note that the hit value is a big value, and the miss value is lower. thus the probability for miss cells is lower than the probability for hit cells. also, observe that a measurement refers to what the robot senses, that is, a green cell or a red cell. normalize the distribution now that we have altered the values of the probability distribution, it is likely that they do not sum up to one, which is a requirement to every probability function. therefore we need to normalize it. so what we would do is: compute the probabilities after the robot \"senses\" a measurement normalize these probabilities by dividing each probability by the total sum of all probabilities exact motion suppose we have a world made up of a grid with 5 cells with the following probabilities: [\\(\\frac{1}{3}\\)][\\(\\frac{1}{3}\\)][\\(\\frac{1}{9}\\)][\\(\\frac{1}{9}\\)][\\(\\frac{1}{3}\\)] we also know that with 100% probability the world moves to the right. theorem of total probability to compute the probability of each cell after the movement, we use the law of total probability that states: given events a and \\(b = \\{b_1, \\cdots, b_n\\}\\) events in a sample space where \\(b\\) is pairwise disjoint, then: \\begin{align} p(a) = \\sum_n p(a \\cap b_n) \\end{align} then, by the conditional probability formula: \\begin{align} p(a) = \\sum_n p(a|b_n)p(b_n) \\end{align} so to apply this theorem what we do is sum the probabilities of ending up in cell \\(j\\) when we come from cell \\(i\\), which is expressed symbolically: \\begin{align} p(x_{j}) = \\sum_{i=1}^5 p(x_{j}|x_{i}) \\cdot p(x_{i}) = 1 \\cdot p({x_j}_{\\{j=(i+1)\\}}) \\end{align} because we know: \\begin{align} p(x_j|x_i) = \\begin{cases} 1, & j = i + 1 \\\\ 0, & \\text{ in any other case} \\end{cases} \\end{align} inexact motion however what if \\(p(x_j|x_i) < 1\\)? suppose: \\(p(x_{i+2}|x_i) = 0.8\\): the robot moved 2 positions/units with \\(0.8\\) probability \\(p(x_{i+1}|x_i) = 0.1\\): the robot moved 1 positions/units with \\(0.1\\) probability \\(p(x_{i+3}|x_i) = 0.1\\): the robot moved 3 positions/units with \\(0.1\\) probability then for each \\(i\\): \\begin{align} p(x_{j}) = \\sum_{i=1}^5 p(x_{j}|x_{i}) \\cdot p(x_{i}) \\end{align} where: \\begin{align} p(x_{j}|x_{i}) = \\begin{cases} 0.8, & j = i + 2 \\\\ 0.1, & j = i + 1 \\\\ 0.1, & j = i + 3 \\\\ 0, & \\text{ otherwhise } \\end{cases} \\end{align} entropy the entropy will decrease after the measurement update (sense) step, and the entropy will increase after the movement step (move). in general, entropy represents the amount of uncertainty in a system. since the measurement update step decreases uncertainty, entropy will decrease. the movement step increases uncertainty, so entropy will increase after this step. the entropy formula for our case is the following: \\begin{align} entropy = \\sum_{i=1}^5(-p(x_i) \\cdot \\log(p(x_i))) \\end{align} bayes rule suppose: \\(x\\) represents the grid cell \\(z\\) represents the measurements then the bayes rule states: \\begin{align} p(x_i|z) = \\frac{p(z|x_i)p(x_i)}{p(z)} \\end{align} where: \\(p(x_i|z)\\) is called the posterior \\(p(z|x_i)\\) is called the likelihood \\(p(z)\\) is known as the evidence or marginal likelihood (that is, it marginalizes \\(z\\)). to compute \\(p(z)\\) we use the theorem of total probability: \\begin{align} p(z) = \\sum_{i=1}^n p(z|x_i)p(x_i) \\end{align} so, to compute \\(p(x_i|z)\\) we follow the steps: for each \\(x_i\\) compute the non-normalized posterior: \\(\\hat{p}(x_i|z) = p(z|x_i)p(x_i)\\) sum all non-normalized posteriors to obtain the evidence: \\(p(z) = \\sum_{i=1}^n \\hat{p}(x_i|z)\\) for each \\(x_i\\) normalize the posterior with the evidence: \\(p(x_i|z) = \\frac{\\hat{p}(x_i|z)}{p(z)}\\) motion using total probability let's say we are at time \\(t\\), and \\(i\\) determines the cell, then the motion is expressed probabilistically as follows: \\begin{align} p(x_i^t) = \\sum_{j} p(x_i|x_j)p(x_j^{t-1}) \\end{align} if we break down this formula: \\(p(x_i|x_j)\\) is the probability that we end up in the cell \\(x_i\\) given we come from the cell \\(x_j\\) \\(p(x_j^{t-1})\\) is the probability of being in cell \\(x_j\\) at the previous time \\(p(x_i^t)\\) is the probability of being in cell \\(x_i\\) at time \\(t\\) summary belief represents where are possible places the robot might be, that is each cell has an associated probability value sense also known as the measurement update function. for each cell we compute the probability that the robot is in that cell, given a measurement sensed by the robot in the moment \\(t\\) (\\(p(x_k|z)\\), where \\(x_k\\) is the cell and \\(z\\) is the measurement). therefore, for each cell in the world we multiply the previous probability value (given by belief) and the probability that the robot moved to the given cell. for example, to satisfy the probability function properties, we need to normalize it, so it sums up to one. move it is a convolution, for each possible location, after the motion, we reverse engineered the situation and guessed where the world might have come from. so what we do is we compute the probability of each cell using the total probability theorem, so given a cell \\(x_k\\), we compute: \\begin{align} p(x_k) = \\sum_{l}p(x_k|x_l)p(x_l) \\end{align} where \\(p(x_k|x_l)\\) is the probabily that the robot moved to cell \\(x_k\\) from cell \\(x_l\\). usually what we do is stablish a motion using a vector (i.e. \\((0,1) \\in \\mathbb{r}^2\\) to indicate the robot moved one unit up in the two dimensional vector space). for example, this probability may refer to how likely it is that the robot moved to the exact cell, how likely it is that the robot moved to a cell \"beyond\" the goal or how likely it is that the robot moved to a cell that lies \"before\" the goal. so if we have these three probabilities, for each cell we sum the probabilities of the robot being in that cell taking into account the three scenarions: if the robot moved to cell \\(x_k\\) from cell \\(x_l\\), and that cell was the goal, then it moved to that cell with probability \\(p_{exact}\\) and maybe the robot moved to cell \\(x_k\\) from cell \\(x_i\\), however the goal was \\(x_{k+1}\\), then it moved to that cell with probability probability \\(p_{undershoot}\\) maybe the robot moved to cell \\(x_k\\) from cell \\(x_j\\), however the goal was \\(x_{k-1}\\), then it moved to that cell with probability probability \\(p_{overshoot}\\) suppose now that then only cells in the world are mentioned: \\(x_k, x_l, x_i, x_j\\). then for \\(x_k\\) we update the belief as follows: \\begin{align} p(x_k) = p(x_l) * p_{exact} + p(x_i) * p_{undershoot} + p(x_j) * p_{overshoot} \\end{align} localization algorithm next we lay out an example of the localization algorithm implemented in \\(\\mathbb{r}^2\\): # the function localize takes the following arguments: # # colors: # 2d list, each entry either 'r' (for red cell) or 'g' (for green cell) # # measurements: # list of measurements taken by the robot, each entry either 'r' or 'g' # # motions: # list of actions taken by the robot, each entry of the form [dy,dx], # where dx refers to the change in the x-direction (positive meaning # movement to the right) and dy refers to the change in the y-direction # (positive meaning movement downward) # note: the *first* coordinate is change in y; the *second* coordinate is # change in x # # sensor_right: # float between 0 and 1, giving the probability that any given # measurement is correct; the probability that the measurement is # incorrect is 1-sensor_right # # p_move: # float between 0 and 1, giving the probability that any given movement # command takes place; the probability that the movement command fails # (and the robot remains still) is 1-p_move; the robot will not overshoot # its destination in this exercise # # the function should return (not just show or print) a 2d list (of the same # dimensions as colors) that gives the probabilities that the robot occupies # each cell in the world. # # compute the probabilities by assuming the robot initially has a uniform # probability of being in any cell. # # also assume that at each step, the robot: # 1) first makes a movement, # 2) then takes a measurement. # # motion: # [0,0] - stay # [0,1] - right # [0,-1] - left # [1,0] - down # [-1,0] - up # compute the probability of \"hit\" cell and the probability of a \"miss\" cell # # :param float z value sensed by the robot (i.e. 'r' or 'g') # :param float cell_measurement value in the cell (i.e. 'r' or 'g') # :param float sensor_right probability that what the robot sensed is correct # # if the value sensed and the value in the cell are equal hit = 1 and miss = 0 # :return [sensor_right, 0] # otherwhise # :return [0, (1-sensor_right)] def probability_hit_miss(z, cell_measurement, sensor_right): hit = (z == cell_measurement) return [hit * sensor_right, (1-hit) * (1-sensor_right)] # compute the probability of a cell after the measurement of the robot # # :param cell_prior probability stored in the cell before measurement # :param float z value sensed by the robot (i.e. 'r' or 'g') # :param float cell_measurement value in the cell (i.e. 'r' or 'g') # :param float sensor_right probability that what the robot sensed is correct # # if the value sensed and the value in the cell are equal hit = sensor_right, else miss = (1-sensor_right) # :return the probability before measurement multiplied by the probability that the measurement is correct for the # given cell def probability_cell_given_measurement(cell_prior, z, cell_measurement, sensor_right): [hit, miss] = probability_hit_miss(z, cell_measurement, sensor_right) return cell_prior * (hit + miss) # for each cell x_k compute the probability that the robot is in the cell x_k given a measurement z # # :param list world measurements in the world # :param list p current world probabilities # :param list z current measurement of the robot # :param float sensor_right probability that the robot's measurement is correct # # for each cell x_k, where k is the cell [i][j]: # compute unnormalized p(x_k|z) = p(z|x_k) * p(x_k) # where p(x_k) = p[i][k] and # p(z|x_k) is computed in probability_cell_given_measurement and equals: # - sensor_right, if measurement in x_k = z # - (1-sensor_right), if measurement in x_k != z # compute the sum over all p(x_k|z), this sum equals p(z). # obtain normalized p(x_k|z) by dividing each p(x_k|z) by p(z) # :return list q of cell probabilies after measurement update def sense(world, p, z, sensor_right): q=[] # obtain probabilities q = [[ probability_cell_given_measurement(p[i][j], z, world[i][j], sensor_right) for j in range(len(p[0]))] for i in range(len(p))] # sum all probabilities s = sum([sum(row) for row in q]) ## normalize q = [[q[i][j]/s for j in range(len(p[0]))] for i in range(len(p))] return q # obtain probabilities of each cell in the grid after the robot moves # # :param list p current world probabilities # :param list u description of the motion (i.e. [0,1] to move to the right) # :param float p_move probability of moving from one cell to another # # for each cell x_k: # p(x_k) = sum over l=1...m of p(x_k|x_l) * p(x_l) # where # - p(x_k|x_l) = p_move and p(x_l) = p[(i-y) % len(p)][(j-x) % len(p[0])] if the robot moves from cell l = [(i-y)][(j-x)] to cell k = [i][j] # - p(x_k|x_l) = (1- p_move) and p(x_l) = p[i][j] if the robot does not move from cell k = [i][j] # :return list q of cell probabilies after the robot moves def move(p, u, p_move): q = [] [y, x] = u q = [[p_move * p[(i-y) % len(p)][(j-x) % len(p[0])] + (1-p_move) * p[i][j] for j in range(len(p[0]))] for i in range(len(p))] return q # for each pair of motion-measurement, update the grid probabilities of probabilities that represents where the robot is in any given moment # # :param matrix colors grid of measurements # :param list measurements measurements sensed by the robot # :param list motions directions in which the robot moved at each moment (i.e. for [0,1] it moves to the right) # :param float sensor_right probability that the robot's measurement is correct # :param float p_move probability of moving from one cell to another # # :return list q of cell probabilies after finishing updating for every measurement-motion def localize(colors,measurements,motions,sensor_right,p_move): # initializes p to a uniform distribution over a grid of the same dimensions as colors pinit = 1.0 / float(len(colors)) / float(len(colors[0])) p = [[pinit for row in range(len(colors[0]))] for col in range(len(colors))] # update probabilities iteratively for k in range((len(measurements))): p=move(p, motions[k], p_move) p=sense(colors, p, measurements[k], sensor_right) return p so for example, for the following data: colors = [['r','g','g','r','r'], ['r','r','g','r','r'], ['r','r','g','g','r'], ['r','r','r','r','r']] measurements = ['g','g','g','g','g'] motions = [[0,0],[0,1],[1,0],[1,0],[0,1]] where: the robot does not move (\\([0,0]\\)) and senses a green cell 'g'. the robot moves down (\\([0,1]\\)) and senses a green cell 'g'. the robot moves right (\\([1,0]\\)) and senses a green cell 'g'. the robot moves right (\\([1,0]\\)) and senses a green cell 'g'. the robot moves down (\\([0,1]\\)) and senses a green cell 'g'. and the colors is the representation of the world. then, we apply the localization algorithm to obtain the probability distribution that the robot is in each cell: p = localize(colors,measurements,motions,sensor_right = 0.7, p_move = 0.8) show(p) note that the probabily that the robot sensed the measurement correctly (\\(p(z|x_i)\\)) is \\(0.7\\) and the probability that the robot moved to the cell given by the motion vector (\\(p(x_i|x_j)\\)) is \\(0.8\\). this outputs: [[0.01106,0.02464,0.06800,0.04472,0.02465], [0.00715,0.01017,0.08697,0.07988,0.00935], [0.00740,0.00894,0.11273,0.35351,0.04066], [0.00911,0.00715,0.01435,0.04313,0.03643]] where each element in the matrix is the probabily of a cell. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/KalmanFilters/Examples.html",
    "title": "Examples",
    "body": " index search search back examples design kalman filters for 2d to design a kalman filter in two dimensions (position, velocity) you need two things: a state transition function, which is usually a matrix \\(f\\): \\begin{align} \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\leftarrow f \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\end{align} a measurement function, represented by the matrix \\(h\\): \\begin{align} z \\leftarrow h \\begin{bmatrix} x \\\\ \\hat{x} \\\\ \\end{bmatrix} \\end{align} for example, suppose we update the location and the velocity as follows: \\begin{align} x' = x + \\hat{x} \\end{align} \\begin{align} \\hat{x}' = \\hat{x} \\end{align} then the transition function is represented as the following matrix: \\begin{align} f = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ \\end{bmatrix} \\end{align} and for the measurement function, we only observe the location not the velocity, therefore: \\begin{align} h = \\begin{bmatrix} 1 & 0 \\\\ \\end{bmatrix} \\end{align} 4d example motion given a state \\((x, y, \\hat{x}, \\hat{y})\\), where \\((x, y)\\) is the position and \\((\\hat{x}, \\hat{y})\\) is the velocity.if in each iteration the motion update for the state is: \\begin{align} \\begin{matrix} x + dt\\cdot \\hat{x} \\\\ y + dt\\cdot \\hat{y} \\\\ \\hat{x} \\\\ \\hat{y} \\\\ \\end{matrix} \\end{align} so the position moves with time and the velocity does not change with time. then the state transition function is represented by the following matrix: \\begin{align} f = \\begin{bmatrix} 1 & 0 & dt & 0\\\\ 0 & 1 & 0 & dt\\\\ 0 & 0 & 1 & 0\\\\ 0 & 0 & 0 & 1\\\\ \\end{bmatrix} \\end{align} measurement and, because we can only measure the position the measurement update is of the form: \\begin{align} z \\leftarrow \\begin{bmatrix} x \\\\ y \\\\ \\end{bmatrix} \\leftarrow h \\cdot \\begin{bmatrix} x \\\\ y \\\\ \\hat{x} \\\\ \\hat{y} \\\\ \\end{bmatrix} \\end{align} therefore the measurement function is represented as follows: \\begin{align} h = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ \\end{bmatrix} \\end{align} code example so for the following data, which deals with 4d data, that is we store a 2d location and a 2d velocity vector in the state. we will have to take this into account in the different update matrices and uncertainty matrix: # location measurements measurements = [[5., 10.], [6., 8.], [7., 6.], [8., 4.], [9., 2.], [10., 0.]] # initial location initial_xy = [4., 12.] dt = 0.1 x = matrix([[initial_xy[0]], [initial_xy[1]], [0.], [0.]]) # initial state (location and velocity) u = matrix([[0.], [0.], [0.], [0.]]) # external motion # initial uncertainty: 0 for positions x and y, 1000 for the two velocities # p = 0 0 0 0 # 0 0 0 0 # 0 0 1000 0 # 0 0 0 1000 p = matrix([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 1000., 0.], [0., 0., 0., 1000.]]) # next state function: generalize the 2d version to 4d # f = 1 0 0.1 0 # 0 1 0 0.1 # 0 0 1 0 # 0 0 0 1 # so, velocity vector (x', y') does not change, and the position vector (x, y) is updated according to the velocity and dt # f · x = x + 0.1x' # y + 0.1y' # x' # y' f = matrix([[1., 0., dt, 0], [0, 1., 0, dt], [0, 0, 1., 0], [0, 0, 0, 1.]]) # measurement function: reflect the fact that we observe x and y but not the two velocities # h = 1 0 0 0 # 0 1 0 0 # so, for the measurement we only take into account the position vector (x,y) and not the velocity # z = h · x = x # y h = matrix([[1., 0., 0., 0.], [0., 1., 0., 0.]]) # measurement uncertainty: use 2x2 matrix with 0.1 as main diagonal # r = 0.1 0 # 0 0.1 r = matrix([[.1, 0.], [0., .1]]) # 4d identity matrix # i = 1 0 0 0 # 0 1 0 0 # 0 0 1 0 # 0 0 0 1 i = matrix([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) and we execute the filter: filter(x, p) to obtain the following state \\(x\\) and uncertainty matrix \\(p\\): x= [9.999340731787717] [0.001318536424568617] [9.998901219646193] [-19.997802439292386] p= [0.03955609273706198, 0.0, 0.06592682122843721, 0.0] [0.0, 0.03955609273706198, 0.0, 0.06592682122843721] [0.06592682122843718, 0.0, 0.10987803538073201, 0.0] [0.0, 0.06592682122843718, 0.0, 0.10987803538073201] $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/KalmanFilters/Kalman Filters.html",
    "title": "Kalman Filters",
    "body": " index search search back kalman filters markov model gaussian distribution measurement and motion motion step measurement step states predicting velocity high dimensional spaces put everything together algorithm this is a tracking technique. it is similar to the histogram localization we talked about previously, however there are some key differences: kalman filter maintains a continuous state (therefore uses a uni-modal distribution: probability density function only has one peak) histogram localization uses discrete state to represent the world (uses a multi-modal distribution: probability density function has multiple peaks) markov model in histogram localization we assigned a probability to each cell in the world: [\\(0.2\\)][\\(0.1\\)][\\(0.5\\)][\\(0.1\\)][\\(0.2\\)] what we did is we divided the continuous space into a finite number of cells, that approximates the posterior distribution (which is continuous: red line) by a histogram (blue bars) over the original distribution. however in kalman filters this distribution is given by a gaussian distribution. gaussian distribution a gaussian distribution is a continuous function which is described in \\(\\mathbb{r}\\) by the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). the formula is the following: \\begin{align} f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp^{-\\frac{1}{2}\\frac{(x-\\mu)}{\\sigma^2}} \\end{align} where \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) is a constant that normalizes \\(\\exp^{-\\frac{1}{2}\\frac{(x-\\mu)}{\\sigma^2}}\\) remark the bigger the covariance \\(\\sigma^2\\) the wider the distribution, which means we are less certain of the state. if it is narrow, that means we are confident about our location. measurement and motion like with localization kalman filters operate within a cycle, that is, it iterates like so: motion (or prediction): where we predict the position of the car based on data we know. to perform this prediction we sum the location distribution and the distribution that describes the motion. measurement (or measurement update/correction): where we combine the prediction with the measurement made by the sensors. to perform this update we multiply the motion prediction with the distribution that describes the measurement. this is illustrated in the following image: here the predicted state estimate corresponds to the motion step, and the measurement distribution corresponds to the measurement. the result of the product between the two eausl the optimal state estimate. motion step suppose, at moment \\(t\\), your location is represented as follows: where: the blue gaussian distribution represents your best guess (prediction) at where your are at \\(t\\), and is characterized by \\((\\mu, \\sigma^2)\\) the green gaussian distribution represents the motion of \\(\\nu\\) units, which has its own uncertainty, and is characterized by \\((\\nu, r^2)\\) the red gaussian distribution represents you location at time \\(t+1\\) after the motion where this last distribution equals the sum of the other two distributions and is characterized by: \\begin{align} \\hat{\\mu} = \\mu + \\nu \\end{align} \\begin{align} \\hat{\\sigma}^2 = \\sigma^2 + r^2 \\end{align} so, basically the mean is shifted \\(\\nu\\) units and the covariance is made larger by summing \\(\\sigma^2\\) and \\(r^2\\), as a result of summing the distributions. motion noise note that the movement may not be certain, that is why we introduce some gaussian noise. this noise is drawn from a normal distribution where the variance is given by a co-variance matrix \\(q\\) (this matrix describes the uncertainty for the initial state). we define this noise as follows: \\begin{align} u \\sim \\mathcal{n}(0, q) \\end{align} measurement step as we have already said, the update is the result of multiplying the prediction distribution (after motion step), which was characterized in the previous section, by the measurement distribution. this would mean \"creating\" a new distribution that models the robot's current state. we now show how the multiplication of two distributions is performed: updating the mean we are going to show how to the mean is computed when multiplying two distributions. suppose the prior distribution is as follows: where the covariance is very large, so we are very uncertain about a location. and we recieve a measurement of the form: which is much more certain about the location. then the mean will shift accordingly (green line): updating the variance we are going to show how to the variance is computed when multiplying two distributions. so, after multiplying the prior and the measurement shown previously, the resulting gaussian y more certain than both of the prior and the measurement gaussians. that is the covariance of this new gaussian is smaller, so the more measurements we have the more certain the are. why does this happen? well, given these two distributions: where the first distribution is characterized by \\((\\mu, \\sigma^2)\\) and the second distribution is characterized by \\((\\nu, r^2)\\). the product of the two is a distribution characterized by \\((\\hat{\\mu}, \\hat{\\sigma}^2)\\), computed as follows: \\begin{align} \\hat{\\mu} = \\frac{r^2\\mu + \\sigma^2\\nu}{r^2 + \\sigma^2} \\end{align} observe, because \\(\\sigma^2 >> r^2\\) in our example, then \\(\\hat{\\mu}\\) will be closer to the second distribution's mean \\(\\nu\\). also: \\begin{align} \\hat{\\sigma}^2 = \\frac{1}{\\frac{1}{r^2} + \\frac{1}{\\sigma^2}} = \\frac{\\sigma^2 r^2}{\\sigma^2 + r^2} \\end{align} thus, the updated covariance is not affected by the means and will always be smaller than \\(\\sigma^2\\) and \\(r^2\\). we illustrate this is the following image, where the updated distribution is the one drawn in blue: note that the wider distribution represents the prior, the measurement represents the likelihood and the updated distribution represents the posterior. measurement noise however, note that the measurement might also be noisy. so we again introduce gaussian noise \\(v\\) that is modeled after a normal distribution with known variance. that is \\(v \\sim \\mathcal{n}(0, r)\\). this indicates how much we trust the measurements provided by the sensors. this variable is called measurement noise covariance matrix states kalman filters are made up from what it's called states, and we differentiate two different kinds of states: observables (in our case the location) hidden (in our case the velocity, which i can never observe) these two types of states interact with each other in the sense that a sequence of observable variables gives us information about the hidden variables. thus we can estimate what these hidden variables are. applied to our case scenario, multiple observations of where we are, that is, our location, we can estimate how fast we are moving, that is, our velocity. predicting velocity given the following graph: where \\(\\hat{x}\\) represents the velocity and \\(x\\) represents the location. in this first instance, we represent the measurement at with an elongated gaussian because the measurement does not tell us anything about the velocity. however, if we now draw our predicition, given by our motion model which is represented by the red gaussian distribution, we obtain: suppose we take a new measurement (a second observation) represeted by the green normal distribution (remember, it tells us nothing about the velocity), it only gives us information about the location as the first observation did. then: multiply the prior (the red gaussian) and the measurement (the green gaussian) to obtain a really good estimate of an object's velocity and location (black distribution): so we were able to infer the velocity by only observing the location. high dimensional spaces up until now we have generally been operating in a one dimensional space, however if we were to work withing higher dimensional spaces we would need to make use of multivariate gaussians. so a multivariate gaussian in a d-dimensional space is characterized as follows: \\begin{align} \\mu = \\begin{bmatrix} \\mu_0 \\\\ \\vdots \\\\ \\mu_d \\\\ \\end{bmatrix}, \\sigma = \\begin{bmatrix} \\sigma_{11} & \\cdots & \\sigma_{1d}\\\\ \\vdots \\\\ \\sigma_{d1} & \\cdots & \\sigma_{dd}\\\\ \\end{bmatrix} \\end{align} also de density function is now, for \\(x \\in \\mathbb{r}^d\\): \\begin{align} f(x) = (2\\pi)^{-\\frac{d}{2}}|\\sigma|^{-\\frac{1}{2}} \\exp^{-\\frac{1}{2}(x - \\mu)^t\\sigma^{-1}(x-\\mu)} \\end{align} here are some examples of how the kalman filter works for spaces with higher dimension: put everything together motion at a given time \\(k-1\\), we have the following prediction: we use a motion model (in our case a gaussian that represents the movement) to update our prediction as follows: the motion model is described as follows: \\begin{align} x_k = f_{k-1}x_{k-1} + u_{k-1} \\end{align} where: \\(f_{k-1}\\) represents the transition function at time \\(k-1\\) \\(u_{k-1}\\) represents the noise at time \\(k-1\\) measurement then, we use the following observation model: we correct our prediction with this observation model as follows: the measurement model is described as follows: \\begin{align} y_k = h_k x_k + v_k \\end{align} where: \\(h_k\\) is the measurement function at time \\(k\\). this function maps the state into the observable state, that does not have to be the same (refer to states) \\(v_k\\) is the noise at time \\(k\\) iterative process first we make a prediction as to where the robot is at time \\(k\\): \\begin{align} \\check{x}_k = f_{k-1}x_{k-1} \\end{align} \\begin{align} \\check{p}_k = f_{k-1}\\hat{p}_{k-1}f_{k-1}^t + q_{k-1} \\end{align} then we compute the optimal gain \\(k\\) as follows: \\begin{align} k_k = \\check{p}_kh_k^t(h_k\\check{p}_kh^t+r_k)^{-1} \\end{align} this gain basically represents how much we trust our motion estimation versus our measurement estimation. finally we obtain the correction using the measurement model: shift the mean: \\begin{align} \\hat{x}_k = \\check{x}_k + k_k(y_k - h_k\\check{x}_k) \\end{align} where \\(y_k - h_k\\check{x}_k\\) represents the difference between the measurement and the prediction we made. lastly, we update the covariance of our motion model: update the variance: \\begin{align} \\hat{p}_k = (1-k_kh_k)\\check{p}_k \\end{align} algorithm so in the kalman filter cycle what we do is: first we perform the prediction and the correction or measurement update. more concretely: prediction we apply the same formulas we defined in motion model and iterative process to make a prediction: \\begin{align} x = fx + u \\end{align} \\begin{align} p = f \\cdot p \\cdot f^t \\end{align} measurement update now, for the correction: first we compute the intermediate \\(s\\) matrix, which equals the second part of the formula for \\(k\\), \\(h_k\\check{p}_kh^t+r_k\\): \\begin{align} s = h \\cdot p \\cdot h^t + r \\end{align} then we compute the kalman gain (as we defined in iterative process): \\begin{align} k = ph^ts^{-1} \\end{align} obtain difference between measurement (\\(z = y_k\\)) and our prediction \\(h \\cdot x = h_k\\check{x}_k\\) (note where \\(\\check{x}_k\\) comes from, \\(h\\) is usually a matrix that selects a concrete part of the kalman state like the position. see examples): \\begin{align} y = z - h \\cdot x \\end{align} finally obtain the correction: \\begin{align} x' = x + (k \\cdot y) \\end{align} \\begin{align} p' = (i- k\\cdot h) \\cdot p \\end{align} code the filter algorithm follows the same steps laid out in the previous section: def filter(x, p): for n in range(len(measurements)): # prediction x = (f * x) + u p = f * p * f.transpose() # measurement update z = matrix([measurements[n]]) y = z.transpose() - (h * x) s = h * p * h.transpose() + r k = p * h.transpose() * s.inverse() x = x + (k * y) p = (i - (k * h)) * p return x, p $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/(DS0002)_AIRobotics/SLAM/SLAM.html",
    "title": "SLAM",
    "body": " index search search back slam how to create maps and localize a robot at the same time? for this we use a technique known as slam: simultaneous localization and mapping graph slam suppose we have a robot whose initial position is \\(x_0 = 0\\) and \\(y_0 = 0\\) at time \\(0\\), then at time \\(1\\) (because of how we mode our motion) the robot is at \\(x_1 = x_0 + 10\\) and \\(y_1 = y_0\\). however we know that our location is uncertain therefore the position at time \\(1\\) is really described by a gaussian distribution centered around \\(10\\) and with a given variance that signifies how certain we are about our position. so to express this with a gaussian, that we do is define a distribution whose pdf peaks when \\(x_1 = x_0 + 10\\) and \\(y_1 = y_0\\), therefore we would like to maximize both the following equations: \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\end{align} \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(y_1 - y_0)^2}{\\sigma^2}} \\end{align} here if \\(x_1 = x_0 + 10\\), then \\(x_1 - x_0 - 10 = 0\\) and if \\(y_1 = y_0\\) ,then \\(y_1 - y_0 = 0\\). these conditions we define are called constraints, so what graph slam does is creating our probabilities defining a sequence of these constraints. suppose we have a robot that has followed the following path: where each \\(x_i\\) is a vector (usually a three dimensional vector) then graph slam defines the following constraints: initial position constraint: \\(x_0\\) relative motion constraints: \\((x-1 - x_0)\\), \\((x-2 - x_1)\\), \\((x-3 - x_2)\\) (indicated by the red lines). ideally these are the same as the robot motion (direction vector), however in reality it tends to bend to accommodate the map. relative measurement constraints: these are the segment between each position vector (not necessarily every position vector) and each landmark defined in the map, and are also captured by gaussian distributions. in our case \\(z_0, z_1, z_2, z_3\\), the lines colored in green. after we have collected these constraints, what the algorithm does is it relaxes the position vectors \\(x_i\\) to find the most likely configuration of robot path for the given landmarks (that is measurements of distance to the landmark). constraint matrix to define our constraints, suppose we have 3 position vectors \\(x_0, x_1, x_2\\) and two landmarks \\(l_0, l_1\\), then we define the following matrix: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} we denote this structure as follows: \\begin{align} \\omega = \\begin{bmatrix} 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\end{align} and: \\begin{align} \\xi = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix} \\end{align} suppose \\(x_0\\) moves to \\(x_1\\) by moving \\(5\\) units to the right, that is \\(x_1 = x_0 + 5\\), then we define this constrain in the matrix as follows: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 5.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} because given the initial constraint \\(x_1 = x_0 + 5\\), if we move around the \\(x_i\\) we get: \\begin{align} x_0 - x_1 = -5 \\end{align} \\begin{align} x_1 - x_0 = 5 \\end{align} now we add another constraint \\(x2 = x_1 - 4\\), therefore: \\begin{align} x_2 - x_1 = -4 \\end{align} \\begin{align} x_1 - x_2 = 4 \\end{align} so the constraint matrix is updated to: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 2.0 & -1.0 & 0.0 & 0.0 \\\\ 0.0 & -1.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 9.0 \\\\ -4.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} we add a relative measurement constraint like \\(l_0 - x_1 = 9\\), therefore: \\begin{align} l_0 - x_1 = 9 \\end{align} \\begin{align} x_1 - l_0 = -9 \\end{align} so the constraint matrix is updated to: \\begin{align} \\begin{array}{c} \\begin{array} - & x_0 & x_1 & x_2 & l_0 & l_1 \\end{array}\\\\[5pt] \\begin{matrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ l_0 \\\\ l_1 \\end{matrix}\\quad \\begin{bmatrix} 1.0 & -1.0 & 0.0 & 0.0 & 0.0 \\\\ -1.0 & 3.0 & -1.0 & -1.0 & 0.0 \\\\ 0.0 & -1.0 & 1.0 & 0.0 & 0.0 \\\\ 0.0 & -1.0 & 0.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\end{bmatrix} \\begin{bmatrix} -5.0 \\\\ 0.0 \\\\ -4.0 \\\\ 9.0 \\\\ 0.0 \\end{bmatrix}\\quad \\end{array} \\end{align} note that whenever we add a constraint to two parameters, let's say \\(x_1\\) and \\(x_2\\), we sum one to the diagonal element of the matrix corresponding to \\(x_1\\) and \\(x_2\\). noise given the following motion: we know that the localization of our robot is not an exact value, but is is modeled after a gaussian distribution, so \\(x_1 \\sim \\mathcal{n}(\\mu_{x_1}, \\sigma_{x_1})\\) and \\(x_2 \\sim \\mathcal{n}(\\mu_{x_2}, \\sigma_{x_2})\\). suppose \\(\\sigma_{x_1} = \\sigma_{x_2}\\). then we want to maximize the expected value, which is given by the expressions: \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\end{align} \\begin{align} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} to maximize both expressions means to maximize their product: \\begin{align} \\max_{x_0, x_1, x_2} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} we also know that constants are irrelevant during maximization: \\begin{align} \\max_{x_0, x_1, x_2} \\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}} \\end{align} this maximization is equivalent to the maximization of its logarithm: \\begin{align} \\max_{x_0, x_1, x_2} \\log \\left(\\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}} \\cdot \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}}\\right) \\end{align} because the logarithm of a product equal the sum of logarithms: \\begin{align} \\max_{x_0, x_1, x_2} \\left(\\log \\exp{-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}}\\right) + \\left(\\log \\exp{-\\frac{1}{2}\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}}\\right) \\end{align} given \\(\\log \\exp (x) = x\\): \\begin{align} \\max_{x_0, x_1, x_2} \\left(-\\frac{1}{2}\\frac{(x_1-x_0-10)^2}{\\sigma^2}\\right) + \\left(-\\frac{1}{2}\\frac{(x_2 - x_1 -50)^2}{\\sigma^2}\\right) \\end{align} again, constants are irrelevant: \\begin{align} \\max_{x_0, x_1, x_2} \\left(\\frac{(x_1-x_0-10)^2}{\\sigma^2}\\right) + \\left(\\frac{(x_2 - x_1 -5)^2}{\\sigma^2}\\right) \\end{align} so, we end up with equations of the form: \\begin{align} \\frac{1}{\\sigma} x_1 - \\frac{1}{\\sigma} x_0 = \\frac{10}{\\sigma} \\end{align} where now \\(\\sigma\\) symbolizes how confident you are in your location/measurement. usually we define a \\(\\sigma\\) for the location and another \\(\\sigma_{measurement}\\) for the measurement (distance to the landmark). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/06. Visualize/index.html",
    "title": "Share Data Through the Art of Visualization",
    "body": " index search search back share data through the art of visualization first week > 11/04/2022 - 11/04/2022 communicating your data insights understand data visualization design data visualizations explore visualization considerations second week > 11/04/2022 - 12/04/2022 $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/learning-log.html",
    "title": "Learning Log",
    "body": " index search search back learning log consider what data means to you 07/04/2022 what does data mean to you? now, i usually picture data like a very big table full of attributes and entries. when you come across a problem and you aren’t sure of the answer or solution, what do you do? i tend to search in google for the answer of a problem how do you identify new and interesting problems to begin with? is there a process you use to identify problems you want to solve? generally, the problems come from me inserting myself into new technologies. this way i am more likely to come up with a new \"problem\" in the form of a project, for me to develop. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Craft effective questions.html",
    "title": "Craft effective questions",
    "body": " index search search back craft effective questions ineffective questions: lead question: it leads you to answer in a certain way. close ended: it can be answer with yes or no. too vague: is too broad an lacks content. effective questions follow the smart methodology: specific: simple and focuses on the problem. measurable: can be quantified and assessed. action-oriented: encourage change. relevant: have significance to the problem at hand. time-bound: specify the time to be studied. also question must assure fairness, that is to make sure they do not reinforce bias. question 1 you are three weeks into your new job as a junior data analyst. the company you work for has just collected data on their weekend sales. your manager asks you to perform a “deep dive” into this data. based on the smart framework, which questions are most important to ask? what is the variation in number of sales between this week and the week before. what products are the ones that are most often purchased. what common characteristics do these products have in common. which kind of costumers purchase more and most often. at what time of the week are the sales higher. how will these questions clarify the requirements and goals for the project? it will allow us to detect if there was a change in the number of sales, and so to investigate as to why it will guide to explore which products to explore it will allows us to know better our costumers how does asking detailed, specific questions benefit you when planning for a project? can vague or unclear questions harm a project? it can be used to lay out clearly what our objectives are, without leading to bias. yes, then can introduce misunderstandings between the clients and the analysts. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Functions in spreadsheets.html",
    "title": "Functions in spreadsheets",
    "body": " index search search back functions in spreadsheets in this activity, you will import a dataset, build a custom data table, and use functions to analyze your data. for this activity, imagine you're a data analyst working for a recruiting agency. this recruiting agency helps all sorts of companies find skilled people to fill open data analytics jobs. the agency has collected data about job applications for opportunities posted on its website for the year 2019. job applications spreadsheet $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Understand the power of data.html",
    "title": "Understand the power of data",
    "body": " index search search back understand the power of data data-inspired decision making: explores different data sources to find out what they have in common data-driven decision making: note that there is a difference between making a decision with incomplete data and making a decision with a small amount of data. you learned that making a decision with incomplete data is dangerous. but sometimes accurate data from a small test can help you make a good decision. quantitative data: measures of numerical facts. qualitative data: subjective measures of qualities and characteristics. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/index.html",
    "title": "Ask Questions to Make Data-Driven Decisions",
    "body": " index search search back ask questions to make data-driven decisions learning log first week > 07/04/2022 - 08/04/2022 problem-solving and effective questioning take action with data solve problems with data craft effective questions weekly challenge second week > 08/04/2022 - 09/04/2022 understand the power of data follow the evidence connecting the data dots weekly challenge third week > 09/04/2022 - 09/04/2022 working with spreadsheets formulas in spreadsheets functions in spreadsheets save time with structured thinking weekly challenge fourth week > 09/04/2022 - 09/04/2022 balance team and stakeholder needs communication is key amazing teamwork weekly challenge course challenge $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Follow the evidence.html",
    "title": "Follow the evidence",
    "body": " index search search back follow the evidence let's see two kinds of visual representation tools: reports: static collection of data given to stakeholders periodically. dashboards: monitors live incoming data. reports pros high level historical data easy to design pre-cleaned sorted data cons continual maintenance less visually appealing static dashboards pros dynamic, automatic, interactive more stakeholder access low maintenance cons labor-intensive design can be confusing potentially uncleaned data pivot table: data summarization tool used in data processing. they are used to summarize, sort, reorganize, group, count, total or average data stored in a database. metric: quantifiable type of data than can be used for measurement. types of dashboards: strategic: focuses on long term goals and strategies at the highest level of metrics operational: short-term performance tracking and intermediate goals analytical: consists of the datasets and the mathematics used in these sets $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Save time with structured thinking.html",
    "title": "Save time with structured thinking",
    "body": " index search search back save time with structured thinking problem domain: specific area of analysis that encompasses every activity affecting or affected by the problem scope of work: outline of the work you are going to perform in a project deliverables: what things are being created as a result of the project milestones timeline reports: how to give status updates to the stakeholders. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Working with spreadsheets.html",
    "title": "Working with spreadsheets",
    "body": " index search search back working with spreadsheets here are the core spreadsheets tasks: organize your data pivot table sort and filter calculate your data formulas functions $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Connecting the data dots.html",
    "title": "Connecting the Data Dots",
    "body": " index search search back connecting the data dots there are two types of data: small data: is specific, recorded in a short-time period and used to make day-to-day decisions. (analyze with spreadsheets) big data: is large and less specific, recorded in a long-time period and used to make big decisions. (analyze with spreadsheets) the three (or four) v words for big data volume: amount of data. variety: different kinds of data. velocity: how fast can it be processed. veracity: quality and reliability of data. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Problem-solving and effective questioning.html",
    "title": "Problem-solving and effective questioning",
    "body": " index search search back problem-solving and effective questioning structured thinking: process of: recognizing the current problem organizing the available information revealing gaps and opportunities identifying options $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Take action with data.html",
    "title": "Take action with data",
    "body": " index search search back take action with data ask define the problem you're trying to solve make sure you fully understand the stakeholder's expectations focus on the actual problem and avoid any distractions collaborate with stakeholders and keep an open line of communication take a step back and see the whole situation in context prepare you will decide what data you need to collect in order to answer your questions and how to organize it. what metrics to measure locate data in your database create security measures to protect that data process you will need to clean up your data to get rid of any possible errors, inaccuracies, or inconsistencies. using spreadsheet functions to find incorrectly entered data using sql functions to check for extra spaces removing repeated entries checking as much as possible for bias in the data analyze now you may need to think analytically about your data. you might sort and format your data to make it easier to: perform calculations combine data from multiple sources create tables with your results share summarize your results with clear and enticing visuals of your analysis using data via tools like graphs or dashboards. act now it’s time to act on your data. you will take everything you have learned from your data analysis and put it to use. this could mean providing your stakeholders with recommendations based on your findings so they can make data-driven decisions. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/02. Ask/Solve problems with data.html",
    "title": "Solve problems with data",
    "body": " index search search back solve problems with data data analysts typically work with six problem types making predictions: using data to make an informed decision about how things might be in the future categorizing things: assigning information to different groups or clusters based on common features spotting something unusual: identifying data that is different from the norm identifying themes: grouping categorized information into broader concepts discovering connections: finding similar challenges faced by different entities and combining data and insights to address them finding patterns: using historical data to understand what happened in the past and is therefore likely to happen again $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/index.html",
    "title": "Google Data Analyst Certificate",
    "body": " index search search back google data analyst certificate > 28/03/2022 - foundations: data, data, everywhere ask questions to make data-driven decisions prepare data for exploration process data from dirty to clean analyze data to answer questions share data through the art of visualization data analysis with r programming google data analytics capstone_ complete a case study $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/learning-log.html",
    "title": "Learning Log",
    "body": " index search search back learning log think about data in daily life 01/04/2022 instructions you can use this document as a template for the learning log activity: think about data in daily life. we recommend that you save every learning log in one folder and include a date in the file name to help you stay organized. important information like course number, title, and activity name are already included. after you finish your learning log entry, you can come back and reread your responses later to understand how your opinions on different topics may have changed throughout the courses. create a list of at least five questions what areas of a city have more pedestrians at what point in their life do people consume less which places are more likely to return an amazon package which kind of films tend to get worse/better reviews on letterbox how many women are in stem at what time in the day do i feel more energized how many words do i read every day how does the number of books a read vary throughout the year what type of movie do people tend to enjoy how many hours am i productive reflection write 2-3 sentences (40-60 words) in response to each of the questions below what are some considerations or preferences you want to keep in mind when making a decision? first of all you have to look at the problem objectively, without any bias towards the probable solution, as to avoid jumping to less efficient solutions. also you may want to summarize each solution/option to its core idea, so it will be easier to pick up the advantages and disadvantages what kind of information or data do you have access to that will influence your decision? i may have a journal where i introduce the number of books a read (as in i registry each book i have read as an entry) are there any other things you might want to track associated with this decision? other characteristics of the books, other than the time when they were finished may help analyze a trend: the time it took for me to read it, the subject of the book, the language of the book, etc. consider how data analysts approach tasks 01/04/2022 instructions you can use this document as a template for the learning log activity: consider how data analysts approach tasks. we recommend that you save every learning log in one folder and include a date in the file name to help you stay organized. important information like course number, title, and activity name are already included. after you finish your learning log entry, you can come back and reread your responses later to understand how your opinions on different topics may have changed throughout the courses. review the 6 phases of data analysis consider how the data analysts at google used the data analysis process to break down their analysis project: the analysts asked questions to define both the issue to be solved and what would equal a successful result. next, they prepared by building a timeline and collecting data with employee surveys, which should be inclusive. they processed the data by cleaning it to make sure it was complete, correct, relevant, and free of errors and outliers. they analyzed the clean employee survey data. then the analysts shared their findings and recommendations with team leaders. afterward, leadership acted on the results and focused on improving key areas. reflection write 2-3 sentences in response to each of the questions below did the details of the case study help to change the way you think about data analysis? why or why not?? even though i am familiar with the process and steps one has to follow to perform data analysis, i did not really think about the asking part of it, or the showing your results in a way everybody can understand. did you find anything surprising about the way the data analysts approached their task? not surprising, every step that was laid out seems to make sense. also coming from a cs background all the workflow of asking questions to define the needs of the client is very similar. what else would you like to learn about data analysis? i would like to know how to extract/find trends in data, be able to turn a heap of information to something useful explore data from your daily life 02/04/2022 instructions create a list exploring an area of your daily life and include details, such as the date, time, cost, quantity, size, etc: 28/03 - slept 6 hours, moderately well rested 29/03 - slept 7 hours, slightly tired 30/03 - slept 5 hours, very tired 31/03 - slept 4.5 hours, very tired 01/04 - slept 7 hours, tired reflection are there any trends you noticed in your behavior? whenever something stressful comes up i get very little sleep. also because i, one, finished a math book (so i searched a for a new resource to learn) and, two, got a new job (the resource i chose was this one) i suddenly was trying to do everything in a frenzy, and so stayed up until way later than usual. are there factors that influence your decision-making? i am used to having 2 to 3 hours free before going to sleep, so if i do not manage to have enough time to have that amount of hours, i will just stay up until later, and kind of make the free time even longer because of the anxiety of knowing i should go to sleep. is there anything you identified that might influence your future behavior? i will sleep better if i can manage better my time (not cram as many tasks a day) and reduce my anxiety whenever a slight change occurs in my life. reflect on your skills and expectations 02/04/2022 instructions the table has a row for each essential aspect of analytical skills: curiosity: a desire to know more about something, asking the right questions understanding context: understanding where information fits into the “big picture” having a technical mindset: breaking big things into smaller steps data design: thinking about how to organize data and information data strategy: thinking about the people, processes, and tools used in data analysis you will put an x in the column that you think best describes your current level with each aspect. the three ratings are: strength: this is an area you feel is one of your strengths developing: you have some experience with this area, but there’s still significant room for growth emerging: this is new to you, and will gain experience in this area from this course then update the comments/plans/goals column with a quick note to yourself about why you chose those ratings. complete the analytical skills table analytical skills strength developing emerging comments/plans/goals curiosity     x i am usually not very curious, i more like to have the knowledge than explore the unknown context   x   i am pretty good at spotting trends technical mindset x     coming from a cs background i am very used to try to resolve different types of problems data design x     for the same reason as above, i am familiar with having to chose a data structure/design that fits well your needs data strategy     x i am not used to having to deal with the people or timeline of a project reflection what do you notice about the ratings you gave yourself in each area? how did you rate yourself in the areas that appeal to you most? generally speaking i am most proficient at the one that i have been trained academically in, and are more technical. the ones that require soft skills, i am worse at. if you are asked to rate your experience level in these areas again in a week, what do you think the ratings will be, and why do you think that? i think most of them will stay the same, but maybe i will become better in data strategy with the help of this course. how do you plan on developing these skills from now on? well curiosity wise, i think the best way for me to go forward is to start projects of my own to see all the caveats a data analysis project can entail. and when it comes to data strategy, my way of improving would be to understand better how to manage a project and also the different tools available to manage data. organize your data in a table introduction you have been collecting data from the beginning of the course. take a moment to consider the data you have gathered in your learning log. now, determine how you could organize your data in a table. structuring your data date time asleep rest level 28/03 6h moderately well rested 29/03 7h slightly tired 30/03 5h very tired 31/03 4.5h very tired 01/04 7h tired reflection in a new learning log entry, follow the instructions in the template, and add a table to organize your data. then, write 3-5 sentences (60-100 words) on opportunities in your personal life or current job to organize data into tables. i could gather data about how many hours a day i am productive at my job how many time i spend logging my work, instead of working generating a chart from a spreadsheet introduction so far, you have planned a project, identified the data you need, and collected the data. earlier in this course, you completed a learning log where you recorded some data from your daily life, then took the practical step of organizing it. now, you’re ready for the most satisfying step of the data analysis project: visualizing your data! for this activity, you will move your data to a spreadsheet and bring it to life in a chart. chart sleep data reflect on the data analysis process reflect which part(s) of the data analysis process did you enjoy the most? what did you enjoy about it? i enjoyed both the process, analysis and share steps because i feel it was the more interactive (as asking questions to oneself is not that fun), and allowed me to use the different tools discussed during the course. what were some of the key ideas you learned in this course? the importance of following through each step to obtain a thorough and clear insight on the data you are analyzing. and also, the need of taking into account what the stakeholder hopes for when analyzing the data. are there concepts or portions of the content that you would like to learn more about? if so, what are they? which upcoming course do you think would teach you the most about this area? i am so very excited about the visualization part of the data analysis. because i have never dug deep enough into it, as i may have into the analysis or process step. now that you've gained experience doing data analysis, how do you feel about becoming a data analyst? have your feelings changed since you began this course? if so, how? well, i am mostly doing this course for fun. but sure being a data analyst still sounds exciting enough $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Optional: Exploring your next job.html",
    "title": "Data analyst roles and job descriptions",
    "body": " index search search back data analyst roles and job descriptions data analysts, data scientists, and data specialists sound very similar but focus on different tasks.the table below illustrates some of the overlap and distinctions between them: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Outlining the data analysis.html",
    "title": "Outlining the data analysis process",
    "body": " index search search back outlining the data analysis process stakeholder: people who have invested time and resources into a project and are interested in the outcome $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Transforming Data into insights.html",
    "title": "Transforming Data into insights",
    "body": " index search search back transforming data into insights the six steps of the data analysis process that you have been learning in this program are: ask, prepare, process, analyze, share, and act. ask: ask effective questions to define the project and what is the desired result prepare: build a timeline, decide how to gather data, how and when to communicate with your client process: clean the data, make sure it is complete, correct, relevant free of errors and outliers analyze: analyze the gathered data to search for solutions, findings, etc. share: know how to show and display your findings to your team leaders act: decide how to best implement changes and take action $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Mastering spreadsheets basics.html",
    "title": "Mastering spreadsheets basics",
    "body": " index search search back mastering spreadsheets basics attribute: characteristic or quality of data used to label a column in a table. observation: all the attributed for something contained in a row of a data table. formula: set of instructions that performs a specific action using the data in a spreadsheet. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/The importance of fair business decisions.html",
    "title": "The importance of fair business decisions",
    "body": " index search search back the importance of fair business decisions issue: topic or subject to investigate question: designed to discover information problem: obstacle or complication that needs to be worked out. business task: question or problem data analysis answers for a business $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Thinking about analytical thinking.html",
    "title": "Thinking about Analytical Thinking",
    "body": " index search search back thinking about analytical thinking what is analytical thinking? is identifying and defining a problem and then solving it by using data in an organized, step-by-step manner. the five key aspects to analytical thinking are: visualization: is the graphical representation of information (helps data analysts understand and explain information more effectively) strategy: it helps data analysts see what they want to achieve with the data and how to get there. it allows them to stay focused and on track. problem-orientation: is about keeping the problem as the priority throughout the entire project. correlation: is about being able to pick up and define relations in data. (correlation does not equal causation) big-picture and detail-oriented thinking: is being able to see the big picture as well as the details of a problem questions frequently asked by data analysts what is the root cause of a problem? where are the gaps in our process? what did we no consider before? use the rule of the five \"why's\" to reveal the root cause. this rule consists of asking five times why, once each time an answer is provided. to identify gaps in a process the technique gap analysis is used. it is a method for examining and evaluating how a process works currently in order to get where you want to be in the future. the steps are: identify where you are now and where you want to be in the future define the gaps between the two stages determine how to bridge them $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/index.html",
    "title": "Foundations: Data, Data, Everywhere",
    "body": " index search search back foundations: data, data, everywhere learning log first week > 28/03/2022 - 01/04/2022 get started transforming data into insights understanding the data ecosystem program expectations weekly challenge 1 second week > 02/04/2022 - 03/04/2022 embrace your data analyst skills thinking about analytical thinking thinking about outcomes weekly challenge 2 third week > 04/04/2022 - 05/04/2022 follow the data life cycle outlining the data analysis the data analysis toolbox weekly challenge 3 fourth week > 06/04/2022 - 06/04/2022 mastering spreadsheets basics structured query language data visualization weekly challenge 4 fifth week > 06/04/2022 - 07/04/2022 data analyst job opportunities the importance of fair business decisions optional: exploring your next job weekly challenge 5 course challenge $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Follow the data life cycle.html",
    "title": "Follow the data life cycle",
    "body": " index search search back follow the data life cycle the life cycle of data is plan: during planning a business decides: what data is needed how to manage the data throughout its life cycle who is responsible for the data what is the desired outcome capture: this is when data is collected from different sources and brought into the company. manage: refers to how we store the data, the tools we use to keep it secure, and the actions taken to make sure it is maintained properly. analyze: use the data to solve problems, make good decisions, and reach the company's goals. archive: means storing data in a place where it is still available, but it may not be used again. destroy: remove data from storage and delete any shared copies of the data. a database is a collection of data stored in a computer system. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Structured Query Language.html",
    "title": "Structured Query Language",
    "body": " index search search back structured query language query: a request for data or information from a database. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Embrace your data analyst skills.html",
    "title": "Embrace your data analyst skills",
    "body": " index search search back embrace your data analyst skills the five key data analyst skills are curiosity understanding context: is the condition in which something exists or happens, could be a structure or environment. having a technical mindset: is the ability to break things down into smaller steps or pieces and work with them in an orderly and logical way data design: is how to organize information (typically has to do with databases) data strategy: is the management of the people: they know how to use the right data to find solutions on the problem you are working on the processes: the path to the solution is clear and accessible the tools: make sure the right technology is being used for the job. analytical skills: are the qualities and characteristics associated with solving problems using facts $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/The data analysis toolbox.html",
    "title": "The data analysis toolbox",
    "body": " index search search back the data analysis toolbox tools data analysts use: spreadsheets query languages for databases: computer programming language that allows you to retrieve and manipulate data from a database (structured query language or sql) visualization tools $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/01. Foundations/Understanding the data ecosystem.html",
    "title": "Understanding the data ecosystem",
    "body": " index search search back understanding the data ecosystem data is a collection of facts data ecosystems are made up of elements that interact with one another in order to procude manage store organize analyze and share data. data analysis is the collection, transformation and organization of data in order to draw conclusions, make predictions, and drive informed decision-making data analytics is the science of data data-driven-decision-making is using facts to guide business strategy data science is a field of study that uses raw data to create new ways of modelling and understanding the unkown dataset is a collection of data that can be manipulated or analyzed as a unit subject matter experts: people that are familiar with the bussiness problem “how do i define success for this project?” ask yourserlf: what kind of results are needed? who will be informed? am i answering the question being asked? how quickly does a decision need to be made? data analysis life cycle ask: business challenge/objective/question prepare: data generation, collection, storage, and data management process: data cleaning/data integrity analyze: data exploration, visualization, and analysis share: communicating and interpreting results act: putting your insights to work to solve the problem $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Data ethics and privacy.html",
    "title": "Data Ethics and Privacy",
    "body": " index search search back data ethics and privacy data ethics: standards of right and wrong about how data is collected, shared and used aspects of data ethics: ownership: individuals own the raw data they provide and they have control over the usage, processing and how it is shared. transaction transparency: all data-processing activities and algorithms should be completely explainable and understood by the individual who provides their data. consent: individual's right to know how and why their data will be used before agreeing to provide it. currency: individuals should be aware of financial transactions resulting from the use of their personal data. privacy: preserving a data subject's information and activity any time a data transaction occurs. openness: free access usage and sharing of data. personally identifiable information , or pii , is information that can be used by itself or with other data to track down a person's identity. data anonymization is the process of protecting people's private or sensitive data by eliminating that kind of information. open data standards: availability and access: the data is available as a whole reuse and redistribution: data is provided under terms that allow it to be reused and redistributed universal participation: everybody should be able to use the data $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Collecting Data.html",
    "title": "Collecting Data",
    "body": " index search search back collecting data these are the key data collection considerations: how the data will be collected determine the time frame existing data? choose the data sources decide what data to use new data? how much data to collected select the right data type type of data collected: first-party data: collected by an individual using their own resources second-party data: collected by a group directly from its audience and then sold third-part data: collected from outside sources who did not collect it directly population: all possible data values in a certain dataset sample: part of a population that is representative of the population $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Effectively organize your data.html",
    "title": "Effectively organize your data",
    "body": " index search search back effectively organize your data file naming do's: work out your conventions early align file naming with your team make sure file names are meaningful keep file names short and simple $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Working with large datasets in SQL.html",
    "title": "Working with Large Datasets in SQL",
    "body": " index search search back working with large datasets in sql big query $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Working with databases.html",
    "title": "Working with Databases",
    "body": " index search search back working with databases a relational database is a database that contains a series of tables that can be connected to show relationships. some tables don't require a primary key. for example, a revenue table can have multiple foreign keys and not have a primary key. a primary key may also be constructed using multiple columns of a table. this type of primary key is called a composite key. ice cream dataset analysis $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Securing data.html",
    "title": "Securing Data",
    "body": " index search search back securing data data security: protecting data from unauthorized access or corruption by adopting safety measures some security measures: encryption: uses a unique algorithm to alter data and make it unusable by users and applications that don’t know the algorithm. tokenization: replaces the data elements you want to protect with randomly generated data referred to as a “token”. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Managing data with metadata.html",
    "title": "Managing Data with Metadata",
    "body": " index search search back managing data with metadata metadata: is data about data. types of metadata: descriptive: describes a piece of data structural: describes how a piece of data is organized. administrative: indicates the technical source of a digital asset. metadata repository: database created to store metadata $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Explore data credibility.html",
    "title": "Explore Data Credibility",
    "body": " index search search back explore data credibility good data is roccc: reliable original: check the quality of the data with the original source comprehensive: contain all critical information needed to find a solution for a problem current cited: should be cited by trusted sources bad data is data that does not satisfies one or more of the characteristics listed above. some good sources for data are: vetted public datasets governmental agency data academic papers $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Unbiased and objective data.html",
    "title": "Unbiased and Objective Data",
    "body": " index search search back unbiased and objective data bias: preference in favor or against a person or thing data bias: type of error that systematically skews results in a certain direction unbiased sampling: when a sample is representative of the population being measured to avoid bias when sampling, make sure to take random samples and make visualizations comparing the population with your sample to check if the sample is representative. types of bias: sampling bias: when a sample is not representative of the population as a whole observer bias (experimenter bias/research bias): tendency for different people to observe things differently interpretation bias: tendency to always interpret ambiguous situations in a positive or negative way confirmation bias: tendency to search for or interpret information in a way that confirms pre-existing beliefs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/index.html",
    "title": "Prepare for Data Exploration",
    "body": " index search search back prepare for data exploration first week > 09/04/2022 - 09/04/2022 data exploration collecting data differentiate between data formats and structures explore data types and values weekly challenge second week > 10/04/2022 - 10/04/2022 unbiased and objective data explore data credibility data ethics and privacy understanding open data weekly challenge third week > 10/04/2022 - 10/04/2022 working with databases managing data with metadata accessing different data sources sorting and filtering working with large datasets in sql weekly challenge fourth week > 10/04/2022 - 10/04/2022 effectively organize your data securing data weekly challenge fifth week > 10/04/2022 - 10/04/2022 create or enhance our online presence build a data analytics network sixth week > 10/04/2022 - 10/04/2022 course challenge $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Sorting and filtering.html",
    "title": "Sorting and Filtering",
    "body": " index search search back sorting and filtering little exercise of sorting and filtering: student performance data $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Understanding open data.html",
    "title": "Understand Open Data",
    "body": " index search search back understand open data sites and resources for open data: u.s. government data site u.s. census bureau open data network google cloud public datasets dataset search $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/03. Prepare/Differentiate between data formats and structures.html",
    "title": "Differentiate between data formats and structures",
    "body": " index search search back differentiate between data formats and structures here are some types of data: discrete data: is counted and has a limited number of values continuous data: is measured and can have almost any numeric value - nominal data: type of qualitative data that is categorized without a set order ordinal data: type of qualitative data that with a set order or scale - internal data: data that lives within a company's own systems external data: data that lives and is generated outside of an organization - structured data: data organized in a certain format such as rows and columns unstructured data: data that is not organized in any easily identifiable manner data model: model that is used for organizing data elements and how they relate to one another. types of data modelling: conceptual data modelling: gives a high level view of the data structure logical data modelling: focuses on relationships, attributes and entities in the database. physical data modelling: shows how a database operates. (column names, table names, data types, etc) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/Convert and Format Data.html",
    "title": "Convert and Format Data",
    "body": " index search search back convert and format data practice concat function $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/VLOOKUP for Data Aggregation.html",
    "title": "VLOOKUP for Data Aggregation",
    "body": " index search search back vlookup for data aggregation data aggregation: process of gathering data from multiple sources in order to combine it into a single summarized collection practice vlookup function $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/index.html",
    "title": "Analyze Data to Answer Questions",
    "body": " index search search back analyze data to answer questions first week > 11/04/2022 - 11/04/2022 let's get organized data analysis basics organize data for analysis sort data in spreadsheets sort data using sql weekly challenge second week > 11/04/2022 - 11/04/2022 convert and format data combine multiple datasets get support during analysis weekly challenge third week > 11/04/2022 - 11/04/2022 vlookup for data aggregation use joins to aggregate data in sql work with subqueries weekly challenge fourth week > 11/04/2022 - 11/04/2022 get started with data calculations pivot pivot pivot learn more sql calculations the data validation process using sql with temporary tables weekly challenge course challenge $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/Data Analysis Basics.html",
    "title": "Data Analysis Basics",
    "body": " index search search back data analysis basics the four phases of analysis: organize data format and adjust data get input from others transform data $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/Use JOINs to Aggregate Data in SQL.html",
    "title": "Use JOINs to Aggregate Data in SQL",
    "body": " index search search back use joins to aggregate data in sql there are four general ways in which to conduct joins in sql queries: inner, left, right, and full outer. inner join or join inner join returns records if the data lives in both tables. left join left join returns all the records from the left table and only the matching records from the right table. the nulls are in the right table, because it returns all of the values in the left table with the joined values (if they exist) in the right table. it might be the case that there is no value in the right table linked to a record in the left, so it is null. right join right join returns all records from the right table and the corresponding records from the left table. the nulls are in the left table, same as above. full outer join full outer join returns all records from the specified tables. so there are null values in both of the table. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/05. Analyze/The Data Validation Process.html",
    "title": "The Data Validation Process",
    "body": " index search search back the data validation process six types of data validation: data type: check that the data matches the data type defined for a field data range: check that the data falls within an acceptable range of values defined for the field. data constraints: check that the data meets certain conditions or criteria for a field. data consistency: check that the data makes sense in the context of other related data. data structure: check that the data follows or conforms to a set structure. code validation: check that the application code systematically performs any of the previously mentioned validations during user data input. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Testing your Data.html",
    "title": "Testing your Data",
    "body": " index search search back testing your data statistical power: probability of getting meaningful results from a test (value between 0 and 1, 0.6 means there is 60% chance as statistically significant result) hypothesis testing: way to see if a survey or experiment has meaningful results statistically significant: it means the results of the test are real and not an error caused by random chance sample size calculators sample size calculator by surveymonkey.com sample size calculator by raosoft.com $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Data Integrity and Analytics Objectives.html",
    "title": "Data Integrity and Analytics Objectives",
    "body": " index search search back data integrity and analytics objectives data integrity: accuracy, completeness, consistency and trustworthiness of data throughout its life cycle. data can be compromised through: data replication: process of storing data in multiple locations (data can be out of sync) data transfer: process of copying data from one place to another (data can be incomplete because the copying process was interrupted) data manipulation: process of changing data $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Manually Cleaning Data.html",
    "title": "Manually Cleaning Data",
    "body": " index search search back manually cleaning data verification: process to confirm that the data-cleaning was well-executed and the resulting data is accurate and reliable. changelog: file containing a chronologically ordered list of modifications made to a project $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/index.html",
    "title": "Process Data from Dirty to Clean",
    "body": " index search search back process data from dirty to clean first week > 10/04/2022 - 10/04/2022 focusing on integrity data integrity and analytics objectives overcoming the challenges of insufficient data testing your data consider the margin of error weekly challenge second week > 10/04/2022 - 10/04/2022 data cleaning is a must begin cleaning data cleaning data in spreadsheets weekly challenge third week > 10/04/2022 - 11/04/2022 using sql to clean data learn basic sql queries transforming data weekly challenge fourth week > 11/04/2022 - 11/04/2022 manually cleaning data documenting results and the cleaning process weekly challenge fifth week > 11/04/2022 - 11/04/2022 the data analyst hiring process understand the elements of a data analyst resume highlighting experiences on resumes exploring areas of interest $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Begin Cleaning Data.html",
    "title": "Begin Cleaning Data",
    "body": " index search search back begin cleaning data what and how to clean data merging: process of combining two or more datasets into a single dataset data compatibility: describes how well two or more datasets are able to work together $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Cleaning Data in Spreadsheets.html",
    "title": "Cleaning Data in Spreadsheets",
    "body": " index search search back cleaning data in spreadsheets data mapping: process of matching fields from one data source to another. data cleaning exercise: boba tea shop data cleaning $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Data Cleaning is a Must.html",
    "title": "Data Cleaning is a Must",
    "body": " index search search back data cleaning is a must dirty data: data that is incomplete, incorrect or irrelevant to the problem clean data: data that is complete, correct and relevant to the problem types of dirty data: duplicate data: data record that shows up more than once outdated data: data that is old which should be replaced with newer and more accurate information incomplete data: data that is missing important fields incorrect/inaccurate data: data that is complete but inaccurate inconsistent data: data that uses different formats to represent the same thing data validation: tool for checking the accuracy and quality of data before adding or importing it. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Using SQL to Clean Data.html",
    "title": "Using SQL to Clean Data",
    "body": " index search search back using sql to clean data differences between spreadsheets and sql: spreadsheets generated with a program access to the data you input stored locally small datasets working independently built-it features sql a language used to interact with database programs can pull information from different sources in the database stored across a database larger datasets tracks changes across teams useful across multiple programs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/GoogleDataAnalyst/04. Process/Overcoming the Challenges of Insufficient Data.html",
    "title": "Overcoming the Challenges of Insufficient Data",
    "body": " index search search back overcoming the challenges of insufficient data types of insufficient data: data from only one source data that keeps updating outdated data geographically-limited data common data errors: random sampling: way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen margin of error: since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. this difference is called the margin of error. confidence level: how confident you are in the survey results. for example, a 95% confidence level means that if you were to run the same survey 100 times, you would get similar results 95 of those 100 times. confidence interval: the range of possible values that the population’s result would be at the confidence level of the study. statistical significance: the determination of whether your result could be due to random chance or not. the greater the significance, the less due to chance. determine the sample size don’t use a sample size less than 30. it has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population. the confidence level most commonly used is 95%, but 90% can work in some cases. for a higher confidence level, use a larger sample size to decrease the margin of error, use a larger sample size for greater statistical significance, use a larger sample size $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Other/UKF/index.html",
    "title": "Unscented Kalman Filter",
    "body": " index search search back unscented kalman filter it approximates a non-linear transformation of a probability distribution, as a gaussian distribution. assume that we have a probability distribution for a random variable vector defined as: \\begin{align} x \\sim \\mathcal{n}(\\overline{x}, \\sigma_x) \\end{align} given a non-linear transofrm that we want to apply to the distribution: \\begin{align} y = f(x) \\end{align} we want to obtain a gaussian approximation of the resulting transformed distribution: \\begin{align} y \\sim \\mathcal{n}(\\overline{y}, \\sigma_y) \\end{align} numerical solution generate \\(n\\) samples from the original distribution \\begin{align} x_i \\sim \\mathcal{n}(\\overline{x}, \\sigma_x) \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} for each sample \\(x_i\\) apply the non-linear transformation to find the corresponding \\(y_i\\) \\begin{align} y_i = f(x_i) \\end{align} fit a gaussian to the transformed points \\begin{align} \\overline{y} = \\frac{1}{n}\\sum^n y_i \\end{align} \\begin{align} \\sigma_y = \\frac{1}{n} \\sum^n (y_i - \\overline{y}) (y_i - \\overline{y})^t \\end{align} selecting less points to sample let \\(x\\) be a \\(n \\times 1\\) random vector with mean \\(\\overline{x}\\) and covariance \\(p\\), that is \\(x \\sim \\mathcal{n}(\\overline{x}, p)\\) we choose \\(2n\\) sigma points as follows: \\begin{align} x^{(i)} = \\overline{x} + \\delta x^{(i)} \\end{align} \\begin{align} i=1, \\cdots, 2n \\end{align} where: \\begin{align} \\delta x^{(i)} = (\\sqrt{np})_i \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} which means \\(\\delta x^{(i)}\\) is the i-th column of the covariance matrix multiplied by \\(\\sqrt{n}\\). for a \\(2\\times 1\\) state vector, this gives us the two following points: we do the same, but inversing, for the remaining \\(n\\) points: \\begin{align} \\delta x^{(n+i)} = -(\\sqrt{np})_i \\end{align} \\begin{align} i=1, \\cdots, n \\end{align} such that we obtain: from the following image we can see the \\(4\\) points we obtained seem to be a good approximation of the shape of the ellipse: so we can use these points to obtain the new ellipse which resulted from applying the transformation. mean approximation we apply the non-linear transformation to the sigma points: \\begin{align} y^{(i)} = h(x^{(i)}), i = 1, \\cdots, 2n \\end{align} such that we have the following situation: we obtain the weighted mean of the transformed sigma points: \\begin{align} \\overline{y} = \\sum_{i=1}^{2n} w^{(i)} y^{(i)} \\end{align} where \\(w^{(i)} = \\frac{1}{2n}\\). such that: \\begin{align} \\overline{y} = \\frac{1}{2n} \\sum_{i=1}^{2n} y^{(i)} \\end{align} and so, we obtain the following estimated mean: covariance approximation we apply this same methodology for the covariance, given the transformed points \\(y^{(i)}\\) we obtain the weighted covariance: \\begin{align} p_y = \\sum_{i=1}^{2n} w^{(i)} (y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} \\begin{align} = \\frac{1}{2n} \\sum_{i=1}^{2n}(y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} which given us the following estimated covariance: general unscented transformation we now show a general definition which allows for more accuracy: sigma points generation we generate \\(2n+1\\) instead of \\(2n\\) and we define \\(x^{(0)}\\) such that it equals the mean: \\begin{align} x^{(i)} = \\overline{x} + \\delta x^{(i)}, i=0, \\cdots, 2n \\end{align} \\begin{align} \\delta x^{(0)} = 0 \\end{align} \\begin{align} \\delta x^{(i)} = \\left(\\sqrt{(n+k)p}\\right)_i, i=1, \\cdots, n \\end{align} \\begin{align} \\delta x^{(n+i)} = -\\left(\\sqrt{(n+k)p}\\right)_i, i=1, \\cdots, n \\end{align} weight definition the weights are now defined as follows: \\begin{align} w^{(0)} = \\frac{k}{n+k} \\end{align} \\begin{align} w^{(i)} = \\frac{1}{2(n+k)} \\end{align} where \\(k=3-n\\) has shown to help improve accuracy. note that \\((n+k)\\neq 0\\) mean and covariance approximation finally we approximate the mean and the covariance of the transformed distribution the same way we did before: for the mean: \\begin{align} \\overline{y} = \\sum_{i=1}^{2n} w^{(i)} y^{(i)} \\end{align} for the covariance: \\begin{align} p_y = \\sum_{i=1}^{2n} w^{(i)} (y^{(i)} - \\overline{y})(y^{(i)} - \\overline{y})^t \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/DataScience/Other/index.html",
    "title": "Standalone topics",
    "body": " index search search back standalone topics unscented kalman filter $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Master/2C_2C/MBJ/index.html",
    "title": "index",
    "body": " index search search $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Master/2C_2C/AAII/index.html",
    "title": "Aprendizaje Automático II",
    "body": " index search search back aprendizaje automático ii tema 1. random forests bias/variance tradeoff boosting bagging random forests $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Master/2C_2C/AAII/T1/01_bias_variance_tradeoff.html",
    "title": "Bias/Variance Tradeoff",
    "body": " index search search back bias/variance tradeoff test error, also referred to as generalization error, is the prediction error over an independent test sample: \\begin{align} err_{\\mathcal{t}} = \\mathbb{e}[l(y | \\hat{f}(x)) | \\mathcal{t}] \\end{align} where \\(l\\) is the loss function. a related quantity is the expected prediction error (or expected test error): \\begin{align} err = \\mathbb{e}[err_{\\mathcal{t}}] \\end{align} where \\(err_{\\mathcal{t}}\\) is the test error. the error can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\) the squared bias of \\(\\hat{f}(x_0)\\) the variance of the error terms \\(\\epsilon\\). that is, \\begin{align} \\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2] = \\mathbb{v}[\\hat{f}(x_0)] + [bias(\\hat{f}(x_0))]^2 + \\mathbb{v}[\\epsilon] \\end{align} this amount is derived from: \\begin{align} err(x_0) = \\mathbb{e}[(y - \\hat{f}(x_0))^2] = \\mathbb{e}[y^2 + \\hat{f}(x_0)^2 - 2y\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[y^2] + \\mathbb{e}[\\hat{f}(x_0)^2] -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} we know that \\(\\mathbb{v}[x] = \\mathbb{e}[(x - \\mathbb{e}[x])^2] = \\mathbb{e}[x^2] - \\mathbb{e}[x]^2\\), such that: \\begin{align} = \\mathbb{v}[y] + \\mathbb{e}[y]^2 + \\mathbb{v}[\\hat{f}(x_0)] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[y]^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[y]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} note that, \\(y = f(x_0) + \\epsilon[/\\)], donde [\\(]\\mathbb{e}[\\epsilon] = 0\\), thus it follows: \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + (\\mathbb{e}[f(x_0)] + \\mathbb{e}[\\epsilon])^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2(\\mathbb{e}[f(x_0)] + \\mathbb{e}[\\epsilon])\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[f(x_0)]^2 + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\mathbb{e}[\\hat{f}(x_0)]^2 -2\\mathbb{e}[f(x_0)]\\mathbb{e}[\\hat{f}(x_0)] \\end{align} we know that \\((a + b)^2 = a^2 + b^2 + 2ab\\) and that \\(\\mathbb{e}[f(x_0)] = f(x_0)\\), such that: \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + f(x_0)^2 + \\mathbb{e}[\\hat{f}(x_0)]^2 -2f(x_0)\\mathbb{e}[\\hat{f}(x_0)] \\end{align} \\begin{align} = \\mathbb{e}[(y - \\mathbb{e}[y])^2] + \\mathbb{e}[(\\hat{f}(x_0) - \\mathbb{e}[\\hat{f}(x_0)])^2] + \\left(\\mathbb{e}[\\hat{f}(x_0)] - f(x_0)\\right)^2 \\end{align} here the notation \\(\\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2]\\) defines the expected test mse, and refers expected to the average test mse that we would obtain if we repeatedly \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). the overall expected test mse can be computed by averaging \\(\\mathbb{e}[\\left(y_0 - \\hat{f}(x_0)\\right)^2]\\) over all possible values of \\(x_0\\) in the test set. the previous equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. hence, we see that *the expected test mse can never lie below \\(\\mathbb{v}[\\epsilon]\\), the irreducible error*. the variance of the error terms, \\(\\mathbb{v}[\\epsilon]\\), is the variance of the target around its true mean \\(f(x_0)\\), and cannot be avoided no matter how well we estimate \\(f(x_0)\\), , unless \\(\\sigma^2 = 0\\). variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. ideally the estimate for f should not vary too much between training sets. this is computed as the expected squared deviation of \\(f^(x_0)\\) around its mean. bias refers to the error that is introduced by approximating a real-life problem by a simpler model. this quentifies the amount by which the average of our estimate differs from the true mean. as a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. as we increase the flexibility, the bias tends to initially decrease faster than the variance increases. consequently, the expected test mse declines. at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. in a real-life situation in which \\(f\\) is unobserved, it is generally not possible to explicitly compute the test mse, bias, or variance for a statistical learning method. training error consistently decreases with model complexity, typically dropping to zero if we increase the model complexity enough. a model with zero training error is overfit to the training dat and will typically generalize poorly. it is important to note that there are in fact two separate goals: model selection: estimating the performance of different models in order to choose the best model. model assessment: having chosen a final model, estimating its prediction error (generalization error) on new data. the training set is used to fit the models. the validation set is used to estimate prediction error for model selection. the test set is used for assessment of the generalization error $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Master/2C_2C/DL/index.html",
    "title": "index",
    "body": " index search search $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Master/index.html",
    "title": "Data Science Master",
    "body": " index search search back data science master 1c 1c 1c 2c 2c 1c 2c 2c aprendizaje automático ii deep learning modelos bayesianos jerárquicos $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/index.html",
    "title": "Notes",
    "body": " index search search back notes math pre-calculus a graphical approach to algebra and trigonometry data science master google data analyst machine learning stanford coursera artificial intelligence in robotics online training mobile robotics standalone topics web development frameworks front react back node.js django spring technologies docker graphql db mongodb rices arch linux installation other vimwiki macos vm $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/VimWiki/index.html",
    "title": "Vim Wiki",
    "body": " index search search back vim wiki key bindings convert current file to html: ,wh see html file in browser: ,whh more info on vimwiki latex inline \\(a = 1\\) equation \\begin{align} a \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Other/MacOS/index.html",
    "title": "Mac OS VM",
    "body": " index search search back mac os vm source initial setup ubuntu/debian: $ sudo apt-get install qemu uml-utilities virt-manager git \\ wget libguestfs-tools p7zip-full make dmg2img -y fedora: $ sudo dnf install @virutalization start libvirt service: $ sudo systemctl start libvirt $ sudo systemctl enable libvirt add user to the kvm and libvirt groups (might be needed). $ sudo usermod -ag kvm $(whoami) $ sudo usermod -ag libvirt $(whoami) $ sudo usermod -ag input $(whoami) note: re-login after executing this command. now edit /etc/libvirt/qemu.conf and set user and group to your user. clone this repository on your qemu system. files from this repository are used in the following steps. $ cd ~ $ git clone --depth 1 --recursive https://github.com/kholia/osx-kvm.git $ cd osx-kvm note: with this you are installing your vm on $home. fetch macos installer. $ ./fetch-macos-v2.py on this step select monterey. convert the downloaded basesystem.dmg file into the basesystem.img file. $ dmg2img -i basesystem.dmg basesystem.img create a virtual hdd image where macos will be installed. if you change the name of the disk image from mac_hdd_ng.img to something else, the boot scripts will need to be updated to point to the new image name. $ qemu-img create -f qcow2 mac_hdd_ng.img 128g be aware that the machine can easily reach that amount of memory. installation cli method (primary). just run the opencore-boot.sh script to start the installation process. $ ./opencore-boot.sh before installing go to disk utility inside the machine and erase the partition we are going to use for the virtual machine (the one that is roughly 128gb). for that click on erase and select mac os extended (journaled). once the erasing procedure is done, you can start the installation normally. edit macos-libvirt-catalina.xml file and change the various file paths (search for changeme strings in that file). the following command should do the trick usually. $ sed \"s/changeme/$user/g\" macos-libvirt-catalina.xml > macos.xml $ virt-xml-validate macos.xml create a vm on virt-manager by running the following command. $ virsh --connect qemu:///system define macos.xml launch virt-manager and start the macos virtual machine. post-installation open virt-manager, select macos and edit cpus and memory so the virtual machine does not lag incredibly. permissions bug (might only happen in fedora) if you get an error when starting the machine related to permissions, they are solved with: $ sudo setenforce permissive if they are related with selinux. if that is the case, refer to. undo the previous command with: $ sudo setenforce enforcing on your $home directory try to fix with: $ sudo chcon -r -u system_u -r object_r -t svirt_image_t osx-kvm/ screen resolution execute the virtual machine and press esc inmmediately. select device management option and change ovmf to 1920x1080p resolution. enter the virtual machine, once it has been booted open a terminal and write: $ diskutil list select the disk where the efi partition is location $ sudo diskutil mount disk1s1 $ vi /volumes/efi/efi/oc/config.plist and edit the entry under resolution to be 1920x1080@32. reboot the machine. once rebooted go to system preferences > displays and check show all resolutions and select 1920x1080. connect to physical iphone open virtual manager, and enter the configuration of the machine. click on add hardware and select usb host, now edit the xml entry just created and substitue the content with: <hostdev mode=\"subsystem\" type=\"usb\" managed=\"yes\"> <source> <vendor id=\"0x05ac\"/> <product id=\"0x12a8\"/> </source> <address type=\"usb\" bus=\"0\" port=\"1\"/> </hostdev> where vendor id and product id is obtained through lsusb on the host machine: $ lsusb ... bus 001 device 004: id 05ac:12a8 apple, inc. iphone 5/5c/5s/6/se ... keyboard is locked if the keyboard seems to be captured when the machine starts, remove the entry on the machine hardware configuration that has this content or similar (this is my keyboard's smart card, may not apply to your case.) <hostdev mode=\"subsystem\" type=\"usb\" managed=\"yes\"> <source> <vendor id=\"0x04f2\"/> <product id=\"0x1469\"/> </source> <address type=\"usb\" bus=\"0\" port=\"2\"/> </hostdev> optimization source only the following are actually important: add more video memory open virtual manager, select macos machine and open the configuration. locate vga and change the xml entry so that vgamem has the value 65536. skip the gui login screen (at your own risk!) $ defaults write com.apple.loginwindow autologinuser -bool true disable spotlight indexing on macos to heavily speed up virtual instances. $ sudo mdutil -i off -a enable performance mode # check if enabled (should contain `serverperfmode=1`) $ nvram boot-args # turn on $ sudo nvram boot-args=\"serverperfmode=1 $(nvram boot-args 2>/dev/null | cut -f 2-)\" disable heavy login screen wallpaper $ sudo defaults write /library/preferences/com.apple.loginwindow desktoppicture \"\" reduce motion & transparency (could be faulty) defaults write com.apple.accessibility differentiatewithoutcolor -int 1 defaults write com.apple.accessibility reducemotionenabled -int 1 defaults write com.apple.universalaccess reducemotion -int 1 defaults write com.apple.universalaccess reducetransparency -int 1 defaults write com.apple.accessibility reducemotionenabled -int 1 to undo any of this changes refer to the reference material. gpu passthrough to be continued $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/events_info.html",
    "title": "Events",
    "body": " index search search back events in node.js it the event-driven approach to programming is very commonly used. meaning the flow of our program is in part controlled by events. event emitter all objects which emit events are instances of eventemitter, which is accessible from the events module: const eventemitter = require('events') const customemitter = new eventemitter() customemitter.on('response', () => { console.log('some other logic here') }) customemitter.emit('response') here we can see that we create an eventemitter object and we listen for the response event with customemitter.on(). the latter function takes the name of the event as its first argument and the callback as its second. in order to emit a concrete event we use customemitter.emit(), which takes the event name as its argument. more listeners we can have more than one listener: const eventemitter = require('events') const customemitter = new eventemitter() customemitter.on('response', (name, id) => { console.log(`data recieved user ${name} with id:${id}`) }) customemitter.on('response', () => { console.log('some other logic here') }) customemitter.emit('response', 'john', 34) where the second listener define a callback that takes name and id as arguments. so when emitting the event we can pass those arguments to the emit function. take into account that the functions' order matter, if you emit and event before you listen for it, the event will never be registered. http events because http.server extends net.server which then extends eventemitter, we can use the methods discussed above. so we can listen for the event request to handle requests from the browser. const http = require('http') // using event emitter api const server = http.createserver() // emits request event // subcribe to it / listen for it / respond to it server.on('request', (req, res) => { res.end('welcome') }) server.listen(5000) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/json.html",
    "title": "JSON",
    "body": " index search search back json the method res.json() allows us to return a array of objects as the body of the http response: const express = require('express') const app = express() app.get('/', (req, res) => { res.json([{name: 'john'}, {name: 'susan'}]) }) app.listen(5000, () => { console.log('server is listening on port 5000....') }) we can also pass a json file to res.json(): const express = require('express') const app = express() const { products } = require('./data') app.get('/', (req, res) => { res.json(products) }) app.listen(5000, () => { console.log('server is listening on port 5000....') }) where data.js contains: const products = [ { id: 1, name: 'albany sofa', image: 'product-3.jpg', price: 39.95, desc: `i'm baby direct trade farm-to-table hell of`, }] module.exports = products $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/globals.html",
    "title": "Globals",
    "body": " index search search back globals some global variables available __dirname: path of current directory __filename require: function to use modules module: info about current module process: info about the environment where the program is bein executed note that in node there is no window object like in javascript. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/npm.html",
    "title": "NPM",
    "body": " index search search back npm the node package manager allows us to: reuse our own code in other projects use code written by other developers and share our own solutions. this tool is installed along node. npm calls the reusable code a package (also modules or dependencies), that is basically a folder that contains some js code. note that there is no quality control applied to the packages that are published, so it is the developer's responsibility to check whether the package is secure or not. installing packages you can install a package locally within your project as a local dependency: $ nmp i <packagename> or you can install the package globally, so it can be accessed from any project: $ npm install -g <packagename> if you want to specify a version for the package: $ npm install <packagename>@1.0.0 package.json this file stores important information about the project and the packages, it can be conceived as a manifest file. there are two ways to create it: manually: create package.json in the root folder of the project and define the properties of the project/packages. using npm following the guide (add -y to skip the questions of the guide): \t \t $ npm init \t when the project is initialized, the package.json file is as follows: { \"name\": \"08_project\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"error: no test specified\\\" && exit 1\" }, \"keywords\": [], \"author\": \"\", \"license\": \"isc\" } where all those properties are set up during the guide of npm init or set as default with the flag -y. after installing a dependency $ npm i lodash the following property is added: \"dependencies\": { \"lodash\": \"^4.17.21\" } and npm creates the folder node_modules, if it does not already exist, which stores the dependencies code. also, in case of wanting to install dependencies needed only during the development process: $ npm i <package> -d $ npm i <package> --save-dev and so, the property devdependencies is created in pakage.json. scripts the object scripts, which is a property of package.json, can contain the definition of different actions, for example: \"scripts\": { \"start\": \"node app.js\" } so when running npm start our app.js will be executed. for some commands you will need to specify run and the command name as follows: $ npm run dev nodemon this is a package that lets you hot reload your project without having to execute your app constantly. for that, after installing nodemon as a local or global dependency, we specify on package.json: \"scripts\": { \"dev\": \"nodemon app.js\" } if we want to run it: $ npm run dev package-lock.json this file stores the dependencies version of the packages installed as dependencies, as to avoid installing newer version that can be the cause of bugs. because within the package.json only our project's dependencies' versions are specified. uninstalling packages in order to uninstall the package we have a command, that follows the syntax: $ npm uninstall <package> we can also remove it from the dependencies object within package.json. so when you remove package-lock.json and the node_modules folder if you run $ npm install the package that was removed will not be installed. git when using git or other version control tool, it is desirable to create a .gitignore and to specify to avoid the node_modules folder, since its size can get big very easily. so, by just pushing the source code, including package.json, if we want to install all of the project's dependencies' again, on the root folder we run: $ npm install $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/api_ssr.html",
    "title": "API vs SSR",
    "body": " index search search back api vs ssr in express when we talk about apis we are talking about http interfaces to interact our data. the main differences between apis and server side rendering (ssr) are:   api ssr content type json template what is sent send data send template method res.json() res.render() $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/http_basics.html",
    "title": "HTTP Basics",
    "body": " index search search back http basics when answering to a request, node requires a method to signal to the server that all of the response headers and body have been sent, and so the server can consider the message complete. that method is res.end() const http = require('http') const server = http.createserver((req, res) => { res.end('home page') }) server.listen(5000) we create a server with http.createserver. this method takes a callback as an argument, which is called every time a user hits the server. next we specify the port on which the server will be listening for requests. the value of this port is somewhat irrelevant in the development environment. headers if we want to provide the metadata about the response we have to provide headers: const http = require('http') const server = http.createserver((req, res) => { res.writehead(200, { 'content-type': 'text/html' }) res.write('<h1>home page</h1>') res.end() }) server.listen(5000) with writehead we specify the headers, in our case we specify the status code (200: ok) and the content type of the response (text/html). the later are called mime-types or media types. then we specify the body of the response with write and finally we finalize the message with end. request object the request object that is an argument of the createserver method has several attributes: req.method: allows you to obtain the method of the user's request, i.e. get, post, put, etc. req.url: contains the url of the user's request. html file as we have seen the method write allows us to define the content of the body as html. however we do not need to write the html code inside the method we can also pass a file as input and the method will serve it's content to the response. const http = require('http') const { readfilesync } = require('fs') const homepage = readfilesync('./index.html') const server = http.createserver((req, res) => { res.writehead(200, { 'content-type': 'text/html' }) res.write(homepage) res.end() }) server.listen(5000) observe that we user readfilesync, we do so because, for one this is an example, and also the file is only read once when the server is created, not every time the user hits the server. external resources when adding external resources to a given html file we also need to handle the request to those resources in our server. const http = require('http') const { readfilesync } = require('fs') const homepage = readfilesync('./index.html') const homestyles = readfilesync('./styles.css') const homeimage = readfilesync('./logo.svg') const server = http.createserver((req, res) => { // home page if (url === '/') { res.writehead(200, { 'content-type': 'text/html' }) res.write(homepage) res.end() } \t// styles else if (url === '/styles.css') { res.writehead(200, { 'content-type': 'text/css' }) res.write(homestyles) res.end() } \t// image/logo else if (url === '/logo.svg') { res.writehead(200, { 'content-type': 'image/svg+xml' }) res.write(homeimage) res.end() } }) note that the content types differ every time, with css we use text/css, with images we use image/svg+xml. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/async_patterns.html",
    "title": "Asynchronous Patterns",
    "body": " index search search back asynchronous patterns blocking code imagine we have the following piece of code: const http = require('http') const server = http.createserver((req, res) => { if (req.url === '/') { res.end('home page') } if (req.url === '/about') { // blocking code for (let i = 0; i < 1000; i++) { for (let j = 0; j < 1000; j++) { console.log(`${i} ${j}`) } } res.end('about page') } res.end('error page') }) server.listen(5000, () => { console.log('server listening on port : 5000....') }) because inside the second conditional we have a nested for loop which is computationally expensive, when a user accesses the about page, the server is blocked, and so it prevents other users from loading any other page. that is essentially because javascript is single threaded, so by running the nested conditional, the thread is occupied for a period of time, during which the server will not be able to answer to any other request until it is freed. promises a promise is an object that represents the eventual completion (or failure) of an asynchronous operation and its resulting value. so, we can wrap the asynchronous readfile function with a promise: const { readfile, writefile } = require('fs') const gettext = (path) => { return new promise((resolve, reject) => { readfile(path, 'utf8', (err, data) => { if (err) { reject(err) } else { resolve(data) } }) }) } the result of a promise can be accessed as follows: gettext('./content/first.txt') .then((result) => console.log(result)) .catch((err) => console.log(err)) and then, we can define an asynchronous function start that will wait for the execution of gettext: const start = async () => { try { const first = await gettext('./content/first.txt') const second = await gettext('./content/second.txt') console.log(first, second) } catch (error) { console.log(error) } } where you can see that we surround the call with a try-catch statement, which allows us to have more control over the execution flow node's native promises we can use the utils module in order to wrap functions with the promise object: const { readfile, writefile } = require('fs') const util = require('util') const readfilepromise = util.promisify(readfile) const writefilepromise = util.promisify(writefile) const start = async () => { try { const first = await readfilepromise('./content/first.txt', 'utf8') const second = await readfilepromise('./content/second.txt', 'utf8') \t\tawait writefilepromise( './content/result-mind-grenade.txt', `this is awesome : ${first} ${second}`, { flag: 'a' } ) console.log(first, second) } catch (error) { console.log(error) } } but, we can also avoid importing the utils module, by adding .promises when importing the asynchronous functions: const { readfile, writefile } = require('fs').promises const start = async () => { try { const first = await readfile('./content/first.txt', 'utf8') const second = await readfile('./content/second.txt', 'utf8') await writefile( './content/result-mind-grenade.txt', `this is awesome : ${first} ${second}`, { flag: 'a' } ) console.log(first, second) } catch (error) { console.log(error) } } start() $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/mongodb.html",
    "title": "MongoDB",
    "body": " index search search back mongodb intro it is a nosql which is structured in collections, where each collection would be used to store a particular type of data in the form of documents: blog collection blog document blog document blog document here each document represent a single item of data, for example, each blog document represents one blog. the data is contained inside the documents in a very similar fashion to json objects, so the documents consist of key-value pairs like so: { \"id\": objectid(12345), \"title\": \"opening party\", \"snippet\": \"all about...\", \"body\": \"lorem ipsum\" } set up we can either install mongodb locally or we can use a cloud database which is already hosted for us. for the latter we will use mongodb atlas. there we create a cluster and inside this new cluster we create a new collection called blog. then we create a user accessing the security -> database access section. once we have our user created, we specify a way to connect to the database, by heading to clusters -> connect your application. we then copy the connection string that we will use as the database uri. observe that this uri needs you to input your password. mongoose now we need to actually connect to the database, we could use the mongodb api package and use the mongodb api, however we will use mongoose that makes it easier to interact with the database. mongoose is a odm (object document mapping) library, which means that it maps the standard mongodb api providing a much easier way to connect to and interact with the database. it does this by allowing us to create simple data models which have query methods to create, get, delete and update database documents. for that we first have to create a schema for the document which define the structure of a type of data or document. for example: blog schema: - title(string), required - snippet(string), required - body(string), required next, what we do is to create a model based on that schema, the model is what actually allows us to communicate with a particular database collection. each model has static methods get, save, delete, etc, that allow us to manage the data. installing $ npm install mongoose connect to mongodb so, now, we import the mongoose package and we use our database uri to connect to it, remember to change password and cluster_name to the values you specified for your database. const express = require('express'); const morgan = require('morgan'); const mongoose = require('mongoose'); // express app const app = express(); // connect to mongodb & listen for requests const dburi = \"mongodb+srv://user:<password>@test.mongodb.net/<cluster_name> mongoose.connect(dburi, { usenewurlparser: true, useunifiedtopology: true }) .then(result => app.listen(3000)) .catch(err => console.log(err)); the connect method is an asynchronous function, so it will execute a callback function when it finished connecting, or an error if the connection failed. in our case, we proceed to start our server when the database is ready. create models & schemas once we have successfully connected to our database, we will create our blog schema. for that, we first create a folder called models and inside it we create blog.js that will contain the following code: const mongoose = require('mongoose'); const schema = mongoose.schema; const blogschema = new schema({ title: { type: string, required: true, }, snippet: { type: string, required: true, }, body: { type: string, required: true }, }, { timestamps: true }); const blog = mongoose.model('blog', blogschema); module.exports = blog; as you can see, we first import mongoose and the schema object that we use to define the blog schema. in order to create a new blog schema we create a new schema object and we specify the different properties and restrictions. we also set and object of options, where we specify that we want mongodb to save the timestamps of updates, creations, etc. next we created a model that is based in the schema we just created with the function model and we pass it the model name (this name is then pluralized, as to then look up the collection that matches it) and the schema instance. getting/saving data in order to work we data, we must import the model we just created. const express = require('express'); const morgan = require('morgan'); const mongoose = require('mongoose'); const blog = require('./models/blog'); // express app const app = express(); // connect to mongodb & listen for requests const dburi = \"mongodb+srv://user:<password>@test.mongodb.net/<cluster_name> mongoose.connect(dburi, { usenewurlparser: true, useunifiedtopology: true }) .then(result => app.listen(3000)) .catch(err => console.log(err)); app.get('/blogs', (req, res) => { blog.find() .then(result => { res.send(result); }) .catch(err => { console.log(err); }); }); app.get('/blogs/:id', (req, res) => { const id = req.params.id; blog.findbyid(id) .then(result => { res.send(result); }) .catch(err => { console.log(err); }); }); here we use the find and findbyid methods to interact with our database. in order to create or delete new blogs: app.post('/blogs', (req, res) => { const blog = new blog(req.body); blog.save() .then(result => { res.redirect('/blogs'); }) .catch(err => { console.log(err); }); }); app.delete('/blogs/:id', (req, res) => { const id = req.params.id; blog.findbyidanddelete(id) .then(result => { res.json({ redirect: '/blogs' }); }) .catch(err => { console.log(err); }); }); in the post method we create a new blog object using the objects from the request body, and then we save it in our database. on the other hand, in order to delete a blog we pass the id as a parameter, we search for it on the database and we delete it. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/routes.html",
    "title": "Routes",
    "body": " index search search back routes set up in order to set up the routes for our project, we first create a folder called routes that will contain all the javascript files that control routing functionality. in this example we create two files within routes, people.js and auth.js. once we have created them, we include them as middleware to the specific endpoints (/api/people for people.js and /login for auth.js), as follows: const express = require('express') const app = express() const people = require('./routes/people') const auth = require('./routes/auth') app.use('/api/people', people) app.use('/login', auth) app.listen(5000, () => { console.log('server is listening on port 5000....') }) router let's focus now on people.js than controls the routing of /api/people. for that we import the controller of this endpoint and we specify the functions to execute for the different http methods and for the different routes. /: this is the default endpoint /api/people there we specify that the logic for a get request is contained in the getpeople function. /:d: this endpoint allows for specifying an id as a parameter. const express = require('express') const router = express.router() const { getpeople, createperson, createpersonpostman, updateperson, deleteperson, } = require('../controllers/people') router.route('/').get(getpeople).post(createperson) router.route('/:id').put(updateperson).delete(deleteperson) module.exports = router controller the people controller contains: let { people } = require('../data') const getpeople = (req, res) => { res.status(200).json({ success: true, data: people }) } const createperson = (req, res) => { const { name } = req.body if (!name) { return res .status(400) .json({ success: false, msg: 'please provide name value' }) } res.status(201).send({ success: true, person: name }) } const createpersonpostman = (req, res) => { const { name } = req.body if (!name) { return res .status(400) .json({ success: false, msg: 'please provide name value' }) } res.status(201).send({ success: true, data: [...people, name] }) } const updateperson = (req, res) => { const { id } = req.params const { name } = req.body const person = people.find((person) => person.id === number(id)) if (!person) { return res .status(404) .json({ success: false, msg: `no person with id ${id}` }) } const newpeople = people.map((person) => { if (person.id === number(id)) { person.name = name } return person }) res.status(200).json({ success: true, data: newpeople }) } const deleteperson = (req, res) => { const person = people.find((person) => person.id === number(req.params.id)) if (!person) { return res .status(404) .json({ success: false, msg: `no person with id ${req.params.id}` }) } const newpeople = people.filter( (person) => person.id !== number(req.params.id) ) return res.status(200).json({ success: true, data: newpeople }) } module.exports = { getpeople, createperson, createpersonpostman, updateperson, deleteperson, } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/index.html",
    "title": "Node.js",
    "body": " index search search back node.js node.js intro globals modules npm event loop asynchronus patterns events streams http express http basics express api vs ssr json route params query strings middleware http methods routes view engines mongodb mock mongodb environment variables json web tokens projects books directory basic users system real-time chat application collaborative drawing app email sender video streaming platform web scraper $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/http.html",
    "title": "HTTP",
    "body": " index search search back http http messages request message: what the user sends response message: what the server sends the messages have the following parts: info about the request: request url, request method (get is the default method), status code, etc. headers: meta information about the request/response, (e.g. \"content type: application/json\" tells the browser that the body is json) body: which is the request payload, or the content of the response. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/middleware.html",
    "title": "Middleware",
    "body": " index search search back middleware when we are talking about middleware we are typically talking about any type of code and function between getting a certain request and sending the corresponding respond. express allows us to pass middleware as an argument to the app methods. note that middleware runs from top to bottom in our server, so order does matter when specifying middleware. const express = require('express') const app = express() const logger = (req, res, next) => { const method = req.method const url = req.url const time = new date().getfullyear() console.log(method, url, time) next() } app.get('/', logger, (req, res) => { res.send('home') }) app.get('/about', logger, (req, res) => { res.send('about') }) app.listen(5000, () => { console.log('server is listening on port 5000....') }) here we have defined a logger function that tells us some information about the request made. this function is passed as an argument to the app.get() method, and then express passes req, res and next as arguments for the middleware. the next argument is a function that is needed in order to pass the flow to the next middleware and it always has to be invoked, unless the current middleware sends a response and so finishes the message. in any other case, if the next method is not invoked then the browser will be stuck loading because the program flow was halted by not calling the next middleware. the middleware functions that we can use can be ones we code ourselves, express functions or third party software. apply middleware with app.use in order to apply a certain middleware to all the routes we first save the logger on a separate file named logger.js, then we import it into our main app, and we specify its usage as a middleware by app.use. const express = require('express') const logger = require('./logger') const app = express() app.use(logger) with this our logger will be executed every time the user accesses our server. we can also specify an argument like so: const express = require('express') const logger = require('./logger') const app = express() app.use('/api/', logger) this tells express to only use the middleware for the /api route and all its subdomains (i.e. /api/*). apply multiple middleware we now define a new middleware function, that goes by the name of authorize.js, we import it into our app.js and we add it as middleware by using an array. const express = require('express') const logger = require('./logger') const authorize = require('./authorize') const app = express() app.use([logger,authorize]) note that the order matters, meaning the first middleware executed is logger, in this instance, and then the control flow is passed to authorize. we can also define more than one middleware function on one concrete end-point: app.get('/api', [logger, authorize], (req, res) => { res.send('api home page') }) as we can see, we have specified two middleware functions, namely logger and authorize by using an array. example const authorize = (req, res, next) => { // de-structure user object const { user } = req.query if(user == 'alice'){ req.user = { name: 'alice', id: 3 } // yield control flow next() \t} else{ res.status(401).send('unauthorized') } } as you can see the authorize middleware function creates a new object within the request object, which can be accessed from the next middleware, or from the server. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/modules.html",
    "title": "Modules",
    "body": " index search search back modules encapsulated code, as to only share what we want. node uses commonjs so every file is treated as a module by default. in any .js file we have the global object module: console.log(module) module { id: '.', path: '/home/alba/desktop/nodejs', exports: {}, filename: '/home/alba/desktop/nodejs/02_constants.js', loaded: false, children: [], paths: [ '/home/alba/desktop/nodejs/node_modules', '/home/alba/desktop/node_modules', '/home/alba/node_modules', '/home/node_modules', '/node_modules' ] } exporting so we can treat the attribute exports as an object and pass it whatever values we would like to show to other app that import our module: module.exports = { value1: 'value1', value2: 'value2' } where value1 is the key of the attribute and 'value1' is its value, e.g.: const name = 'john' const surname = 'tuckey' module.exports = { name: name, surname: surname } also, if we only export one object it is sufficient to type: const name = 'john' module.exports = name another way to export is to define explicitly the name of the attributes to export: module.exports.items = ['item1', 'item2'] const person = { name: 'bob' } module.exports.singleperson = person importing now, a module can be imported with the keyword require as follows: const externalmodule = require('./module') console.log(externalmodule) { name: 'john', surname: 'tukey' } another type of syntax could be unrolling the attributes of the export object: const { name, surname } = require('./module') built-in modules some built-in modules are: os path fs (filesystem) http even though there are several more built-in modules. os to import the os built-in module we do: const os = require('os') and we call it by: console.log(`the system uptime is ${os.uptime()} seconds`) fs we can also interact with the file system via the fs module. there are two ways to do so: asynchronously, which is non-blocking synchronously, that is blocking synchronous to exemply both setups, we first de-structure the read and write synchronous methods from the fs module, and then we read and write files. const { readfilesync, writefilesync } = require('fs') // read file with a given path and the corresponding encoding const first = readfilesync('./file.txt', 'utf8') const second = readfilesync('./file2.txt', 'utf8') // write to a file given a path, the content is overwritten writefilesync('./writefile', 'this content will be written') // write to a file given a path, the content is appended writefilesync('./writefile', 'this content will be written', {flag: 'a'}) asynchronous now, in order to access the file system asynchronously, we need a callback, and so we do: const { readfilesync, writefilesync } = require('fs') readfile('./file', 'utf8', (error, result) => { if(error){ console.log(error) return } else{ console.log(result) const first = result // here we can add another read call } }) writefile('./file', 'this is the content', (error, result) => { if(error){ console.log(error) return }else{ console.log(result) } }) where we specify a callback function with the es6 syntax. its first parameter is the error parameter and the second is the result of the operation. the problem with synchronous calls is that they can be very time consuming and they halt the execution, which can be critical when working on time sensitive tasks or when several user call upon these type of functions at a time. http to show the bare basics, we will set up a server: const http = require('http') const server = http.createserver((request, response) => { response.write('this is the index!') response.end() }) // define the port server.listen(5000) that can be accessed on localhost:5000. next, we can code something a little more complex, where the content handed as a response depends on the request: const http = require('http') const server = http.createserver((request, response) => { if(request.url === '/'){ response.end('this is the index') }else if(request.url === '/about'){ response.end('this is the about') }else{ response.end('404') } }) // define the port server.listen(5000) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/intro.html",
    "title": "NodeJS",
    "body": " index search search back nodejs nodejs in an environment to run javascript outside of the browser that was build on top of chrome's v8 js engine. it allows for easy development of full stack apps, since both the frontend and the backend are build in the same language, javascript. differences between to the browser and nodejs browser nodejs dom no dom window no window interactive apps server side apps no filesystem filesystem fragmentation versions es6 modules commonjs how to get node to evaluate our code repl (read, eval, print loop) $ node welcome to node.js v16.9.1. type \".help\" for more information. > cli executable $ node 00_app.js large number hey it is my first node app $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/view_engines.html",
    "title": "View Engines",
    "body": " index search search back view engines we have different options: pug ejs (embedded javascript) express handlebars ejs installing we will use ejs in this example. first we download it: $ npm install ejs set up now we specify in our application that we want to use it: const express = require('express') const app = express() // specify view engine and settings app.set('view engine', 'ejs') app.set('views', './views') we use the function set() that is used to specify app settings. there we define ejs as our view engine and then we indicate that the folder where our views are located is /views, which is the default folder. this means we could have omitted that last line and the functionality would remain the same. rendering inside our root folder, we create the folder views and the file index.ejs which has the same syntax as html: <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"> <title>blog ninja | <%= title %></title> </head> <body> <div class=\"blogs content\"> <h2>all blogs</h2> <% if (blogs.length > 0) { %> <% blogs.foreach(blog => { %> <h3 class=\"title\"><%= blog.title %></h3> <p class=\"snippet\"><%= blog.snippet %></p> <% }) %> <% } else { %> <p>there are no blogs to display...</p> <% } %> </div> </body> </html> so in order to send this template as a response we do: app.get('/', (req, res) => { const blogs = [ {title: 'yoshi finds eggs', snippet: 'lorem ipsum dolor sit amet consectetur'}, {title: 'mario finds stars', snippet: 'lorem ipsum dolor sit amet consectetur'}, {title: 'how to defeat bowser', snippet: 'lorem ipsum dolor sit amet consectetur'}, ]; res.render('index', { title: 'home', blogs }); }); note that we define an array of blog objects, and we pass them as an argument to the template. which then iterates over them to visualize each item. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/http_methods.html",
    "title": "HTTP Methods",
    "body": " index search search back http methods in this section we will show a few examples of the different http methods in express, take into account that we are not using a database. get app.get('/api/people', (res, req) => { res.status(200).json({ success: true, data: people }) }) post observe that we use a middleware provided by express that lets us parse incoming requests with urlencoded payload, and another middleware function to parse json. app.use(express.urlencoded({ extended: false })) app.use(express.json()) app.post('/api/people', (res, req) => { const { name } = req.body if(!name){ return res .status(400) .json({ success: false, msg: 'please provide a name'}) } // send array of people adding the new person (this is not permanent) res.status(201).json({ success: true, data: [...data, { name, id: data.length + 1}] }) }) put app.put('/api/people/:id', (res, req) => { // de-structure params const { id } = req.params const { name } = req.body const person = people.find((person) => person.id === number(id)) // the person does not exist if(!person){ return res .status(400) .json({ success: false, msg: `no person with id: ${id}`}) } // update the person data const newpeople = people.map((person) => { if(person.id === number(id)){ person.name = name } return person }) res.status(200).json({ success: true, data: newpeople }) }) delete app.delete('/api/people/:id', (res, req) => { // de-structure params const { id } = req.params const { name } = req.body const person = people.find((person) => person.id === number(id)) // the person does not exist if(!person){ return res .status(400) .json({ success: false, msg: `no person with id: ${id}`}) } // filter the person data const newpeople = people.filter((person) => person.id !== id) res.status(200).json({ success: true, data: newpeople }) }) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/env.html",
    "title": "Environment Variables",
    "body": " index search search back environment variables installing in order to pass environment variables, like mongodb credentials, to our app we can use a third party package called cross-env: $ npm install --save-dev cross-env and then we can pass environment variables as arguments to our node application like so: npx cross-env node_env=development node app.js and the environment variables can be accessed from our app as follows: console.log(process.env.node_env) command to make it easier we can modify our package.json scripts to pass these variables for us: { \"name\": \"project\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"dev\": \"npx cross-env node_env=development node app\" }, \"keywords\": [], \"author\": \"\", \"license\": \"isc\" } and we start the application with: $ npm dev file another way to do it is using a .env file: node_env=development port=3000 host=localhost to pass those variables to node.js we use the eval command: $ eval $(cat .env) node app and we can also include it to package.json. { \"name\": \"project\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"app.js\", \"scripts\": { \"dev\": \"eval $(cat .env) node app\" }, \"keywords\": [], \"author\": \"\", \"license\": \"isc\" } dotenv in case of not wanting to use commands that are exclusive to our operative system, we can use the package dotenv $ npm install --save-dev dotenv and in our app we do: require('dotenv').config() $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/express.html",
    "title": "Express",
    "body": " index search search back express express is a minimal and flexible node.js web app framework that allows us to develop and design web sites and apis much faster. to install: $ npm install express --save they suggest including the flag --save because in earlier versions of express if it was not specified the package would not be saved as a dependency on package.json. initializing express app in order to do so we import the express module, and the we create the instance, more or less like we did with our http servers: const express = require('express') const app = express() app methods the app instance we just created has several methods, we now list the most common: app.get: http method to read data. app.get('/', (req, res) => { res.status(200).send('home page') }) app.post: http method to insert data. app.put: http method to update data. app.delete: http method to delete data. app.all: usually used to respond when we cannot locate a resource on the server. app.all('*', (req, res) => { res.status(404).send('<h1>resource not found</h1>') }) app.use: it is responsible for the middleware. app.listen: this method listens for any requests made to the server. app.listen(5000, () => { console.log('server is listening on port 5000...') }) send html files to send html files as a response instead of plain text we have to use the sendfile method: const express = require('express') const path = require('path') const app = express() app.get('/', (req, res) => { res.sendfile(path.resolve(__dirname, './index.html')) }) app.listen(5000, () => { console.log('server is listening on port 5000...') }) now, we have to import the external resources needed by the html file: const express = require('express') const path = require('path') const app = express() app.use(express.static('./public')) app.get('/', (req, res) => { res.sendfile(path.resolve(__dirname, './index.html')) }) app.listen(5000, () => { console.log('server is listening on port 5000...') }) so we invoke app.use as to tell the server that there are static resources stored in the public folder. however, because in this case index.html is also a static file we can remove the sendfile method if we store index.html inside the public folder: const express = require('express') const path = require('path') const app = express() app.use(express.static('./public')) app.listen(5000, () => { console.log('server is listening on port 5000...') }) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/route_params.html",
    "title": "Route Params",
    "body": " index search search back route params if, for example, we have a list of products, and we want to get a certain product by its id, we use route params. they can have any name, and are specified by :param. this is then stored in the request object. app.get('/api/products/:productid', (req, res) => { \t // de-structure param const { productid } = req.params // filter products by id const singleproduct = products.find( (product) => product.id === number(productid) ) \t // if it does not exist if (!singleproduct) { return res.status(404).send('product does not exist') } return res.json(singleproduct) }) note that the route params are always strings, in our case we had to convert it to a number. we can also have more that one route parameter like so: app.get('/api/products/:productid/reviews/:reviewid', (req, res) => { res.send('hello world') }) where we define productid and reviewid as route parameters, and can, therefore, filter by them. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/streams.html",
    "title": "Streams",
    "body": " index search search back streams streams are used to read or write sequentially. there are different types: writeable readable duplex: for both writing and reading. transform: to modify data while writing or reading. they are used in order to read files which are too big to store on variables, as it would pose an error. const { createreadstream } = require('fs') const stream = createreadstream('./content/big.txt') stream.on('data', (result) => { console.log(result) }) when logging result when listening on the data event we get the amount of data that is being read, as streams read data chunk by chunk, whose default value is 64kb. in order to modify this value we specify, on the options object, the property highwatermark: const { createreadstream } = require('fs') const stream = createreadstream('./content/big.txt', { highwatermark: 90000 }) stream.on('data', (result) => { console.log(result) }) stream.on('error', (err) => console.log(err)) so, now we are reading 90kb chunks of data. in order to read the data, we specify the encoding of the file: const { createreadstream } = require('fs') const stream = createreadstream('./content/big.txt', { encoding: 'utf8' }) stream.on('data', (result) => { console.log(result) }) in order to listen for errors: const { createreadstream } = require('fs') const stream = createreadstream('./content/big.txt') stream.on('error', (err) => console.log(err)) streams on the web when reading and writing files on servers, it is highly advisable to use chunks instead of the hole file, like so: var http = require('http') var fs = require('fs') http .createserver(function (req, res) { const text = fs.readfilesync('./content/big.txt', 'utf8') res.end(text) }) .listen(5000) instead of this approach, we use streams, both for reading and for writing: var http = require('http') var fs = require('fs') http .createserver(function (req, res) { const filestream = fs.createreadstream('./content/big.txt', 'utf8') filestream.on('open', () => { filestream.pipe(res) }) filestream.on('error', (err) => { res.end(err) }) }) .listen(5000) here, we see that we use the on method to listen for the open event. and then, we use pipe to write on the stream. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/mock_mongo.html",
    "title": "Mocking MongoDB",
    "body": " index search search back mocking mongodb we will now use the node library mongomemoryserver in order to mock our mongodb database using node.js inside a docker container. mongomemoryserver as we have mentioned we need mongomemoryserver, so we install it as a development depencendy. for that we head to our node app's root folder and we execute: $ npm install mongodb-memory-server-core --save-dev docker so, now we create our dockerfile, which holds our app source code, and where we install mongodb: from alpine:latest maintainer albamr09 # install dependencies run apk add --no-cache nodejs npm # install mongodb run echo 'http://dl-cdn.alpinelinux.org/alpine/v3.6/main' >> /etc/apk/repositories run echo 'http://dl-cdn.alpinelinux.org/alpine/v3.6/community' >> /etc/apk/repositories run apk update run apk add mongodb run apk add mongodb-tools run mkdir -p /data/db/ run chmod -r 777 /data/db # add common user run adduser -d user #run useradd --create-home --shell /bin/bash user # create app directory workdir /home/user/src/ # change permissions run chown -r user:user /home/user/src/ run chmod -r 755 /home/user/src/ user user # copy with user as owner copy --chown=user:user ./package*.json ./ # install app dependencies run npm install # copy and override src folder copy . . note that this version of mongodb is 3.4.4, mainly because we are using the alpine image. this version may not coincide with our mongodb docker image, and is not desirable. so make sure (or force) that you are installing the save versions. mongomemoryserver configuration also, we only need to install it for those images that are not supported by mongodb. furthermore, if instead of the package mongo-memory-server-core we install mongo-memory-server, the latter will include a post-install hook that will install mongodb if it is not already installed on the system. in case of manually installing mongodb we have to let know mongomemoryserver where the binary lays. so, within our package.json file we add: \"config\": { \"mongodbmemoryserver\": { \"systembinary\": \"/usr/bin/mongod\", \"version\": \"3.4.4\" } example of usage we, now, exemplify how to mock our database in our tests: const { mongomemoryserver } = require('mongodb-memory-server-core'); const mongoose = require('mongoose'); const usermodel = require('../../models/user'); const userdata = { 'name': 'test', 'email': 'test@test.com', 'password': 'test1234', 'username': 'testname' }; describe('user model tests', ()=> { let mongoserver; beforeall(async () => { mongoserver = await mongomemoryserver.create(); await mongoose.connect(mongoserver.geturi(), { usenewurlparser: true, useunifiedtopology: true, }).catch(error => console.log(error)); }); afterall(async () => { await mongoserver.stop(); await mongoose.connection.close(); }); aftereach(() => { mongoose.connection.collections['users'].drop( function() {}); }); it('create a new user', async ()=> { const user = new usermodel(userdata); const saveduser = await user.save(); expect(saveduser._id).tobedefined(); expect(saveduser.name).tobe(userdata.name); expect(saveduser.email).tobe(userdata.email); expect(saveduser.password).tobe(userdata.password); expect(saveduser.username).tobe(userdata.username); }) it('create a user with invalid fields', async ()=> { var invaliduserdata = {...userdata}; delete invaliduserdata.email; const user = new usermodel(invaliduserdata); let error; try{ const saveduser = await user.save(); error = saveduser; }catch(err){ error = err; } expect(error).tobeinstanceof(mongoose.error.validationerror); expect(error.errors.email).tobedefined(); }) it('create user that already exists', async ()=>{ await new usermodel(userdata).save(); let error; try{ const repeateduser = new usermodel(userdata); await repeateduser.save(); }catch(err){ error = err; } expect(error).tobedefined(); expect(error.code).tobe(11000); }) it('create user with undefined fields', async ()=>{ var newuserdata = {...userdata}; delete newuserdata.name; const user = new usermodel(newuserdata); await user.save(); expect(user._id).tobedefined(); expect(user.name).tobeundefined(); }) } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/jwt.html",
    "title": "JSON Web Tokens",
    "body": " index search search back json web tokens installation $ npm install jsonwebtoken example of usage we first create our express application and so, we import express and jsonwebtoken. and then we start the server. const express = require(\"express\"); const jwt = require(\"jsonwebtoken\"); const app = express(); app.listen(3000, () => { console.log(\"nodejs app running...\"); }); now, we define two new endpoints: /api and /api/login. app.get(\"/api\", (req , res) => { res.json({ mensaje: \"nodejs and jwt\" }); }); app.post(\"/api/login\", (req , res) => { const user = { id: 1, nombre : \"henry\", email: \"henry@email.com\" } jwt.sign({user}, 'secretkey', {expiresin: '32s'}, (err, token) => { res.json({ token }); }); }); where we use the sign method to create a new token. so, if we want to define an endpoint that requires authentication we do: // middleware function verifytoken(req, res, next){ const bearerheader = req.headers['authorization']; if(typeof bearerheader !== 'undefined'){ const bearertoken = bearerheader.split(\" \")[1]; req.token = bearertoken; next(); }else{ res.sendstatus(403); } } app.post(\"/api/posts\", verifytoken, (req , res) => { jwt.verify(req.token, 'secretkey', (error, authdata) => { if(error){ res.sendstatus(403); }else{ res.json({ mensaje: \"post fue creado\", authdata }); } }); }); where verifytoken is a middleware function that gets the token from the header, and then we use the verify method to check if the token is valid. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/query_strings.html",
    "title": "Query Strings",
    "body": " index search search back query strings we can use the query attribute from the request object in order to further filter our data. so whenever the user types localhost:5000/whateverendpoint?name=john, the request object passed as an argument of the callback defined for whateverendpoint will have the object {name: 'john'} stored in request.query. app.get('/whateverendpoint', (req, res) => { console.log(req.query) }) now we code the way to filter by the keywords search and limit: app.get('/api/v1/query', (req, res) => { \t \t// de-structure keys const { search, limit } = req.query // get a copy of the products let sortedproducts = [...products] // if search was specified if (search) { \t // return only the products whose name start with sortedproducts = sortedproducts.filter((product) => { return product.name.startswith(search) }) } // if limit was specified if (limit) { // return as many products as the limit specified sortedproducts = sortedproducts.slice(0, number(limit)) } // if no product matched the search if (sortedproducts.length < 1) { return res.status(200).json({ sucess: true, data: [] }) } \t // return the products filtered res.status(200).json(sortedproducts) }) so now, if we go to localhost:5000/api/v1/query?search=a&limit=2 the server will return a json object that contains at most 2 products whose name start with an \"a\". observe, that in order to avoid error for sending more than one response (note that we have two res.json() in our function), we must add the return keyword after sending each response, then the method exits. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/NodeJS/event_loop.html",
    "title": "Event Loop",
    "body": " index search search back event loop it is what allows node.js to perform non-blocking i/o operations, despite the fact that javascript is single-threaded- by offloading operations to the system kernel whenever possible. the event loop follows the next steps: an asynchronous request is made by a user the event loop registers the callback of the request when the request is completed and we are ready to execute the callback the event loop stores the callback at the end of the execution line, meaning, once the immediate tasks are done (i.e. synchronous code) the callback is executed for example, we have the following code: const { readfile, writefile } = require('fs') console.log('started a first task') readfile('./content/first.txt', 'utf8', (err, result) => { if (err) { console.log(err) return } console.log(result) console.log('completed first task') }) console.log('starting next task') which outputs: started first task starting next task hello this is first text file completed first task so we can see that the synchronous code is run first, and then the callback of the asynchronous function readfile is called upon finishing reading the file. in the next example: // started operating system process console.log('first') settimeout(() => { console.log('second') }, 0) console.log('third') // completed and exited operating system process which outputs: first third second so even though the timeout is initialized to 0, because it is an asynchronous function it is offloaded and so it is put to the end of the execution line, and then it is executed after the synchronous code. it is important to note that the listen function of the http module is also asynchronous. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/XML configuration file.html",
    "title": "Configure Spring Container with an XML file",
    "body": " index search search back configure spring container with an xml file first we create the config file <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <bean id=\"mycoach\" class=\"com.luv2code.springdemo.trackcoach\"> </bean> </beans> then we create the spring container in our application: package com.springdemo; /* class to create a spring container using xml files */ import org.springframework.context.support.classpathxmlapplicationcontext; public class myapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container by its id \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/IoC/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control the spring container (generally known as applicationcontext) has two main functions: create and manage objects (inversion of control) inject object's dependencies (dependency injection) so inversion control is externalizing the construction and management of objects which will be handled by and object factory. this is illustrated in the following image: myapp has the main method myapp asks spring to retrieve the appropiate object based on a configuration file or an annotation, instead of having to code it manually like: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\tcoach thecoach = new trackcoach(); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t} } where we have defined an interface coach that is implemented by both trackcoach and baseballcoach package com.springdemo; public interface coach { \tpublic string getdailyworkout(); \t } package com.springdemo; public class trackcoach implements coach { \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} } to avoid this approach we create a spring container. to configure a spring container we can use: xml configuration file (legacy) java annotations java source code however what is a spring bean? a \"spring bean\" is simply a java object. when java objects are created by the spring container, then spring refers to them as \"spring beans\". spring beans are created from normal java classes just like java objects. why do we specify the coach interface in getbean()? when we pass the interface to the method, behind the scenes spring will cast the object for you. context.getbean(\"mycoach\", coach.class) however, there are some slight differences than normal casting. behaves the same as getbean(string), but provides a measure of type safety by throwing a beannotofrequiredtypeexception if the bean is not of the required type. this means that classcastexception can't be thrown on casting the result correctly, as can happen with getbean(string). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/View.html",
    "title": "View",
    "body": " index search search back view create view inside web-inf/view we create a file main-menu.jsp: <!doctype> <html> <body> <h2>spring mvc demo - home page</h2> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Configuration.html",
    "title": "Configuration",
    "body": " index search search back configuration add configurations to file: web-inf/web.xml configure spring mvc dispatcher servlet set up url mappings to spring mvc dispatcher servlet add configurations to spring configuration file: web-inf/spring-mvc-demo-servlet.xml add support for spring component scanning add support for conversion, formatting and validation configure spring mvc view resolver configuration on web.xml we have to add an entry for our front controller: dispatcherservlet <?xml version=\"1.0\" encoding=\"utf-8\"?> <web-app xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" \txmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" \txsi:schemalocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" \tid=\"webapp_id\" version=\"3.1\"> \t<display-name>spring-mvc-demo</display-name> \t<absolute-ordering /> \t<!-- step 1: configure spring mvc dispatcher servlet --> \t<servlet> \t\t<!-- name to reference this servlet --> \t\t<servlet-name>dispatcher</servlet-name> \t\t<servlet-class>org.springframework.web.servlet.dispatcherservlet</servlet-class> \t\t<!-- file of configuration of spring application --> \t\t<init-param> \t\t\t<param-name>contextconfiglocation</param-name> \t\t\t<param-value>/web-inf/spring-mvc-demo-servlet.xml</param-value> \t\t</init-param> \t\t<load-on-startup>1</load-on-startup> \t</servlet> \t<!-- step 2: set up url mapping for spring mvc dispatcher servlet --> \t<servlet-mapping> \t\t<servlet-name>dispatcher</servlet-name> \t\t<!-- for any url that comes in pass it to the \"dispatcher\" servlet --> \t\t<url-pattern>/</url-pattern> \t</servlet-mapping> </web-app> configuration on spring-mvc-demo-servlet.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" \txmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" \txmlns:context=\"http://www.springframework.org/schema/context\" \txmlns:mvc=\"http://www.springframework.org/schema/mvc\" \txsi:schemalocation=\" \t\thttp://www.springframework.org/schema/beans \thttp://www.springframework.org/schema/beans/spring-beans.xsd \thttp://www.springframework.org/schema/context \thttp://www.springframework.org/schema/context/spring-context.xsd \thttp://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"> \t<!-- step 3: add support for component scanning --> \t<context:component-scan base-package=\"com.springdemo\" /> \t<!-- step 4: add support for conversion, formatting and validation support --> \t<mvc:annotation-driven/> \t<!-- step 5: define spring mvc view resolver --> \t<bean \t\tclass=\"org.springframework.web.servlet.view.internalresourceviewresolver\"> \t\t<!-- specify where to look for view files --> \t\t<property name=\"prefix\" value=\"/web-inf/view/\" /> \t\t<property name=\"suffix\" value=\".jsp\" /> \t</bean> </beans> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Radio Buttons.html",
    "title": "Radio Buttons",
    "body": " index search search back radio buttons to pass and bind data from radio buttons to controllers an another views we use the form tag form:radiobutton which is surrounded by a form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and performs data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\tcountry: \t\t<!-- drop down list of country options --> \t\t<!-- we specify the variable where we store the selected value in the student object: which is country --> \t\t<form:select path=\"country\"> \t\t\t<!-- this is a list that was populated when we created the student object --> \t\t\t<!-- remember spring calls student.getcountryoptions() --> \t\t\t<form:options items=\"${student.countryoptions}\" /> \t\t</form:select> \t\t<br><br> \t\t<br><br> \t\tfavorite language: \t\t \t\t<!-- the \"path\" specifies the name of the property we are going to bind the radiobutton to, in this case \"favoritelanguage\" --> \t\t<!-- note these can also be populated from the student class or using a properties file --> \t\tjava <form:radiobutton path=\"favoritelanguage\" value=\"java\" /> \t\tc# <form:radiobutton path=\"favoritelanguage\" value=\"c#\" /> \t\tphp <form:radiobutton path=\"favoritelanguage\" value=\"php\" /> \t\truby <form:radiobutton path=\"favoritelanguage\" value=\"ruby\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \tthe student is confirmed: ${student.firstname} ${student.lastname} \t<br><br> \tselected coutry: ${student.country} ${student.lastname} \t<br><br> \t<!-- obtain the value using the binded variable inside the student object --> \tfavorite language: ${student.favoritelanguage} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t \tprivate string country; \t \tprivate linkedhashmap<string, string> countryoptions; \t \t// property we are going to bind to the radio buttons \tprivate string favoritelanguage; \t \tpublic student() { \t\t \t\t// populate country options: used iso country code \t\tcountryoptions = new linkedhashmap<>(); \t\t \t\tcountryoptions.put(\"br\", \"brazil\"); \t\tcountryoptions.put(\"fr\", \"france\"); \t\tcountryoptions.put(\"de\", \"germany\"); \t\tcountryoptions.put(\"in\", \"india\"); \t\tcountryoptions.put(\"us\", \"united states of america\");\t\t \t\t \t\t// we can also populate the favoritelanguage options from here \t\t// in the same manner we did with the country options \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getcountry() { \t\treturn country; \t} \tpublic void setcountry(string country) { \t\tthis.country = country; \t} \tpublic linkedhashmap<string, string> getcountryoptions() { \t\treturn countryoptions; \t} \t \t// setter and getter handlers for the new binded attribute \tpublic string getfavoritelanguage() { \t\treturn favoritelanguage; \t} \tpublic void setfavoritelanguage(string favoritelanguage) { \t\tthis.favoritelanguage = favoritelanguage; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Controller.html",
    "title": "Controller",
    "body": " index search search back controller create controller class package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.requestmapping; // add controller annotation @controller public class homecontroller { \t \t// add request mapping: this method controls the request coming to this url \t@requestmapping(\"/\") \tpublic string showpage() { \t // name of the view that is returned: note they are stored in web-inf/view/ \t\treturn \"main-menu\"; \t} } now, we create the view $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Resources.html",
    "title": "Add CSS and JS",
    "body": " index search search back add css and js here are the steps on how to access static resources in a spring mvc. for example, you can use this to access images, css, javascript files etc. you can configure references to static resources in the spring-mvc-demo-servlet.xml. add the following entry to your spring mvc configuration file: spring-mvc-demo-servlet.xml <mvc:resources mapping=\"/resources/**\" location=\"/resources/\"></mvc:resources> now in your view pages, you can access the static files using this syntax: <img src=\"${pagecontext.request.contextpath}/resources/images/spring-logo.png\"> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Form Tags.html",
    "title": "Form Tags",
    "body": " index search search back form tags form tags are configurable an reusable: they can make use of data binding (you can automatically set and retrieve data from a java object) you can mix them in with you html web page some examples are: reference spring mvc form tags to use these tags in your web page you have to specify the spring namespace at the beginning of the jsp file: <!-- reference to the namespace --> <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head></head> <body> </body> </html> text fields drop down lists radio buttons checkbox $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Text Fields.html",
    "title": "Text Fields",
    "body": " index search search back text fields to pass and bind data from input text fields to controllers an another views we use the form tag form:input along with form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and perfoms data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<!-- note the modelattribute equals the attribute we added to the model in the controller--> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\t<!-- to retrieve the data this maps to student.getfirstname() --> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> when we submit spring will call student.setfirstname() and student.setlastname() to save the data in the student object, so we can retrieve it from our controller method. for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \t<!-- obtain data from the model: note we use the attribute's name (i.e. student) to access the object --> \tthe student is confirmed: ${student.firstname} ${student.lastname} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t\t \tpublic student() {} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Adding Data.html",
    "title": "Model",
    "body": " index search search back model the model is a container for the application data. so in your controller you can put anything in the model (strings, objects, info from db, etc). and then you view page (jsp) can access data from the model. example controller package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; @controller public class helloworldcontroller { // new a controller method to read form data and // add data to the model @requestmapping(\"/processformversiontwo\")\t // the httpservletrequest allows you to retrieve information from the request (like the parameters of a form) // the model is our model where we will store data public string parsestring(httpservletrequest request, model model) { // read the request parameter from the html form string thename = request.getparameter(\"studentname\"); // convert the data to all caps thename = thename.touppercase(); // create the message string result = \"yo! \" + thename; // add message attribute to the model model.addattribute(\"message\", result); \t\t return \"helloworld\"; } } view now, on the view, we can access the model data: <!doctype html> <html> <body> hello world of spring! <br><br> student name: ${param.studentname} <br><br> <!-- access model data by the attribute's name--> the message: ${message} </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Number Validation.html",
    "title": "Number Range Validation",
    "body": " index search search back number range validation in this section we will show how to perform a number range validation. add validation rule to bean we create a customer class, whose freepasses variable must be a number between 0 and 10. public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t// minimum value we will expect \t@min(value=0, message=\"must be greater than or equal to zero\") \t// maximum value we will expect \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate int freepasses; \t \t... perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the lastname attribute in the customer class --> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the freepasses attribute in the customer class --> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Read HTML Form Data.html",
    "title": "Read HTML Form Data",
    "body": " index search search back read html form data the flow of our example will be the following: when the user accesses the url /showform, the browser will send a request to our controller, and our controller will return the corresponding view when the user hits submit on the form the action /processform is passed to the browser that will send a request to our controller, and our controller will process the request controller package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.requestmapping; @controller public class helloworldcontroller { \t// need a controller method to show the initial html form \t@requestmapping(\"/showform\") // the method name can be anything \tpublic string showform() { \t\treturn \"helloworld-form\"; \t} \t\t \t// need a controller method to process the html form \t@requestmapping(\"/processform\") \tpublic string processform() { \t\treturn \"helloworld\"; \t} \t\t } view we create web-inf/view/helloworld-form.jsp <!doctype html> <html> <head> \t<title>hello world - input form</title> </head> <body> <!-- the action is the request url --> \t<form action=\"processform\" method=\"get\"> \t\t<input type=\"text\" name=\"studentname\" \t\t\tplaceholder=\"what's your name?\" /> \t\t<input type=\"submit\" /> \t</form> </body> </html> and we create web-inf/view/helloworld-form.jsp <!doctype html> <html> <body> hello world of spring! <br><br> <!-- name of html form field from previous jsp view --> student name: ${param.studentname} </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/index.html",
    "title": "Spring MVC",
    "body": " index search search back spring mvc spring mvc is a framework for building web applications in java based on the model-view-controller design patter. the front controller is known as dispatcherservlet: it is part of the spring framework pre-processes and delegates requests from the web browser to your controllers the mvc pattern is made up of: model objects: contains the data view templates: ui of the app that displays data (most common templates: jsp + jslt) controller classes: business logic (handle request, access db, etc.) it includes the features of the core spring framework (inversion of control and dependency injection) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Drop Down Lists.html",
    "title": "Drop Down Lists",
    "body": " index search search back drop down lists to pass and bind data from drop down lists to controllers an another views we use the form tags form:select that encloses a set of options represented with form:option tags. and all these are surrounded by a form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and performs data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\tcountry: \t\t<!-- drop down list of country options --> \t\t<!-- we specify the variable where we store the selected value in the student object: which is country --> \t\t<form:select path=\"country\"> \t\t\t<!-- this is a list that was populated when we created the student object --> \t\t\t<!-- remember spring calls student.getcountryoptions() --> \t\t\t<form:options items=\"${student.countryoptions}\" /> \t\t</form:select> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \tthe student is confirmed: ${student.firstname} ${student.lastname} \t<!-- obtain the value saved in the coutry variable inside the student's object (corresponds to the selected value) --> \tselected coutry: ${student.country} ${student.lastname} </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t \tprivate string country; \t \tprivate linkedhashmap<string, string> countryoptions; \t \tpublic student() { \t\t \t\t// populate country options: used iso country code \t\tcountryoptions = new linkedhashmap<>(); \t\t \t\tcountryoptions.put(\"br\", \"brazil\"); \t\tcountryoptions.put(\"fr\", \"france\"); \t\tcountryoptions.put(\"de\", \"germany\"); \t\tcountryoptions.put(\"in\", \"india\"); \t\tcountryoptions.put(\"us\", \"united states of america\");\t\t \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getcountry() { \t\treturn country; \t} \t// setter and getter handlers for the new binded attribute \tpublic void setcountry(string country) { \t\tthis.country = country; \t} \tpublic linkedhashmap<string, string> getcountryoptions() { \t\treturn countryoptions; \t} } country options from a properties file we create web-inf/countries.properties: br=brazil fr=france co=colombia in=india update configuration's file spring-mvc-dmo-servlet.xml header (to use a new set of spring tags: utils): <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xsi:schemalocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd\"> load the country options properties file in the spring configuration file, with a bean id equal to \"countryoptions\": <util:properties id=\"countryoptions\" location=\"classpath:../countries.properties\" /> inject properties inside our controller: @value(\"#{countryoptions}\") private map<string, string> countryoptions; add countryoptions as an attribute of the model inside the controller method: @requestmapping(\"/showform\") public string showform(model themodel) { // create a student object student student thestudent = new student(); // add student object to the model themodel.addattribute(\"student\", thestudent); // add the country options to the model themodel.addattribute(\"thecountryoptions\", countryoptions); return \"student-form\"; } update the view as follows: <form:select path=\"country\"> <form:options items=\"${thecountryoptions}\" /> </form:select> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/CheckBox.html",
    "title": "Check Box",
    "body": " index search search back check box to pass and bind data from check boxes to controllers an another views we use the form tag form:checkbox which is surrounded by a form:form: controller add a model to the controller method for the form and create the model attribute, that holds the data and performs data binding package com.springdemo.mvc; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/student\") public class studentcontroller { \t \t// request to show the view that contains the form \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\t// create a student object \t\tstudent thestudent = new student(); \t\t \t\t// add student object to the model \t\tthemodel.addattribute(\"student\", thestudent); \t\t \t\treturn \"student-form\"; \t} \t \t// process the submit event on the form \t@requestmapping(\"/processform\") \t// we obtain the model attribute with the following annotation \tpublic string processform(@modelattribute(\"student\") student thestudent) { \t\t \t\t// now we can retrieve the updated information from the form \t\tsystem.out.println(\"thestudent: \" + thestudent.getfirstname() \t\t\t\t\t\t\t+ \" \" + thestudent.getlastname()); \t\t \t\treturn \"student-confirmation\"; \t} } view setting the html for data binding: for student-form.jsp: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!doctype html> <html> <head> \t<title>student registration form</title> </head> <body> \t<form:form action=\"processform\" modelattribute=\"student\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name: <form:input path=\"lastname\" /> \t\t<br><br> \t\tcountry: \t\t<!-- drop down list of country options --> \t\t<!-- we specify the variable where we store the selected value in the student object: which is country --> \t\t<form:select path=\"country\"> \t\t\t<!-- this is a list that was populated when we created the student object --> \t\t\t<!-- remember spring calls student.getcountryoptions() --> \t\t\t<form:options items=\"${student.countryoptions}\" /> \t\t</form:select> \t\t<br><br> \t\t<br><br> \t\tfavorite language: \t\t \t\tjava <form:radiobutton path=\"favoritelanguage\" value=\"java\" /> \t\tc# <form:radiobutton path=\"favoritelanguage\" value=\"c#\" /> \t\tphp <form:radiobutton path=\"favoritelanguage\" value=\"php\" /> \t\truby <form:radiobutton path=\"favoritelanguage\" value=\"ruby\" /> \t\t<br><br> \t\toperating systems: \t\t \t\t<!-- the \"path\" specifies the name of the property we are going to bind the radiobutton to, in this case \"operatingsystems\" --> \t\t<!-- note these can also be populated from the student class or using a properties file --> \t\tlinux <form:checkbox path=\"operatingsystems\" value=\"linux\" /> \t\tmac os <form:checkbox path=\"operatingsystems\" value=\"mac os\" /> \t\tms windows <form:checkbox path=\"operatingsystems\" value=\"ms window\" />\t \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> for student-confirmation.jsp: <%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %> <!doctype html> <html> <head> \t<title>student confirmation</title> </head> <body> \tthe student is confirmed: ${student.firstname} ${student.lastname} \t<br><br> \tselected coutry: ${student.country} ${student.lastname} \t<br><br> \t<!-- obtain the value using the binded variable inside the student object --> \tfavorite language: ${student.favoritelanguage} \t<br><br> \toperating systems: \t<!-- create an unordered list of the selected values in the checkbox --> \t\t<ul> \t\t\t<c:foreach var=\"temp\" items=\"${student.operatingsystems}\"> \t\t\t\t<li> ${temp} </li> \t\t\t</c:foreach> \t\t</ul> </body> </html> model the model attribute \"student\" is populated with an instance of the following student class: package com.springdemo.mvc; import java.util.linkedhashmap; public class student { \tprivate string firstname; \tprivate string lastname; \t \tprivate string country; \t \tprivate linkedhashmap<string, string> countryoptions; \t \tprivate string favoritelanguage; // attribute bound to the checkbox (multiple options so it is an array) private string[] operatingsystems; \t \tpublic student() { \t\t \t\t// populate country options: used iso country code \t\tcountryoptions = new linkedhashmap<>(); \t\t \t\tcountryoptions.put(\"br\", \"brazil\"); \t\tcountryoptions.put(\"fr\", \"france\"); \t\tcountryoptions.put(\"de\", \"germany\"); \t\tcountryoptions.put(\"in\", \"india\"); \t\tcountryoptions.put(\"us\", \"united states of america\");\t\t \t\t \t\t// we can also populate the favoritelanguage options from here \t\t// in the same manner we did with the country options \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getcountry() { \t\treturn country; \t} \tpublic void setcountry(string country) { \t\tthis.country = country; \t} \tpublic linkedhashmap<string, string> getcountryoptions() { \t\treturn countryoptions; \t} \t \tpublic string getfavoritelanguage() { \t\treturn favoritelanguage; \t} \tpublic void setfavoritelanguage(string favoritelanguage) { \t\tthis.favoritelanguage = favoritelanguage; \t} \t// setter and getter handlers for the new bound attribute public string[] getoperatingsystems() { \t\treturn operatingsystems; \t} \tpublic void setoperatingsystems(string[] operatingsystems) { \t\tthis.operatingsystems = operatingsystems; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Validation with Regular Expressions.html",
    "title": "Validation with Regular Expressions",
    "body": " index search search back validation with regular expressions in this section we will show how to perform a validation with regular expressions. add validation rule to bean we create a customer class, whose freepasses variable must be a number between 0 and 10. public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t@min(value=0, message=\"must be greater than or equal to zero\") \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate int freepasses; // define the regular expression for the postalcode attribute @pattern(regexp=\"^[a-za-z0-9]{5}\", message=\"only 5 chars/digits\") \tprivate string postalcode; \t \t... perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> <br><br> \t\tpostal code: <form:input path=\"postalcode\" /> <!-- the message shown equals the messages from both of the validation annotations defined for the postalcode attribute in the customer class --> \t\t<form:errors path=\"postalcode\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Form Validation.html",
    "title": "Form Validation",
    "body": " index search search back form validation java has a standard bean validation api that defines a metadata model and an api for entity validation. here is a list of bean validation features you can check: required validate length validate numbers validate with regular expressions custom validation some annotations to perform the validation are the following: set up add hibernate's library (hibernate validator)for bean validation which is fully compliant with java's bean validation api. required validation number range validation validation with regular expressions handle string in integer field custom validation $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Required Validation.html",
    "title": "Required Validation",
    "body": " index search search back required validation in this section we will show how to perform a required validation. add validation rule to bean we create a customer class, whose lastname attribute must be non-null, that is, lastname is a required attribute: package com.springdemo.mvc; import javax.validation.constraints.notnull; import javax.validation.constraints.size; public class customer { \tprivate string firstname; \t // validation annotation \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} } note that if we wanted to make an integer required, we must use the wrapper java classes (i.e. integer), that will be able to handle empty strings as inputs and nulls. the primitive types will throw an exception. perform validation in the controller we also package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> <!-- we use the error form tag to display an error when the input is not valid --> <!-- the message shown equals the messages from both of the validation annotations defined for the lastname attribute in the customer class --> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Request Params and Request Mappings.html",
    "title": "Request Params and Request Mappings",
    "body": " index search search back request params and request mappings request params spring provides for a specific annotation that allows you to retrieve request parameters directly without using the httpservletrequest object. given the form: package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.requestparam; @controller public class helloworldcontroller { \t@requestmapping(\"/processformversionthree\")\t \tpublic string processformversionthree( // we use the annotation to obtain the parameter \t\t\t@requestparam(\"studentname\") string thename, \t\t\tmodel model) { \t\t\t\t \t\t// convert the data to all caps \t\tthename = thename.touppercase(); \t\t \t\t// create the message \t\tstring result = \"hey my friend from v3! \" + thename; \t\t \t\t// add message to the model \t\tmodel.addattribute(\"message\", result); \t\t\t\t \t\treturn \"helloworld\"; \t}\t } controller request mappings they serve as a parent mapping for the controller all request mappings on methods in the controller are relative for example: package com.springdemo.mvc; import javax.servlet.http.httpservletrequest; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.requestparam; @controller // this is the request mapping for the controller @requestmapping(\"/hello\") public class helloworldcontroller { \t// both of these request mappings are relative to the parent mapping \t// that is the mapping translates to domain/hello/showform \t// need a controller method to show the initial html form \t@requestmapping(\"/showform\") \tpublic string showform() { \t\treturn \"helloworld-form\"; \t} \t\t \t// need a controller method to process the html form \t@requestmapping(\"/processform\") \tpublic string processform() { \t\treturn \"helloworld\"; \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Handle String in Integer Field.html",
    "title": "Handle String Input in Integer Field",
    "body": " index search search back handle string input in integer field if we want to avoid the trace returned by errors like inputting the wrong data type (string instead of int), we can define a custom message that will override those messages. create a custom message create a properties file in resources/messages.properties // errortype.springmodelattributename.fieldname typemismatch.customer.freepasses=invalid number specify properties file in configuration we add the following in our configuration file spring-mvc-demo-servlet.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" \txmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" \txmlns:context=\"http://www.springframework.org/schema/context\" \txmlns:mvc=\"http://www.springframework.org/schema/mvc\" \txsi:schemalocation=\" \t\thttp://www.springframework.org/schema/beans \thttp://www.springframework.org/schema/beans/spring-beans.xsd \thttp://www.springframework.org/schema/context \thttp://www.springframework.org/schema/context/spring-context.xsd \thttp://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"> \t<context:component-scan base-package=\"com.luv2code.springdemo\" /> \t<mvc:annotation-driven/> \t<bean \t\tclass=\"org.springframework.web.servlet.view.internalresourceviewresolver\"> \t\t<property name=\"prefix\" value=\"/web-inf/view/\" /> \t\t<property name=\"suffix\" value=\".jsp\" /> \t</bean> \t <!-- load custom message resources --> <bean id=\"messagesource\" class=\"org.springframework.context.support.resourcebundlemessagesource\"> \t\t\t\t<!-- path where the properties file is stored --> <property name=\"basenames\" value=\"resources/messages\" /> </bean> </beans> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/MVC/Custom Validation.html",
    "title": "Custom Validation",
    "body": " index search search back custom validation create a custom java annotation create annotation clas package com.springdemo.mvc.validation; import java.lang.annotation.elementtype; import java.lang.annotation.retention; import java.lang.annotation.retentionpolicy; import java.lang.annotation.target; import javax.validation.constraint; import javax.validation.payload; // specify the class that holds the validation logic @constraint(validatedby = coursecodeconstraintvalidator.class) // where you can use this annotation: on a method or on a field @target( { elementtype.method, elementtype.field } ) @retention(retentionpolicy.runtime) // note the @interface (it is needed to create the annotation) public @interface coursecode { \t// define default course code \tpublic string value() default \"luv\"; \t \t// define default error message \tpublic string message() default \"must start with luv\"; \t \t// define default groups \tpublic class<?>[] groups() default {}; \t \t// define default payloads \tpublic class<? extends payload>[] payload() default {}; } create validator class this class holds the validation logic package com.springdemo.mvc.validation; import javax.validation.constraintvalidator; import javax.validation.constraintvalidatorcontext; // implements the previous constraintvalidator interface, with generics: <annotation interface, data type> public class coursecodeconstraintvalidator \timplements constraintvalidator<coursecode, string> { \tprivate string courseprefix; \t \t@override \tpublic void initialize(coursecode thecoursecode) { \t\t// obtain prefix from the \"value\" attribute of our annotation \t\tcourseprefix = thecoursecode.value(); \t} \t@override \t// called when we use the @valid annotation \tpublic boolean isvalid(string thecode, constraintvalidatorcontext theconstraintvalidatorcontext) { \t\tboolean result; \t\t \t\t// validation logic \t\tif (thecode != null) { \t\t\tresult = thecode.startswith(courseprefix); \t\t} \t\telse { \t\t\tresult = true; \t\t} \t\t \t\treturn result; \t} } add custom validation public class customer { \tprivate string firstname; \t \t@notnull(message=\"is required\") \t@size(min=1, message=\"is required\") \tprivate string lastname; \t@notnull(message=\"is required\") \t@min(value=0, message=\"must be greater than or equal to zero\") \t@max(value=10, message=\"must be less than or equal to 10\") \tprivate integer freepasses; \t@pattern(regexp=\"^[a-za-z0-9]{5}\", message=\"only 5 chars/digits\") \tprivate string postalcode; \t // use our custom validation tag \t@coursecode(value=\"tops\", message=\"must start with tops\") \tprivate string coursecode; perform validation on controller package com.springdemo.mvc; import javax.validation.valid; import org.springframework.beans.propertyeditors.stringtrimmereditor; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.validation.bindingresult; import org.springframework.web.bind.webdatabinder; import org.springframework.web.bind.annotation.initbinder; import org.springframework.web.bind.annotation.modelattribute; import org.springframework.web.bind.annotation.requestmapping; @controller @requestmapping(\"/customer\") public class customercontroller { \t// add an initbinder ... to convert trim input strings \t// remove leading and trailing whitespace \t// resolve issue for our validation \t \t@initbinder \t//@initbinder annotation works as a pre-processor \t// it will pre-process each web request to our controller \tpublic void initbinder(webdatabinder databinder) { \t\t \t\t// trim strings (true: empty strings to null) \t\tstringtrimmereditor stringtrimmereditor = new stringtrimmereditor(true); \t \t\t// for every string class apply the trim editor \t\tdatabinder.registercustomeditor(string.class, stringtrimmereditor); \t} \t \t \t@requestmapping(\"/showform\") \tpublic string showform(model themodel) { \t\t \t\tthemodel.addattribute(\"customer\", new customer()); \t\t \t\treturn \"customer-form\"; \t} \t \t@requestmapping(\"/processform\") \t// @valid: tells spring to perform validation on the customer object \t// bindingresult: results of the validation will be placed in bindingresult \tpublic string processform( \t\t\t@valid @modelattribute(\"customer\") customer thecustomer, \t\t\tbindingresult thebindingresult) { \t\t \t\tsystem.out.println(\"last name: |\" + thecustomer.getlastname() + \"|\"); \t\t \t\t// check if validation was sucessfull \t\tif (thebindingresult.haserrors()) { \t\t\t// if not sucessfull send back \t\t\treturn \"customer-form\"; \t\t} \t\telse { \t\t\t// if sucessfull \t\t\treturn \"customer-confirmation\"; \t\t} \t} } when performing spring mvc validation, the location of the bindingresult parameter is very important. in the method signature, the bindingresult parameter must appear immediately after the model attribute. display error on html <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>customer registration form</title> \t \t<style> \t\t.error {color:red} \t</style> </head> <body> <i>fill out the form. asterisk (*) means required.</i> <br><br> \t<form:form action=\"processform\" modelattribute=\"customer\"> \t\tfirst name: <form:input path=\"firstname\" /> \t\t<br><br> \t\tlast name (*): <form:input path=\"lastname\" /> \t\t<form:errors path=\"lastname\" cssclass=\"error\" /> \t\t<br><br> \t\tfree passes: <form:input path=\"freepasses\" /> \t\t<form:errors path=\"freepasses\" cssclass=\"error\" /> <br><br> \t\tpostal code: <form:input path=\"postalcode\" /> \t\t<form:errors path=\"postalcode\" cssclass=\"error\" /> \t\t<br><br> <!-- the message shown equals the messages from both of the validation annotations defined for the coursecode attribute in the customer class --> \t\t\tcourse code: <form:input path=\"coursecode\" /> \t\t\t<form:errors path=\"coursecode\" cssclass=\"error\" /> \t\t<br><br> \t\t<input type=\"submit\" value=\"submit\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Maven/Overview.html",
    "title": "Maven",
    "body": " index search search back maven maven is a project management tool. the most popular use of maven is for build management and dependencies. behind the scenes what maven does is: reads the configuration file of our application: pom.xml checks on the local repository if the library is already stored (like a cache) if not, it goes to the remote repository and searches for it then it saves it to the local repository finally it uses the downloaded library to build and run the application maven also downloads the libraries' dependencies. and when you build and run your application, maven will handle the class/build path for you, based on the configuration file. standard directory structure $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Maven/Additional Repositories.html",
    "title": "Additional Repositories",
    "body": " index search search back additional repositories as we have said, if maven does not find some dependency in your local repository it goes to the central repository to search for it. but what if the dependency is not in the central repository. then we have to define the repository in our pom.xml: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Maven/POM File Structure.html",
    "title": "POM File Structure",
    "body": " index search search back pom file structure the pom file has the following structure: project metadata: information about the project dependencies: list of dependencies for the project plug-ins: additional custom tasks to run (junit tests, reports, etc) project coordinates project coordinates uniquely identify a project: where: group id: name of company, group or organization artifact id: name for the project version: a specific release version dependency coordinates to add a given dependency project, we need: group id artifact id optional: version (best practice to include the version) find dependencies search maven maven repository $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Maven/Maven Archetypes.html",
    "title": "Maven Archetypes",
    "body": " index search search back maven archetypes archetypes are used to create new maven projects, you can think of them as starter projects. some archetypes are: for standalone projects: maven-archetype-quickstart for web projects: maven-archetype-webapp $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Maven/Private Repositories.html",
    "title": "Private Repositories",
    "body": " index search search back private repositories if you want to create repositories with restricted access you can: set up your own private maven repository in your server, that is secure with credentials: id/password some maven repository manager products are: archiva artifactory nexus if you do not want to create your own server, there are also cloud based solutions like: package cloud my maven repo $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/index.html",
    "title": "Spring",
    "body": " index search search back spring introduction core spring framework spring mvc hibernate spring rest spring boot thymeleaf maven spring security aspect oriented programming intro spring docs getting ready spring framework set up core spring framework spring with xml configuration inversion of control dependency injection bean scopes and life cycle spring with java annotations java annotations spring with only java spring configuration with java code spring mvc overview configuration controller view read html form data model add css and js request params and request mappings form tags form validation hibernate overview configuration annotations usage sessions database operations advanced annotations concepts onetoone onetomany eager vs lazy loading manytomany spring rest json data binding spring rest controller pojos as json exception handling spring boot overview controller spring boot project structure spring boot starters spring boot devtools spring boot actuator application properties jpa spring data jpa spring data rest thymeleaf overview tables maven overview pom file structure maven archetypes additional repositories private repositories spring security overview java configuration basic security custom login form log out cross site request forgery display user and roles authorization jdbc database authentication aspect oriented programming (aop) overview before advice pointcut expressions pointcut declarations control aspect order joinpoints afterreturning advice afterthrowing advice after advice around advice $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Set Up.html",
    "title": "Set Up",
    "body": " index search search back set up requirements: jdk java application server (i.e. tomcat) java integrated development environment (ide) spring 5 jar files (download manually or use maven) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Custom Login Form.html",
    "title": "Custom Login Form",
    "body": " index search search back custom login form now we are going to configure the security of the access to web path in application, login, logout, etc: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t \t\tauth.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} \t@override \tprotected void configure(httpsecurity http) throws exception { // here is the control of the access to web path http.authorizerequests() // require authentication for every request .anyrequest().authenticated() // and for form login customize the login page shown .and() .formlogin() \t\t\t\t\t\t// custom jsp page .loginpage(\"/showmyloginpage\") \t\t\t\t\t\t// you do not need to create a method in your controller for this endpoint, it is handled by spring .loginprocessingurl(\"/authenticatetheuser\") .permitall(); \t\t \t} } create the form we create the login page /showmyloginpage as follows: <!-- reference the spring and jsp tags --> <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %> <html> <head> \t<title>custom login page</title> \t<style> \t\t.failed { \t\t\tcolor: red; \t\t} \t</style> </head> <body> <h3>my custom login page</h3> \t\t<!-- the form points to the endpoint specified preivously: \"authenticatetheuser\" --> \t\t<!-- contextpath is the domain of our app, i.e. localhost:8080 --> \t<form:form action=\"${pagecontext.request.contextpath}/authenticatetheuser\" \t\t\t method=\"post\"> \t\t<!-- check for login error --> \t\t<c:if test=\"${param.error != null}\"> \t\t\t<i class=\"failed\">sorry! you entered invalid username/password.</i> \t\t</c:if> \t\t<p> \t\t\tuser name: <input type=\"text\" name=\"username\" /> \t\t</p> \t\t<p> \t\t\tpassword: <input type=\"password\" name=\"password\" /> \t\t</p> \t\t<input type=\"submit\" value=\"login\" /> \t</form:form> </body> </html> note that spring appends a parameter error when the user fails to login. that is what we use as a condition to show our error message, that is, we check if param.error exists. also, spring security defines default names for login form fields: user name field: username password field: password login controller we also need a controller method for requests to /showmyloginpage: package com.springsecurity.demo.controller; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.getmapping; @controller public class logincontroller { \t@getmapping(\"/showmyloginpage\") \tpublic string showmyloginpage() { \t\t \t\t// this is the custom-login.jsp we created in the previous section \t\treturn \"custom-login\"; \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/JDBC Database Authentication.html",
    "title": "JDBC Database Authentication",
    "body": " index search search back jdbc database authentication spring security can read user account info from database by default, you have to follow spring security's predefined table schemas. you can customize the table schemas, but you will be responsible for writing the code to access the data. set up database the tables we have to create are the following: password encryption in spring security 5, passwords are stored using a specific format: {id}encodedpassword the id references the operation used to encrypt the password: noop: plain text. so the password is stored as follows in the database: {noop}test123 bcrypt: bcrypt password hashing. so the password is stored as follows in the database: {bcrypt}$2a$12$r9h/cipz0gi.urnnx3kh2opst9/pgbkqquzi.ss7kiugo2t0jwmuw etc. add dependiencies we define the dependencies in our pom.xmlfile that are needed to add support to connect to databases: \t\t<!-- add mysql and c3p0 support --> \t\t<dependency> \t\t\t<groupid>mysql</groupid> \t\t\t<artifactid>mysql-connector-java</artifactid> \t\t\t<version>8.0.16</version> \t\t</dependency> \t\t \t\t<dependency> \t\t\t<groupid>com.mchange</groupid> \t\t\t<artifactid>c3p0</artifactid> \t\t\t<version>0.9.5.4</version> \t\t</dependency> jdbc properties files inside /src/main/resources we create the properties file persistence-mysql.properties for our database connections: # # jdbc connection properties # jdbc.driver=com.mysql.jdbc.driver jdbc.url=jdbc:mysql://localhost:3306/spring_security_demo_plaintext?usessl=false jdbc.user=springstudent jdbc.password=springstudent # # connection pool properties # connection.pool.initialpoolsize=5 connection.pool.minpoolsize=5 connection.pool.maxpoolsize=20 connection.pool.maxidletime=3000 spring security configuration we have to modify our main configuration class, to include our database properties file and create the datasource package com.luv2code.springsecurity.demo.config; import java.beans.propertyvetoexception; import java.util.logging.logger; import javax.sql.datasource; import org.springframework.beans.factory.annotation.autowired; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.propertysource; import org.springframework.core.env.environment; import org.springframework.web.servlet.viewresolver; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.view.internalresourceviewresolver; import com.mchange.v2.c3p0.combopooleddatasource; @configuration @enablewebmvc @componentscan(basepackages=\"com.luv2code.springsecurity.demo\") @propertysource(\"classpath:persistence-mysql.properties\") public class demoappconfig { \t// set up variable to hold the properties \t@autowired \tprivate environment env; \t \t// set up a logger for diagnostics \tprivate logger logger = logger.getlogger(getclass().getname()); \t \t \t// define a bean for viewresolver \t@bean \tpublic viewresolver viewresolver() { \t\t \t\tinternalresourceviewresolver viewresolver = new internalresourceviewresolver(); \t\t \t\tviewresolver.setprefix(\"/web-inf/view/\"); \t\tviewresolver.setsuffix(\".jsp\"); \t\t \t\treturn viewresolver; \t} \t \t// define a bean for our security datasource \t \t@bean \tpublic datasource securitydatasource() { \t\t \t\t// create connection pool \t\tcombopooleddatasource securitydatasource \t\t\t\t\t\t\t\t\t= new combopooleddatasource(); \t\t\t\t \t\t// set the jdbc driver class \t\ttry { // obtain driver from properties file \t\t\tsecuritydatasource.setdriverclass(env.getproperty(\"jdbc.driver\")); \t\t} catch (propertyvetoexception exc) { \t\t\tthrow new runtimeexception(exc); \t\t} \t\t \t\t // obtain database info from properties file \t\tlogger.info(\">>> jdbc.url=\" + env.getproperty(\"jdbc.url\")); \t\tlogger.info(\">>> jdbc.user=\" + env.getproperty(\"jdbc.user\")); \t\t \t\t \t\t// set database connection props \t\tsecuritydatasource.setjdbcurl(env.getproperty(\"jdbc.url\")); \t\tsecuritydatasource.setuser(env.getproperty(\"jdbc.user\")); \t\tsecuritydatasource.setpassword(env.getproperty(\"jdbc.password\")); \t\t \t\t// set connection pool props \t\tsecuritydatasource.setinitialpoolsize( \t\t\t\tgetintproperty(\"connection.pool.initialpoolsize\")); \t\tsecuritydatasource.setminpoolsize( \t\t\t\tgetintproperty(\"connection.pool.minpoolsize\")); \t\tsecuritydatasource.setmaxpoolsize( \t\t\t\tgetintproperty(\"connection.pool.maxpoolsize\")); \t\tsecuritydatasource.setmaxidletime( \t\t\t\tgetintproperty(\"connection.pool.maxidletime\")); \t\t \t\treturn securitydatasource; \t} \t \t// need a helper method \t// read environment property and convert to int \t \tprivate int getintproperty(string propname) { \t\t \t\tstring propval = env.getproperty(propname); \t\t \t\t// now convert to int \t\tint intpropval = integer.parseint(propval); \t\t \t\treturn intpropval; \t} } now in our security configuration we do two things: inject the datasource we defined previouly that holds authentication information tell spring to use jdbc for authentication package com.springsecurity.demo.config; import javax.sql.datasource; import org.springframework.beans.factory.annotation.autowired; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t// add a reference to our security data source \t@autowired \tprivate datasource securitydatasource; \t \t \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// use jdbc authentication \t\tauth.jdbcauthentication().datasource(securitydatasource); \t\t \t} \t@override \tprotected void configure(httpsecurity http) throws exception { \t\thttp.authorizerequests() \t\t\t.antmatchers(\"/\").hasrole(\"employee\") \t\t\t.antmatchers(\"/leaders/**\").hasrole(\"manager\") \t\t\t.antmatchers(\"/systems/**\").hasrole(\"admin\") \t\t\t.and() \t\t\t.formlogin() \t\t\t\t.loginpage(\"/showmyloginpage\") \t\t\t\t.loginprocessingurl(\"/authenticatetheuser\") \t\t\t\t.permitall() \t\t\t.and() \t\t\t.logout().permitall() \t\t\t.and() \t\t\t.exceptionhandling().accessdeniedpage(\"/access-denied\"); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Authorization.html",
    "title": "Authorization",
    "body": " index search search back authorization in this section we are going to show how to restrict access based on roles. our example follows the following scheme: where only managers and above can access the /leaders endpoint and only admins can access the /systems endpoint. create controllers we create a basic controller for every endpoint: package com.springsecurity.demo.controller; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.getmapping; @controller public class democontroller { \t// add request mapping for index page \t@getmapping(\"/\") \tpublic string showhome() { \t\t \t\treturn \"home\"; \t} \t \t// add request mapping for /leaders \t@getmapping(\"/leaders\") \tpublic string showleaders() { \t\t \t\treturn \"leaders\"; \t} \t \t// add request mapping for /systems \t@getmapping(\"/systems\") \tpublic string showsystems() { \t\t \t\treturn \"systems\"; \t} \t } we also create a controller for the /acess-denied endpoint: package com.springsecurity.demo.controller; import org.springframework.stereotype.controller; import org.springframework.web.bind.annotation.getmapping; @controller public class logincontroller { \t@getmapping(\"/showmyloginpage\") \tpublic string showmyloginpage() { \t\t \t\t// return \"plain-login\"; \t\treturn \"fancy-login\"; \t} \t \t// add request mapping for /access-denied \t@getmapping(\"/access-denied\") \tpublic string showaccessdenied() { \t\t \t\treturn \"access-denied\"; \t} } define user roles and restrict accessand restrict access in our configuration file we had saved in-memory a list of users with some defined roles, we are going to update it to have more roles. we are also going to define the authorization scheme we showed earlier. package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { @override protected void configure(authenticationmanagerbuilder auth) throws exception { // add our users for in memory authentication userbuilder users = user.withdefaultpasswordencoder(); // add more roles auth.inmemoryauthentication() .withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) .withuser(users.username(\"mary\").password(\"test123\").roles(\"employee\", \"manager\")) .withuser(users.username(\"susan\").password(\"test123\").roles(\"employee\", \"admin\")); } @override protected void configure(httpsecurity http) throws exception { // handle requests http.authorizerequests() // set role for index page .antmatchers(\"/\").hasrole(\"employee\") // set role for leaders page .antmatchers(\"/leaders/**\").hasrole(\"manager\") // set role for systems page .antmatchers(\"/systems/**\").hasrole(\"admin\") .and() .formlogin() .loginpage(\"/showmyloginpage\") .loginprocessingurl(\"/authenticatetheuser\") .permitall() .and() .logout().permitall() // also define the page where the user is redirected if it does not have access to the resource it requests .and() .exceptionhandling().accessdeniedpage(\"/access-denied\"); } } display content based on roles in our home page, we add two conditionals so only managers can see the link to the leaders page, and only admins can see the link to the systems page: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <%@ taglib prefix=\"security\" uri=\"http://www.springframework.org/security/tags\" %> <html> <head> \t<title>luv2code company home page</title> </head> <body> \t<h2>luv2code company home page</h2> \t<hr> \t<p> \twelcome to the luv2code company home page! \t</p> \t<hr> \t<!-- display user name and role --> \t<p> \t\tuser: <security:authentication property=\"principal.username\" /> \t\t<br><br> \t\trole(s): <security:authentication property=\"principal.authorities\" /> \t</p> \t<!-- check if user has the manager role, if so show the link --> \t<security:authorize access=\"hasrole('manager')\"> \t\t<!-- add a link to point to /leaders ... this is for the managers --> \t\t<p> \t\t\t<a href=\"${pagecontext.request.contextpath}/leaders\">leadership meeting</a> \t\t\t(only for manager peeps) \t\t</p> \t</security:authorize>\t \t<!-- check if user has the admin role, if so show the link --> \t<security:authorize access=\"hasrole('admin')\"> \t\t<!-- add a link to point to /systems ... this is for the admins --> \t\t<p> \t\t\t<a href=\"${pagecontext.request.contextpath}/systems\">it systems meeting</a> \t\t\t(only for admin peeps) \t\t</p> \t</security:authorize> \t<hr> \t<!-- add a logout button --> \t<form:form action=\"${pagecontext.request.contextpath}/logout\" \t\t\t method=\"post\"> \t\t<input type=\"submit\" value=\"logout\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Java Configuration.html",
    "title": "Java Configuration",
    "body": " index search search back java configuration we are going to show the demoappconfig.java that holds the base configuration of our application: package com.springsecurity.demo.config; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.web.servlet.viewresolver; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.view.internalresourceviewresolver; // tell spring this is a configuration file @configuration // enables annotations @enablewebmvc // search for components in \"com.springsecurity.demo\" package @componentscan(basepackages=\"com.springsecurity.demo\") public class demoappconfig { \t// define a bean for viewresolver \t@bean \tpublic viewresolver viewresolver() { \t\t \t\tinternalresourceviewresolver viewresolver = new internalresourceviewresolver(); \t\t \t\tviewresolver.setprefix(\"/web-inf/view/\"); \t\tviewresolver.setsuffix(\".jsp\"); \t\t \t\treturn viewresolver; \t} } as you can see we have defined a viewresolver that prepends /web-inf/view/ to every view, and appends .jsp to every view. web app initializer spring mvc provides support for web app initialization, and makes sure your code is automatically detected. your code is used to initialize the servlet container. as an example: package com.springsecurity.demo.config; import org.springframework.web.servlet.support.abstractannotationconfigdispatcherservletinitializer; public class myspringmvcdispatcherservletinitializer extends abstractannotationconfigdispatcherservletinitializer { \t@override \tprotected class<?>[] getrootconfigclasses() { \t\t// todo auto-generated method stub \t\treturn null; \t} \t@override \t// tell spring where the configuration for the servlet is \tprotected class<?>[] getservletconfigclasses() { \t\treturn new class[] { demoappconfig.class }; \t} \t@override \t// map the servlet to the path \"/\" \tprotected string[] getservletmappings() { \t\treturn new string[] { \"/\" }; \t} } here is the correspondence with the xml servlet configuration file: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Basic Security.html",
    "title": "Basic Security",
    "body": " index search search back basic security create security spring initializer spring security provides support for security initialization. your security code is used to initialize the servlet container. there is a special class to register the spring security filters. you need this class for the spring security filters to \"activate\". next we show an example: package com.springsecurity.demo.config; import org.springframework.security.web.context.abstractsecuritywebapplicationinitializer; public class securitywebapplicationinitializer \t\t\t\t\t\textends abstractsecuritywebapplicationinitializer { } create spring security configuration (@configuration) now we create our spring security configuration file: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; // tell spring this is a configuration file @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication (this is for test purposes only, you would usually retrieve this information encrypted from the database) \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t // use the authenticationmanagerbuilder given by spring to handle authentication \t\tauth \t\t\t.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} } add users, passwords and roles $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/index.html",
    "title": "Overview",
    "body": " index search search back overview spring security is implemented using servlet filters in the background there are two methods of securing a web app: declarative programmatic servlet filters servlet filters are used to pre-process/post-process web requests. they can route web requests based on security logic. spring provides a bulk of security functionality with servlet filters. this is described in the following picture: we can see spring intercepts the request to /mytopsecretstuff and uses the app's security configuration, alongside information about the user, passwords and roles to pre and post-process the request. spring security in action next we show a flowchart of the pre-processing made by spring security filters: if the resource is protected we go to step (2), else we go to step (4) if the user is authenticated we go to step (3), else we go to step (6) if the user is authorized to access the resource we go to step (4), else we go to step (5) the resource is shown to the user the access to the resource is denied we send the user to the login page, if the user logins correctly we go to step (3) declarative security you define your application's security constraints in configuration. for that, you can either: use all java configuration (@configuration) use a spring configuration file (xml) programmatic security you can also do it programmatically: spring security provides an api for custom application coding. it also provides greater customization for specific apps. authentication/authorization information about users/passwords/roles, etc can be stored: in-memory jdbc ldap custom etc maven dependencies to use this framework, you have to add the following dependency to your project: \t<dependencies> ... \t\t<!-- spring security --> \t\t<!-- spring-security-web and spring-security-config --> \t\t \t\t<dependency> \t\t <groupid>org.springframework.security</groupid> \t\t <artifactid>spring-security-web</artifactid> \t\t <version>${springsecurity.version}</version> \t\t</dependency> ... \t<dependencies> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Cross Site Request Forgery.html",
    "title": "Cross Site Request Forgery",
    "body": " index search search back cross site request forgery spring security protects against cross-site request forgery. csrf is a security attack where a website tricks you into executing an action on a web application that you are currently logged in. protection from this type of attack is embedded in the spring security filters. this protection is enabled by default. spring security uses the synchronizer token pattern, where each request includes a session cookie and a randomly generated token. so for request processing, spring security verifies the token before processing. how to use it? for form submissions use \"post\" instead of \"get\" the spring security tag <form:form> automatically adds the csrf token. if you do not use the tag, you must manually add the csrf token. if you do not add the token you get an error message: 403 forbidden, and further information about how the token cannot be null. how to see the csrf token? when your jsp with the <form:form> tag is processed into an html page, you will be able to see the token inside the form tag: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Display User and Roles.html",
    "title": "Display User and Roles",
    "body": " index search search back display user and roles in this section we are going to show how to display in our jsp files the user id and its role: add jsp tag library as dependency first we add to our pom.xml file the jsp tag library: \t\t<!-- add spring security taglibs support --> \t\t<dependency> \t\t <groupid>org.springframework.security</groupid> \t\t <artifactid>spring-security-taglibs</artifactid> \t\t <version>${springsecurity.version}</version> \t\t</dependency>\t jsp page then add the tag library to the jsp page, and we use its tags to access the user id and its role: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <!-- add tag library --> <%@ taglib prefix=\"security\" uri=\"http://www.springframework.org/security/tags\" %> <html> <head> \t<title>luv2code company home page</title> </head> <body> \t<h2>luv2code company home page</h2> \t<hr> \t<p> \twelcome to the luv2code company home page! \t</p> \t<hr> \t<!-- display user name and role --> \t<p> \t\tuser: <security:authentication property=\"principal.username\" /> \t\t<br><br> \t\trole(s): <security:authentication property=\"principal.authorities\" /> \t</p> \t<hr> \t<!-- add a logout button --> \t<form:form action=\"${pagecontext.request.contextpath}/logout\" \t\t\t method=\"post\"> \t\t<input type=\"submit\" value=\"logout\" /> \t</form:form> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Security/Log Out.html",
    "title": "Log Out",
    "body": " index search search back log out we are going to show in this section how to add the logout functionality to our spring application. configuration to our existing configuration we add: package com.springsecurity.demo.config; import org.springframework.context.annotation.configuration; import org.springframework.security.config.annotation.authentication.builders.authenticationmanagerbuilder; import org.springframework.security.config.annotation.web.builders.httpsecurity; import org.springframework.security.config.annotation.web.configuration.enablewebsecurity; import org.springframework.security.config.annotation.web.configuration.websecurityconfigureradapter; import org.springframework.security.core.userdetails.user; import org.springframework.security.core.userdetails.user.userbuilder; @configuration @enablewebsecurity public class demosecurityconfig extends websecurityconfigureradapter { \t@override \tprotected void configure(authenticationmanagerbuilder auth) throws exception { \t\t// add our users for in memory authentication \t\tuserbuilder users = user.withdefaultpasswordencoder(); \t\t \t\tauth.inmemoryauthentication() \t\t\t.withuser(users.username(\"john\").password(\"test123\").roles(\"employee\")) \t\t\t.withuser(users.username(\"mary\").password(\"test123\").roles(\"manager\")) \t\t\t.withuser(users.username(\"susan\").password(\"test123\").roles(\"admin\")); \t} \t@override \tprotected void configure(httpsecurity http) throws exception { // here is the control of the access to web path http.authorizerequests() // require authentication for every request .anyrequest().authenticated() .and() .formlogin() .loginpage(\"/showmyloginpage\") .loginprocessingurl(\"/authenticatetheuser\") .permitall(); // add logout functionality .and() .logout().permitall() \t\t \t} } the default url for logging out is /logout. log out button now we create the logout button in our home page: <%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %> <html> <head> \t<title>luv2code company home page</title> </head> <body> \t<h2>luv2code company home page</h2> \t<hr> \t<p> \twelcome to the luv2code company home page! \t</p> \t<!-- add a logout button: it point to \"/logout\" endpoint --> \t<form:form action=\"${pagecontext.request.contextpath}/logout\" \t\t\t method=\"post\"> \t\t<input type=\"submit\" value=\"logout\" /> \t</form:form> </body> </html> note that the logout logic is handled directly by spring, what it does is: invalidate the user's http session and remove cookies, etc. sends the user back to the login page appends a logout parameter: ?logout $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Thymeleaf/Overview.html",
    "title": "Overview",
    "body": " index search search back overview thymeleaf is a java templating engine. a thymeleaf template can be an html page with some thymeleaf expressions and include dynamic content from thymeleaf expressions. in a web app, thymeleaf is processed on the server. to use thymeleaf you have to include it in your dependencies: ... \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-thymeleaf</artifactid> \t\t</dependency> ... placement in spring boot, your thymeleaf template files go in src/main/resources/templates. and for web apps, thymeleaf templates have an .html extension. example given the following controller: @controller public class democontroller { \t// create a mapping for \"/hello\" \t@getmapping(\"/hello\") \tpublic string sayhello(model themodel) { \t\t \t\tthemodel.addattribute(\"thedate\", new java.util.date()); \t\t \t\treturn \"helloworld\"; \t} } we create the corresponding template helloworld.html: <!doctype html> <html xmlns:th=\"http://www.thymeleaf.org\"> <head> \t<title>thymeleaf demo</title> </head> <!-- we obtain the date from the model --> <body> \t<p th:text=\"'time on the server is ' + ${thedate}\" /> </body> </html> to add styles, we create a css files in src/main/resources/static/css, and then we reference the styles: <!doctype html> <html xmlns:th=\"http://www.thymeleaf.org\"> <head> \t<title>thymeleaf demo</title> \t<!-- reference css file --> \t<link rel=\"stylesheet\" \t\t th:href=\"@{/css/demo.css}\" /> </head> <body> \t<p th:text=\"'time on the server is ' + ${thedate}\" class=\"funny\" /> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Thymeleaf/Tables.html",
    "title": "Tables in Thymeleaf",
    "body": " index search search back tables in thymeleaf in this section we are going to show how to create a table with thymeleaf: controller we create a controller for employee, to list and add employees. package com.springboot.thymeleafdemo.controller; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.stereotype.controller; import org.springframework.ui.model; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.requestmapping; import com.springboot.thymeleafdemo.model.employee; @controller @requestmapping(\"/employees\") public class employeecontroller { \t// load employee data \t \tprivate list<employee> theemployees; \t \t@postconstruct \tprivate void loaddata() { \t\t \t\t// create employees \t\temployee emp1 = new employee(1, \"leslie\", \"andrews\", \"leslie@luv2code.com\"); \t\temployee emp2 = new employee(2, \"emma\", \"baumgarten\", \"emma@luv2code.com\"); \t\temployee emp3 = new employee(3, \"avani\", \"gupta\", \"avani@luv2code.com\"); \t\t// create the list \t\ttheemployees = new arraylist<>(); \t\t \t\t// add to the list \t\ttheemployees.add(emp1); \t\ttheemployees.add(emp2); \t\ttheemployees.add(emp3); \t \t} \t \t// add mapping for \"/list\" \t@getmapping(\"/list\") \tpublic string listemployees(model themodel) { \t\t \t\t// add to the spring model \t\tthemodel.addattribute(\"employees\", theemployees); \t\t \t\treturn \"list-employees\"; \t} } entity we create the entity employee: package com.springboot.thymeleafdemo.model; public class employee { \tprivate int id; \tprivate string firstname; \tprivate string lastname; \tprivate string email; \tpublic employee() { \t\t \t} \tpublic employee(int id, string firstname, string lastname, string email) { \t\tthis.id = id; \t\tthis.firstname = firstname; \t\tthis.lastname = lastname; \t\tthis.email = email; \t} \tpublic int getid() { \t\treturn id; \t} \tpublic void setid(int id) { \t\tthis.id = id; \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \tpublic string getemail() { \t\treturn email; \t} \tpublic void setemail(string email) { \t\tthis.email = email; \t} \t@override \tpublic string tostring() { \t\treturn \"employee [id=\" + id + \", firstname=\" + firstname + \", lastname=\" + lastname + \", email=\" + email + \"]\"; \t} \t\t } template finally we create the template for list-employees.html: <!doctype html> <html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"> <head> <!-- required meta tags --> <meta charset=\"utf-8\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"> <!-- bootstrap css --> <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css\" integrity=\"sha384-gjzzqfgwb1qttn6wy59fff1bugjplsa9dkkmp0dgimdm4iymj70gzwkybi706tws\" crossorigin=\"anonymous\"> <title>employee directory</title> </head> <body> <div class=\"container\"> <h3>employee directory</h3> <hr> <table class=\"table table-bordered table-striped\"> <thead class=\"thead-dark\"> <tr> <th>first name</th> <th>last name</th> <th>email</th> </tr> </thead> <tbody> <!-- for loop for all employees, stored in the model --> <tr th:each=\"tempemployee : ${employees}\"> <td th:text=\"${tempemployee.firstname}\" />\t <td th:text=\"${tempemployee.lastname}\" />\t <td th:text=\"${tempemployee.email}\" />\t </tr> </tbody>\t\t </table> </div> </body> </html> $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Bean Scopes/Life Cycle.html",
    "title": "Bean Life Cycle",
    "body": " index search search back bean life cycle the bean life cycle is as follows: as you can see you can add method/hooks: add custom code during bean initialization calling business logic methods setting up handles to resources (db, sockets, etc) add custom code during bean destruction calling business logic methods clean up handles to resources (db, sockets, etc) define methods first of all we define the methods in our bean: package com.springdemo; public class trackcoach implements coach { \tprivate fortuneservice fortuneservice; \tpublic trackcoach() { \t\t \t} \t \tpublic trackcoach(fortuneservice fortuneservice) { \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn \"just do it: \" + fortuneservice.getfortune(); \t} \t// add an init method \tpublic void domystartupstuff() { \t\tsystem.out.println(\"trackcoach: inside method domystartupstuff\"); \t} \t \t// add a destroy method \tpublic void domycleanupstuffyoyo() { \t\tsystem.out.println(\"trackcoach: inside method domycleanupstuffyoyo\");\t\t \t} } configure hooks in the configuration file once the initialization and clean-up methods have been defined, we configure them in our configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- define your beans here --> \t \t<!-- define the dependency --> \t<bean id=\"myfortuneservice\" \t class=\"com.springdemo.happyfortuneservice\"> \t</bean> \t \t<!-- note the new tag \"scope\" --> \t<bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\" \t\tinit-method=\"domystartupstuff\" \t\tdestroy-method=\"domycleanupstuffyoyo\">\t \t\t \t\t<!-- set up constructor injection --> \t\t<constructor-arg ref=\"myfortuneservice\" /> \t</bean> </beans> main method now in our app, we create the bean to check that our methods are being called: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class beanlifecycledemoapp { \t \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"beanlifecycle-applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// close the context \t\tcontext.close(); \t} } notes when using xml configuration, i want to provide additional details regarding the method signatures of the init-method and destroy-method . access modifier: the method can have any access modifier (public, protected, private) return type: the method can have any return type. however, \"void' is most commonly used. if you give a return type just note that you will not be able to capture the return value. as a result, \"void\" is commonly used. method name: the method can have any method name. arguments: the method can not accept any arguments. the method should be no-arg. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Bean Scopes/Bean Scopes and Life cycle.html",
    "title": "Spring Bean Scopes and Life Cycle",
    "body": " index search search back spring bean scopes and life cycle scope life cycle $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Bean Scopes/Scope.html",
    "title": "Bean Scopes",
    "body": " index search search back bean scopes intro the scope of a bean refers to the life cycle of the bean: how long does it live how many instances are created how is the bean shared the default scope of the bean is a singleton: the spring container creates only one instance of the bean it is cached in memory all requests to the bean will return a shared reference to the same bean other scopes are: a singleton scope is good for stateless data a prototype scope is good for stateful data (the container returns a new bean for each request). note that for this type of bean, spring does not call the destroy method. specify scope in xml config file <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <!-- note the new tag \"scope\" --> <bean id=\"mycoach\" class=\"com.springdemo.trackcoach\" scope=\"prototype\">\t <!-- set up constructor injection --> <constructor-arg ref=\"myfortuneservice\" /> </bean> </beans> main method now, from our application we do: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class beanscopedemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"beanscope-applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\tcoach alphacoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// check if they are the same \t\tboolean result = (thecoach == alphacoach); \t\t \t\t// print out the results \t\tsystem.out.println(\"\\npointing to the same object: \" + result); \t\t \t\tsystem.out.println(\"\\nmemory location for thecoach: \" + thecoach); \t\tsystem.out.println(\"\\nmemory location for alphacoach: \" + alphacoach + \"\\n\"); \t \t\t// close the context \t\tcontext.close(); \t} } observe, the result variable should be set to false, because we are using the prototype scope. also the values of the memory location for the two objects should be distinct for that same reason. however if we were using scope=\"singleton\", then result should be true, and both objects should have the same memory location. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Spring Rest/Spring Rest Controller.html",
    "title": "Spring Rest Controller",
    "body": " index search search back spring rest controller spring web mvc provides support for spring rest. for that we use a new annotation called restcontroller which is an extension of controller and handles rest requests and responses. spring rest will also automatically convert java pojos to json as long as the jackson project is on the classpath or pom.xml. hello world to exemplify how to set up a rest controller in spring we will create an application that upong request sends back a hello world! message: configuration first of all, make sure you have the jackson project, mvc and rest and also servlet libraries as a maven dependency or as a library in your classpath. \t<dependencies> \t\t<!-- add spring mvc and rest support --> \t\t<dependency> \t\t\t<groupid>org.springframework</groupid> \t\t\t<artifactid>spring-webmvc</artifactid> \t\t\t<version>5.0.5.release</version> \t\t</dependency> \t\t \t\t<!-- add jackson for json converters --> \t\t<dependency> \t\t\t<groupid>com.fasterxml.jackson.core</groupid> \t\t\t<artifactid>jackson-databind</artifactid> \t\t\t<version>2.9.9.2</version> \t\t</dependency> \t\t<!-- add servlet support for \t\t\t spring's abstractannotationconfigdispatcherservletinitializer --> \t\t<dependency> \t\t\t<groupid>javax.servlet</groupid> \t\t\t<artifactid>javax.servlet-api</artifactid> \t\t\t<version>3.1.0</version> \t\t</dependency> \t\t<!-- add support for jsp ... get rid of eclipse error -->\t\t\t\t \t\t<dependency> \t\t\t<groupid>javax.servlet.jsp</groupid> \t\t\t<artifactid>javax.servlet.jsp-api</artifactid> \t\t\t<version>2.3.1</version> \t\t</dependency> \t\t\t\t \t</dependencies> general we create a configuration class as follows: package com.springdemo.config; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.web.servlet.config.annotation.enablewebmvc; import org.springframework.web.servlet.config.annotation.webmvcconfigurer; // mark it as a configuration class @configuration @enablewebmvc // enable component scanning in our source code @componentscan(\"com.springdemo\") public class demoappconfig implements webmvcconfigurer { } servlet initializer we have to specify the configuration of our servlet, for this we extend abstractannotationconfigdispatcherservletinitializer: package com.springdemo.config; import org.springframework.web.servlet.support.abstractannotationconfigdispatcherservletinitializer; public class myspringmvcdispatcherservletinitializer extends abstractannotationconfigdispatcherservletinitializer { \t@override \tprotected class<?>[] getrootconfigclasses() { \t\t// todo auto-generated method stub \t\treturn null; \t} \t@override \tprotected class<?>[] getservletconfigclasses() { // specify our config class \t\treturn new class[] { demoappconfig.class }; \t} \t@override \tprotected string[] getservletmappings() { \t\treturn new string[] { \"/\" }; \t} } controller for this we need to create our server with the controller that handles this request: package com.springdemo.rest; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; @restcontroller @requestmapping(\"/test\") public class demorestcontroller { \t// add code for the \"/hello\" endpoint \t \t@getmapping(\"/hello\") \tpublic string sayhello() { \t\treturn \"hello world!\"; \t} \t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Spring Rest/POJOs as JSON.html",
    "title": "POJOs as JSON",
    "body": " index search search back pojos as json to test converting pojos to json we are going to create a service that allows us to retrieve a list of students: create pojo we are going to create the student entity: package com.springdemo.entity; public class student { \tprivate string firstname; \tprivate string lastname; \t \tpublic student() { \t\t \t} \tpublic student(string firstname, string lastname) { \t\tthis.firstname = firstname; \t\tthis.lastname = lastname; \t} \tpublic string getfirstname() { \t\treturn firstname; \t} \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \tpublic string getlastname() { \t\treturn lastname; \t} \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t } create service we now code the logic that handles the controller. package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.luv2code.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t \t \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t \t \t \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\treturn thestudents.get(studentid); \t\t \t} } note that the endpoint \"/students/{studentid}\" has a path variable studentid $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Spring Rest/JSON Data Binding.html",
    "title": "Java JSON Data Binding",
    "body": " index search search back java json data binding data binding is the process of converting json data to a java pojo (the conversion goes both ways) data binding is the same as serialization/deserialization and marshalling/unmarshalling. spring uses the jackson project behind the scenes which handles data binding between json and java pojos. for conversion we use object mapper by default jackson will call appropiate getter and setter methods to populate a pojo from a json or to create a json object from a pojo. to convert from json to java, jackson calls the setter methods to convert from java to json, jackson calls the getter methods set up add jackson project as a dependency in the maven file: \t<dependencies> \t\t<!-- todo: add your dependency here --> \t\t<dependency> \t\t\t<groupid>com.fasterxml.jackson.core</groupid> \t\t\t<artifactid>jackson-databind</artifactid> \t\t\t<version>2.10.0.pr1</version> \t\t</dependency>\t \t\t\t \t</dependencies> create pojo class we now create the class we are going to convert to json (serialize): package com.jackson.json.demo; public class student { \tprivate int id; \tprivate string firstname; \tprivate string lastname; \tprivate boolean active; \t \tpublic student() { \t\t \t} \t \tpublic int getid() { \t\treturn id; \t} \t \tpublic void setid(int id) { \t\tthis.id = id; \t} \t \tpublic string getfirstname() { \t\treturn firstname; \t} \t \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \t \tpublic string getlastname() { \t\treturn lastname; \t} \t \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t \tpublic boolean isactive() { \t\treturn active; \t} \t \tpublic void setactive(boolean active) { \t\tthis.active = active; \t} \t } main app now, to test it we are going to create a student object by reading from a json object: package com.jackson.json.demo; import java.io.file; import com.fasterxml.jackson.databind.objectmapper; public class driver { \tpublic static void main(string[] args) { \t\t \t\ttry { \t\t\t// create object mapper \t\t\tobjectmapper mapper = new objectmapper(); \t\t\t \t\t\t// read json file and map/convert to java pojo: \t\t\t// data/sample-lite.json \t\t\t \t\t\tstudent thestudent = mapper.readvalue( \t\t\t\t\t\tnew file(\"data/sample-lite.json\"), student.class); \t\t} \t\tcatch (exception exc) { \t\t\texc.printstacktrace(); \t\t} \t} } nested objects but, how can we read nested properties inside a json file, like the following: { \t\"id\": 14, \t\"firstname\": \"mario\", \t\"lastname\": \"rossi\", \t\"active\": true, \t\"address\": { \t\t\"street\": \"100 main st\", \t\t\"city\": \"philadelphia\", \t\t\"state\": \"pennsylvania\", \t\t\"zip\": \"19103\", \t\t\"country\": \"usa\" \t}, \t\"languages\" : [\"java\", \"c#\", \"python\", \"javascript\"] } as you can see the address property has properties inside it. what we are going to do is create a new attribute address inside the student object, which will be a pojo object in itself. package com.jackson.json.demo; public class student { \tprivate int id; \tprivate string firstname; \tprivate string lastname; \tprivate boolean active; \t \tprivate address address; \t \tprivate string[] languages; \t \tpublic student() { \t\t \t} \t \tpublic int getid() { \t\treturn id; \t} \t \tpublic void setid(int id) { \t\tthis.id = id; \t} \t \tpublic string getfirstname() { \t\treturn firstname; \t} \t \tpublic void setfirstname(string firstname) { \t\tthis.firstname = firstname; \t} \t \tpublic string getlastname() { \t\treturn lastname; \t} \t \tpublic void setlastname(string lastname) { \t\tthis.lastname = lastname; \t} \t \tpublic boolean isactive() { \t\treturn active; \t} \t \tpublic void setactive(boolean active) { \t\tthis.active = active; \t} \tpublic address getaddress() { \t\treturn address; \t} \tpublic void setaddress(address address) { \t\tthis.address = address; \t} \tpublic string[] getlanguages() { \t\treturn languages; \t} \tpublic void setlanguages(string[] languages) { \t\tthis.languages = languages; \t} \t } we also need to create the address class: package com.jackson.json.demo; public class address { \tprivate string street; \tprivate string city; \tprivate string state; \tprivate string zip; \tprivate string country; \t \tpublic address() { \t\t \t} \tpublic string getstreet() { \t\treturn street; \t} \tpublic void setstreet(string street) { \t\tthis.street = street; \t} \tpublic string getcity() { \t\treturn city; \t} \tpublic void setcity(string city) { \t\tthis.city = city; \t} \tpublic string getstate() { \t\treturn state; \t} \tpublic void setstate(string state) { \t\tthis.state = state; \t} \tpublic string getzip() { \t\treturn zip; \t} \tpublic void setzip(string zip) { \t\tthis.zip = zip; \t} \tpublic string getcountry() { \t\treturn country; \t} \tpublic void setcountry(string country) { \t\tthis.country = country; \t} } ignore unknwon properties to ignore properties from the json file that cannot be mapped to an attribute in the pojo we use the annotation: package com.jackson.json.demo; @jsonignoreproperties(ignoreunkown=true) public class student { \tprivate int id; \tprivate string firstname; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Spring Rest/Exception Handling.html",
    "title": "Exception Handling",
    "body": " index search search back exception handling in this section we are going to show how to create an error page to display when there is an error on a request. create error response class package com.springdemo.rest; public class studenterrorresponse { \tprivate int status; \tprivate string message; \tprivate long timestamp; \tpublic studenterrorresponse() { \t\t \t} \t \tpublic studenterrorresponse(int status, string message, long timestamp) { \t\tthis.status = status; \t\tthis.message = message; \t\tthis.timestamp = timestamp; \t} \tpublic int getstatus() { \t\treturn status; \t} \tpublic void setstatus(int status) { \t\tthis.status = status; \t} \tpublic string getmessage() { \t\treturn message; \t} \tpublic void setmessage(string message) { \t\tthis.message = message; \t} \tpublic long gettimestamp() { \t\treturn timestamp; \t} \tpublic void settimestamp(long timestamp) { \t\tthis.timestamp = timestamp; \t} \t \t } create exception class package com.springdemo.rest; public class studentnotfoundexception extends runtimeexception { \tpublic studentnotfoundexception(string message, throwable cause) { \t\tsuper(message, cause); \t} \tpublic studentnotfoundexception(string message) { \t\tsuper(message); \t} \tpublic studentnotfoundexception(throwable cause) { \t\tsuper(cause); \t} \t } rest service with exception what we need to know is: define an exception handler method with @exceptionhandler annotation the exception handler will return a response entity response entity is a wrapper for the http response object resposneentity provides a fine-grained control to specify: http status code http headers response body package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.exceptionhandler; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t \t \t \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\t// check the studentid against list size \t\tif ( (studentid >= thestudents.size()) || (studentid < 0) ) {\t\t\t \t\t\tthrow new studentnotfoundexception(\"student id not found - \" + studentid); \t\t} \t\t \t\treturn thestudents.get(studentid); \t\t \t} // tag it as an exception handling method \t@exceptionhandler // type of response body exception type to handle \tpublic responseentity<studenterrorresponse> handleexception(studentnotfoundexception exc) { \t\t \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t // json error object \t\terror.setstatus(httpstatus.not_found.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t // return response with the error object and the status code \t\treturn new responseentity<>(error, httpstatus.not_found); \t } // another exception handler \t@exceptionhandler // catch any exception thrown \tpublic responseentity<studenterrorresponse> handleexception(exception exc) { \t\t \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t \t\terror.setstatus(httpstatus.bad_request.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\treturn new responseentity<>(error, httpstatus.bad_request); \t}\t } global exception handler instead of having the exception handling methods in every controller, we defined them globally. for that we use controlleradvice that acts as a filter between the requests and the controller. it: pre-processes requests to controllers post-processes responses to handle exceptions so, we create a class with the @controlleradvice annotation: package com.springdemo.rest; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.controlleradvice; import org.springframework.web.bind.annotation.exceptionhandler; @controlleradvice public class studentrestexceptionhandler { \t// add exception handling code here \t// add an exception handler using @exceptionhandler \t@exceptionhandler \tpublic responseentity<studenterrorresponse> handleexception(studentnotfoundexception exc) { \t\t \t\t// create a studenterrorresponse \t\tstudenterrorresponse error = new studenterrorresponse(); \t\t \t\terror.setstatus(httpstatus.not_found.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\t// return responseentity \t\treturn new responseentity<>(error, httpstatus.not_found); \t} \t \t// add another exception handler ... to catch any exception (catch all) \t@exceptionhandler \tpublic responseentity<studenterrorresponse> handleexception(exception exc) { \t\t \t\t// create a studenterrorresponse \t\tstudenterrorresponse error = new studenterrorresponse(); \t\terror.setstatus(httpstatus.bad_request.value()); \t\terror.setmessage(exc.getmessage()); \t\terror.settimestamp(system.currenttimemillis()); \t\t \t\t// return responseentity\t\t \t\treturn new responseentity<>(error, httpstatus.bad_request); \t} \t } and now we modify the controller to make use of this paradigm: package com.springdemo.rest; import java.util.arraylist; import java.util.list; import javax.annotation.postconstruct; import org.springframework.http.httpstatus; import org.springframework.http.responseentity; import org.springframework.web.bind.annotation.exceptionhandler; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.pathvariable; import org.springframework.web.bind.annotation.requestmapping; import org.springframework.web.bind.annotation.restcontroller; import com.springdemo.entity.student; @restcontroller @requestmapping(\"/api\") public class studentrestcontroller { \tprivate list<student> thestudents; \t \t// define @postconstruct to load the student data ... only once! \t@postconstruct \tpublic void loaddata() { \t \t\tthestudents = new arraylist<>(); \t\t \t\tthestudents.add(new student(\"poornima\", \"patel\")); \t\tthestudents.add(new student(\"mario\", \"rossi\")); \t\tthestudents.add(new student(\"mary\", \"smith\"));\t\t \t} \t// define endpoint for \"/students\" - return list of students \t@getmapping(\"/students\") \tpublic list<student> getstudents() { \t\t\t \t\treturn thestudents; \t} \t \t// define endpoint for \"/students/{studentid}\" - return student at index \t@getmapping(\"/students/{studentid}\") \tpublic student getstudent(@pathvariable int studentid) { \t\t \t\t// just index into the list ... keep it simple for now \t\t// check the studentid against list size \t\tif ( (studentid >= thestudents.size()) || (studentid < 0) ) { \t\t\tthrow new studentnotfoundexception(\"student id not found - \" + studentid); \t\t} \t\t\t \t\treturn thestudents.get(studentid); \t}\t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Boot Project Structure.html",
    "title": "Spring Boot Project Structure",
    "body": " index search search back spring boot project structure application properties by default, spring boot will load properties from: application.properties in the src project directory. we inject it in our code the same way we did it with spring static content by default, spring boot wil load static resources from \"/static\" directory testing unit tests are stored on the src directory under the /test folder $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Boot Starters.html",
    "title": "Spring Boot Starters",
    "body": " index search search back spring boot starters spring boot staters offer a curated list of dependencies that are grouped together and tested by the spring development team. so now, if your application depends on the web and security module and also uses thymeleaf and jpa, you add the following dependencies: ... \t<dependencies> <!-- web --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-web</artifactid> \t\t</dependency> <!-- security --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-security</artifactid> \t\t</dependency> <!-- jpa --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-data-jpa</artifactid> \t\t</dependency> <!-- thymeleaf --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-thymeleaf</artifactid> \t\t</dependency> \t\t<!-- add support for automatic reloading --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-devtools</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... spring boot starter parent this is a special starter that provides defaults: default compiler level utf-8 source encoding you include it in your pom.xml file as follows: ... \t<parent> \t\t<groupid>org.springframework.boot</groupid> \t\t<artifactid>spring-boot-starter-parent</artifactid> \t\t<version>2.1.2.release</version> \t\t<relativepath/> <!-- lookup parent from repository --> \t</parent> \t<dependencies> \t... \t</dependencies> ... if you want to override a default, you use properties: ... \t<parent> \t\t<groupid>org.springframework.boot</groupid> \t\t<artifactid>spring-boot-starter-parent</artifactid> \t\t<version>2.1.2.release</version> \t\t<relativepath/> <!-- lookup parent from repository --> \t</parent> \t \t<!-- override default java version --> \t<properties> \t\t<java.version>1.8</java.version> \t</properties> \t<dependencies> \t... \t</dependencies> ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Data JPA.html",
    "title": "Spring Data JPA",
    "body": " index search search back spring data jpa with jpa api we created a employee dao, however what if we need to create one for each entity we manage. then we would duplicate a lot of code, because the calls to the api are basically the same. that is what we use spring data jpa, we plug in the entity type and the primary key to the dao, and spring creates it an manages it for us. create repository so now the employee dao is as follows: package com.springboot.cruddemo.dao; import org.springframework.data.jpa.repository.jparepository; import com.springboot.cruddemo.entity.employee; public interface employeerepository extends jparepository<employee, integer> { } use repository and the employee service is: package com.springboot.cruddemo.service; import java.util.list; import java.util.optional; import org.springframework.beans.factory.annotation.autowired; import org.springframework.stereotype.service; import com.springboot.cruddemo.dao.employeerepository; import com.springboot.cruddemo.entity.employee; @service public class employeeserviceimpl implements employeeservice { // here we make use of the above implemented repository \tprivate employeerepository employeerepository; \t \t@autowired \tpublic employeeserviceimpl(employeerepository theemployeerepository) { \t\temployeerepository = theemployeerepository; \t} \t \t@override \tpublic list<employee> findall() { \t\treturn employeerepository.findall(); \t} \t@override \tpublic employee findbyid(int theid) { \t\toptional<employee> result = employeerepository.findbyid(theid); \t\t \t\temployee theemployee = null; \t\t \t\tif (result.ispresent()) { \t\t\ttheemployee = result.get(); \t\t} \t\telse { \t\t\t// we didn't find the employee \t\t\tthrow new runtimeexception(\"did not find employee id - \" + theid); \t\t} \t\t \t\treturn theemployee; \t} \t@override \tpublic void save(employee theemployee) { \t\temployeerepository.save(theemployee); \t} \t@override \tpublic void deletebyid(int theid) { \t\temployeerepository.deletebyid(theid); \t} } this employeeservice implements the interface: package com.springboot.cruddemo.service; import java.util.list; import com.springboot.cruddemo.entity.employee; public interface employeeservice { \tpublic list<employee> findall(); \t \tpublic employee findbyid(int theid); \t \tpublic void save(employee theemployee); \t \tpublic void deletebyid(int theid); \t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Application Properties.html",
    "title": "Application Properties",
    "body": " index search search back application properties by default spring boot reads information from a standard properties file in src/main/resources/application.properties. you can define any custom properties in this file and your app can access properties using the annotation @value(we have done this before). configuring the spring boot server some properties offered by spring are: core # log levels severity mapping logging.level.org.springframework=debug logging.level.org.hibernate=trace logging.level.org.luv2code=info # log file name logging.file=date.log web # http server port server.port=7070 # context path of the application server.servlet.context-path=/my-app # default http session timeout server.servlet.session.timeout=15m actuator properties # endpoints to include by name or wildcard management.endpoints.web.exposure.include=* # endpoints to exclude by name or wildcard management.endpoints.web.exposure.exclude=beans,mapping security # default username spring.security.user.name=admin # password for default user spring.security.user.password=mypass data properties # jdbc url of the database spring.datasource.url=jdbc:mysql://localhost:3306/myapp # login username of the database spring.datasource.username=alba # login password of the database spring.datasource.password=testpass $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Controller.html",
    "title": "Rest Controller",
    "body": " index search search back rest controller in this section we are going to show how to create a rest controller in a spring boot application: create controller the controller is the same as in spring rest: package com.springboot.demo.mycoolapp.rest; import java.time.localdatetime; import org.springframework.beans.factory.annotation.value; import org.springframework.web.bind.annotation.getmapping; import org.springframework.web.bind.annotation.restcontroller; @restcontroller public class funrestcontroller { \t\t \t// expose \"/\" that return \"hello world\" \t \t@getmapping(\"/\") \tpublic string sayhello() { \t\treturn \"hello world! time on server is \" + localdatetime.now(); \t} \t main app the springbootapplication is made up of three annotations: auto configuration (@enableautoconfiguration) component scanning (@componentscan) additional configuration (@configuration) package com.springboot.demo.mycoolapp; import org.springframework.boot.springapplication; import org.springframework.boot.autoconfigure.springbootapplication; // annotation to tell spring this is an spring application @springbootapplication public class mycoolappapplication { \tpublic static void main(string[] args) { \t\t// boostrap spring boot application \t\tspringapplication.run(mycoolappapplication.class, args); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Boot DevTools.html",
    "title": "Spring Boot DevTools",
    "body": " index search search back spring boot devtools spring boot dev tools automatically restart your application when code is updated. the only thing you need to do is add the module to the dependencies: ... \t<dependencies> \t\t<!-- add support for automatic reloading --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-devtools</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/index.html",
    "title": "Overview",
    "body": " index search search back overview spring boot is a framework that: make it easier to get started with spring development minimize the amount of manual configuration perform auto-configuration based on props files and jar classpath help to resolve dependency conflicts (maven or gradle) provide an embedded http server so you can get started quickly to create a new project you just have to go to spring initiliazr, where you simply select your dependencies and lets you create a maven/gradle project and import it into an ide. so now our app is a jar file, and it includes the source code and also the embedded http server, so can be ran from the command line, from your ide, etc. however if you want to export your code as a war file, you can also do that by exporting only your source code, without the embedded server. with the jar file you can run your application by executing: $ java -jar app.jar $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/JPA.html",
    "title": "JPA",
    "body": " index search search back jpa until now, to manage data we have been using the entitymanager along with the hibernate api. however now we are going to use the standard jpa api. the jpa api methods are similar to native hibernate api. it also supports a query language jpql (jpa query language) comparing hibernate to jpa: example: for managing employees with jpa, we first create the data access object: package com.springboot.cruddemo.dao; import java.util.list; import javax.persistence.entitymanager; import javax.persistence.query; import org.springframework.beans.factory.annotation.autowired; import org.springframework.stereotype.repository; import com.luv2code.springboot.cruddemo.entity.employee; @repository public class employeedaojpaimpl implements employeedao { \tprivate entitymanager entitymanager; \t \t@autowired \tpublic employeedaojpaimpl(entitymanager theentitymanager) { \t\tentitymanager = theentitymanager; \t} \t \t@override \tpublic list<employee> findall() { \t\t// create a query \t\tquery thequery = \t\t\t\tentitymanager.createquery(\"from employee\"); \t\t \t\t// execute query and get result list \t\tlist<employee> employees = thequery.getresultlist(); \t\t \t\t// return the results\t\t \t\treturn employees; \t} \t@override \tpublic employee findbyid(int theid) { \t\t// get employee \t\temployee theemployee = \t\t\t\tentitymanager.find(employee.class, theid); \t\t \t\t// return employee \t\treturn theemployee; \t} \t@override \tpublic void save(employee theemployee) { \t\t// save or update the employee \t\temployee dbemployee = entitymanager.merge(theemployee); \t\t \t\t// update with id from db ... so we can get generated id for save/insert \t\ttheemployee.setid(dbemployee.getid()); \t\t \t} \t@override \tpublic void deletebyid(int theid) { \t\t// delete object with primary key \t\tquery thequery = entitymanager.createquery( \t\t\t\t\t\t\t\"delete from employee where id=:employeeid\"); \t\t \t\tthequery.setparameter(\"employeeid\", theid); \t\t \t\tthequery.executeupdate(); \t} } and then we call it from the employee service: package com.springboot.cruddemo.service; import java.util.list; import org.springframework.beans.factory.annotation.autowired; import org.springframework.beans.factory.annotation.qualifier; import org.springframework.stereotype.service; import org.springframework.transaction.annotation.transactional; import com.springboot.cruddemo.dao.employeedao; import com.springboot.cruddemo.entity.employee; @service public class employeeserviceimpl implements employeeservice { \tprivate employeedao employeedao; \t \t@autowired \tpublic employeeserviceimpl(@qualifier(\"employeedaojpaimpl\") employeedao theemployeedao) { \t\temployeedao = theemployeedao; \t} \t \t@override \t@transactional \tpublic list<employee> findall() { \t\treturn employeedao.findall(); \t} \t@override \t@transactional \tpublic employee findbyid(int theid) { \t\treturn employeedao.findbyid(theid); \t} \t@override \t@transactional \tpublic void save(employee theemployee) { \t\temployeedao.save(theemployee); \t} \t@override \t@transactional \tpublic void deletebyid(int theid) { \t\temployeedao.deletebyid(theid); \t} } this class implements the following interface: package com.springboot.cruddemo.service; import java.util.list; import com.springboot.cruddemo.entity.employee; public interface employeeservice { \tpublic list<employee> findall(); \t \tpublic employee findbyid(int theid); \t \tpublic void save(employee theemployee); \t \tpublic void deletebyid(int theid); \t } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Boot Actuator.html",
    "title": "Spring Boot Actuator",
    "body": " index search search back spring boot actuator spring boot actuator automatically exposes endpoints to monitor and manage your application. you only need to add the dependency to you pom.xml file: ... \t<dependencies> ... \t\t<!-- add support for spring boot actuator --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-actuator</artifactid> \t\t</dependency> \t\t\t\t \t</dependencies> ... the endpoints are prefixed by /actuator, some of them are: /health: health information about your application /info: information about your project. by default it return an empty json object. you can add info through application.properties as follows: info.app.name=my super cool app info.app.description=a crazy fun app, yoohoo! info.app.version=1.0.0 /auditevents: audit events for your application /beans: list of all beans registered in the spring application context /mappings: list of all @requestmapping path by default only /health and /info are exposed, to expose all actuator endpoints you need to specify on application.properties (you can also specify only the ones you want separated by commas): management.endpoints.web.exposure.include=* add security first you need to add spring security as a dependency in your pom.xml: ... \t<dependencies> \t\t... <!-- security --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-security</artifactid> \t\t</dependency> \t\t... \t</dependencies> ... now, when we access some endpoints like /actuator/beans spring will prompt a login to grant access to the endpoint. the default user name is \"user\" the password will be printed on the console where you start the application to override these defaults edit the application.properties file as follows: spring.security.user.name=alba spring.security.user.password=mypassword we can also exclude endpoints by adding the following declarations to the application.properties file: management.endpoints.web.exposure.exclude=health,info $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/SpringBoot/Spring Data REST.html",
    "title": "Spring Data Rest",
    "body": " index search search back spring data rest what if we want to also reduce the code for creating api, that is, what if spring could create a rest api for us using our jparepository, such that it would expose all of the basic rest api crud features automatically. what does it do? it scans your project fro jparepository it exposes rest apis for each entity type for your jparepository so now, we can remove our employee services and our rest controllers, because it is handled automatically by spring. the only thing needed is adding spring data rest as a dependency: \t<dependencies> ... \t\t<!-- add dependency for spring data rest --> \t\t<dependency> \t\t\t<groupid>org.springframework.boot</groupid> \t\t\t<artifactid>spring-boot-starter-data-rest</artifactid> \t\t</dependency> ... \t</dependencies> to sum up, now in your application you only will have: your entity: employee the corresponding jpa repository: employeerepository dependency main application the first one applies to each entity your application has. spring data rest is hateoas compliant (the responses include metadata about itself). configuration you can specify the name of the endpoint that is exposed (by the default is the plural of the entity) with: @repositoryrestresource(path=\"members\") public interface employeerepository extends jparepository<employee, integer> { } the default number of elements returned are 20, then we can use pagination to retrieve the next ones with query parameters (?page=0). some properties available to tweak in application.properties are: spring.data.rest.base-path: base path used to expose repository resources spring.data.rest.default-page-size: default size pages spring.data.rest.max-page-size: maximum size of pages sorting you can sort by the property names of your entity. on the employee example we have firstname, lastname and email, therefore we can do: http://localhost:8080/employees?sort=firstname or http://localhost:8080/employees?sort=firstname,desc $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Dependency Injection/Injecting Literal Values.html",
    "title": "Injecting Literal Values",
    "body": " index search search back injecting literal values to inject concrete attributes into our beans: define the attributes first we define the attributes emailaddress and team in the object. also we create the set and get methods for both of them: package com.luv2code.springdemo; public class cricketcoach implements coach { \tprivate fortuneservice fortuneservice; \t \t// add new fields for emailaddress and team \tprivate string emailaddress; \tprivate string team; \t \t\t \tpublic cricketcoach() { \t\tsystem.out.println(\"cricketcoach: inside no-arg constructor\"); \t} \t /* setters and getters */ \tpublic string getemailaddress() { \t\treturn emailaddress; \t} \tpublic void setemailaddress(string emailaddress) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setemailaddress\"); \t\tthis.emailaddress = emailaddress; \t} \tpublic string getteam() { \t\treturn team; \t} \tpublic void setteam(string team) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setteam\"); \t\tthis.team = team; \t} /* setter injection */ \tpublic void setfortuneservice(fortuneservice fortuneservice) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setfortuneservice\"); \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice fast bowling for 15 minutes\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } configuration file now we define the properties in the configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- define your beans here --> \t<!-- define the dependency --> \t<bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> \t</bean> \t \t<bean id=\"mycoach\" \t\tclass=\"com.springdemo.trackcoach\">\t \t\t<!-- set up constructor injection --> \t\t<constructor-arg ref=\"myfortuneservice\" /> \t</bean> \t \t<bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> \t <!-- set up setter injection --> \t <!-- ref: references the id of the bean we define previously --> \t <!-- name: name of the setter method set<name>, where the first \t letter of the name is capitalized --> \t <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> \t\t<!-- inject literal values, where name is the name of the attribute in the bean \t\tand value is the value to set the value to --> \t <property name=\"emailaddress\" value=\"email@email.com\" /> \t <property name=\"team\" value=\"best team\" /> \t</bean> </beans> main method now in the main method of our app we can call the getters and setters for these new attributes: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// retrieve attribute values \t\tsystem.out.println(thecoach.getteam()); \t\tsystem.out.println(thecoach.getemailaddress()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Dependency Injection/Inject Values From a Properties File.html",
    "title": "Inject Values from the Properties Files",
    "body": " index search search back inject values from the properties files create the properties file let's define our properties inside a properties file sport.properties: foo.email=myeasycoach@email.com foo.team=royal challengers bangalore load the properties file now we load the properties file using the context tag inside our config file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- load the properties file: sport.properties --> <context:property-placeholder location=\"classpath:sport.properties\"/> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\">\t \t<!-- set up constructor injection --> \t<constructor-arg ref=\"myfortuneservice\" /> </bean> <bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> <!-- set up setter injection --> <!-- ref: references the id of the bean we define previously --> <!-- name: name of the setter method set<name>, where the first letter of the name is capitalized --> <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> <!-- inject literal values, where name is the name of the attribute in the bean and value is the value to set the value to --> <!-- note that we are now referencing the values from the properties file --> <property name=\"emailaddress\" value=\"${foo.email})\" /> <property name=\"team\" value=\"${foo.team}\" /> </bean> </beans> main method in the main method, we create our object as usual, and if we invoke the getter methods, we retrieve the values passed in the property file: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// retrieve attribute values from property file \t\tsystem.out.println(thecoach.getteam()); \t\tsystem.out.println(thecoach.getemailaddress()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Dependency Injection/Constructor Injection.html",
    "title": "Constructor Injection",
    "body": " index search search back constructor injection now we will show an example where the baseballcoach has fortuneservice as a dependency. so, first we create the dependency interface as follows: create dependency object package com.springdemo; public interface fortuneservice { \tpublic string getfortune(); \t } next we create the dependency class than implements the interface: package com.springdemo; public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } establish dependency let's also update the coach interface to add a method getdailyfortune (note that all classes that implement this interface have to implement this new method): package com.springdemo; public interface coach { \tpublic string getdailyworkout(); \t \tpublic string getdailyfortune(); } now create a constructor for the dependency in the class that has the dependency package com.springdemo; public class baseballcoach implements coach { \t// define a private field for the dependency \tprivate fortuneservice fortuneservice; \t \t// define a constructor for dependency injection \tpublic baseballcoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"spend 30 minutes on batting practice\"; \t} \t@override \tpublic string getdailyfortune() {\t\t \t\t// use my fortuneservice to get a fortune\t\t \t\treturn fortuneservice.getfortune(); \t} } configuration file finally define the dependency in the configuration file: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <!-- bean with the dependency --> <bean id=\"mycoach\" class=\"com.springdemo.trackcoach\">\t <!-- set up constructor injection, note ref=id of bean --> <constructor-arg ref=\"myfortuneservice\" /> </bean> </beans> behind the scenes, spring framework does: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\t// create object \t\t// from the bean with id = myfortuneservice in the config file \t\thappyfortuneservice myfortuneservice = new happyfortuneservice(); \t\t \t\t// add dependency via constructor \t\t// from the bean with id = mycoach in the config file \t\ttrackcoach mycoach = new trackcoach(fortuneservice); \t} } main method we do not need to make any modifications to the app, when we create the coach bean using spring, the framework deals with the dependency injection: package com.luv2code.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class hellospringapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t\t\t \t\t// retrieve bean from spring container (with the dependency) \t\tcoach thecoach = context.getbean(\"mycoach\", coach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\t// let's call our new method for fortunes \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Dependency Injection/Dependency Injection.html",
    "title": "Dependency Injection",
    "body": " index search search back dependency injection the dependencies of the objects are managed by the spring container object factory: so instead of having to build the object and all of its dependencies, the spring factory will do this work for you. injection types there are several injection types in spring. the more common are: constructor injection setter injection injecting literal values inject values from a properties file $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Dependency Injection/Setter Injection.html",
    "title": "Setter Injection",
    "body": " index search search back setter injection inject dependencies by calling setter methods on your class create dependency object refer to create dependency object define dependency we include a setter method that takes the dependency as an argument like: package com.springdemo; public class cricketcoach implements coach { \tprivate fortuneservice fortuneservice;\t \t \t// create a no-arg constructor \tpublic cricketcoach() { \t\tsystem.out.println(\"cricketcoach: inside no-arg constructor\"); \t} \t \t// our setter method \tpublic void setfortuneservice(fortuneservice fortuneservice) { \t\tsystem.out.println(\"cricketcoach: inside setter method - setfortuneservice\"); \t\tthis.fortuneservice = fortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice fast bowling for 15 minutes\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } configuration file <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> <!-- define your beans here --> <!-- define the dependency --> <bean id=\"myfortuneservice\" class=\"com.springdemo.happyfortuneservice\"> </bean> <bean id=\"mycoach\" \tclass=\"com.springdemo.trackcoach\">\t \t<!-- set up constructor injection --> \t<constructor-arg ref=\"myfortuneservice\" /> </bean> <bean id=\"mycricketcoach\" class=\"com.springdemo.cricketcoach\"> <!-- set up setter injection --> <!-- ref: references the id of the bean we define previously --> <!-- name: name of the setter method set<name>, where the first letter of the name is capitalized --> <property name=\"fortuneservice\" ref=\"myfortuneservice\" /> </bean> </beans> behind the scenes, spring framework does: package com.springdemo; public class myapp { \tpublic static void main(string[] args) { \t\t// create object \t\t// from the bean with id = myfortuneservice in the config file \t\thappyfortuneservice myfortuneservice = new happyfortuneservice(); \t\t \t\t// from the bean with id = mycricketcoach in the config file \t\tcricketcoach mycricketcoach = new cricketcoach(fortuneservice); \t\t// add dependency via setter \t\tmycricketcoach.setfortuneservice(myfortuneservice); \t} } main method now, on the main method of our spring app, we create the object by reading the config file, and spring automatically injects the dependency via the setter method: package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class setterdemoapp { \tpublic static void main(string[] args) { \t\t// load the spring configuration file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// retrieve bean from spring container \t\tcricketcoach thecoach = context.getbean(\"mycricketcoach\", cricketcoach.class); \t\t \t\t// call methods on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Overview.html",
    "title": "Overview",
    "body": " index search search back overview aspect-oriented programming is a programming technique based on the concept of an aspect (that is something that encapsulated cross-cutting logic/functionality, which means logic that affect the project transversally like logging or security). what aop does behind the scenes is call methods from the classes/aspects (like a logging class) whenever a method is called (this depends on the configuration): advantages reusable modules resolve code tangling resolve code scatter applied selectively based on configuration disadvantages too many aspects and app flow is hard to follow minor performance cost for aspect execution terminology aspect: module of code for a cross-cutting concern (logging, security...) advice: what action is takes and when it should be applied joint point: when to apply code during program execution pointcut: a predicate expression for where advice should be applied advice types before advice: run before the method after finally advice: run after the method (like finally clause in try catch) after returning advice: run after the method (success execution) after throwing advice: run after the method (if exception if thrown) around advice: run before and after the method weaving it refers to the connection being made between aspects and target objects to create an advised object. there are different types: compile-time load-time run-time note that the slowest is the run-time weaving best practices keep the code inside the advices small keep the code fast do not perform any expensive/slow operations $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/After Advice.html",
    "title": "After Advice",
    "body": " index search search back after advice this advice runs always when the method is completed (like a finally clause inside a try catch). for example if we want to always run the advice afterfinallyfindaccountsadvice when the method findaccounts inside accountdao finishes: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.after; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t@after(\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\") \tpublic void afterfinallyfindaccountsadvice(joinpoint thejoinpoint) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @after (finally) on method: \" \t\t\t\t\t\t\t+ method); \t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/AfterThrowing Advice.html",
    "title": "AfterThrowing Advice",
    "body": " index search search back afterthrowing advice this advice is run whenever the target object throws and execption. for example: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.luv2code.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@afterthrowing( \t\t\tpointcut=\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\", \t\t\t// define the name of the parameter that holds the exception object \t\t\tthrowing=\"theexc\") \tpublic void afterthrowingfindaccountsadvice( \t\t\t\t\tjoinpoint thejoinpoint, throwable theexc) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @afterthrowing on method: \" + method); \t\t \t\t// log the exception \t\tsystem.out.println(\"\\n=====>>> the exception is: \" + theexc); \t \t} } in this code sample we have the advice afterthrowingfindaccountsadvice that is run whenever the method findaccounts inside accountdao throws an exception. we also make use of the throwing attribute that lets us map the exception object to a parameter inside our advice. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/JoinPoints.html",
    "title": "JoinPoints",
    "body": " index search search back joinpoints when we are in an aspect, how can we access method parameters? display method signature to display the method signature we do the following: package com.aopdemo.aspect; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice(joinpoint thejoinpoint) { \t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t \t\t \t\t// display the method signature \t\tmethodsignature methodsig = (methodsignature) thejoinpoint.getsignature(); \t\t \t\tsystem.out.println(\"method: \" + methodsig); \t} } display method arguments also, to display the method arguments: package com.aopdemo.aspect; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice(joinpoint thejoinpoint) { \t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t \t\t// display method arguments \t\t// get args \t\tobject[] args = thejoinpoint.getargs(); \t\t \t\t// loop through args \t\tfor (object temparg : args) { \t\t\tsystem.out.println(temparg); \t\t\t \t\t\tif (temparg instanceof account) { \t\t\t\t \t\t\t\t// downcast and print account specific stuff \t\t\t\taccount theaccount = (account) temparg; \t\t\t\t \t\t\t\tsystem.out.println(\"account name: \" + theaccount.getname()); \t\t\t\tsystem.out.println(\"account level: \" + theaccount.getlevel());\t\t\t\t\t\t\t\t \t\t\t} \t\t}\t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Before Advice.html",
    "title": "Before Advice",
    "body": " index search search back before advice we use the tag @before to execute some code before we call the target object function: add dependencies we have to download the aspectj jar file, because spring aop depends on some on their framework's classes create target object we create a dao object: package com.aopdemo.dao; import org.springframework.stereotype.component; @component public class accountdao { \tpublic void addaccount() { \t\tsystem.out.println( \t\t\tgetclass() \t\t\t+ \": doing my db work: adding an account\" \t\t); \t} } spring configuration we now have to enable aop proxying in our app configuration: package com.aopdemo; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.enableaspectjautoproxy; @configuration // enable proxying to add before advice @enableaspectjautoproxy @componentscan(\"com.aopdemo\") public class democonfig { } create aspect with @before now it is time to create an aspect with @before advice: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t// this is where we add all of our related advices for logging \t// here we specify we want to run this code before calling the \t// object method public void addaccount \t@before(\"execution(public void addaccount())\") \tpublic void beforeaddaccountadvice() { \t\tsystem.out.println(\"\\n=====>>> executing @before advice on addaccount()\"); \t} } main app we now create a demo app: package com.aopdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; import com.aopdemo.dao.accountdao; public class maindemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(democonfig.class); \t\t \t\t// get the bean from spring container \t\taccountdao theaccountdao = context.getbean(\"accountdao\", accountdao.class); \t\t \t\t// call the business method \t\ttheaccountdao.addaccount(); \t\t// do it again! \t\tsystem.out.println(\"\\nlet's call it again!\\n\"); \t\t \t\t// call the business method again \t\ttheaccountdao.addaccount(); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Around Advice.html",
    "title": "Around Advice",
    "body": " index search search back around advice this type of advice is always called before and after the target object. when using the @around advice we have access to a reference of a proceeding join point. which is a handle to the target method, and will let us execute the taget method. so for example if we want to measure the performance of the getfortunemethod: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.proceedingjoinpoint; import org.aspectj.lang.annotation.after; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.afterthrowing; import org.aspectj.lang.annotation.around; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\t// now, let's execute the method \t\tobject result = theproceedingjoinpoint.proceed(); \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} } the advice aroundgetfortune is called before the getfortune is called, then it proceeds to call from inside the advice and we measure how long does the method take to run. exception handling inside an advice, to handle exceptions you can: handle the exception inside the advice \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\ttry { \t\t\tresult = theproceedingjoinpoint.proceed(); \t\t} catch (exception e) { \t\t\t// log the exception \t\t\tmylogger.warning(e.getmessage()); \t\t\t \t\t\t// give users a custom messagee \t\t\tresult = \"major accident! but no worries, \" \t\t\t\t\t+ \"your private aop helicopter is on the way!\"; \t\t} \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} simply rethrow the exception \t@around(\"execution(* com.aopdemo.service.*.getfortune(..))\")\t \tpublic object aroundgetfortune( \t\t\tproceedingjoinpoint theproceedingjoinpoint) throws throwable { \t\t \t\t// print out method we are advising on \t\tstring method = theproceedingjoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @around on method: \" + method); \t\t \t\t// get begin timestamp \t\tlong begin = system.currenttimemillis(); \t\t \t\ttry { \t\t\tresult = theproceedingjoinpoint.proceed(); \t\t} catch (exception e) { \t\t\t// log the exception \t\t\tmylogger.warning(e.getmessage()); \t\t\t// rethrow exception \t\t\tthrow e; \t\t} \t\t \t\t// get end timestamp \t\tlong end = system.currenttimemillis(); \t\t \t\t// compute duration and display it \t\tlong duration = end - begin; \t\tsystem.out.println(\"\\n=====> duration: \" + duration / 1000.0 + \" seconds\"); \t\t \t\treturn result; \t} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Pointcut Expressions.html",
    "title": "Pointcut Expressions",
    "body": " index search search back pointcut expressions a pointcut expression is a predicate expression that tells spring when to apply a given advice. spring aop uses aspectj's pointcut expression language. execution pointcut the expression pattern is the following: execution(modifiers-pattern? return-type-pattern declaring-type-pattern? method-name-pattern(param-pattern) throws-pattern?) modifiers-pattern?: spring aop only supports public or * return-type-pattern: void, boolean, string, list<costumer>, etc declaring-type-pattern?: the class name method-name-pattern(param-pattern): method name to match, and parameters type to match throws-pattern?: exception types to match if the parameter is optional it is followed by an ?. you can also add wildcards inside the patterns. match methods some examples are: match concrete method inside a class: @before(\"execution(public void com.aopdemo.dao.accountdao.addaccount())\") match a method inside any class: @before(\"execution(public void addaccount())\") match any method that starts with add: @before(\"execution(public void add*())\") match all methods inside a given package: @before(\"execution(* com.aopdemo.dao.*.*(..))\") the first * denotes the return type, it can be anything the second * denotes the class name, it can be anything inside the package the third * denotes the method name, it can be anything lastly, .. denotes the param-type, there can be 0 or more parameters match parameters there are the following parameter pattern wildcards: (): matches a method with no arguments (*): matches a method with one argument of any type (..): matches a method with 0 or more arguments of any type for example: match addaccount methods with no arguments: @before(\"execution(* addaccount())\") match addacount methods with one account parameter: @before(\"execution(* addaccount(com.aopdemo.account))\") match addacount methods with any number of parameters: @before(\"execution(* addaccount(*))\") $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/AfterReturning Advice.html",
    "title": "AfterReturning Advice",
    "body": " index search search back afterreturning advice this advice is run after the method is done executing, and it executed successfully. the flow of this advice is the following: so for example, if you want to have an advice run everytime we call the findaccounts method inside a concrete class, and we also want to print out the result we obtained we do the following: package com.aopdemo.aspect; import java.util.list; import org.aspectj.lang.joinpoint; import org.aspectj.lang.annotation.afterreturning; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.reflect.methodsignature; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; import com.aopdemo.account; @aspect @component @order(2) public class mydemologgingaspect { \t\t \t// add a new advice for @afterreturning on the findaccounts method \t@afterreturning( \t\t\tpointcut=\"execution(* com.aopdemo.dao.accountdao.findaccounts(..))\", // this is the parameter name of the list of accounts returned by findaccounts \t\t\treturning=\"result\") \tpublic void afterreturningfindaccountsadvice( \t\t\t\t\tjoinpoint thejoinpoint, list<account> result) { \t\t \t\t// print out which method we are advising on \t\tstring method = thejoinpoint.getsignature().toshortstring(); \t\tsystem.out.println(\"\\n=====>>> executing @afterreturning on method: \" + method); \t\t\t\t \t\t// print out the results of the method call \t\tsystem.out.println(\"\\n=====>>> result is: \" + result); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Pointcut Declarations.html",
    "title": "Pointcut Declarations",
    "body": " index search search back pointcut declarations how can we reuse a pointcut expression? we need to: create a pointcut declaration apply the pointcut declaration to the advices we want create pointcut declaration we define the pointcut declaration with the pointcut annotation and we bind it to an arbitrary method. package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} } reuse pointcut declaration to reuse this declaration we simply call the method that is bound to the pointcut declaration: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} \t // reuse declaration \t@before(\"fordaopackage()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t // reuse declaration \t@before(\"fordaopackage()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} \t } combine pointcut declarations how can we apply multiple pointcut expressions to a single advice? well we can combine pointcut expressions using logic operators: and (&&) or (||) not (!) for example: @before(\"expressionone() && expressiontwo()\") @before(\"expressionone() || expressiontwo()\") @before(\"expressionone() && !expressiontwo()\") imagine we want to execute an advice for every method in the package except for getters and setters, then we do: package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.aspectj.lang.annotation.pointcut; import org.springframework.stereotype.component; @aspect @component public class mydemologgingaspect { \t@pointcut(\"execution(* com.aopdemo.dao.*.*(..))\") \tprivate void fordaopackage() {} \t \t// create pointcut for getter methods \t@pointcut(\"execution(* com.aopdemo.dao.*.get*(..))\") \tprivate void getter() {} \t \t// create pointcut for setter methods \t@pointcut(\"execution(* com.aopdemo.dao.*.set*(..))\") \tprivate void setter() {} \t \t// create pointcut: include package ... exclude getter/setter \t@pointcut(\"fordaopackage() && !(getter() || setter())\") \tprivate void fordaopackagenogettersetter() {} \t \t@before(\"fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t \t@before(\"fordaopackagenogettersetter()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/AOP/Control Aspect Order.html",
    "title": "Control Aspect Order",
    "body": " index search search back control aspect order how do we control the order of advices being applied when they all match the pointcut expressions? to control order we should: refactor: place advices in separate aspects control order on aspects using the @order annotation refactor and order we are going to create three aspects separate from each other as follows: so with the ordering the aspect flow looks something like this: log to cloud aspect package com.luv2code.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set order @order(1) public class mycloudlogasyncaspect { \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void logtocloudasync() { \t\tsystem.out.println(\"\\n=====>>> logging to cloud in async fashion\");\t\t \t} } logging aspect package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set the order @order(2) public class mydemologgingaspect { \t \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void beforeaddaccountadvice() {\t\t \t\tsystem.out.println(\"\\n=====>>> executing @before advice on method\");\t\t \t} \t } analytics aspect package com.aopdemo.aspect; import org.aspectj.lang.annotation.aspect; import org.aspectj.lang.annotation.before; import org.springframework.core.annotation.order; import org.springframework.stereotype.component; @aspect @component // set the order @order(3) public class myapianalyticsaspect { \t@before(\"com.aopdemo.aspect.luvaopexpressions.fordaopackagenogettersetter()\") \tpublic void performapianalytics() { \t\tsystem.out.println(\"\\n=====>>> performing api analytics\");\t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Field Injection.html",
    "title": "Field Injection",
    "body": " index search search back field injection field injection allows you to inject dependencies by setting field values on your class directly (even private ones). this is accomplished by using java reflection. for this, we need to configure the autowired annotation as follows: apply it directly to the field which saves us from using setter methods for dependency injection. define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface @autowired \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Method Injection.html",
    "title": "Method Injection",
    "body": " index search search back method injection one thing to note is that you can add dependency injection on any method you want, does not have to be a setter method: define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency we now create a setter method in our class for the injection: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired public anymethod(fortuneservice fortuneservice){ this.fortuneservice = fortuneservice; } \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/index.html",
    "title": "Java Annotations",
    "body": " index search search back java annotations java annotations are special labels added to classes. they provide metadata about the class, and can be processed at compile time or run-time for special processing. we use annotations to minimize the xml configuration. spring scans the classes to find beans and configure them internally (as we have done with the xml configuration). in order to use this approach we need to: enable component scanning in our spring configuration file and add the @component annotation to our class inversion of control dependency injection scopes life cycles $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Constructor Injection.html",
    "title": "Constructor Injection",
    "body": " index search search back constructor injection define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired \tpublic tenniscoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Qualifier Annotation.html",
    "title": "Qualifier",
    "body": " index search search back qualifier in order to specify which specific implementation of an interface we want to use, when this interface is implemented by several beans, then we use the qualifier annotation. the qualifier annotation can be used in any dependency injection implementation: constructor injection (has different syntax) setter injection method injection field injection define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \t \t// we tell spring to search for beans (classes with @component annotation) \t// that implement the fortuneservice interface whose name is \"happyfortuneservice\" \t// (note this is the default name of the component if you set one explicitly you \t// will have to specify that one in the qualifier annotation) \t@autowired \t@qualifier(\"happyfortuneservice\") \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } qualifier in constructor package com.springdemo; import org.springframework.beans.factory.annotation.autowired; import org.springframework.beans.factory.annotation.qualifier; import org.springframework.stereotype.component; @component public class tenniscoach implements coach { private fortuneservice fortuneservice; // define a default constructor public tenniscoach() { system.out.println(\">> tenniscoach: inside default constructor\"); } @autowired public tenniscoach(@qualifier(\"happyfortuneservice\") fortuneservice thefortuneservice) { system.out.println(\">> tenniscoach: inside constructor using @autowired and @qualifier\"); fortuneservice = thefortuneservice; } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Scopes.html",
    "title": "Scopes",
    "body": " index search search back scopes to explicitly specify scopes with java annotations you do as follows: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.context.annotation.scope; @component @scope(\"singleton\") public class tenniscoach implements coach { ... or package com.springdemo; import org.springframework.stereotype.component; import org.springframework.context.annotation.scope; @component @scope(\"prototype\") public class tenniscoach implements coach { ... refer to more information about scopes are in bean scopes: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control let's see how to make us of inversion of control with annotations: enable component scanning we remove all of the beans we defined, and enable component scanning: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/xmlschema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemalocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"> \t<!-- add entry to enable component scanning --> \t<context:component-scan base-package=\"com.springdemo\" /> </beans> now spring will scan recursively all of the files in this package. add @component annotation to classes we add the @component annotation to our classes (note we do not add it to the interfaces like coach). package com.springdemo; import org.springframework.stereotype.component; @component // we can also set the explicit name like // @component(\"mytenniscoach\") public class tenniscoach implements coach { \t \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} } note that we can name the component explicitly or by default. main method in our application we do not really need to change anything. we create our bean the same way we did before. the only thing to note is that if we set the name of the component explicitly, then when we instantiate the bean, we should refer to it by said name. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\t// if we set the name explicitly \t\tcoach thecoach = context.getbean(\"mytenniscoach\", coach.class); // else \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Life Cycles.html",
    "title": "Life Cycle",
    "body": " index search search back life cycle to define methods to add when the beans is constructed or destroyed we use the postconstruct and predestroy annotation. package com.springdemo; public class trackcoach implements coach { \t \tprivate fortuneservice fortuneservice; \t \tpublic trackcoach() { \t\t \t} \t \tpublic trackcoach(fortuneservice fortuneservice) { \t\tthis.fortuneservice = fortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"run a hard 5k\"; \t} \t \t@override \tpublic string getdailyfortune() { \t\treturn \"just do it: \" + fortuneservice.getfortune(); \t} \t \t// run when the bean is done creating \t@postconstruct \tpublic void domystartupstuff() { \t\tsystem.out.println(\"trackcoach: inside method domystartupstuff\"); \t} \t \t// run before the bean is destroyed \t@predestroy \tpublic void domycleanupstuffyoyo() { \t\tsystem.out.println(\"trackcoach: inside method domycleanupstuffyoyo\");\t\t \t} } refer to more information about scopes are in bean life cycle: notes access modifier: the method can have any access modifier (public, protected, private) return type: the method can have any return type. however, \"void' is most commonly used. if you give a return type just note that you will not be able to capture the return value. as a result, \"void\" is commonly used. method name: the method can have any method name. arguments: the method can not accept any arguments. the method should be no-arg. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Dependency Injection.html",
    "title": "Dependency Injection",
    "body": " index search search back dependency injection we will introduce dependency injection with annotation using autowiring: spring looks for a class that matches the attribute type (call or interface) (i.e. fortuneservice) spring will inject it automatically if there are multiple implementations: tell spring which specific bean to use with the qualifier annotation constructor injection setter injection method injection field injection inject using properties file qualifier annotation which dependency to use choose a style and stay consistent in your project. you get the same functionality regardless of the type of dependency injection you use. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Setter Injection.html",
    "title": "Setter Injection",
    "body": " index search search back setter injection define dependency as component package com.springdemo; import org.springframework.stereotype.component; // we tell spring this is a bean @component public class happyfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is your lucky day!\"; \t} } specify dependency we now create a setter method in our class for the injection: package com.springdemo; import org.springframework.stereotype.component; import org.springframework.beans.factory.annotation.autowired; @component public class tenniscoach implements coach { \tprivate fortuneservice fortuneservice; \t \tpublic tenniscoach() {} // we tell spring to search for beans (classes with @component annotation) // that implement the fortuneservice interface \t@autowired public setfortuneservice(fortuneservice fortuneservice){ this.fortuneservice = fortuneservice; } \t@override \tpublic string getdailyworkout() { \t\treturn \"practice your backhand volley\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } the main method and the configuration files remain unchanged. and when we execute this piece of code, spring will automatically inject the dependency because of the autowired annotation. package com.springdemo; import org.springframework.context.support.classpathxmlapplicationcontext; public class annotationdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config file \t\tclasspathxmlapplicationcontext context = \t\t\t\tnew classpathxmlapplicationcontext(\"applicationcontext.xml\"); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t// call method to get daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t \t\t// close the context \t\tcontext.close(); \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Annotations/Inject using Properties File.html",
    "title": "Inject properties file using Java annotations",
    "body": " index search search back inject properties file using java annotations this solution will show you how inject values from a properties file using annotations. the values will no longer be hard coded in the java code. create a properties file we create new text file: src/sport.properties foo.email=myeasycoach@luv2code.com foo.team=silly java coders load the properties we load the properties in the configuration xml file. for that we add the line: <context:property-placeholder location=\"classpath:sport.properties\"/> inject values lastly we inject the properties values into our bean like so: @value(\"${foo.email}\") private string email; @value(\"${foo.team}\") private string team; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/Concepts.html",
    "title": "Database Concepts",
    "body": " index search search back database concepts cascade: perform an operations on related entities on save: if we save an object, if it is related to another object, we need to also save that other object on delete: if we delete an object that is related to another object, we might need to delete that other object (depends on the use case) fetch types: when we fetch data, should we retrieve everything? eager: will retrieve everything lazy: will retrieve on request cascade types persist: if entity is persisted/saved, the related entity will also be persisted remove: if entity is removed/deleted, the related entity will also be deleted refresh: if entity is refreshed, the related entity will also be refreshed detach: if entity is detached (not associated with session), the related entity will also be detached merge: if entity is merged, the related entity will also be merged all: all of the above cascade types by default, no operations are cascaded. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/Sessions.html",
    "title": "Sessions",
    "body": " index search search back sessions there are two key components when it comes to session handling: sessionfactory: reads the hibernate configuration file, creates sessions objects, and is created only once in the application and reused over and over again session: is a wrapper around a jdbc connection, which is the main object used to save/retrieve objects. this object is created multiple times. so to create a sessionfactory and then create session from it: public class demo { public static void main(string[] args) { // create session factory sessionfactory factory = new configuration() // configuration file in src/ (if it is not specified, hibernate will look for a file named hibernate.cfg.xml) .configure(\"hibernate.cfg.xml\") // class that was annotated to be mapped .addannotatedclass(student.class) // you can add multiple classes .addannotatedclass(...) // create the factory .buildsessionfactory(); // create session session session = factory.getcurrentsession(); try {\t\t\t // use session object to perform crud operations\t } finally { // delete session factory factory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/ManyToMany.html",
    "title": "Many To Many Relationship",
    "body": " index search search back many to many relationship here we demonstrate how to implement a many to many relationship between two entities. for this we need a join table: well, first of all you have to define the two database tables corresponding to these two entities. and then we define a intermediate table to act as the join table called course_student. entities we now code the two entities: package com.hibernate.demo.entity; // annotate the class as an entity and map to db table @entity @table(name=\"course\") public class course { // define the fields // annotate the fields with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"title\") private string title; // set up one to many relationship @manytoone(cascade= // on delete course, do not delete instructor {cascadetype.persist, cascadetype.merge, cascadetype.detach, cascadetype.refresh}) @joincolumn(name=\"instructor_id\") private instructor instructor; // set up unidirectional one to many relationship @onetomany(fetch=fetchtype.lazy, cascade=cascadetype.all) \t@joincolumn(name=\"course_id\") \tprivate list<review> reviews; // set up many to many relationship with lazy loading // so only courses are retrieved, and the students associated // are obtained only if needed \t@manytomany(fetch=fetchtype.lazy, \t\t\tcascade= {cascadetype.persist, cascadetype.merge, \t\t\t cascadetype.detach, cascadetype.refresh}) // specifying the join table, and the corresponding // foreign keys @jointable( // table name name=\"course_student\", // this entity's pk joincolumns=@joincolumn(name=\"course_id\"), // related entity's pk inversejoincolumns=@joincolumn(name=\"student_id\") ) private list<student> students; public course() { }\t\t ... // setters and getters } and now the student: package com.hibernate.demo.entity; @entity @table(name=\"student\") public class student { @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"first_name\") private string firstname; @column(name=\"last_name\") private string lastname; @column(name=\"email\") private string email; // set up many to many relationship with lazy loading // so only students are retrieved, and the courses associated // are obtained only if needed @manytomany(fetch=fetchtype.lazy, \t\tcascade= {cascadetype.persist, cascadetype.merge, \t\t cascadetype.detach, cascadetype.refresh}) // specifying the join table, and the corresponding // foreign keys @jointable( // table name \t\tname=\"course_student\", // this entity's pk \t\tjoincolumns=@joincolumn(name=\"student_id\"), // related entity's pk \t\tinversejoincolumns=@joincolumn(name=\"course_id\") \t\t)\t private list<course> courses; // constructor, getters, setters .... main app to test our code, we are going to get a course and add it to a student: package com.hibernate.demo; public class createdemo { public static void main(string[] args) { // create session factory // ... // create session session session = factory.getcurrentsession(); try {\t\t // start a transaction session.begintransaction(); // get the student mary from database int studentid = 2; student tempstudent = session.get(student.class, studentid); // create more courses course tempcourse1 = new course(\"rubik's cube - how to speed cube\"); course tempcourse2 = new course(\"atari 2600 - game development\"); \t\t\t // add student to courses tempcourse1.addstudent(tempstudent); tempcourse2.addstudent(tempstudent); \t\t\t // save the courses session.save(tempcourse1); session.save(tempcourse2); // commit transaction session.gettransaction().commit(); } finally { session.close(); \tfactory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/Database Operations.html",
    "title": "Database Operations",
    "body": " index search search back database operations save java object to save a java object: public ... { try {\t\t\t \t// create a student object \tstudent tempstudent = new student(\"paul\", \"doe\", \"paul@luv2code.com\"); \t \t// start a transaction \tsession.begintransaction(); \t \t// save the student object \tsession.save(tempstudent); \t \t// commit transaction \tsession.gettransaction().commit(); } finally { \tfactory.close(); } } read java object public ... { try {\t\t\t // from the student created and saved previously // find out the student's id: primary key // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); // retrieve student based on the id: primary key system.out.println(\"\\ngetting student with id: \" + tempstudent.getid()); // get from the db by the primary key of the student student mystudent = session.get(student.class, tempstudent.getid()); // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } query java object hibernate has a query language for retrieving objects: hql which is similar to sql. public class querystudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t // start a transaction session.begintransaction(); // note we use the java object name for the table name // and the name of the attribute in the class for the name // of the column (firstname istd of first_name) // query students: lastname='doe' or firstname='daffy' thestudents = session.createquery(\"from student s where\" + \" s.lastname='doe' or s.firstname='daffy'\").getresultlist(); // query students where email like '%gmail.com' thestudents = session.createquery(\"from student s where\" \t\t+ \" s.email like '%gmail.com'\").getresultlist(); // commit transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } update java objects public class updatestudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t\t\t\t\t\t // update one student int studentid = 1; // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); student mystudent = session.get(student.class, studentid); // update name of student mystudent.setfirstname(\"scooby\"); // commit the transaction session.gettransaction().commit(); // update several students session = factory.getcurrentsession(); session.begintransaction(); // update email for all students system.out.println(\"update email for all students\"); session.createquery(\"update student set email='foo@gmail.com'\") \t.executeupdate(); \t\t\t // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } } delete java objects public class deletestudentdemo { public static void main(string[] args) { // create session factory ... // create session session session = factory.getcurrentsession(); try {\t\t\t\t\t\t\t\t int studentid = 1; \t\t\t // now get a new session and start transaction session = factory.getcurrentsession(); session.begintransaction(); // retrieve student based on the id: primary key student mystudent = session.get(student.class, studentid); // delete the student session.delete(mystudent); // delete student id=2 session.createquery(\"delete from student where id=2\").executeupdate(); // commit the transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/Annotations.html",
    "title": "Configure Hibernate with Annotations",
    "body": " index search search back configure hibernate with annotations add hibernate configuration file we create the following hibernate.cfg.xml file: <!doctype hibernate-configuration public \"-//hibernate/hibernate configuration dtd 3.0//en\" \"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\"> <hibernate-configuration> <!-- a session factory allows us to get sessions objects to connect to the database --> <session-factory> <!-- jdbc database connection settings --> <property name=\"connection.driver_class\">com.mysql.cj.jdbc.driver</property> <property name=\"connection.url\">jdbc:mysql://localhost:3306/hb_student_tracker?usessl=false&amp;servertimezone=utc</property> <property name=\"connection.username\">hbstudent</property> <property name=\"connection.password\">hbstudent</property> <!-- jdbc connection pool settings ... using built-in test pool --> <property name=\"connection.pool_size\">1</property> <!-- select our sql dialect --> <property name=\"dialect\">org.hibernate.dialect.mysqldialect</property> <!-- echo the sql to stdout --> <property name=\"show_sql\">true</property> <!-- set the current session context --> <property name=\"current_session_context_class\">thread</property> </session-factory> </hibernate-configuration> annotate java class hibernate deals with the concept of entity, which is basically a java class with its attributes, setters and getters, that is mapped to a database table with the help of annotations. note that there are two ways of configuring the mapping: xml config file (legacy) java annotations (modern, preferred) with java annotations we have to follow these steps: map the class to a database table // let spring know this is an entity we want to map to a database table @entity // provides the actual name of the table (observe in this case it is optional // because the name of the class = the name of the database table) @table(name=\"student\") public class student { ... } map the fields to database columns public class student { // primary key @id // how to generate primary key @generatedvalue(strategy=generationtype.identity) // column name (also not needed if the name in the database and the name here are the same) @column(name=\"id\") private int id; @column(name=\"first_name\") private string firstname; @column(name=\"last_name\") private string lastname; @column(name=\"email\") private string email; ... } some other id generation strategies are: auto: pick the appropiate strategy for the given database identity: assign primary keys using database identidy column sequence: assign primary keys using a database sequence table: assign primary keys using an uderlying database table to ensure uniqueness you can also create your custom generator $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/Eager vs Lazy Loading.html",
    "title": "Eager vs Lazy Loading",
    "body": " index search search back eager vs lazy loading eager: fetches all data all at once (with dependencies of the entity) lazy: fetches required data only lazy loading is usually preferred, that is only load data when absolutely needed. the flow of lazy loading is: load the main entity first load dependent entities on demand note than when using lazy loading you need an open hibernate session, else if you close the session and try to retrieve the data hibernate will throw an exception. default fetch types mapping defaul fetch type @onetoone fetchtype.eager @onetomany fetchtype.lazy @manytoone fetchtype.eager @manytomany fetchtype.lazy specify fetch type on entity we can specify the fetching type on the entity as follows: @entity @table(name=\"instructor\") public class instructor { \t@id \t@generatedvalue(strategy=generationtype.identity) \t@column(name=\"id\") \tprivate int id; \t \t@onetoone(cascade=cascadetype.all) \t@joincolumn(name=\"instructor_detail_id\") \tprivate instructordetail instructordetail; \t // specify fetch type (only load the courses on demand, their retrieval // is delayed) \t@onetomany(fetch=fetchtype.lazy, \t\t\t mappedby=\"instructor\", \t\t\t cascade= {cascadetype.persist, cascadetype.merge, \t\t\t\t\t\t cascadetype.detach, cascadetype.refresh}) \tprivate list<course> courses; ... avoid closed session exception to avoid the error we use the join fetch (we do override lazy loading with eager loading) of hql: public class fetchjoindemo { public static void main(string[] args) { // create session factory sessionfactory factory = ... // create session session session = factory.getcurrentsession(); try {\t\t\t // start a transaction session.begintransaction(); // hibernate query with hql to avoid exception of lazy loading when closing session // get the instructor from db int theid = 1; query<instructor> query = \t\tsession.createquery(\"select i from instructor i \" \t\t\t\t\t\t+ \"join fetch i.courses \" \t\t\t\t\t\t+ \"where i.id=:theinstructorid\", \t\t\t\tinstructor.class); // set parameter on query query.setparameter(\"theinstructorid\", theid); // execute query and get instructor instructor tempinstructor = query.getsingleresult(); system.out.println(\"luv2code: instructor: \" + tempinstructor);\t // commit transaction session.gettransaction().commit(); // close the session session.close(); system.out.println(\"\\nluv2code: the session is now closed!\\n\"); // get courses for the instructor system.out.println(\"luv2code: courses: \" + tempinstructor.getcourses()); system.out.println(\"luv2code: done!\"); } finally { // add clean up code session.close(); factory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/index.html",
    "title": "index",
    "body": " index search search back hibernate is a framework for persisting/saving java objects in a database handles all of the low-level sql minimizes the amount jdbc code to develop provides the object-to-relational mapping (orm): the developer defines a mapping between a java class and a database table hibernate uses jdbc for all database communications: $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/OneToMany.html",
    "title": "One To Many Relationship",
    "body": " index search search back one to many relationship unidirectional bidirectional unidirectional here we demonstrate how to implement a unidirectional one to many relationship between two entities: well, first of all you have to define the two database tables corresponding to these two entities. entities we now code the two entities: package com.hibernate.demo.entity; // annotate the class as an entity and map to db table @entity @table(name=\"course\") public class course { // define the fields // annotate the fields with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"title\") private string title; // set up one to many relationship @manytoone(cascade= // on delete course, do not delete instructor {cascadetype.persist, cascadetype.merge, cascadetype.detach, cascadetype.refresh}) @joincolumn(name=\"instructor_id\") private instructor instructor; // set up unidirectional one to many relationship @onetomany(fetch=fetchtype.lazy, cascade=cascadetype.all) \t@joincolumn(name=\"course_id\") \tprivate list<review> reviews; public course() { }\t\t ... // setters and getters } and now the review: package com.hibernate.demo.entity; @entity @table(name=\"review\") public class review { \t@id \t@generatedvalue(strategy=generationtype.identity) \t@column(name=\"id\") \tprivate int id; \t \t@column(name=\"comment\") \tprivate string comment; \t \tpublic review() { \t\t \t} note that there is no reference in the review to the course. main app to test our code, we are going to get a course and the list of review objects associated. the test main app is the following: package com.hibernate.demo; public class createdemo { public static void main(string[] args) { // create session factory // ... // create session session session = factory.getcurrentsession(); try {\t\t // start a transaction session.begintransaction(); // get the course int theid = 10; course tempcourse = session.get(course.class, theid); // get reviews tempcourse.getreviews(); // commit transaction session.gettransaction().commit(); } finally { session.close(); \tfactory.close(); } } } bidirectional now we will define the following relationship: let's now see how to code a bidirectional relationship: entities package com.hibernate.demo.entity; /* annotate the class as an entity and map to db table */ @entity @table(name=\"instructor\") public class instructor { // define the fields and annotate the fields // with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"first_name\") private string firstname; @column(name=\"last_name\") private string lastname; @column(name=\"email\") private string email; // set up mapping to instructordetail entity // note the cascade type @onetoone(cascade=cascadetype.all) // define the foreign key @joincolumn(name=\"instructor_detail_id\") private instructordetail instructordetail; // bidirectional relationship with courses // the mapping information is in the instructor // property in the course class @onetomany(mappedby=\"instructor\", // on delete instructor, do not delete courses cascade= {cascadetype.persist, cascadetype.merge, cascadetype.detach, cascadetype.refresh}) private list<course> courses; public instructor() { \t } ... // setters and getters } and now the course class: package com.hibernate.demo.entity; // annotate the class as an entity and map to db table @entity @table(name=\"course\") public class course { // define the fields // annotate the fields with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"title\") private string title; // set up one to many relationship @manytoone(cascade= // on delete course, do not delete instructor {cascadetype.persist, cascadetype.merge, cascadetype.detach, cascadetype.refresh}) @joincolumn(name=\"instructor_id\") private instructor instructor; public course() { }\t\t ... // setters and getters } main app in our test main app we are going to search for an instructordetail object, and we are going to retrieve the related instructor object: package com.hibernate.demo; public class getinstructordetaildemo { public static void main(string[] args) { session = factory.getcurrentsession(); try { // start a transaction session.begintransaction(); // get the instructor from db int theid = 1; instructor tempinstructor = session.get(instructor.class, theid);\t\t // create some courses course tempcourse1 = new course(\"air guitar - the ultimate guide\"); course tempcourse2 = new course(\"the pinball masterclass\"); // add courses to instructor tempinstructor.add(tempcourse1); tempinstructor.add(tempcourse2); // save the courses session.save(tempcourse1); session.save(tempcourse2); // commit transaction session.gettransaction().commit(); } catch(exception exc){ exc.printstacktrace(); } finally { // finish session session.close(); // remove factory\t factory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Hibernate/OneToOne.html",
    "title": "One To One Relationship",
    "body": " index search search back one to one relationship unidirectional bidirectional unidirectional here we demonstrate how to implement a unidirectional one to one relationship between two entities: well, first of all you have to define the two database tables corresponding to these two entities. entities we now code the two entities: package com.hibernate.demo.entity; /* annotate the class as an entity and map to db table */ @entity @table(name=\"instructor\") public class instructor { // define the fields and annotate the fields // with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"first_name\") private string firstname; @column(name=\"last_name\") private string lastname; @column(name=\"email\") private string email; // set up mapping to instructordetail entity // note the cascade type @onetoone(cascade=cascadetype.all) // define the foreign key @joincolumn(name=\"instructor_detail_id\") private instructordetail instructordetail; public instructor() { \t } ... // setters and getters } note the specification of the cascade type. and now the instructordetail: package com.hibernate.demo.entity; // annotate the class as an entity and map to db table @entity @table(name=\"instructor_detail\") public class instructordetail { \t// define the fields \t// annotate the fields with db column names \t \t@id \t@generatedvalue(strategy=generationtype.identity) \t@column(name=\"id\") \tprivate int id; \t \t@column(name=\"youtube_channel\") \tprivate string youtubechannel; \t \t@column(name=\"hobby\") \tprivate string hobby; \t \tpublic instructordetail() { }\t\t ... // setters and getters } main app to test our code, we are going to create an instructor object and an instructordetail object and save them. the test main app is the following: package com.hibernate.demo; public class createdemo { public static void main(string[] args) { // create session factory // ... // create session session session = factory.getcurrentsession(); try {\t\t\t // create the objects instructor tempinstructor = \t\tnew instructor(\"madhu\", \"patel\", \"madhu@mail.com\"); instructordetail tempinstructordetail = \t\tnew instructordetail( \t\t\t\t\"http://www.youtube.com\", \t\t\t\t\"guitar\");\t\t // associate the objects tempinstructor.setinstructordetail(tempinstructordetail); // start a transaction session.begintransaction(); // save the instructor // // note: this will also save the details object // because of cascadetype.all // session.save(tempinstructor);\t\t\t\t\t // commit transaction session.gettransaction().commit(); } finally { \tfactory.close(); } } } bidirectional now we will define the following bidirectional one to one relationship: let's now see how to code a bidirectional relationship: entities package com.hibernate.demo.entity; /* annotate the class as an entity and map to db table */ @entity @table(name=\"instructor\") public class instructor { // define the fields and annotate the fields // with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"first_name\") private string firstname; @column(name=\"last_name\") private string lastname; @column(name=\"email\") private string email; // set up mapping to instructordetail entity // note the cascade type @onetoone(cascade=cascadetype.all) // define the foreign key @joincolumn(name=\"instructor_detail_id\") private instructordetail instructordetail; public instructor() { \t } ... // setters and getters and now the instructordetail: package com.hibernate.demo.entity; // annotate the class as an entity and map to db table @entity @table(name=\"instructor_detail\") public class instructordetail { // define the fields // annotate the fields with db column names @id @generatedvalue(strategy=generationtype.identity) @column(name=\"id\") private int id; @column(name=\"youtube_channel\") private string youtubechannel; @column(name=\"hobby\") private string hobby; \t // add @onetoone annotation // mappedby refers to the instructordetail property // in the instructor class // this uses the information from the instructor class in @joincolumn // to define the mapping @onetoone(mappedby=\"instructordetail\", // different cascade types cascade={ cascadetype.detach, cascadetype.merge, cascadetype.persist, cascadetype.refresh}) private instructor instructor; public instructordetail() { }\t\t ... // setters and getters } main app in our test main app we are going to search for an instructordetail object, and we are going to retrieve the related instructor object: package com.hibernate.demo; public class getinstructordetaildemo { \tpublic static void main(string[] args) { session = factory.getcurrentsession(); try { // start a transaction session.begintransaction(); // get the instructor detail object int theid = 2999; instructordetail tempinstructordetail = \t\tsession.get(instructordetail.class, theid); \t\t\t // print the associated instructor system.out.println(\"the associated instructor: \" + \t\t\t\t\ttempinstructordetail.getinstructor()); // commit transaction session.gettransaction().commit(); } catch(exception exc){ exc.printstacktrace(); } finally { // finish session \t\t\tsession.close(); \t\t // remove factory\t \t\t\tfactory.close(); } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Configuration/Load Properties from File.html",
    "title": "Load Properties from File",
    "body": " index search search back load properties from file in order to inject values read from a properties file we do the following: create the file first, we create the file sport.properties foo.email=myeasycoach@luv2code.com foo.team=awesome java coders load the file now, we load the file from our configuration class: package com.springdemo; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.configuration; import org.springframework.context.annotation.propertysource; import org.springframework.context.support.propertysourcesplaceholderconfigurer; @configuration @propertysource(\"classpath:sport.properties\") public class sportconfig { \t \t// define bean for our sad fortune service \t@bean \tpublic fortuneservice sadfortuneservice() { \t\treturn new sadfortuneservice(); \t} \t \t// define bean for our swim coach and inject dependency \t@bean \tpublic coach swimcoach() { \t\tswimcoach myswimcoach = new swimcoach(sadfortuneservice()); \t\t \t\treturn myswimcoach; \t} \t } inject values we inject the values at field level in our bean: package com.springdemo; import org.springframework.beans.factory.annotation.value; public class swimcoach implements coach { \tprivate fortuneservice fortuneservice; \t@value(\"${foo.email}\") \tprivate string email; \t \t@value(\"${foo.team}\") \tprivate string team; ... $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Configuration/index.html",
    "title": "Spring Configuration with Java",
    "body": " index search search back spring configuration with java we are now going to use java to configure our application instead of using xml, to do that we follow the next steps: create a java class and annotate as @configuration add component scanning support with @componentscan (optional), which is xml we did as: <?xml version=\"1.0\" encoding=\"utf-8\"?> <beans xmlns=\"http://www.springframework.org/schema/beans\" ....> \t<!-- add entry to enable component scanning --> \t<context:component-scan base-package=\"com.springdemo\" /> </beans> in the main app read the spring java configuration class configuration with java inversion of control load properties from file dependency injection $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Configuration/Inversion of Control.html",
    "title": "Inversion of Control",
    "body": " index search search back inversion of control to define a bean, we now use our configuration class: create the bean package com.springdemo; // note there are no special annotations public class swimcoach implements coach { \tprivate fortuneservice fortuneservice; \tpublic swimcoach(fortuneservice thefortuneservice) { \t\tfortuneservice = thefortuneservice; \t} \t \t@override \tpublic string getdailyworkout() { \t\treturn \"swim 1000 meters as a warm up.\"; \t} \t@override \tpublic string getdailyfortune() { \t\treturn fortuneservice.getfortune(); \t} } we also create the sadfortuneservice bean: package com.springdemo; import org.springframework.stereotype.component; @component public class sadfortuneservice implements fortuneservice { \t@override \tpublic string getfortune() { \t\treturn \"today is a sad day :(\"; \t} } define the bean in the configuration class package com.springdemo; import org.springframework.context.annotation.bean; import org.springframework.context.annotation.configuration; @configuration public class sportconfig { \t \t// define bean for our sad fortune service \t@bean \tpublic fortuneservice sadfortuneservice() { \t\treturn new sadfortuneservice(); \t} \t \t// define bean for our swim coach and inject dependency // without springs dependency injection \t@bean \tpublic coach swimcoach() { \t\tswimcoach myswimcoach = new swimcoach(sadfortuneservice()); \t\t \t\treturn myswimcoach; \t} \t } the @bean annotation tells spring that we are creating a bean component manually. we didn't specify a scope so the default scope is singleton. public coach swimcoach(){ specifies that the bean will bean id of \"swimcoach\". the @bean annotation will intercept any requests for \"swimcoach\" bean. since we didn't specify a scope, the bean scope is singleton. so now in our main method: main method package com.luv2code.springdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; public class javaconfigdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(sportconfig.class); \t\t \t\t// get the bean from spring container by its id \t\tcoach thecoach = context.getbean(\"swimcoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// call method to get the daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Java Configuration/Configuration With Java.html",
    "title": "Configuration With Java",
    "body": " index search search back configuration with java create configuration class package com.springdemo; import org.springframework.context.annotation.componentscan; import org.springframework.context.annotation.configuration; // 1. define configuration class @configuration // 2. add component scanning support @componentscan(\"com.springdemo\") public class sportconfig { \t } load the configuration class package com.springdemo; import org.springframework.context.annotation.annotationconfigapplicationcontext; public class javaconfigdemoapp { \tpublic static void main(string[] args) { \t\t// read spring config java class \t\tannotationconfigapplicationcontext context = \t\t\t\tnew annotationconfigapplicationcontext(sportconfig.class); \t\t \t\t// get the bean from spring container \t\tcoach thecoach = context.getbean(\"tenniscoach\", coach.class); \t\t \t\t// call a method on the bean \t\tsystem.out.println(thecoach.getdailyworkout()); \t\t\t\t \t\t// call method to get the daily fortune \t\tsystem.out.println(thecoach.getdailyfortune()); \t\t\t\t\t \t\t// close the context \t\tcontext.close(); \t\t \t} } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Spring/Spring Framework.html",
    "title": "Spring Framework",
    "body": " index search search back spring framework components: core container beans (define entities) core (management of beans) spel: spring expression language (annotations) context (store entities) aop (aspect oriented programming): allows you to create application wide services like messaging, logging, security, etc. and add this functionality to your objects in a declarative fashion. data access layer: establishes the connection with the database jdbc helper classes orm: provides hook to hibernate transactions oxm jms (java message service) for async messages web layer: all web related classes, holds all of the spring mvc framework servlet websocket web portlet test layer: supports tdd: unit integration mock spring projects spring modules built on top of the core spring framework: spring boot spring cloud spring batch etc spring projects $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Reducer.html",
    "title": "Reducers and Actions",
    "body": " index search search back reducers and actions let's now see an example of a reducer, more concretely the reducer of the user slice we defined previously: // use create slice to define the slice import { createslice } from \"@reduxjs/toolkit\"; // define initial state const initialstatevalue = { name: \"\", age: 0, email: \"\" }; export const userslice = createslice({ // name of slice name: \"user\", // initial state of reducer initialstate: { value: initialstatevalue }, // possible reducers reducers: { login: (state, action) => { state.value = action.payload; }, logout: (state) => { state.value = initialstatevalue; }, }, }); // de-structure actions export const { login, logout } = userslice.actions; // export reducer export default userslice.reducer; we now $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/useReducer.html",
    "title": "useReducer",
    "body": " index search search back usereducer an alternative to usestate. accepts a reducer of type (state, action) => newstate, and returns the current state paired with a dispatch method. usereducer is usually preferable to usestate when you have complex state logic that involves multiple sub-values or when the next state depends on the previous one. usereducer also lets you optimize performance for components that trigger deep updates because you can pass dispatch down instead of callbacks. for example: import react, { usestate, usereducer } from 'react'; // components import modal from './modal'; // data import { data } from '../../../data'; // reducer dispatch function import { reducer } from './reducer'; // initial state for the reducer const defaultstate = { people: [], ismodalopen: false, modalcontent: '', }; const index = () => { // define state variables const [name, setname] = usestate(''); // define reducer: (dispatch fuction, initial state) const [state, dispatch] = usereducer(reducer, defaultstate); const handlesubmit = (e) => { // avoid the re-rendering caused by the submit event e.preventdefault(); if (name) { const newitem = { id: new date().gettime().tostring(), name }; // call reducer to update state dispatch({ type: 'add_item', payload: newitem }); setname(''); } else { // call reducer to update state dispatch({ type: 'no_value' }); } }; const closemodal = () => { // call reducer to update state dispatch({ type: 'close_modal' }); }; return ( <> {/*render modal component conditionally */} {state.ismodalopen && ( <modal closemodal={closemodal} modalcontent={state.modalcontent} /> )} {/* form to add a new person to the reducer state variable */} <form onsubmit={handlesubmit} classname='form'> <div> <input type='text' value={name} onchange={(e) => setname(e.target.value)} /> </div> <button type='submit'>add </button> </form> {/* show the people stored in the reducer state variable */} {state.people.map((person) => { return ( <div key={person.id} classname='item'> <h4>{person.name}</h4> <button onclick={() => // call reducer to update state dispatch({ type: 'remove_item', payload: person.id }) } > remove </button> </div> ); })} </> ); }; export default index; now, let's see the reducer function: /* reducer function */ export const reducer = (state, action) => { // define logic for each type of action if (action.type === 'add_item') { // add new person (action.payload) to existing people array (state.people) const newpeople = [...state.people, action.payload]; return { // always copy the value from the previous state ...state, // update the people array people: newpeople, ismodalopen: true, modalcontent: 'item added', }; } if (action.type === 'no_value') { // always copy the value from the previous state return { ...state, ismodalopen: true, modalcontent: 'please enter value' }; } if (action.type === 'close_modal') { return { ...state, ismodalopen: false }; } if (action.type === 'remove_item') { // filter people array, by removing the person const newpeople = state.people.filter( (person) => person.id !== action.payload ); // copy the previous state (...state) and update the people the array (newpeople) return { ...state, people: newpeople }; } throw new error('no matching action type'); }; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Event Basics.html",
    "title": "Event Basics",
    "body": " index search search back event basics list of all possible events to define an event we have to specify: attribute: like onclick, onmousehover, etc. eventhandler: the function to apply. this can be specified as a reference or as an in-line function. next, we present an example: import react from 'react' const book = ({ title, author }) => { const clickhandler = () => {alert('hello!!')} return ( <article classname='book'> <!-- here we have the eventhandler as an in-line function --> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <!-- here we have the eventhandler as a reference --> <button type=\"button\" onclick={clickhandler}>this is a button</button> </article> ); }; export default book to pass an argument to the eventhandler we have to use a lambda function, else when we load the application will invoke the function clickhandler(author) import react from 'react' const book = ({ title, author }) => { const clickhandler = (author) => {alert(author)} return ( <article classname='book'> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <!-- wrap function with an in-line function --> <button type=\"button\" onclick={() => clickhandler(author)}>this is a button</button> </article> ); }; export default book we can also access the event object from within the function, like: import react from 'react' const book = ({ title, author }) => { // you can always access the event object from an eventhandler const clickhandler = (author, e) => {console.log(e)} return ( <article classname='book'> <h1 onclick={() => alert('hello!!')}>{title}</h1> <h4>{author}</h4> <button type=\"button\" onclick={() => clickhandler(author)}>this is a button</button> </article> ); }; export default book $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Controlled Inputs.html",
    "title": "Controlled Inputs",
    "body": " index search search back controlled inputs let's see how to handle inputs in a form using react: import react, { usestate } from 'react'; const controlledinputs = () => { const [firstname, setfirstname] = usestate(''); const [people, setpeople] = usestate([]); const handlesubmit = (e) => { // avoid the default behaviour in submit which re-renders the page e.preventdefault(); // our own logic for the submit action if (firstname) { // create new person object const person = { id: new date().gettime().tostring(), firstname }; // update our state (remember you need to spred the people state variable we have before, else the you would // override the people state variable and it would be assigned to only the person object) setpeople((people) => { return [...people, person]; }); // set to empty, so the value of the input is the empty string setfirstname(''); } else { // no values to create new person console.log('empty values'); } }; return ( <> <article> {/*event handler for the submit event*/} <form onsubmit={handlesubmit}> <div > <label htmlfor='firstname'>name : </label> <input type='text' id='firstname' name='firstname' {/*set the value of the input, it updates every time we change the input*/} value={firstname} {/*event handler for the change event: use a lambda function to pass the event e and get the value in the input*/} onchange={(e) => setfirstname(e.target.value)} /> </div> <button type='submit'>add person</button> </form> {/*show each person in the people array */} {people.map((person, index) => { const { id, firstname } = person; return ( <div classname='item' key={id}> <h4>{firstname}</h4> </div> ); })} </article> </> ); }; export default controlledinputs; multiple inputs how can we define an event handler for the onchange event that is generic, instead of defining one for each input? to showcase this scenario, we will use the same code as before, but with two new inputs. all of the inputs have the same onchange event handler. import react, { usestate } from 'react'; const controlledinputs = () => { // create a new state variable person, that holds the properties of the person we are currently creating const [person, setperson] = usestate({ firstname: '', email: '', age: '' }); // array of people we have already created const [people, setpeople] = usestate([]); // generic event handler const handlechange = (e) => { // obtain the name of the input/state variable const name = e.target.name; // obtain the new value for the input const value = e.target.value; // update the value of the property for the current person setperson({ ...person, [name]: value }); }; const handlesubmit = (e) => { e.preventdefault(); if (person.firstname && person.email && person.age) { const newperson = { ...person, id: new date().gettime().tostring() }; setpeople([...people, newperson]); setperson({ firstname: '', email: '', age: '' }); } }; return ( <> <article classname='form'> <form> <div classname='form-control'> <label htmlfor='firstname'>name : </label> <input type='text' id='firstname' name='firstname' // access the firstname of the person object value={person.firstname} // generic event handler onchange={handlechange} /> </div> <div classname='form-control'> <label htmlfor='email'>email : </label> <input type='email' id='email' name='email' // access the email of the person object value={person.email} // generic event handler onchange={handlechange} /> </div> <div classname='form-control'> <label htmlfor='age'>age : </label> <input type='number' id='age' name='age' // access the age of the person object value={person.age} // generic event handler onchange={handlechange} /> </div> <button type='submit' classname='btn' onclick={handlesubmit}> add person </button> </form> </article> <article> {people.map((person) => { const { id, firstname, email, age } = person; return ( <div key={id} classname='item'> <h4>{firstname}</h4> <p>{email}</p> <p>{age}</p> </div> ); })} </article> </> ); }; export default controlledinputs; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Get State.html",
    "title": "Get State",
    "body": " index search search back get state in order to access the state saved in our state, we do the following: import react from \"react\"; import { useselector } from \"react-redux\"; function profile() { // use the useselector hook const user = useselector((state) => state.user.value); return ( <div style={{ color: themecolor }}> <h1> profile page</h1> <!--obtain the user state--> <p> name: {user.name} </p> <p> age: {user.age}</p> <p> email: {user.email}</p> </div> ); } export default profile; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/npm.html",
    "title": "npm",
    "body": " index search search back npm it is the node package manager: create package.json (manifest) file, with the list of dependencies $ npm init install package locally and add it to package.json, under the keyword \"dependencies\" $ npm install <package name> install package globally (requires sudo) $ npm install -g <package name> install package only for development $ npm install <package name> --save-dev the packages installed with be saved under the file node_modules to install all the dependencies listed in package.json, just run: $ npm install where the package.json is. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Prop Drilling.html",
    "title": "Prop Drilling",
    "body": " index search search back prop drilling prop drilling refers to the scenario where we have to pass props to anidated components recursively. next up, we show and example import react, { usestate } from 'react'; // data import { data } from '../../../data'; // outer component const propdrilling = () => { // state passed as a prop const [people, setpeople] = usestate(data); // event handler passed as a prop const removeperson = (id) => { setpeople((people) => { return people.filter((person) => person.id !== id); }); }; return ( <section> <h3>prop drilling</h3> {/* pass props to the list elements */} <list people={people} removeperson={removeperson} /> </section> ); }; // middle component const list = ({ people, removeperson }) => { return ( <> {people.map((person) => { {/* pass props to the singleperson elements */} return ( <singleperson key={person.id} {...person} removeperson={removeperson} /> ); })} </> ); }; // inner component const singleperson = ({ id, name, removeperson }) => { return ( <div classname='item'> <h4>{name}</h4> <button onclick={() => removeperson(id)}>remove</button> </div> ); }; export default propdrilling; in these cases we can use the context api $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Custom Hooks.html",
    "title": "Custom Hooks",
    "body": " index search search back custom hooks customs hooks allow us to avoid duplicating code that uses hooks and essentially in different places of your code. for example, the fetching function is very common, so we create a usefetch hook. when you define a custom hook, that is, if you define a function outside a component that uses hooks, you will have to name it use<functionname>, else you will get an error. import react, { usestate, useeffect } from 'react' // import custom hook import { usefetch } from './2-usefetch' const url = 'https://course-api.com/javascript-store-products' const example = () => { // values returned by usefetch const { loading, products } = usefetch(url) return ( <div> <h2>{loading ? 'loading...' : 'data'}</h2> </div> ) } export default example import { usestate, useeffect, usecallback } from 'react'; export const usefetch = (url) => { // state within the hook const [loading, setloading] = usestate(true); const [products, setproducts] = usestate([]); // functionality of the hook const getproducts = usecallback(async () => { const response = await fetch(url); const products = await response.json(); setproducts(products); setloading(false); }, [url]); // run whenever the url or the getproducts function changes useeffect(() => { getproducts(); }, [url, getproducts]); // values returned by the custom hook return { loading, products }; }; note we are using the hook usecallback (refer to performance optimization), we do this because we are specifying getproducts as a dependency for useeffect. however getproducts is created every time the state changes. so when we call useeffect, we change the state, and therefore create the function getproducts, which triggers useeffect, thus the state changes, and we create getproducts, and so on and so forth. to avoid this, we use usecallback, which will create the function whenever any of the dependencies in the list change. so this means, now getproducts is only created when the url changes. this allows us to avoid the infinite loop we ran into before. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Performance Optimization.html",
    "title": "Performance Optimization",
    "body": " index search search back performance optimization even though react is fast by default (you do not need to use it), we can use different optimization techniques (mind, they do add their own cost): react.memo react.memo stores a component, and only re-renders if the props of the component change (it memoizes the component). in the next example, that means that we only re-render biglist if products change, thus, we do not re-render any singleproduct component unless products change. import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' const index = () => { const { products } = usefetch(url) const [count, setcount] = usestate(0) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <biglist products={products} /> </> ) } // each time a prop or the state changes, the component re-renders, so all // the elements of the list are processed again. // however if we use react.memo we only re-render the component if products change const biglist = react.memo(({ products }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> </article> ) } export default index usecallback what happens if we pass a function to biglist, well if the state changes (whichever variable of the state) then the function is created again, and so the function is different. which means the props of biglist list changes, and causes react.memo to re-render the entire component. that is why we use usecallback. usecallback allows us to define when to create a function, by specifying the dependencies like we did with useeffect: if the dependency is []: then only create in the first render if there are variables in the []: create whenever those variables change if there is nothing: create always. refer to customs hooks for an use case of usecallback inside the custom hook usefetch. import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' const index = () => { const { products } = usefetch(url); const [count, setcount] = usestate(0); const [cart, setcart] = usestate(0); // we only create this function when we update the cart value // that is we memoize the function const addtocart = usecallback(() => { setcart(cart + 1) }, [cart]) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <biglist products={products} addtocart={addtocart}/> </> ) } // each time a prop or the state changes, the component re-renders. because now // addtocart is define with usecallback, the re-render is not triggered const biglist = react.memo(({ products, addtocart }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} addtocart={addtocart} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields, addtocart }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> <button onclick={addtocart}>add to cart</button> </article> ) } export default index; usememo note that this hook deals with values (which is the traditional functionality of the idea of memoizing), whilst react.memo look for changes in the props. in the next example we create a function that returns a value, and we memoize the function, so it only computes the value whenever the products change (the argument of the function), else it returns the value stored before: import react, { usestate, usecallback, usememo } from 'react' // custom hook import { usefetch } from 'usefetch' const url = 'https://course-api.com/javascript-store-products' // define the function we are going to memoize const calculatemostexpensive = (data) => { return ( data.reduce((total, item) => { const price = item.fields.price if (price >= total) { total = price } return total }, 0) / 100 ) } const index = () => { const { products } = usefetch(url); const [count, setcount] = usestate(0); const [cart, setcart] = usestate(0); const addtocart = usecallback(() => { setcart(cart + 1) }, [cart]) // memoize the function with usememo const mostexpensive = usememo(() => calculatemostexpensive(products), [ products, ]) return ( <> <h1>count : {count}</h1> <button classname='btn' onclick={() => setcount(count + 1)}> click me </button> <!-- show most expensive product --> <h1>most expensive : ${mostexpensive}</h1> <biglist products={products} addtocart={addtocart}/> </> ) } const biglist = react.memo(({ products, addtocart }) => { return ( <section classname='products'> {products.map((product) => { return ( <singleproduct key={product.id} {...product} addtocart={addtocart} ></singleproduct> ) })} </section> ) }) const singleproduct = ({ fields, addtocart }) => { let { name, price } = fields price = price / 100 const image = fields.image[0].url return ( <article classname='product'> <img src={image} alt={name} /> <h4>{name}</h4> <p>${price}</p> <button onclick={addtocart}>add to cart</button> </article> ) } export default index; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Basics.html",
    "title": "Basics",
    "body": " index search search back basics redux is a complex state management tool, with a single store (javascrip object) as a cds (central data storage). components store: object that holds the state reducers: events handler that manages the state and returns the new updated state. the reducers get the arguments and return the state modified. actions: describe the event handler by the reducer and has two properties: type: is the identifier of the action payload: holds the data dispatch: is used to send actions to update the data so redux is composed by: handle an action the process of handling an action is the following: we create an action object and dispatch it: the store forwards the action to the reducer: the reducer updates the state and returns it the store notifies the ui components of the change of the state install redux $ npm install redux react-redux first steps inside src create a store folder inside the store folder create an index.js that holds all of the react states in this file $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Children in Props.html",
    "title": "Children in Props",
    "body": " index search search back children in props you can nest content inside your component. if we have the following: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' const singlebook = { title: 'book title', author: 'book author' } reactdom.render( <book {...singlebook}> <p> i am nested!</p> </book>, document.getelementbyid('root') ); you can access the nested object from your component: import react from 'react' // de-structure the children prop const book = ({ title, author, children }) => { return ( <article classname='book'> <h1>{title}</h1> <h4>{author}</h4> {children} </article> ); }; export default book $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/PropTypes.html",
    "title": "PropTypes",
    "body": " index search search back proptypes proptypes allows us to validate our props. import react from 'react' import product from './product' import { usefetch } from './usefetch' const url = 'https://course-api.com/react-prop-types-example' const index = () => { const { products } = usefetch(url) return ( <div> <h2>products</h2> <section classname='products'> {products.map((product) => { // pass the props return <product key={product.id} {...product} /> })} </section> </div> ) } export default index in this product component we show how to use proptypes to parametrize the props, and how to use conditional render to avoid getting an error when some of the props are missing. import react from 'react'; import proptypes from 'prop-types'; import defaultimage from './assets/default-image.jpeg'; const product = ({ image, name, price }) => { const url = image && image.url; return ( <article classname='product'> {/*use conditional rendering in case the data does not exist */} <img src={url || defaultimage} alt={name || 'default name'} /> <h4>{name}</h4> <p>${price || 3.99}</p> </article> ); }; // define the proptypes for the object product.proptypes = { image: proptypes.object.isrequired, name: proptypes.string.isrequired, price: proptypes.number.isrequired, }; export default product; default props in this other product component, we show how to use defaultprops instead of conditional rendering. import react from 'react'; import proptypes from 'prop-types'; import defaultimage from './assets/default-image.jpeg'; const product = ({ image, name, price }) => { return ( <article classname='product'> {/*use conditional rendering in case the data does not exist */} <img src={image.url} alt={name} /> <h4>{name}</h4> <p>${price}</p> </article> ); }; // define the proptypes for the object product.proptypes = { image: proptypes.object.isrequired, name: proptypes.string.isrequired, price: proptypes.number.isrequired, }; product.defaultprops = { name: 'default name', price: 3.99, image: defaultimage, }; export default product; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/useState.html",
    "title": "useState",
    "body": " index search search back usestate error in the next piece of code we show how, if we change the value of a variable in react, it does not change in our web app because it is not re-rendered: import react from 'react'; const errorexample = () => { let title = 'random title'; const handleclick = () => { title = 'hello people'; console.log(title); }; return ( <react.fragment> <h2>{title}</h2> <button type='button' onclick={handleclick}> change title </button> </react.fragment> ); }; export default errorexample; that is why we will need to use the hook usestate, so we change handle state changes. import react, { usestate } from 'react'; const usestatebasics = () => { const [text, settext] = usestate('random title'); const handleclick = () => { if (text === 'random title') { settext('hello world'); } else { settext('random title'); } }; return ( <react.fragment> <h1>{text}</h1> <button type='button' onclick={handleclick}> change title </button> </react.fragment> ); }; export default usestatebasics; when we invoke usestate we have to pass as an argument the initial value of the state variable. usestate is a function that returns an array: the first element: the state variable the second element: the handler that controls the value of the state value when using usestate with objects, whenever you update one property of the object, you have to pass the object to the handler (with the spread operator), and then override the property you want to update: import react, { usestate } from 'react'; const usestateobject = () => { // object const [person, setperson] = usestate({ name: 'peter', age: 24, message: 'random message', }); const changemessage = () => { // pass the person object with the spread operator // and override the message property setperson({ ...person, message: 'hello world' }); }; return ( <> <h3>{person.name}</h3> <h3>{person.age}</h3> <h4>{person.message}</h4> <button classname='btn' onclick={changemessage}> change message </button> </> ); }; export default usestateobject; asynchronous functions if we want to update a value asynchronally, and fetch the value of the state variable when the change happens, and not when the function is defined, then: import react, { usestate } from 'react'; const usestatecounter = () => { const [value, setvalue] = usestate(0); const reset = () => { setvalue(0); }; const complexincrease = () => { settimeout(() => { // value is the value of the state variable when the timeout is defined // if you call it multiple times consecutively you get the same value, because they all get value = 0 // setvalue(value + 1); // prevstate is the value of the state variable when the timeout finished // if you call it multiple times consecutively you get different values, because value has already been updated // by another settimeout. // if you call it multiple times setvalue((prevstate) => { return prevstate + 1; }); }, 2000); }; return ( <> <section style={{ margin: '4rem 0' }}> <h2>more complex counter</h2> <h1>{value}</h1> <button classname='btn' onclick={complexincrease}> increase later </button> </section> </> ); }; export default usestatecounter; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Dispatch.html",
    "title": "Dispatch",
    "body": " index search search back dispatch in order to dispatch actions in our reducers we do as follows: import react from \"react\"; // get dispatch hook import { usedispatch } from \"react-redux\"; // get actions import { login, logout } from \"../features/user\"; function login() { // initialize dispatch hook const dispatch = usedispatch(); return ( <div> <button onclick={() => { // dispatch login action dispatch(login({ name: \"pedro\", age: 20, email: \"pedro@gmail.com\" })); }} > login </button> <button onclick={() => { // dispatch logout action dispatch(logout()); }} > logout </button> </div> ); } export default login; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/index.html",
    "title": "React",
    "body": " index search search back react basic react npm create-react-app babel file structure start in indexjs jsx rules css in jsx props children in props list of components event basics advanced react properties hooks usestate useeffect conditional rendering controlled inputs useref usereducer prop drilling context api custom hooks proptypes react router performance optimization redux basics index reducers and actions get state dispatch $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/List of components.html",
    "title": "List of Components",
    "body": " index search search back list of components react has one restriction for list of objects, and that is: they have to have a key. so, for example: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' // data to create book object const books = [ { id: '1', title: 'book title', author: 'book author' }, { id: '2', title: 'book title', author: 'book author' }, ] const booklist = books.map((book) => { // de-structure book object return <book key={book.id} {...book} />; }) reactdom.render( <div> booklist </div>, document.getelementbyid('root') ); $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/React Router.html",
    "title": "React Router",
    "body": " index search search back react router in react routing behaves differently than in traditional html applications: it does not fetch the html page from the server, it is done in the client side. there is no re-rendering even though we change the url. here we have an example: import react from 'react'; // react router import { browserrouter as router, route, switch } from 'react-router-dom'; // pages import home from './home'; import about from './about'; import people from './people'; import error from './error'; import person from './person'; // navbar import navbar from './navbar'; const reactroutersetup = () => { return ( <router> <navbar /> <!-- with the switch component only the first one that matches is displayed --> <switch> <!-- match the path exactly, else this will be rendered always along the other components --> <route exact path='/'> <!-- component to display --> <home /> </route> <!-- match the path --> <route path='/about'> <about /> </route> <!-- match the path --> <route path='/people'> <people /> </route> <!-- match the path and pass id as a parameter --> <!-- specify children property because it will be a list of components --> <route path='/person/:id' children={<person />}></route> <!-- match any path (this is only displayed when the other paths do not match if we use the switch component)--> <route path='*'> <error /> </route> </switch> </router> ); }; export default reactroutersetup; links how do we navigate through our application, well by using links. so, for example, in the navbar: import react from 'react'; import { link } from 'react-router-dom'; const navbar = () => { return ( <nav> <ul> <li> <!-- specify the path --> <link to='/'>home</link> </li> <li> <!-- specify the path --> <link to='/about'>about</link> </li> <li> <!-- specify the path --> <link to='/people'>people</link> </li> </ul> </nav> ); }; export default navbar; to pass a parameter to the link we can do the following: import react, { usestate } from 'react'; import { data } from '../../../data'; import { link } from 'react-router-dom'; const people = () => { // list of people const [people, setpeople] = usestate(data); return ( <div> <h1>people page</h1> {people.map((person) => { return ( <div key={person.id} classname='item'> <h4>{person.name}</h4> <!-- specify the path and pass the id of the current person as a parameter --> <link to={`/person/${person.id}`}>learn more</link> </div> ); })} </div> ); }; export default people; now in the person component, we can fetch the parameter: import react, { usestate, useeffect } from 'react'; import { data } from '../../../data'; import { link, useparams } from 'react-router-dom'; const person = () => { // state const [name, setname] = usestate('default name'); // useparams hook to fetch the parameter // the name of the parameter (id), is specified in the \"route\" component // in our case the path to person was: /person/:id const { id } = useparams(); useeffect(() => { const newperson = data.find((person) => person.id === parseint(id)); setname(newperson.name); }, []); return ( <div> <h1>{name}</h1> <!-- go to the previous page of the list of people --> <link to='/people' classname='btn'> back to people </link> </div> ); }; export default person; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/JSX Rules.html",
    "title": "JSX Rules",
    "body": " index search search back jsx rules always return something always return a single element or div, section, article or react.fragment (does not create a div) enclosing the element use camelcase for html attribute use classname instead of class close every element $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/File Structure.html",
    "title": "File Structure",
    "body": " index search search back file structure node_modules: folder that contains all of the dependencies package.json: is the manifest file for the project scripts start: runs the development server build: creates a production version for the project inside a folder called build, where the optimized files resulting of the build are stored. the rest of the files created by create-react-app are mostly useless: app.js app.css app.test.js logo.svg serviceworker.js setuptests.js also all of the contents of index.js can be removed. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/CSS in JSX.html",
    "title": "CSS in JSX",
    "body": " index search search back css in jsx we can define the style inside jsx, for that we use the prop style. the first curly braces takes us back to javascript, and the second are to specify the creation of an object. also note that we do not write font-size but we use the react convention of writing fontsize const author = () => ( <h4 style={{fontsize: '1px'}}> test </h4> ); this level has higher preference (overrides) than the css imported from a css file. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/create-react-app.html",
    "title": "create-react-app",
    "body": " index search search back create-react-app you do not need create-react-app to create a react app, but it makes it way easier: npx create-react-app <app-name> cd <app-name> npm start $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/useRef.html",
    "title": "useRef",
    "body": " index search search back useref useref returns a mutable ref object whose .current property is initialized to the passed argument. some properties: preserves the value of the object does not trigger re-render assigned to dom nodes/elements import react, { useeffect, useref } from 'react'; const userefbasics = () => { // create the container const refcontainer = useref(null); const handlesubmit = (e) => { e.preventdefault(); // print the value inside the input console.log(refcontainer.current.value); }; useeffect(() => { // focus on the input element whenever we render the application refcontainer.current.focus(); }); return ( <> <form classname='form' onsubmit={handlesubmit}> <div> {/*the refcontainer points to the input element*/} <input type='text' ref={refcontainer} /> </div> <button type='submit'>submit</button> </form> </> ); }; export default userefbasics; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Index.html",
    "title": "Index file",
    "body": " index search search back index file in the following piece of code we create our store object, where we are going to save the state of our application. as you may note, in this store there are three slices defined. that is because we differentiate three different states (slices). so our store is defined as: { user: {...} theme: {...} } import { configurestore } from \"@reduxjs/toolkit\"; // different slices import userslice from \"./features/userslice.js\"; import themeslice from \"./features/themeslice.js\"; // create store const store = configurestore({ reducer: { // in each case obtain the reducer user: userslice.reducer, theme: themeslice.reducer, }, }); export default store; now, we have to wrap our application with our store: import react from \"react\"; import reactdom from \"react-dom\"; import app from \"./app\"; // import our store as a provider import { provider } from \"react-redux\"; import store from \"./store\"; reactdom.render( <react.strictmode> <provider store={store}> <app /> </provider> </react.strictmode>, document.getelementbyid(\"root\") ); $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/useEffect.html",
    "title": "useEffect",
    "body": " index search search back useeffect this hook is used for any work needed to be made outside of the component: fetch data, changing the document title, signing up for a subscription, setting up an event listener. runs after every re-render cleanup function second parameter import react, { usestate, useeffect } from 'react'; const useeffectbasics = () => { // callback called whenever the component is rendered useeffect(() => { document.title = `new messages(${value})`; }); return ( <> <h1>{value}</h1> <button classname='btn'}> click me </button> </> ); }; export default useeffectbasics; dependencies the useeffect definition allows you to pass an array of dependencies: if it is specified as []: useeffect will only be triggered in the first render if it is an array of state variables: it will be triggered every time the state variable is updated. import react, { usestate, useeffect } from 'react'; const useeffectbasics = () => { const [value, setvalue] = usestate(0); // only trigger on first render // useeffect(() => { // document.title = `new messages(${value})`; // }, []); // call whenever value is updated useeffect(() => { document.title = `new messages(${value})`; }, [value]); return ( <> <h1>{value}</h1> <button classname='btn'}> click me </button> </> ); }; export default useeffectbasics; clean up function useeffect lets us define a function that is invoked once we exit the function: import react, { usestate, useeffect } from 'react'; const useeffectcleanup = () => { const [size, setsize] = usestate(window.innerwidth); const checksize = () => { setsize(window.innerwidth); }; useeffect(() => { console.log('useeffect'); window.addeventlistener('resize', checksize); // clean up function return () => { console.log('cleanup'); window.removeeventlistener('resize', checksize); }; }, []); console.log('render'); return ( <> <h1>window</h1> <h2>{size} px</h2> </> ); }; export default useeffectcleanup; fetch data up next we will show how to get data using useeffect. note, if we do not specify the restriction of only triggering on the first render: useeffect calls getusers getusers updates the state, and so the component re-renders because there is a re-render, useeffect is called again thus, we end in an infinite loop import react, { usestate, useeffect } from 'react'; const url = 'https://api.github.com/users'; const useeffectfetchdata = () => { const [users, setusers] = usestate([]); const getusers = async () => { const response = await fetch(url); const users = await response.json(); setusers(users); }; useeffect(() => { getusers(); // specify [] so we only run useeffect on the first render. }, []); return ( <> <h3>github users</h3> <ul classname='users'> {users.map((user) => { const { id, login, avatar_url, html_url } = user; return ( <li key={id}> <img src={avatar_url} alt={login} /> <div> <h4>{login}</h4> <a href={html_url}>profile</a> </div> </li> ); })} </ul> </> ); }; export default useeffectfetchdata; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Conditional Rendering.html",
    "title": "Conditional Rendering",
    "body": " index search search back conditional rendering in the following example we show how we can have react display different elements conditionally: import react, { usestate, useeffect } from 'react'; const url = 'https://api.github.com/users/quincylarson'; const multiplereturns = () => { const [isloading, setisloading] = usestate(true); const [iserror, setiserror] = usestate(false); const [user, setuser] = usestate('default user'); // fetch data useeffect(() => { fetch(url) .then((resp) => { if (resp.status >= 200 && resp.status <= 299) { return resp.json(); } else { // update the control state variables setisloading(false); setiserror(true); throw new error(resp.statustext); } }) .then((user) => { const { login } = user; setuser(login); // update the control state variables setisloading(false); }) .catch((error) => console.log(error)); }, []); // different display depending on the state of the get if (isloading) { return ( <div> <h1>loading...</h1> </div> ); } if (iserror) { return ( <div> <h1>error....</h1> </div> ); } return ( <div> <h1>{user}</h1> </div> ); }; export default multiplereturns; short circuit evlauation now, let's see an example of short circuit evaluation in action: import react, { usestate } from 'react'; const shortcircuit = () => { const [text, settext] = usestate(''); const [iserror, setiserror] = usestate(false); // if text is falsy, then return 'hello world' // else return text // const firstvalue = text || 'hello world'; // if text is true, then return 'hello world' // else return text // const secondvalue = text && 'hello world'; return ( <> {/*if text is false, return h1 with 'john doe value'*/} <h1>{text || 'john doe'}</h1> {/*if text is true, return h1 with 'john doe value'*/} {text && <h1>'john doe'</h1>} </> ); }; export default shortcircuit; ternary operators we can also use ternary operators to render conditionally in react. import react, { usestate } from 'react'; const shortcircuit = () => { const [iserror, setiserror] = usestate(false); return ( <> <button classname='btn' onclick={() => setiserror(!iserror)}> toggle error </button> {/*check the value of iserror, if is error is true, return the first value after the ? else return the second value*/} {iserror ? ( <p>there is an error...</p> ) : ( <div> <h2>there is no error</h2> </div> )} </> ); }; export default shortcircuit; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Context API.html",
    "title": "Context API",
    "body": " index search search back context api context api and usecontext allows us to resolve the issue of the prop drilling. the context has two components: the provider: works as a distributer the consumer we use them as follows: import react, { usestate, usecontext } from 'react'; import { data } from '../../../data'; // create context object const personcontext = react.createcontext(); const contextapi = () => { // state saved in the context const [people, setpeople] = usestate(data); // event handler saved in the context const removeperson = (id) => { setpeople((people) => { return people.filter((person) => person.id !== id); }); }; return ( {/*wrap the components in the context provider, so all the nested components have access to the variables defined in the context object*/} <personcontext.provider value={{ removeperson, people }}> <h3>context api / usecontext</h3> <list /> </personcontext.provider> ); }; const list = () => { // obtain data from the context with the usecontext hook const maindata = usecontext(personcontext); return ( <> {maindata.people.map((person) => { return <singleperson key={person.id} {...person} />; })} </> ); }; const singleperson = ({ id, name }) => { // obtain data from the context with the usecontext hook const { removeperson } = usecontext(personcontext); return ( <div classname='item'> <h4>{name}</h4> <button onclick={() => removeperson(id)}>remove</button> </div> ); }; export default contextapi; $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Start in indexjs.html",
    "title": "Start in index.js",
    "body": " index search search back start in index.js keep in mind, index.js is the entry point: first of all refer to file structure, and then basically remove everything from index.js, and replace it for: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; we use the reactdom module to make use of the react dom api, which let's us render components, etc. next we call reactdom.render() to output our html: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; function component() { return ( <h4> hi! </h4> ); } reactdom.render( <component/>, document.getelementbyid(\"root\") ) note the function must start with a capital letter the tag that encloses the component must be closed, so either: <component/> or <component></ component> we use document.getelementbyid(\"root\"), this tells react where to place the component inside the html $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Babel.html",
    "title": "Babel",
    "body": " index search search back babel babel is a javascript compiler that converts es7, es6 to e5 so it can run smoothly in older browsers. this way we can use new features of es7 and es6 while maintaining compatibility. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Props.html",
    "title": "Props",
    "body": " index search search back props in react to define parameters in our components, we do as follows: import react from 'react' // this are the props const book = (props) => { return ( <article classname='book'> <img src={props.img} alt='' /> <h1>{props.title}</h1> <h4>{props.author}</h4> </article> ); }; export default book another way (more readable), is to spread the object: import react from 'react' // this are the props const book = ({ title, author }) => { return ( <article classname='book'> <h1>{title}</h1> <h4>{author}</h4> </article> ); }; export default book now, to pass these props we do: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' reactdom.render( <book title='book title' author='book author'/>, document.getelementbyid('root') ); spread operator let's define an object singlebook that contains all of the book's properties and pass it to the book component: import react from 'react'; import reactdom from 'react-dom'; // css import './index.css'; import book from './book' const singlebook = { title: 'book title', author: 'book author' } reactdom.render( // use the spread operator <book {...singlebook}/>, document.getelementbyid('root') ); $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/React/Properties Hooks.html",
    "title": "Properties of Hooks",
    "body": " index search search back properties of hooks all the hooks have the following properties: they start with the word use the component where they are created must be named in uppercase they cannot be invoked inside a function/component body. you cannot call hooks conditionally $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/MongoDB/commands.html",
    "title": "MongoDB Commands",
    "body": " index search search back mongodb commands to log into mongodb with the created user and database: $ mongo -u <your username> -p <your password> \\\\ --authenticationdatabase <your database name> or $ mongo -u <your username> \\\\ --authenticationdatabase <your database name> to connect to the database use the following uri: mongodb://yourusername:yourpasswordhere@127.0.0.1:27017/your-database-name $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/MongoDB/index.html",
    "title": "MongoDB",
    "body": " index search search back mongodb mongodb commands $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Django/django_notes.html",
    "title": "Django Notes",
    "body": " index search search back django notes in this section we lay out some concepts about the django framework. apps models the models can be thought of as objects, in the sense of oop, that have certain attributes. this objects are then mapped by django to the database of choice. to define new models, or modify existing model (e.g. the user model) you need to modify the models.py file in the root folder of every app that is created. alternatively, you can centralize all of your models on the core app. an example of a simple model is the following tag model: class tag(models.model): \"\"\"tag to be used for a book\"\"\" # define the attributes of the table name = models.charfield(max_length=255) # define the relation between the tag and the user user = models.foreignkey( settings.auth_user_model, on_delete=models.cascade, ) # define the string representation of the tag def __str__(self): return self.name once the model is define, it needs to be registered on the admin.py file: admin.site.register(models.tag) specifically when modifying existing models, you will need to extend the classes defined by django (e.g. abstractbaseuser, useradmin). for example: class user(abstractbaseuser, permissionsmixin): \"\"\"custom user model that suppors using email instead of username\"\"\" email = models.emailfield(max_length=255, unique=true) name = models.charfield(max_length=255) is_active = models.booleanfield(default=true) is_staff = models.booleanfield(default=false) objects = usermanager() which has to be registered as follows: admin.site.register(models.user, useradmin) where useradmin is a class define in the admin.py file, that defines the custom user model: class useradmin(baseuseradmin): ordering = ['id'] list_display = ['email', 'name'] # user edit page fields fieldsets = ( (none, {'fields': ('email', 'password')}), (_('personal info'), {'fields': ('name',)}), ( _('permissions'), {'fields': ('is_active', 'is_staff', 'is_superuser')} ), (_('important dates'), {'fields': ('last_login',)}) ) # user create page fields add_fieldsets = ( (none, { 'classes': ('wide',), 'fields': ('email', 'password', 'password2') }), admin this is the feature that allows you to manage your models, let it be create them, modify them or delete them. the functionality of the admin model is defined within the admin.py file on the root folder of every app that is created. in order to create a superuser execute the following command: $ python manage.py createsuperuser on docker: $ docker-compose run app sh -c \"python manage.py createsuperuser\" then, you will be prompted to enter an email and a password. once you have filled said fields, you can start the server with $ docker-compose up and enter to the admin page located on 127.0.0.1:8000/admin, where you can log in with your credentials. urls django allows us to define relative urls on a very modular way. first off, we have the core file when it comes to url definition: app/app/urls.py. here we may have something like this: from django.contrib import admin from django.urls import path, include urlpatterns = [ path('admin/', admin.site.urls), path('api/user/', include('user.urls')), ] this example shows that the urlpatterns variable is a list that holds all of the urls defined in our project. the modularization comes from the way the urls defined on the user’s app are specified. first we specify the endpoint for these urls (namely api/user/), and then we pull all the relative urls from the user’s app, defined on the file app/user/urls.py. which are then concatenated with api/user/. the urls defined on the user app are as follows: app_name = 'user' urlpatterns = [ path('create/', views.createuserview.as_view(), name='create'), ] this the can be used like this: # create user api endpoint dinamically create_user_url = reverse('user:create') serializers this files are defined to specify how to serialize (map to the database) the json objects received, in our case, from http requests. for that we create, for each model, a class that extends serializers.modelserializer. in this class we define an inner class called meta that tells the framework which fields does the object have and so allows the mapping to take place. you can also add extra arguments to this inner class, for example to restrict or exercise a stronger control on the fields. next on, we have a simple example of our user model serializer: from django.contrib.auth import get_user_model from rest_framework import serializers class userserializer(serializers.modelserializer): \"\"\"serializer for the users object\"\"\" class meta: \"\"\"info about how to serialize the user model\"\"\" model = get_user_model() fields = ('email', 'password', 'name') # extra requirements for the user model extra_kwargs = {'password': {'write_only': true, 'min_length': 5}} def create(self, validated_data): \"\"\"create a new user with encrypted password and return it\"\"\" # validation_data: json data passed in the http post return get_user_model().objects.create_user(**validated_data) we can also serialize an object that is not related to a model per se, for example: class authtokenserializer(serializers.serializer): \"\"\"serializer for the user authentication object\"\"\" email = serializers.charfield() password = serializers.charfield( style={'input_type': 'password'}, trim_whitespace=false ) views this is, on simple terms, a python function that takes a web request and returns a web response. in our case, we will mostly use views for our api, so we use pre-make view that allows us to easily make an api that creates, updates, etc an object on the database using the serializer that we specify, for example, the api for creating a user is as follows: class createuserview(generics.createapiview): \"\"\"create a new user in the system\"\"\" serializer_class = userserializer in case of wanting to update an object we extend generics.retrieveupdateapiview instead of generics.createapiview. because this view is private, we need to indicate an authentication mechanism and the level of permissions the user has, in our case the authentication is made via token and the permissions are that the user needs to be logged in. class manageuserview(generics.retrieveupdateapiview): \"\"\"manage the authenticated user\"\"\" serializer_class = userserializer # authentication mechanism by which the authentication happens authentication_classes = (authentication.tokenauthentication,) permission_classes = (permissions.isauthenticated,) def get_object(self): \"\"\"retrieve and return authentication user\"\"\" return self.request.user actions start the server observe that this is executed on the docker-compose configuration file $ python manage.py runserver 0.0.0.0:8000 sync django settings (app/app/settings.py) $ python manage.py migrate on docker: $ docker-compose run app sh -c \"python manage.py migrate\" sync changes made on models $ docker-compose run app sh -c \"python manage.py migrate\" on docker: $ docker-compose run app sh -c \"python manage.py makemigrations\" you can also specify the name off the app that contains the model $ python manage.py makemigrations app_name $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Django/index.html",
    "title": "Django",
    "body": " index search search back django django notes $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/relationships.html",
    "title": "Relationships",
    "body": " index search search back relationships one to many we are now going to illustrate the situation where an animal belongs to only one category whilst a category contains several animals: type animal { id: id! category: category! name: string! parameter: string! } type category { id: id! name: string! animals: [animal!]! parameter: string! } where we have stored in our database the id of the category as a foreign key of the animal entity. in order to query for animals from a category we create a new resolver: const resolvers = { query: { animals: () => animals, animal: (parent, args, ctx) => { let animal = animals.find((animal) => { return animal.paramenter === args.id }) return animal } } category: { animals: (parent, args, ctx) => { return animals.filter((animal) >= { return animal.category == parent.id }) } } } so if we query for: { category(parameter: \"mammal\"){ category animals { name } } } we get all the names of the animals that are mammals. the parent object symbolizes the object resulting from category(parameter: \"mammal\"), this object will be a category object and will have an id, that we will use in our resolver to filter the animals. observe that the animals have a attribute called category, which is *not* the same as the type definition we have made for our animal object, this attribute is defined on the database. note that we have created a category resolver that acts as the query resolver but for queries within the category object. we, now, do the same for the animals, meaning we want to get the category object that we specified in the animal object, for that we create a new resolver: animal: { category: (parent, args, ctx) => { return categories.find((category) => { return category.id === parent.category }) } } so what we do is go through all of the categories until we find the one that has the same id. { animal(parameter: \"cat\"){ name category { name } } and with this query we retrieve the name and the category name of a cat. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/updating.html",
    "title": "Mutations",
    "body": " index search search back mutations in order to update, delete or add new data using graphql we use mutations. typedef we create the type definition for the mutation object (which is reserved in graphql to modify/add data, much like the query object). in it, we define all the modifying functions we want, along with the data that must be provided to execute the modification, and also the type of object that is returned. type mutation { addanimal( name: string! description: [string!]! parameter: string! category: string! ): animal removeanimal(id: id!): boolean! } with this we have defined the addanimal method, which creates and animal by specifying the name, description, url parameter and the category. this function will return an animal object. we have also defined the removeanimal method, that only takes an id as a parameter and returns a boolean. resolvers we now define the logic behind both of these methods, so we create a mutation.js file as follows: const { v4 } = require(\"uuid\") const mutation = { addanimal: (parent, { name, description, parameter, category }, { animals }) => { let newanimal = { id: v4(), name, description, parameter, category, } // only because this is an object: here we would create in the database animals.push(newanimal) return newanimal }, removeanimal: (parent, { id }, { animals }) => { // here we would delete in the database let index = animals.findindex((animal) => { return animal.id === id }); animals.splice(index, 1); return true } } module.exports = mutation note that we de-structure the parameters from the args object for readability sake. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/file_structure.html",
    "title": "File Structure",
    "body": " index search search back file structure what is best practice is to separate the type definitions and the resolvers: typedefs: stored in schema.js for example. resolvers: stored in a folder called resolvers, and then for each resolver we create a file, for example for the query resolver: const category = { animals: (parent, args, { animals }) => { return animals.filter(animal => { return animal.category === parent.id }) } } module.exports = category then we create an index.js inside the resolvers folder where we can import and export all of our resolvers together: const query = require('./query') const category = require('./category') const animal = require('./animal') module.exports = { query, category, animal } and we put everything together in our index.js inside the root folder: const { apolloserver } = require('apollo-server'); const { maincards, animals, categories } = require('./db') const typedefs = require('./schema') const { query, category, animal } = require('./resolvers/index') const server = new apolloserver({ typedefs, resolvers: { query, animal, category }, context: { maincards, animals, categories } }); // the `listen` method launches a web server. server.listen().then(({ url }) => { console.log(`🚀 server ready at ${url}`); }); we now use the context object in order to make our \"database\" available to all of the resolvers through ctx. (note that we de-structure the object to the get animal object). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/terminology.html",
    "title": "Terminology",
    "body": " index search search back terminology schema it defines the data associated with an entity: type person { id: id! name: string! email: string! age: int! phone: string gender: boolean! } that is to say, it defines the type definitions of the data that conforms a given entity. resolver the data that we get back is dependent on the resolvers. they are functions that return data that follow a certain schema, it does not need to follow the schema, but then when querying it, it may throw and error. people(parent, args, ctx, info){ return[ { id: \"1\", name: \"laith\", email: \"email@email.com\", age: 23, phone: \"623198135\", gender: true } ] } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/fetch.html",
    "title": "Fetch Data",
    "body": " index search search back fetch data you can start requesting data with usequery. usequery is a react hook that shares graphql data with your ui. so for example to fetch the image and the title of the cards: import { usequery, gql } from '@apollo/client' const fetch_data = gql` { maincards { image title } } ` function mainhero(){ const maindata = usequery(fetch_data) return(<div></div>) } where maincards is one of our resolvers, and we specify that we want to select the image and the title. so now, we can de-structure the different attributes offered by the apolloprovider, namely loading, error and data. and therefore control the flow of our application by using them. import { usequery, gql } from '@apollo/client' const fetch_data = gql` { maincards { image title } } ` function mainhero(){ const { loading, error, data } = usequery(fetch_data) return(<div></div>) } variables in order to make a query by passing parameters we do: const animal_query = gql` query($slug: string!){ animal(slug: $slug){ title image stock description price } } ` where $string is the variable we want to pass in, and we specify its type and the fact that it is required with string!. now to make the query we do: function animalpage() { const { slug } = useparameters() const { loading, data, error } = usequery( variables: { slug: 'cat' } ) } with variables we pass in all of the parameters needed in the query. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/index.html",
    "title": "GraphQL",
    "body": " index search search back graphql backend intro terminoligy graphql server queries, typedefs and resolvers relationships file structure mutations frontend graphql client fetch data mutations $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/intro.html",
    "title": "Intro",
    "body": " index search search back intro graphql is a query language used to communicate to our api and query for data. difference with apis whenever we use rest apis and we hit specific endpoints, more often than not, we are going to retrieve some data that we have no use for. this is what is called overfetching. for example when you access https://my-rest-api/animals you get an object with a list of animal objects, and you may not need all of the information of every animal. graphql solves this problem by: only having one endpoint. from this endpoint we use the graph query language to select whatever data that we want. for example, to retrieve the same information stated above: query{ animals{ title ratings img price } } which gets only the specified attributes for each animal. graphql also solves underfetching, which is the situation where you cannot get enough data with a call to only one endpoint, forcing you to call a second endpoint. for example, if you want information about the animals and the categories you have to access https://my-rest-api/animals, and https://my-rest-api/categories, however with graphql: query{ animals{ title ratings img price } categories{ id title img } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/mutations_client.html",
    "title": "Mutations",
    "body": " index search search back mutations in order to execute a mutation from the client side we create a mutation request: const add_animal_mutation = gql` mutation( $name: string!, $description: [string!] $parameter: string!, $category: string! ) { addanimal( name: $name, description: $description, parameter: $parameter, category: $category ) } ` and now we use the usemutation hook to obtain the function that will be called in order to update our animal: import { usemutation, gql } from '@apollo/client' function animal(){ const [addanimal] = usemutation(add_animal_mutation) return( <div> <button onclick={() => addanimal({ variables: { name: 'cat', description: ['this is a description'], parameter: 'cat', category: 'mammal' } } )}/> </div> ) } with this we get the function addanimal with the usemutation hook and we use it in our button, so when it is clicked we add a cat to our animal collection. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/client.html",
    "title": "Client",
    "body": " index search search back client as well as with the server there are several clients for graphql within different languages and frameworks, visit the official page to check them out. we are going to use apollo client so for that we need to install apollo and graphql on the client side of our application: $ npm install @apollo/client graphql in our case we are going to connect our client to react (reference). so, first we import the necessary modules. import react from 'react'; import { render } from 'react-dom'; import { apolloclient, inmemorycache, apolloprovider, usequery, gql } from \"@apollo/client\"; const client = new apolloclient({ uri: 'http://localhost:4000', cache: new inmemorycache() }); function app() { return ( <div> <h2>my first apollo app 🚀</h2> </div> ); } render( <apolloprovider client={client}> <app /> </apolloprovider>, document.getelementbyid('root'), ); we tell apollo that our graphql server is listening for request on our localhost on the port 4000. where apollo allows us to cache our queries, with the inmemorycache module. that way we do not need to make the same request twice, because the data is cached in memory. and then, we wrap our app with the apolloprovider, so all of our components have access to our client. note that we pass our client as a prop. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/server.html",
    "title": "GraphQL Server",
    "body": " index search search back graphql server graphql supports several languages, and has several servers that do mainly the same. consult the official page for the one that suits your needs. we are going to use apollo-server to demonstrate how to use graphql in a node.js application: so, first, we install the apollo-server along with graphql dependency with npm: $ npm install apollo-server graphql now we use graphql to define our type definitions: const { apolloserver, gql } = require('apollo-server'); const typedefs = gql` type book { title: string author: string } type query { books: [book] } and we also create our resolvers: const resolvers = { query: { books: () => books, } } where books is an already defined array of books. finally we create the actual server: const server = new apolloserver({typedefs, resolvers}); server.listen().then(({ url }) => { console.log(`🚀 server ready at ${url}`); }); $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/GraphQL/qtr.html",
    "title": "Queries TypeDefs and Resolvers",
    "body": " index search search back queries typedefs and resolvers typedefs: define how the data should look. resolvers: resolve what the actual data is going to be. here we could introduce some logic, like calling the database or applying validation. query: defines how we can query our data type query{ books: [book] } so the book resolver would return an array of books. data specification arrays: to define an array on typedefs or queries you use []. type book { author: [string] } non nullable field: to specify that an attribute cannot be null you use !. type book { author: string! author: [string]! // the array must not be null author: [string!]! // the elements of the array and the array must not be null } queries parameters: on the query object you add an argument between brackets (the ! specifies the argument must be provided). type animal { id: id! name: string! description: [string!]! } type query { animals: [animal!]! animal(id: string!): animal } on the resolver we use the arg parameter to retrieve the parameter passed: const resolvers = { query: { animals: () => animals, animal: (parent, args, ctx) => { let animal = animals.find((animal) => { retunr animal.id === args.id }) return animal } } } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/engine.html",
    "title": "Docker Engine",
    "body": " index search search back docker engine when you install docker on your linux system you are installing: docker daemon: this is the background process that manages docker objects (i.e. images, containers, volumes and networks). rest api: it is the interface programs can access to provide instructions to the daemon. docker cli: command line interface to manage our docker objects. this uses the rest api to interact with the docker daemon. note that the docker cli can be run from a remote machine, that is to say the rest api and the docker daemon are running on a different machine. so, in order to interact with the api we use the -h flag, indicating the ip where the api and the daemon reside with the 2375 port. $ docker -h=10.123.2.1:2375 <docker-command> $ docker -h=10.123.2.1:2375 run nginx containerization as we have seen all of our containers run on top of the same operative system, so it is a given that the processes will be handled by the same kernel. this means that the processes of our containers are run along with the rest of processes in the host machine, in other words the pids of all the processes must be different. what docker does to isolate these processes is the container is using namespaces and maps the process id to another process id within the container, and that is visible only on this container. cgroups because all docker containers share the hosts resources it could be possible that a container takes up all of the machine's resources. so, to restrict the amount of resources used by a container docker uses cgroups. you can specify the amount of cpu or ram that the container is allowed to have: $ docker run --cpus=.5 ubuntu $ docker run --memory=100m ubuntu $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/environmentvar.html",
    "title": "Environment Variables",
    "body": " index search search back environment variables in order to pass an environment variables to our container we run: $ docker run -e env_var=value <image_name> this way we set up and environment variable within the container. if you inspect a running container, you will be able to see the environment variables defined, inside the \"env\" object: $ docker inspect <image_name> { . . \"config\": { \"hostname\": \"51049352a8ee\", \"domainname\": \"\", \"user\": \"\", \"attachstdin\": false, \"attachstdout\": false, \"attachstderr\": false, \"exposedports\": { \"3456/tcp\": {}, \"80/tcp\": {} }, \"tty\": false, \"openstdin\": false, \"stdinonce\": false, \"env\": [ \"path=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"nginx_version=1.19.10\", \"njs_version=0.5.3\", \"pkg_release=1\" ], \"cmd\": [ \"nginx\", \"-g\", \"daemon off;\" ] . . . } $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/registry.html",
    "title": "Docker Registry",
    "body": " index search search back docker registry public registry in the following example you are pulling the nginx image, which in reality is stored as nginx/nginx where the first nginx corresponds to the user name, and the second to the image name. image: nginx this is a public image so it is stored in a public registry, usually in docker.io which is the default registry. so a more verbose configuration file would be: image: docker.io/nginx/nginx private registry when you have applications that should no be made available to the public private registries are used. to pull or use an image from a private registry: register into the private registry: $ docker login private-registry.io run the image indicating the registry: $ docker run private-registry.io/apps/internal-app deploy private registry a private registry is in itself a docker image, so first you have to have your registry image running: $ docker run -p 5000:5000 --name registry registry:2 so now you have your registry running on port 5000. the next step is to assign a tag to your image as follows: $ docker image tag my-image localhost:5000/my-image where my-image is the name of the image and localhost:5000/my-image is the tag assigned. finally you push your image to your registry $ docker push localhost:5000/my-image now you can pull your image: $ docker pull localhost:5000/my-image $ docker pull 192.168.56.100:5000/my-image $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/kubernetes.html",
    "title": "Kubernetes",
    "body": " index search search back kubernetes some of its functionalities are: running several instances of a service. scaling up or down the number of instances. rolling updates. rolling back from updates. supports many different network and storage renderers. provides autoscaling. helps you test new features of your application by only upgrading a percentage of the instances, which allows for doing ab testing. architecture a kubernetes cluster consists of several nodes, the worker nodes are where containers will be launched, so even if one node fails the application is still available. kubernetes clusters are managed by the master, which is a node that watches over worker nodes and is responsible of the orchestration of containers in the worker nodes. components when you install kubernetes in your system you are actually installing: api server: acts as the front-end for kubernetes, so all of the programs talk to this server to interact with the kubernetes server. etcd: it is the distributed reliable key value store to store all data to manage the cluster. scheduler: responsible for distributing work. controller: responsible for noticing/responding to nodes/containers going down. container runtime: underline software used to run containers (e.g. docker). kubelet: is the agent that runs in each node in the cluster, and is responsible of making sure the containers are running on the nodes as expected. one of the command line utilities used by kubernetes is kubectl, that is the kubernetes cli and is used to deploy and manage applications on a kubernetes cluster. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/compose_networks.html",
    "title": "Docker Compose Networks",
    "body": " index search search back docker compose networks let's start with an example application, which is made up of five services: voting-app: a front-end application for the user to vote. redis: and in-memory database to store the vote. worker: application in the back-end that processes the vote and stores it in the database. db: database in which the vote is stored. result-app: front-end application that shows the voting results. in this architecture we have two networks: front-end: voting-app and result-app back-end: all the services. therefore it is desirable to define two networks in our docker-compose and attach the networks to the services: version: 2 services: redis: image: redis networks: - back-end db: image: postgres networks: - front-end vote: image: voting-app networks: - front-end - back-end result: image: result networks: - front-end - back-end worker: image: worker networks: - front-end - back-end networks: - front-end: - back-end: as you can see we define two networks: front-end and back-end (note that we have omitted the configuration of the networks) and then for each service we specify the network to which the service has access (also, observe that the configuration of the services has been trimmed down for readability purposes). $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/swarm.html",
    "title": "Docker Swarm",
    "body": " index search search back docker swarm you can combine multiple docker machines together into a single cluster and docker swarm will take care of managing your services. you need to have different hosts with docker installed on them. you must designate one to be the manager, so the rest are the workers. run the docker swarm init command on the manager and that will initialize the manager. on the workers run docker swarm join <token> where <token> is specified in the output of docker swarm init. now you can deploy your services in your cluster, and will be run on the nodes (i.e. workers). docker service docker services are one or more instances of application or services that run along the nodes in the swarm cluster. $ docker services create --replicas=3 <image-name> this creates three instances of my image and runs them in the nodes of the cluster. this command must be run on the manager node, not on the worker nodes. it is similar to the docker run command in terms of the options to pass (networks, ports, interactive mode, etc.) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/storage.html",
    "title": "Storage",
    "body": " index search search back storage the data pertaining docker is stored within the folder /var/lib/docker/. this includes containers, images, volumes created, etc. layers because of docker's layered architecture when creating very similar images that share a lot of instructions, it uses the cached layers and is, therefore more efficient by not building each image from scratch. for example, when you update your application's source code, only the instructions after the copy instruction, this one included, from your dockerfile is run. image and container layers the layers created from each instruction on the dockerfile constitute the image layers and are all read-only files. when you run your image a new layer is created, denoted by container layer which is a writable file which is a writable file. however, when the container is destroyed, this layer is removed. this is the reason why we use volumes for permanent storage. this is needed because all the containers use the same image, so the changes made in the image by the different containers should not affect the image. copy-on-write also, the changes made on files stored in the image are not made on the original file. the file is copied to the container layer and the changes are made onto this copy. volumes as we have said, we need volumes to store permanent data. so, first we create the volume: $ docker volume <volume_name> which is stored in /var/lib/docker/volumes volume mounting once we have created the volume, we specify that we want to mount this volume within our container: $ docker run -v <volume_name>:/var/lib/mysql mysql if you run this same command, without creating the volume first, docker will automatically create the volume for you. bind mounting if you want to mount another directory that is not inside /var/lib/docker/volumes, then you have to specify the whole directory's (may be an absolute or relative path). $ docker run -v /data/mysql:/var/lib/mysql mysql mount this is the new way to mount: $ docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql which is preferred as it is more verbose. storage administration the responsible for all of these operations that happen under the hood are the storage drivers, which are chosen depending on the hosts' os: aufs zfs btrfs device mapper overlay overlay2 $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/cmd_vs_entrypoint.html",
    "title": "CMD vs ENTRYPOINT",
    "body": " index search search back cmd vs entrypoint cmd a command allows us to append to the command executed when the container start of the base image. for example, ubuntu's cmd is bash, so if we append sleep 5 our container will sleep for 5 seconds when started and then exit. from ubuntu cmd sleep 5 the command can also be specified as cmd [\"sleep\", \"5\"]. entrypoint this other instruction also adds to the base image starting command, but this lets us add arguments from the command line, for example, if we define the following dockerfile: from ubuntu entrypoint [\"sleep\"] we build the image $ docker build dockerfile -t ubuntu-sleeper and then we running with 10 as and argument: $ docker run ubuntu-sleeper 10 our container will sleep for 10 seconds and then exit. to define a default value for sleep, when no argument is passed from the command line, we use both entrypoint and cmd from ubuntu entrypoint [\"sleep\"] cmd [\"5\"] to override the entrypoint command specified in the dockerfile we use the flag --entrypoint: $ docker run --entrypoint sleep2.0 ubuntu-sleeper 10 difference when using cmd when running: $ docker run ubuntu-sleeper sleep 10 the argument sleep 10 replaces entirely the starting command. however with entrypoint if we run: $ docker run ubuntu-sleeper 10 the argument 10 is passed and appended to the entrypoint command. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/windows_mac.html",
    "title": "Docker on Windows and Mac",
    "body": " index search search back docker on windows and mac on these systems we have two options: docker toolbox (usually for older pc's): installs docker along with virtualbox to create a linux system on which docker is run. hypervirtualization: installs docker and uses hyper-v (comes with windows server or professional edition) or hyperkit on mac. this allows docker to create a linux machine under the hood and run docker in it. windows containers the options just discussed will only work for linux applications and containers. in 2016 microsoft announced support for windows containers, there are two types: windows server container: the containers share the kernel, as regular linux containers do. hyper-v isolation: each container is run within a highly optimized virtual machine, so complete kernel isolation between the containers and the underline host is guaranteed. base images where in linux we had the debian, ubuntu or alpine base images in windows we have two options: windows server core nano server: this is a headless deploy of the windows server, that is, the lightweight option. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/images_commands.html",
    "title": "Image",
    "body": " index search search back image list lists downloaded images: $ docker images or alternatively: $ docker image ls remove remove an image $ docker rmi nginx you must stop and remove all the containers that are instances of the image before removing said image. download to only download an image and not also run a container: $ docker pull nginx create your own image first create a dockerfile specifying all of the steps required to set up your application: from ubuntu run apt-get update run apt-get install python run pip install flask run pip install flask-mysql copy . /opt/source-code entrypoint flask_app=/opt/source-code/app.py flask run then build your image, to store locally: $ docker build dockerfile -t mycustomapp here we specify our dockerfile as input for building the image and the tag of the image with the flag -t. to make it available on the dockerhub: $ docker push mycustomapp dockerfile this is configuration file that follows a certain syntax and tells docker how to build the image. the syntax is the following: instruction argument in the previous example we have: from: defines the base image, which can be an os or another image (every image have to be based off another image). run: run a particular command on the base image. copy: copies files from the host system onto the docker image. entrypoint: specifies the command that will be run when the container is started. layered architecture docker follows a layered architecture so each instruction represents a different layer, which contains only the changes from the layer before, and may serve as a snapshot from which to start the build from a particular layer. also, docker caches the layers, so if there is an error, the build would start from the last layer that did not produce a failure. also, if you were to add additional steps, docker would not start the build from scratch. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/index.html",
    "title": "index",
    "body": " index search search back docker intro set up container image environment variables cmd vs entrypoint networking storage docker compose docker compose networks docker registry docker engine docker on windows and mac container orchestration docker swarm kubernetes $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/nerworking.html",
    "title": "Networking",
    "body": " index search search back networking when you install docker it creates three networks automatically: bridge: default network the container get attached to. $ docker run ubuntu none: $ docker run ubuntu --network=none host $ docker run ubuntu --network=host bridge this is a private internal network created by docker on the host. all containers can access each other using their internal ip (usually subnets of 172.17.0.3). to access from outside you have to map a port of the container to a port of the host. host another way to configure the network is to associate the container to the host's network, removing all kind of network isolation between the docker host and the docker container. this way when you run a server on port 5000 it would automatically accessible from the host on the port 5000 without needing to map it to a host's port. this prevents you from using the same ports for different applications. none the containers are not attached to any network and are, therefore, isolated from any other containers so they do not have any access to the external network or other containers. user defined networks because with the default internal network, the containers can access each other, it is sometimes desirable to create new internal networks: $ docker network create --drive bridge --subnet 172.18.0.0/16 <network_name> to list the created networks: $ docker network ls inspect network in order to see the network configuration use inspect and head to the networks section: $ docker inspect ( container_name | container_id ) . . . \"macaddress\": \"aa:bb:cc:dd:ee:ff\", \"networks\": { \"bridge\": { \"ipamconfig\": null, \"links\": null, \"aliases\": null, \"networkid\": \"24af0d...\", \"endpointid\": \"3449a29...\", \"gateway\": \"172.17.0.1\", \"ipaddress\": \"172.17.0.3\", \"ipprefixlen\": 16, \"ipv6gateway\": \"\", \"globalipv6address\": \"\", \"globalipv6prefixlen\": 0, \"macaddress\": \"02:42:ac:11:00:03\", \"driveropts\": null } } . . . embedded dns when containers in the same subnet may want to access each other, for that you could hard code the internal ip assigned to the containers. however this is not advisable, as this ip may change when the container is started in another occasion in the future. because of that all containers in a docker host can resolve each other using their names. this is possible has a built-in dns server for this purpose that runs at 172.0.0.11. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/compose.html",
    "title": "Docker Compose",
    "body": " index search search back docker compose it is used to set up a complex application running multiple services. docker commands map to docker compose as follows: to start the application we run: $ docker-compose up build if we would like to tell docker compose to build a docker build instead of pulling an image we use the build keyword inside a service instead of the image keyword. and we specify the location of the directory which contains the application code and a dockerfile. vote: build: ./vote ports: - 5000:80 links: - redis versions different docker compose versions have different formats and functionality. version 2 from version 2 on, you must specify the docker compose version by adding to the top of the file: version: 2 also, all of the different containers should be listed under a sevices section. and now, links are no longer needed as docker creates a virtual network and attaches all of the services to this network with the name of the service. finally, a depends_on keyword is introduced to force a order of startup. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/intro.html",
    "title": "Intro",
    "body": " index search search back intro what are containers they are completely isolated environments, they have their own processes, network interfaces, etc. however they share the same os kernel. docker uses lxc containers, which are very low lever, so docker provides a high level tool that allows us to manage our containers easily. sharing the kernel as we have said, docker uses the system's kernel, so it is capable of running any distributions whose underlying kernel is linux (e.g. docker running on ubuntu can run a container based on debian, fedora, etc.) containers vs virtual machines containers: application 1 application 2 libs/dependencies 1 libs/dependencies 2 container 1 container 2 docker   os   hardware   virtual machines: application 1 application 2 libs/dependencies 1 libs/dependencies 2 os 1 os 2 virtual machine 1 virtual machine 2 hypervisor   hardware   the main differences are the use of hypervisors in virtual machines and how on these, each instance has its own os. which results in needing more hardware resources. also virtual machines have total isolation, as they use their own os, which does not happen with containers, because these do share the same kernel. however the key is combining both technologies, so each virtual machine runs several applications hosted in different containers. container vs image an image can be thought as a package or a template that is used to create one or more containers. that is to say, containers are running instances of images that are isolated and have their own environment. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/setup.html",
    "title": "Set Up",
    "body": " index search search back set up in this first step, we will present how to install the necessary tools to use docker and docker compose in arch linux. install docker in the current section we will lay out the steps to carry out in order to get docker up and running on an arch linux machine. docker engine before installing anything we will update the system as follows $ sudo pacman -syu when it is done updating we will proceed rebooting the system, and then we enable the loop module: $ sudo tee /etc/modules-load.d/loop.conf <<< \"loop\" $ sudo modprobe loop install using static binaries for reference go to the official documentation on docker's website. firstly we will download the static binary archive on https://download.docker.com/linux/static/stable/. once the file is downloaded extract it executing the following command, and substituting our docker-20.10.8 for your package's version. $ tar xzvf docker-20.10.8.tgz copy the binaries to your executable path (/usr/bin or /bin). this is optional. $ sudo cp docker/* /usr/bin/ start docker's daemon: $ sudo dockerd finally run to check that the installation was correct (it will download an example image that outputs a message informing the user that the installation was successful, among other things). $ sudo docker run hello-world official repo this other approach will allows to have a docker service so we do not have to always run sudo dockerd & to start docker's daemon. we install docker using pacman: $ sudo pacman -s docker afterwards, we enable the docker service executing: $ sudo systemctl start docker.service $ sudo systemctl enable docker.service finally run to check that the installation was correct (it will download an example image that outputs a message informing the user that the installation was successful, among other things). $ sudo docker run hello-world configure docker running as normal user in order to use docker as a normal user we need to add said user to the docker group. add the docker group $ sudo groupadd docker add your user to the docker group $ sudo usermod -ag docker $user log out, log in and verify that it runs properly $ docker run hello-world install docker compose download the current stable release of docker compose. mind you, this command downloads the 1.29.2 version, check the official page for new releases. $ sudo curl -l \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose make the binary executable $ sudo chmod +x /usr/local/bin/docker-compose test the installation $ docker-compose --version docker-compose version 1.29.2, build 5becea4c $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/orchestration.html",
    "title": "Container Orchestration",
    "body": " index search search back container orchestration when in production, it is often needed that several instances of containers are run (because of a heavy load on the application for example). so in these cases you need to monitor the instances as well as the host itself in case any of them crash. for that we use container orchestration that offers a set of tools and scripts that allow us to manage the hosts and containers. the typical approach is to create several instances of containers in different hosts, so if one fails the application can still offer the service. for example: $ docker service create --replicas=100 nodejs some solutions let us automatically scale the number of containers depending on the demand. others can help in automatically adding new hosts to support the user load. they also provide complex networking between the containers as well as load balancing user requests across different hosts or sharing storage between the hosts, configuration management or security. there are several solutions: docker swarm from docker kubernetes from google mesos from apache. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/WebDev/Docker/containers.html",
    "title": "Container",
    "body": " index search search back container run basics run a container from an image, the attached way, (i.e. it is not run on the background). $ docker run nginx if the image is not present on the host it will be downloaded from docker hub. when it is downloaded it runs and exits right away, because there is not application running in the container. to run the container in the detach mode, so it run on the background: $ docker run -d nginx to bring the container to the foreground: $ docker attach ( container_id | container_name ) run a container with a specific tag: $ docker run redis:4.0 this way we run the redis image where redis's version is 4.0. run a container listening to the standard input (because by default docker does not listen for input): $ docker run -i <image_name> this way we are running our container in interactive mode. in order to attach a terminal: $ docker run -it <image_name> port mapping each container is assigned a port (e.g. 5000) and an internal ip by default (e.g. 127.17.0.2) but this ip is only accessible from the host. so to access it from outside, we would use our host's ip (e.g. 192.168.1.5), however we still need to map our container's port to a free port in our host. so to map, for example, the port 5000 of our docker container to the port 80 of our host: $ docker run -p 80:5000 <image_name> and now, we can access the service running in our docker container by heading to 192.168.1.5:80. this way all traffic in this specific url will be routed to the port 5000 in our docker container. volume mapping our container has its own file system, so the changes made to data stored in it are only made in the container. if you want certain data to persist (because when removing the docker container the files stored within are also removed) you use the flag -v to map a certain file/folder in the container to a certain file/folder in our host: $ docker run -v /opt/datadir:/var/lib/mysql mysql in this particular example we store the data we saved in our mysql database in a directory in our container (/var/lib/mysql), and we map this directory to a directory in our host (/opt/datadir) this way docker mounts implicitly the folder in our host to the folder in the container. linking if we have a web application that connects to a redis instance, we need to tell the web app's container which redis instance to wait for (because there may be multiple). so, first we start the redis container: $ docker run -d --name=redis redis and now we start our web app's container and we link it with the redis container: $ docker run -d --name=vote -p 5000:80 --link redis:redis voting-app the redis before the colon is the name of our redis container, and the redis after the name is the name used in the web app container. this option is soon to be deprecated because new concepts are technologies are being introduced. information of a container in order to get more detailed information about a certain container: $ docker inspect ( container_name | container_id ) logs to see the logs of a container (usually printed to the stdout): $ docker log ( container_name | container_id ) list lists all running containers and some information about it. $ docker ps to see all containers, even if they are not currently running: $ docker ps -a stop stop running a container who matches the id or the name provided: docker stop ( container_id | container_name ) remove removes a container permanently docker rm ( container_id | container_name ) execute commands to execute a command after creating a new container: $ docker run ubuntu sleep 5 this commands starts the container and run the command sleep 5 and then exits. to execute a command in a currently running container: $ docker exec ( container_id | container_name ) cat /etc/hosts $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/06_05.html",
    "title": "Determinants and Cramer's Rule",
    "body": " index search search back determinants and cramer's rule determinants of \\(2 \\times 2\\) matrices the determinant of a \\(2 \\times 2\\) matrix \\(a\\) is a real number defined as \\(det(a) = a_{11} a_{22}− a_{21} a_{12}\\). determinants of \\(3 \\times 3\\) matrices the determinant of a \\(3 \\times 3\\) matrix \\(a\\) is a real number defined as \\(det(a) = (a_{11}a_{22}a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21}a_{32}) − (a_{31}a_{22}a_{13} + a_{32} a_{23} a_{11} + a_{33} a_{21}a_{12})\\). cofactor let \\(m_{ij}\\) be the minor for element \\(a_{ij}\\) in an \\(n \\times n\\) matrix. the cofactor of \\(a_{ij}\\), written \\(a_{ij}\\), is defined as follows. \\begin{align} a_{ij} = (-1)^{i + j} \\cdot m_{ij} \\end{align} cramer's rule for \\(2 \\times 2\\) systems the solution of the system: \\begin{align} a_1x + b_1y = c_1 \\end{align} \\begin{align} a_2x + b_2y = c_2 \\end{align} is given by: \\begin{align} x = \\frac{d_x}{d}, y = \\frac{d_y}{d} \\end{align} where: \\begin{align} d_x = det( \\begin{bmatrix} c_1 & b_1 \\\\ c_2 & b_2 \\\\ \\end{bmatrix}) \\end{align} \\begin{align} d_y = det( \\begin{bmatrix} a_1 & c_1 \\\\ a_2 & c_2 \\\\ \\end{bmatrix}) \\end{align} \\begin{align} d = det( \\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\\\ \\end{bmatrix}) \\neq 0 \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/06_02.html",
    "title": "Solutions of Linear Systems in Three Variables",
    "body": " index search search back solutions of linear systems in three variables we can extend the ideas of systems of equations in two variables to linear equations of the form: \\begin{align} ax + by + cz = d \\end{align} considering the possible intersections of the planes representing three equations in three unknowns shows that the solution set of such a system may be either a single ordered triple \\((x, y, z)\\), an infinite set of ordered triples (dependent equations), or the empty set (an inconsistent system). the following steps can be used to solve a linear system with three variables. eliminate a variable from any two of the equations. eliminate the same variable from a different pair of equations. eliminate a second variable using the resulting two equations in two variables to get an equation with just one variable. find the values of the remaining variables by substitution. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/06_03.html",
    "title": "Solution of Linear Systems by Row Transformations",
    "body": " index search search back solution of linear systems by row transformations solving linear systems of equations can be streamlined by using matrices. consider a system of three equations and three unknowns. \\begin{align} a_1 x + b_1y + c_1z = d_1 \\end{align} \\begin{align} a_2 x + b_2y + c_2z = d_2 \\end{align} \\begin{align} a_3 x + b_3y + c_3z = d_3 \\end{align} can be written as the following augmented matrix: \\begin{align} \\begin{bmatrix} a_1 & b_1 & c_1 & d_1 \\\\ a_2 & b_2 & c_2 & d_2 \\\\ a_3 & b_3 & c_3 & d_3 \\\\ \\end{bmatrix} \\end{align} matrix row transformations for any augmented matrix of a system of linear equations, the following row transformations will result in the matrix of an equivalent system. any two rows may be interchanged. the elements of any row may be multiplied by a nonzero real number. any row may be changed by adding to its elements a multiple of the corresponding elements of another row. row echelon method the echelon (triangular) form of an augmented matrix has 1s down the diagonal from upper left to lower right and 0s below each 1. once a system of linear equations is in echelon form, back-substitution can be used to find the solution set. the row echelon method uses matrices to solve a system of linear equations. start by obtaining a 1 as the first entry in the first column and then transform all entries below it to a 0. continue through the columns obtaining a 1 as the second entry in the second column (zeros below), the third entry in the third column (zeros below), and so on. repeat this process to row echelon form. the following matrix is an augmented matrix in row echelon form: \\begin{align} \\begin{bmatrix} 1 & 2 & 3 & 4\\\\ 0 & 5 & 6 & 7\\\\ 0 & 0 & 0 & 9\\\\ \\end{bmatrix} \\end{align} reduced row echelon method the reduced row echelon form has 1s along the main diagonal and 0s both below and above. for example \\begin{align} \\begin{bmatrix} 1 & 1 & 1 & 6\\\\ 2 & -1 & 1 & 5\\\\ 3 & 1 & -1 & 9\\\\ \\end{bmatrix} \\end{align} by using row transformations, this augmented matrix can be transformed to \\begin{align} \\begin{bmatrix} 1 & 0 & 0 & 3\\\\ 0 & 1 & 0 & 2\\\\ 0 & 0 & 1 & 1\\\\ \\end{bmatrix} \\end{align} which represents \\(x = 3, y = 2, z = 1\\). there is no need for back-substitution with reduced row echelon form. special cases whenever a row of the augmented matrix is of the form \\begin{align} \\begin{bmatrix} 0 & 0 & \\cdots & a\\\\ \\end{bmatrix} \\end{align} where \\(a \\neq 0\\) the system is inconsistent and there will be no solution. a row of the matrix of a linear system in the form: \\begin{align} \\begin{bmatrix} 0 & 0 & \\cdots & 0\\\\ \\end{bmatrix} \\end{align} indicates that the equations of the system are dependent. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/06_06.html",
    "title": "Solution of Linear Systems by Matrix Inverses",
    "body": " index search search back solution of linear systems by matrix inverses multiplicative inverses of square matrices in a similar way, if \\(a\\) is an \\(n \\times n\\) matrix, then its multiplicative inverse, written \\(a^{-1}\\), must satisfy both: \\begin{align} aa^{-1} = a^{-1}a = i_n \\end{align} this result means that only a square matrix can have a multiplicative inverse. the inverse matrix of an \\(n \\times n\\) matrix \\(a\\) (if it exists) can be found analytically by first forming the augmented matrix \\([a|i_n]\\) such that \\(ax = i_n\\), thus \\(x = a^{-1}\\). this means you are solving \\(n\\) systems of linear equations of the form \\(ax_i = i_{n_i}\\). this system is solved by performing matrix row operations, until the left side of the augmented matrix becomes the identity matrix. the resulting augmented matrix can be written as \\([i_n|a^{-1}]\\), where the right side of the matrix is \\(a^{-1}\\). if \\(a^{-1}\\) exists, then it is unique. if \\(a^{-1}\\) does not exist, then \\(a\\) is a singular matrix. using determinants to find inverses if \\begin{align} a = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\end{align} and \\(det(a) \\neq 0\\) then \\begin{align} a^{-1} = \\frac{1}{det(a)}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix} \\end{align} if \\(det(a) = 0\\), then \\(a^{−1}\\) does not exist and \\(a\\) is a singular matrix. solving linear systems using inverse matrices to solve the matrix equation \\(ax = b\\), first see if \\(a^{-1}\\) exists. assuming that it does, use the facts that \\(a^{-1}a = ir and \\)ix = x$. \\begin{align} ax = b \\end{align} \\begin{align} a^{-1}(ax) = a^{-1}b \\end{align} \\begin{align} (a^{-1}a)x = a^{-1}b \\end{align} \\begin{align} ix = a^{-1}b \\end{align} \\begin{align} x = a^{-1}b \\end{align} $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/index.html",
    "title": "A Graphical Approach to Algebra and Trigonometry",
    "body": " index search search back a graphical approach to algebra and trigonometry linear functions, equations and inequalities analysis of graphs of functions polynomial functions rational, power and root functions inverse, exponential and logarithmic functions systems and matrices systems of equations solutions of linear systems in three variables solution of linear systems by row transformations matrix properties and operations determinants and cramer's rule solution of linear systems by matrix inverses $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Notes/Math/Pre-Calculus/AGAAT/06_04.html",
    "title": "Matrix Properties and Operations",
    "body": " index search search back matrix properties and operations in general, a matrix with \\(m\\) rows and \\(n\\) columns has dimension \\(m \\times n\\). the number of rows is always given first certain matrices have special names. an \\(n \\times n\\) matrix is a square matrix of order n. also, a matrix with just one row is a row matrix, and a matrix with just one column is a column matrix. two matrices are equal if they have the same dimension and if corresponding elements, position by position, are equal a matrix containing only zeros as elements is called a zero matrix. matrix addition the sum of two \\(m \\times n\\) matrices \\(a\\) and \\(b\\) is the \\(m \\times n\\) matrix \\(a + b\\) in which each element is the sum of the corresponding elements of \\(a\\) and \\(b\\). only matrices with the same dimension can be added. multiplication of a matrix by a scalar the product of a scalar \\(k\\) and a matrix \\(a\\) is the matrix \\(ka\\), each of whose elements is \\(k\\) times the corresponding element of \\(a\\). matrix multiplication the product \\(ab\\) of an \\(m \\times n\\) matrix \\(a\\) and an \\(n \\times k\\) matrix \\(b\\) is an \\(m \\times k\\) matrix and is found as follows. to find the \\(i\\)th row, \\(j\\)th column element of \\(ab\\), multiply each element in the \\(i\\)th row of \\(a\\) by the corresponding element in the \\(j\\)th column of \\(b\\). the sum of these products gives the element of row \\(i\\), column \\(j\\) of \\(ab\\). \\begin{align} c_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj} \\end{align} the product ab can be found only if the number of columns of a is the same as the number of rows of b. the final product will have as many rows as a and as many columns as b. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/DataScience/google-analytics.html",
    "title": "Google Data Analyst Certificate",
    "body": " index search search back google data analyst certificate source foundations: data, data, everywhere introducing data analytics all about analytical thinking the wonderful world of data set up your toolbox endless career possibilities ask questions to make data-driven decisions effective questions data driven decisions more spreadsheets basics always remember the stakeholder prepare data for exploration data types and structures bias, credibility, privacy, ethics and access databases: where data lives organizing and protecting your data optional: engaging in the data community process data from dirty to clean the importance of integrity sparkling-clean data cleaning data with sql verify and report on your cleaning results optional: adding data to your resume course challenge analyze data to answer questions organizing data to begin analysis formatting and adjusting data aggregating data for analysis performing data calculations share data through the art of visualization visualizing data creating data visualization with tableau crafting data stories developing presentations and slideshows data analysis with r programming programming and data analytics programming using rstudio working with data in r more about visualizations, aesthetics, and annotations documentations and reports google data analytics capstone: complete a case study learn about capstone basics optional: building your portfolio optional: using your portfolio putting your certificate to work $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/DataScience/freecodecamp.html",
    "title": "FreeCodeCamp",
    "body": " index search search back freecodecamp source there are data visualization, machine learning, data analysis in python courses $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/DataScience/index.html",
    "title": "Data Science Study Guide",
    "body": " index search search back data science study guide practical courses google data analyst certificate (absolutely useless) ibm data analyst certificate freecodecamp kaggle courses topics mainly covered in mathematics for machine learning: optimization linear algebra (orthogonal matrices) books to study in this order: mathematics for machine learning pattern recognition and machine learning $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/DataScience/ibm-analytics.html",
    "title": "IBM Data Science Certificate",
    "body": " index search search back ibm data science certificate source $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/index.html",
    "title": "Study",
    "body": " index search search back study study guides math cs datascience web development course plan current college algebra: start from chapter 2 nand2tetris review react do the react crash course. do the redux crash course choose the markdown previewer or the 25 + 5 clock (or both!!!! or neither!!!) future ordered by priority, one choose one at a time backend dev spring boot (list of courses): do the spring course (max 2 weeks) projects do the back end dev course from freecodecamp (max a week) continue the bookish project (add graphql) learn c++ tutorialspoints course (max a week) learn how to use cmake search for projects, some are: data structure data analysis with python course from freecodecamp data visualization course from freecodecamp review college subject on machine learning notes practice projects (add snippets to notes) to search for more project go to project ideas, there are project listings for a lot of languages, frameworks. achive copy ml notes (at least 1h a day) read math for machine learning finish sicp berkeley (at least 1h a day) $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Computer Architecture.html",
    "title": "Computer Architecture",
    "body": " index search search back computer architecture todo nand2tetris computer systems: a programmer's perspective resources nand2tetris each chapter involves building a small piece of the overall system, from writing elementary logic gates in hdl, through a cpu and assembler, all the way to an application the size of a tetris game. the elements of computing systems (nand2tetris) part i: hardware course on coursera part ii: software course on coursera we recommend reading through the first six chapters of the book and completing the associated projects. this will develop your understanding of the relationship between the architecture of the machine and the software that runs on it. in seeking simplicity and cohesiveness, nand2tetris trades off depth. in particular, two very important concepts in modern computer architectures are pipelining and memory hierarchy, but both are mostly absent from the text. computer systems: a programmer's perspective most courses go from chapter 1 to 6. do this course $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Operating Systems.html",
    "title": "Operating Systems",
    "body": " index search search back operating systems books operating systems: three easy pieces labs: xv6 labs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Mathematics for Computer Science.html",
    "title": "Mathematics for Computer Science",
    "body": " index search search back mathematics for computer science books mit lecture notes video lectures mit video lectures $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Algorithms and Data Structures.html",
    "title": "Algorithms and Data Structures",
    "body": " index search search back algorithms and data structures books the algorithm design manual video lectures skiena's or tim's on coursera practice: leetcode problem solving book after the manual: how to solve it $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/index.html",
    "title": "Computer Science",
    "body": " index search search back computer science source programming computer architecture algorithms and data structures mathematics for computer science operating systems computer networking databases languages and compilers distributed systems ai and machine learning ai and machine learning check the source. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Computer Networking.html",
    "title": "Computer Networking",
    "body": " index search search back computer networking books computer networking: a top-down approach video lectures standford: introduction to computer networking course labs: wireshark labs $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Programming.html",
    "title": "Programming",
    "body": " index search search back programming books structure and interpretation of computer programs video lectures brian harvey’s sicp lectures resources for the berkley course code online on scheme scheme on arch (download mit/gnu scheme) plan 1 notes notes we recommend working through at least the first three chapters of sicp and doing the exercises. for additional practice, work through a set of small programming problems like those on exercism. alternatives same course but with python instead of scheme (stk) books: composing programs lectures: 61a taught by john denero at berkley. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Databases.html",
    "title": "Databases",
    "body": " index search search back databases start with the recording and the go through the book (paper compilation) video lectures cs186b berkley book readings in database systems data modelling book: data and reality: a timeless perspective on perceiving and managing information in our imprecise world. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Distributed Systems.html",
    "title": "Distributed Systems",
    "body": " index search search back distributed systems books practice oriented: designing data-intensive applications more traditional: distributed systems, 3rd edition $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/CS/Languages and Compilers.html",
    "title": "Languages and Compilers",
    "body": " index search search back languages and compilers books introductory: crafting interpreters as supplementary reference for video lectures: compilers: principles, techniques & tools video lecutres alex aiken’s, on edx notes for introductory book: we suggest taking the time to work through the whole thing, attempting whichever of the \"challenges\" sustain your interest. $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/src/Study/Math/index.html",
    "title": "Math",
    "body": " index search search back math first year first semester dicrete maths discrete mathematics with applications, 2nd edition by susanna s. epp discrete mathematics structures, 4th edition by kolman, busby and ross proof writing (very advanced, do not expect to master anything in these books) mathematical proofs: a transition to advanced mathematics by gary chartrand et. al (this one is better than the next one) an introduction to abstract mathematics by robert j. bond and wiliam j. keane second semester pre-algebra (refresh really basic math) ags pre-algebra (has solutions) fearon's pre-algebra (this one is better) college algebra (after the pre-algebra one, if the pre-algebra books are too easy skip onto these ones) college algebra by kaufmann (more begginer friendly) college algebra by blitzer second year first semester pre-calculus (once you are done with college algebra. if you know some basic algebra you can skip the college algebra and start in this section) a graphical approach to algebra and trigonometry by hornsby, lial, and rockswold. 6th edition (get the instructor's edition) second semester calculus calculus by james stewart, 5th edition (very famous book, to learn basic calculus. it has a lot of problems. used to teach calculus i, ii and iii) calculus by michael spivak, 3rd edition (it has less material but it is more advanced) third year first semester differential equations a first course in differential equations by zill ordinary differential equations with applications by andrews (it is easier, good for beginners) linear algebra (try to learn as much as possible) elementary linear algebra by howard anton (beginner friendly, with exercises) linear algebra by friedgber, insel, and spence (it is harder and more difficult to read. it is proof based) second semester statistics mathematical statistics by wackerly, mendenhall, and scheaffer a first course in probability by ross complex analysis (calculus with complex numbers. both are pretty much the same, very good beginner books) fundamentals of complex analysis by saff and snider, 3rd edition complex variables and applications by brown and churchill, 7th edition fourth year first semester real analysis (one of the hardest subjects) analysis 1 and analysis 2 by terrance tao (easier to read, but the other two are standard) advanced calculus by fitzpatrick principles of mathematical analysis by rudin elements ofanalysis by ross (expends a lot of time for proofs) abstract algebra (study of groups, rings and fields. very proof based) abstract algebra by saracino (very good for beginners) contemporary abstract algebra by gallian (also good for beginners) second semester topology (optional) introduction to topology by gamelin and greene (it has full solutions for all of the problems) combinatorics (optional) applied combinatorics by tucker naive set theory (optional) naive set theory by halmos functional analysis (optional) functional analysis by kreyszig graph theory (optional) graph theory by gould $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  },
  {
    "id": "https://albamr09.github.io/index.html",
    "title": "Registry Index",
    "body": " index search search registry index notes study $(\"pre\").each(function (index, item) { $(item).html(\"<code>\" + $(item).html() + \"</code>\"); }); hljs.highlightall(); "
  }
]