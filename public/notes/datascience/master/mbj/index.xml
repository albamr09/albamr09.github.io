<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alba&#39;s Notes – Modelos Bayesianos Jerárquicos</title>
    <link>//localhost:1313/notes/datascience/master/mbj/</link>
    <description>Recent content in Modelos Bayesianos Jerárquicos on Alba&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	  <atom:link href="//localhost:1313/notes/datascience/master/mbj/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Introduccion a la Inferencia Bayesiana</title>
      <link>//localhost:1313/notes/datascience/master/mbj/01_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/mbj/01_intro/</guid>
      <description>
        
        
        &lt;h2&gt;The three steps of Bayesian data analysis&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-three-steps-of-bayesian-data-analysis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-three-steps-of-bayesian-data-analysis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The process of Bayesian data analysis can be idealized by dividing it into the following three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Setting up a &lt;strong&gt;full probability model&lt;/strong&gt;: a joint probability distribution for all observable and unobservable quantities in a problem.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conditioning on observed data&lt;/strong&gt;: calculating and interpreting the appropriate posterior distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluating the fit of the model&lt;/strong&gt; and the implications of the resulting posterior distribution&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We distinguish between two kinds of estimands&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quantities that are &lt;strong&gt;not directly observable&lt;/strong&gt;: for example, parameters that govern the hypothetical process leading to the observed data, for which statistical inferences are made.&lt;/li&gt;
&lt;li&gt;Potentially &lt;strong&gt;observable quantities&lt;/strong&gt; (such as future observations of a process, or the outcome under the treatment not received)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Notation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;notation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#notation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;$\theta$: Unobservable vector quantities or population parameters.&lt;/li&gt;
&lt;li&gt;$y$: Observed data&lt;/li&gt;
&lt;li&gt;$\tilde{y}$: Unknown, but potentially observable, quantities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Exchangeability&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;exchangeability&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#exchangeability&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We assume assume that the $n$ values $y_i$ may be regarded as &lt;strong&gt;exchangeable&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We express uncertainty as a joint probability density $p(y_1, \cdots, y_n)$ that is invariant to permutations of the indexes.&lt;/p&gt;
&lt;p&gt;We commonly model data from an exchangeable distribution as &lt;strong&gt;independently and identically distributed (iid)&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Explanatory variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;explanatory-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#explanatory-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Observations on each unit that we do not model as random.&lt;/p&gt;
&lt;h3&gt;Hierarchical modeling&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-modeling&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-modeling&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Hierarchical models (also called &lt;strong&gt;multilevel models&lt;/strong&gt;), which are used when information is available on several different levels of observational units.&lt;/p&gt;
&lt;h2&gt;Bayesian inference&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bayesian-inference&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bayesian-inference&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We define a &lt;strong&gt;prior distribution&lt;/strong&gt; $p(\theta)$, and a &lt;strong&gt;sampling distribution&lt;/strong&gt; (or &lt;strong&gt;data distribution&lt;/strong&gt;) is given by $p(y|\theta)$, such that the joint probability distribution for $\theta$ and $y$ is obtained as follows:&lt;/p&gt;
$$
\begin{aligned}
p(\theta,y) = p(\theta|y)p(y)
\end{aligned}
$$&lt;p&gt;By Baye&amp;rsquo;s rule:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|y) = \frac{p(\theta, y)}{p(y)} = \frac{p(y|\theta)p(\theta)}{p(y)}
\end{aligned}
$$&lt;p&gt;where $p(y) = \sum_y p(y, \theta) = \sum_y p(y|\theta) p(y) = \int_y p(y|\theta) p(y) dy$&lt;/p&gt;
&lt;p&gt;An equivalent form is omitting the factor $p(y)$, yielding the &lt;strong&gt;unnormalized posterior density&lt;/strong&gt;:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|y) \propto p(y|\theta)p(\theta)
\end{aligned}
$$&lt;h3&gt;Prediction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;prediction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#prediction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The &lt;strong&gt;marginal distribution&lt;/strong&gt; of $y$ or &lt;strong&gt;prior predictive distribution&lt;/strong&gt; is given by:&lt;/p&gt;
$$
\begin{aligned}
p(y) = \int p(y, \theta) dy = \int p(y|\theta) p(\theta) d\theta
\end{aligned}
$$&lt;p&gt;The distribution of $\tilde{y}$ is called the &lt;strong&gt;posterior predictive distribution&lt;/strong&gt;, posterior because it is conditional on the observed $y$ and predictive because it is a prediction for an observable $\tilde{y}$. It is defined as the marginalization of $\tilde{y}$ over $y$.&lt;/p&gt;
$$
\begin{aligned}
p(\tilde{y}|y) = \int p(\tilde{y}, \theta|y)d\theta
\end{aligned}
$$&lt;p&gt;We note that the statistical process is also conditioned on the unobservable data $\theta$. Por la propiedad $P(X, Y|Z) = P(X|Y, Z)P(Z)$:&lt;/p&gt;
$$
\begin{aligned}
= \int p(\tilde{y}|y, \theta)p(\theta|y) d\theta
\end{aligned}
$$&lt;p&gt;Asumimos independencia condicional entre $y$ y $\tilde{y}$:&lt;/p&gt;
$$
\begin{aligned}
= \int p(\tilde{y}|\theta)p(\theta|y) d\theta
\end{aligned}
$$&lt;h3&gt;Likelikhood&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;likelikhood&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#likelikhood&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When regarded as a function of $\theta$, for fixed y $p(y|\theta)$ is the &lt;strong&gt;likelihood function&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Likelihood and odds ratios&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;likelihood-and-odds-ratios&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#likelihood-and-odds-ratios&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Odds a posteriori:&lt;/p&gt;
$$
\begin{aligned}
\frac{p(\theta_1|y)}{p(\theta_2|y)} = \frac{\frac{p(y|\theta_1)p(\theta_1)}{p(y)}}{\frac{p(y|\theta_2)p(\theta_2)}{p(y)}} = \frac{p(y|\theta_1)p(\theta_1)}{p(y|\theta_2)p(\theta_2)}
\end{aligned}
$$&lt;p&gt;Odds a priori:&lt;/p&gt;
$$
\begin{aligned}
\frac{p(\theta_1)}{p(\theta_2)}
\end{aligned}
$$&lt;p&gt;Likelihood ratio:&lt;/p&gt;
$$
\begin{aligned}
\frac{p(\theta_1|y)}{p(\theta_2|y)}
\end{aligned}
$$&lt;h2&gt;Probability theory&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;probability-theory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#probability-theory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The expected value of a continuous random variable $u$ is given by:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[u] = \int u p(u)du
\end{aligned}
$$&lt;p&gt;The variance for a continuous random variable $u$ is given by:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[u] = \int (u - \mathbb{E}[u])^2 p(u)du
\end{aligned}
$$&lt;p&gt;The expected value of a discrete random variable $u$ is given by:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[u] = \sum u p(u)
\end{aligned}
$$&lt;p&gt;The variance for a discrete random variable $u$ is given by:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[u] = \sum (u - \mathbb{E}[u])^2 p(u)
\end{aligned}
$$&lt;h3&gt;Means and variances of conditional distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;means-and-variances-of-conditional-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#means-and-variances-of-conditional-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Given two continuous random variables $u$ and $y$, the mean of $u$ can be obtained by &lt;strong&gt;averaging the conditional mean over the marginal distribution of&lt;/strong&gt; $v$:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}(u) = \int u p(u) du = \int u \left(\int p(u, v)\right) dv du
\end{aligned}
$$$$
\begin{aligned}
= \int \left(\int u p(u|v) du \right) p(v) dv = \int \mathbb{E}_u[u|v] p(v) dv = \mathbb{E}_v[\mathbb{E}_u[u|v]]
\end{aligned}
$$&lt;p&gt;The corresponding result for the variance:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{V}[u] = \mathbb{E}[\mathbb{V}[u|v]] - \mathbb{V}[\mathbb{E}[u|v]]
\end{aligned}
$$&lt;h3&gt;Transformation of variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;transformation-of-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#transformation-of-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Suppose $p_u(u)$ is the density of the vector $u$, and we transform to $v = f(u)$, where $v$ has the same number of components as $u$. If $p_u$ is a discrete distribution, and $f$ is a one-to-one function, then the density of $v$ is given by:&lt;/p&gt;
$$
\begin{aligned}
p_v(v) = p_u(f^{-1}(v))
\end{aligned}
$$&lt;p&gt;If $p_u$ is a continuous distribution, and $v = f(u)$ is a one-to-one transformation, then the joint density of the transformed vector is:&lt;/p&gt;
$$
\begin{aligned}
p_v(v) = |J| p_u(f^{-1}(v))
\end{aligned}
$$&lt;p&gt;where $|J|$ is the absolute value of the determinant of the Jacobian of the transformation $u = f^{−1}(v)$ as a function of $v$.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Modelos Jerarquicos</title>
      <link>//localhost:1313/notes/datascience/master/mbj/02_models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/mbj/02_models/</guid>
      <description>
        
        
        &lt;h2&gt;Constructing a parametrized prior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;constructing-a-parametrized-prior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#constructing-a-parametrized-prior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Suppose the aim is to estimate $\theta$, the probability of tumor in a population of female laboratory rats of type &amp;ldquo;F344&amp;rdquo; that receive a zero dose of the drug (control group).&lt;/p&gt;
&lt;p&gt;It is natural to assume a &lt;a href=&#34;#appendix&#34; &gt;binomial model&lt;/a&gt; for the number of tumors, given $\theta$. For convenience, we select a prior distribution for $\theta$ from the conjugate family, $\theta \sim Beta(\alpha, \beta)$&lt;/p&gt;
&lt;h3&gt;Analysis with a fixed prior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;analysis-with-a-fixed-prior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analysis-with-a-fixed-prior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;From historical data, suppose we knew that the tumor probabilities $\theta$ among groups of female lab rats of type &amp;ldquo;F344&amp;rdquo; follow an approximate beta distribution, with known mean and standard deviation.&lt;/p&gt;
&lt;p&gt;The use of a fixed prior distribution from historical data allows for the construction of a parameterized prior distribution, which in turn influences the posterior distribution for the current experiment.&lt;/p&gt;
&lt;p&gt;Then, assuming a $Beta(\alpha, \beta)$ prior distribution for $\theta$ yields a $Beta(\alpha + 4, \beta + 10)$ posterior distribution for $\theta$.&lt;/p&gt;
&lt;h3&gt;Approximate estimate of the population distribution using the historical data&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;approximate-estimate-of-the-population-distribution-using-the-historical-data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#approximate-estimate-of-the-population-distribution-using-the-historical-data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Contrary to last section, typically, the mean and standard deviation of underlying tumor risks are not available, rather historical data is available from previous experiments on similar conditions.&lt;/p&gt;
&lt;p&gt;In the $j$th historical experiment, let the number of rats with tumors be $y_j$ and the total number of rats be $n_j$. We model the $y_j$&amp;rsquo;s (probability that $p$ rats have tumor given a total of $n$ rats) as independent binomial data, given sample sizes $n_j$ and study-specific means $\theta_j$.&lt;/p&gt;
&lt;p&gt;We can display the hierarchical model schematically as follows:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/rat_hierarchical_model.png&#34; alt=&#34;Rat Hierarchical Model&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The observed sample mean and standard deviation of the 70 values $y_j$, $n_j$ are $0.136$ and $0.103$. If we set the mean and standard deviation of the population distribution to these values we can solve for $\theta$ and $\beta$.&lt;/p&gt;
&lt;p&gt;The resulting estimate for $(\alpha, \beta)$ is $(1.4, 8.6)$. &lt;strong&gt;This is not a Bayesian calculation because it is not based on any specified full probability model.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Appendix&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;appendix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#appendix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Binomial model&lt;/strong&gt;: In probability theory and statistics, the binomial distribution with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ independent experiments (&lt;a href=&#34;https://en.mdpedia.org/wiki/Binomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binomial Distribution&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Exchangeability and hierarchical models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;exchangeability-and-hierarchical-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#exchangeability-and-hierarchical-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Exchangeability&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;exchangeability&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#exchangeability&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When we have no additional data on the parameters, we assume exchangeability between them, such that $p(\theta_1, \cdots, \theta_J)$ is invariant to permutation of the indexes.&lt;/p&gt;
&lt;p&gt;The simplest form of an exchangeable distribution has each of the parameters $\theta_j$ as an &lt;strong&gt;independent&lt;/strong&gt; sample from a prior distribution governed by some unknown parameter vector $\phi$; thus:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|\phi) = \prod_{j = 1}^J p(\theta_j|\phi)
\end{aligned}
$$&lt;p&gt;In general, $\phi$ is unknown, so our distribution for $\theta$ must average over our uncertainty in $\phi$:&lt;/p&gt;
$$
\begin{aligned}
p(\theta) = \int_{\phi} p(\theta|\phi)p(\phi) d \phi
\end{aligned}
$$$$
\begin{aligned}
= \int_{\phi} \left(\prod_{j = 1}^J p(\theta_j|\phi)\right) p(\phi) d \phi
\end{aligned}
$$&lt;p&gt;This form, the mixture of independent identical distributions, is usually all that we need to
capture exchangeability in practice.&lt;/p&gt;
&lt;h4&gt;Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We use a nonhierarchical example with exchangeability at the level of $y$ rather than $\theta$.&lt;/p&gt;
&lt;p&gt;In this example, eight states in the United States were selected, and the divorce rate per $1000$ population in each state in $1981$ was recorded. Since you have no information to distinguish any of the eight states from the others, you must model them exchangeably.&lt;/p&gt;
&lt;p&gt;However, you can&amp;rsquo;t assign an exchangeable prior to the set of eight diverse states when there&amp;rsquo;s specific information about one of them. For example, if we know that Nevada differentiates itself from the others because it divorce rate is known to be unusually high, that lets us know before even seeing the data (observed values), that there&amp;rsquo;s a strong reason to believe that Nevada&amp;rsquo;s divorce rate is higher than the other states.&lt;/p&gt;
&lt;p&gt;This means that in a Bayesian analysis, the prior distribution should reflect this belief, assigning more probability mass to Nevada having a higher divorce rate in comparison to the other states.&lt;/p&gt;
&lt;h3&gt;Exchangeability when additional information is available on the units&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;exchangeability-when-additional-information-is-available-on-the-units&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#exchangeability-when-additional-information-is-available-on-the-units&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Sometimes obervations are partially or conditionally exchangeable. For example, when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the case where observations can be grouped, a hierarchical model can be created. In this context, each group has unknown properties. The assumption of exchangeability allows for the use of a common prior distribution for these group properties, meaning that any group can be considered as a random sample of the same underlying population.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If $y_i$ has additional information $x_i$ so that $y_i$ are not exchangeable but $(y_i, x_i)$ still are exchangeable, then we can make a joint model for $(y_i, x_i)$ or a conditional model for $y_i|x_i$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, the usual way to model exchangeability with covariates is through conditional independence:&lt;/p&gt;
$$
\begin{aligned}
p(\theta_1, \cdots, \theta_J) = \int \left[ p(\theta_j|\phi,x_j)\right]p(\phi, x) d\phi
\end{aligned}
$$&lt;p&gt;whith $x = [x_1, \cdots, x_J]$&lt;/p&gt;
&lt;h4&gt;Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Let&amp;rsquo;s consider an example in the field of education where we want to analyze the test scores of students from different schools. We can view the test scores as observations that can be grouped by schools.&lt;/p&gt;
&lt;p&gt;Let $y_{ij}$ be the test score of the student $i$ in school $j$, where $i = 1, 2, \cdots, n_j$ and $j = 1, 2, \cdots, J$ and $n_j$ are the number of students at school $j$.&lt;/p&gt;
&lt;h5&gt;Assumptions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;assumptions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#assumptions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Each school $j$ has an unknown mean test score $\mu_j$.&lt;/li&gt;
&lt;li&gt;The mean test scores $\mu_j$ are assumed to follow a common distribution.&lt;/li&gt;
&lt;li&gt;*Exchangeability: The test scores within each school are exchangeable, implying that any school could be considered a random sample from the overall population of schools.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common prior distribution&lt;/strong&gt;: we assume a common prior distribution for the group mean test scores $\mu_j$ across schools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Model Formulation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;model-formulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#model-formulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: The likelihood of the test scores given the group mean and variance&lt;/p&gt;
$$
\begin{aligned}
p(y_{ij}|\mu_j, \sigma^2)
\end{aligned}
$$&lt;p&gt;&lt;strong&gt;Prior&lt;/strong&gt;: Common prior distribution for the group mean test scores&lt;/p&gt;
$$
\begin{aligned}
p(\mu_j|\theta) \sim \mathcal{N}(\theta, \tau^2)
\end{aligned}
$$&lt;p&gt;where $\theta$ represents the overall mean test score and $\tau$ is the variance parameter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hyperprior&lt;/strong&gt;: Prior distribution for the overall test score&lt;/p&gt;
$$
\begin{aligned}
p(\theta) \sim \mathcal{N}(\mu_0, \sigma_0^2)
\end{aligned}
$$&lt;p&gt;where $\mu_0$ is the prior mean and $\sigma_0^2$ is the prior variance.&lt;/p&gt;
&lt;h5&gt;Bayesian Inference&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bayesian-inference&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bayesian-inference&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The posterior distribution of the group mean test scores and the overall mean test score can be obtained using Bayesian inference techniques, such as Markov Chain Monte Carlo (MCMC) sampling.&lt;/p&gt;
&lt;h3&gt;Objections to exchangeable models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;objections-to-exchangeable-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#objections-to-exchangeable-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In statistical applications, it is common to raise objections to the assumption that different data or experiments are exchangeable. For example experiments may which may have been conducted at different times, with different subjects, and likely in different places.&lt;/p&gt;
&lt;p&gt;Despite these differences, the text suggests that it might be acceptable to consider the data as if they were from the same distribution due to model ignorance.&lt;/p&gt;
&lt;h3&gt;The full Bayesian Treatment of the hierarchical model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-full-bayesian-treatment-of-the-hierarchical-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-full-bayesian-treatment-of-the-hierarchical-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The true &amp;lsquo;hierarchical&amp;rsquo; part of the models is that some parameters are not known and thus have their own prior distributions, denoted as $p(\phi)$. The Bayesian posterior distribution is of the vector $(\phi, \theta)$. The joint prior distribution is:&lt;/p&gt;
$$
\begin{aligned}
p(\phi, \theta) = p(\phi)p(\theta|\phi)
\end{aligned}
$$&lt;p&gt;and the joint posterior distribution (after seeing the data $y$) is:&lt;/p&gt;
$$
\begin{aligned}
p(\phi, \theta|y) \propto p(\phi, \theta)p(y|\phi, \theta)
\end{aligned}
$$&lt;p&gt;Given that $p(y|\phi, \theta)$ depends only on $\theta$:&lt;/p&gt;
$$
\begin{aligned}
= p(\phi, \theta)p(y|\theta)
\end{aligned}
$$&lt;p&gt;In order to create a joint probability distribution for $(\phi, \theta)$, we must assign a prior distribution to $\phi$.&lt;/p&gt;
&lt;p&gt;It is often practical to start with a simple, relatively noninformative, prior distribution on $\phi$ and seek to add more prior information if there remains too much variation in the posterior distribution.&lt;/p&gt;
&lt;h3&gt;Posterior predictive distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;posterior-predictive-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#posterior-predictive-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Hierarchical models are characterized both by parameters $\theta$ and hyperparameters, $\phi$, that parametrize the prior distribution over $\theta$.&lt;/p&gt;
&lt;p&gt;There are two posterior predictive distributions that might be of interest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The distribution of future observations $\tilde{y}$ corresponding to an existing $j$ &amp;ldquo;group&amp;rdquo; described by $\theta_j$.&lt;/li&gt;
&lt;li&gt;The distribution of future observations $\tilde{y}$ corresponding to future $\theta_j$ (a &amp;ldquo;new group&amp;rdquo;), denoted by $\tilde{\theta}$, drawn from the superpopulation $p(\theta|\phi)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the rat tumor example, future observations can be (1) additional rats from an existing experiment, or (2) results from a future experiment (explained by a different set of parameters $\theta$).
For (1) the posterior predictive draws $\tilde{y}$ are based on the posterior draws of $\theta_j$ ($p(\theta_j|y)$) for the existing experiment.&lt;/p&gt;
&lt;p&gt;For (2) one must first draw $\tilde{\theta}$ for the new experiment from the population distribution, given the posterior draws of $\phi$, and then draw $\tilde{y}$ given the simulated $\tilde{\theta}$.&lt;/p&gt;
&lt;h2&gt;Bayesian analysis of conjugate hierarchical models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bayesian-analysis-of-conjugate-hierarchical-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bayesian-analysis-of-conjugate-hierarchical-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Analytic derivation of conditional and marginal distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;analytic-derivation-of-conditional-and-marginal-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analytic-derivation-of-conditional-and-marginal-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Hierarchical models involve multiple levels of parameters and dependencies between them, making the analysis more intricate. The following steps are necessary to disentangle the relationships between parameters at different levels of the hierarchy and to estimate their distributions accurately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Joint Posterior Density&lt;/strong&gt;: Combines the prior information (hyperprior distribution $p(\phi)$), the population distribution ($p(\theta|\phi)$), and the likelihood function $p(y|\theta)$ to form the joint posterior distribution.&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p(\theta, \phi|y) \propto p(y|\theta)p(\theta|\phi)p(\phi)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Conditional Posterior Density of the parameters&lt;/strong&gt;: Calculates the posterior distribution of $\theta$ given the hyperparameters $\phi$, allows us to understan how parameters interact and influence each other. This is usually done using &lt;a href=&#34;https://es.mdpedia.org/wiki/Prior_conjugada&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a priori conjugate distributions&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p(\theta|\phi, y)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hyperparameter Estimation&lt;/strong&gt;: estimating $\phi$ through the Bayesian paradigm helps in updating our knowledge about the higher-level parameters based on the observed data. This step can be perfomed by integrating the joint posterior distribution over $\theta$ in to be able to marginalize $\phi$ conditionally on $y$.&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p(\phi|y) = \int p(\theta, \phi|y)d\theta
\end{aligned}
$$&lt;p&gt;For many standard models the marginal posterior distribution of $\phi$ can be computed algebraically using the conditional probability formula:&lt;/p&gt;
$$
\begin{aligned}
p(\phi|y) = \frac{p(\theta, \phi|y)}{p(\theta|\phi, y)}
\end{aligned}
$$&lt;h3&gt;Drawing simulations from the posterior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;drawing-simulations-from-the-posterior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#drawing-simulations-from-the-posterior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The following strategy is useful for simulating a draw from the joint posterior distribution $p(\theta, \phi|y)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Draw the vector of hyperparameters, $\phi$, from its marginal posterior distribution, $p(\phi|y)$.&lt;/li&gt;
&lt;li&gt;Draw the parameter vector $\theta$ from its conditional posterior distribution, $p(\theta|\phi, y)$. For the examples we consider in this chapter, the factorization $p(\theta|\phi, y) = \prod_j p(\theta_j|\phi, y)$ holds.&lt;/li&gt;
&lt;li&gt;If desired, draw predictive values $\tilde{y}$ from the posterior predictive distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above steps are performed $L$ times in order to obtain a set of $L$ draws. From the joint posterior simulations of $\theta$ and $\tilde{y}$, we can compute the posterior distribution of any estimand or predictive quantity of interest.&lt;/p&gt;
&lt;h3&gt;Application to the model for rat tumors&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;application-to-the-model-for-rat-tumors&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#application-to-the-model-for-rat-tumors&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The data from experiments $j = 1, \cdots, J$, $J = 71$, are assumed to follow independent binomial distributions:&lt;/p&gt;
$$
\begin{aligned}
y_j \sim Bin(n_j, \theta_j)
\end{aligned}
$$&lt;p&gt;This models the probability of getting exactly $\theta_j$ successes in $n_j$ independent Bernoulli trials.&lt;/p&gt;
&lt;p&gt;with the number of rats $n_j$ unknown. The parameters $\theta_j$ are assumed to be independent samples from a beta distribution:&lt;/p&gt;
$$
\begin{aligned}
\theta_j \sim Beta(\alpha, \beta)
\end{aligned}
$$&lt;p&gt;and we shall assign a noninformative hyperprior distribution to reflect our ignorance about the unknown hyperparameters $\alpha, \beta$. We defer the choice of noninformative hyperprior distribution, a relatively arbitrary and unimportant part of this particular analysis, until we inspect the integrability of the posterior density.&lt;/p&gt;
&lt;h4&gt;Joint, conditional, and marginal posterior distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;joint-conditional-and-marginal-posterior-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#joint-conditional-and-marginal-posterior-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The &lt;strong&gt;joint posterior distribution&lt;/strong&gt; of all the parameters is:&lt;/p&gt;
$$
\begin{aligned}
p(\theta, \alpha, \beta|y) \propto p(\alpha, \beta)p(\theta|\alpha,\beta)p(y|\theta, \alpha, \beta)
\end{aligned}
$$&lt;p&gt;where $p(\alpha, \beta)$ is the hyperprior distribution ($p(\phi)$).&lt;/p&gt;
&lt;p&gt;Then $p(\theta|\alpha,\beta)$ is the population distribution ($p(\theta|\phi)$). The pdf of $x \sim Beta(\alpha, \beta)$, ignoring the normalization constant, &lt;a href=&#34;https://en.mdpedia.org/wiki/Beta_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;is given by&lt;/a&gt;:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|\alpha, \beta) \propto \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1- x)^{\beta - 1}
\end{aligned}
$$&lt;p&gt;For $j = 1, \cdots, J$ i.i.d $\theta_j \sim Beta(\alpha, \beta)$:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|\alpha, \beta) \propto \prod_{j=1}^J \theta_j^{\alpha - 1} (1- \theta_j)^{\beta - 1}
\end{aligned}
$$&lt;p&gt;And $p(y|\theta, \alpha, \beta)$ is the likelihood ($p(y|\theta)$). The pdf of $x \sim Bin(n, p)$ &lt;a href=&#34;https://en.mdpedia.org/wiki/Binomial_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;is given by&lt;/a&gt;:&lt;/p&gt;
$$
\begin{aligned}
p(x = k|n, p) \propto p^k (1 - p)^{n - k}
\end{aligned}
$$&lt;p&gt;For $j = 1, \cdots, J$ i.i.d $y_j \sim Bin(n_j, \theta_j)$:&lt;/p&gt;
$$
\begin{aligned}
p(y_j|n_j, \theta_j) \propto \theta_j^{y_j} (1 - \theta_j)^{n_j - y_j}
\end{aligned}
$$&lt;p&gt;Therefore we obtain:&lt;/p&gt;
$$
\begin{aligned}
\propto p(\alpha, \beta) \left(\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1}\right) \left(\prod_{j=1}^J \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}\right)
\end{aligned}
$$&lt;p&gt;The &lt;strong&gt;conditional posterior density of&lt;/strong&gt; $\theta$ given the hyperparameters is defined using a &lt;a href=&#34;https://compcogsci-3016.djnavarro.net/technote_betabinomial.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beta-binomial conjugate prior (page 7)&lt;/a&gt;, therefore if:&lt;/p&gt;
$$
\begin{aligned}
y_i|n_j, \theta_j \sim Bin(n_j, \theta_j)
\end{aligned}
$$$$
\begin{aligned}
\theta_j|\alpha,\beta \sim Beta(\alpha, \beta)
\end{aligned}
$$&lt;p&gt;then&lt;/p&gt;
$$
\begin{aligned}
\theta_j|\alpha, \beta, y_j, n_j \sim Beta(\alpha + y_j, \beta + n_j - y_j)
\end{aligned}
$$&lt;p&gt;which gives us the following &lt;a href=&#34;https://en.mdpedia.org/wiki/Beta_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdf for a Beta distribution&lt;/a&gt; of i.i.d $\theta$:&lt;/p&gt;
$$
\begin{aligned}
p(\theta|\alpha, \beta, y) = \prod_{j=1}^J \frac{\Gamma(\alpha + y_j + \beta + n_j - y_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}
\end{aligned}
$$$$
\begin{aligned}
= \prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}
\end{aligned}
$$&lt;p&gt;We can determine the &lt;strong&gt;marginal posterior distribution of the hyperparameters&lt;/strong&gt; $(\alpha, \beta)$ by substituting on the previous equations on the following formula:&lt;/p&gt;
$$
\begin{aligned}
p(\phi|y) = \frac{p(\theta, \phi|y)}{p(\theta|\phi, y)}
\end{aligned}
$$&lt;p&gt;where $\phi = (\alpha, \beta)$, so:&lt;/p&gt;
$$
\begin{aligned}
p(\alpha, \beta|y) = \frac{p(\theta, \alpha, \beta|y)}{p(\theta|\alpha, \beta, y)}
\end{aligned}
$$$$
\begin{aligned}
\propto \frac{p(\alpha, \beta) \left(\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1}\right) \left(\prod_{j=1}^J \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}\right)}{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \frac{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\prod_{j=1}^J \frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \frac{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}}{\frac{\Gamma(\alpha + \beta + n_j)}{\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}}\frac{\theta_j^{\alpha - 1}(1 - \theta_j)^{\beta - 1} \theta_j^{y_j}(1 - \theta_j)^{n_j - y_j}}{\theta_j^{\alpha + y_j - 1}(1-\theta_j)^{\beta + n_j - y_j - 1}}\right)
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}\theta_j^{\alpha - 1 + y_j - \alpha - y_j + 1}(1-\theta_j)^{\beta - 1 + n_j - y_j - \beta - n_j + y_j + 1}\right)
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \left(\frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}\theta_j^{0}(1-\theta_j)^{0}\right)
\end{aligned}
$$$$
\begin{aligned}
\propto p(\alpha, \beta) \prod_{j=1}^J \frac{\Gamma(\alpha + \beta)\Gamma(\alpha + y_j)\Gamma(\beta + n_j - y_j)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha + \beta + n_j)}
\end{aligned}
$$&lt;h4&gt;Choosing a standard parameterization and setting up a &amp;rsquo;noninformative&amp;rsquo; hyperprior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;choosing-a-standard-parameterization-and-setting-up-a-noninformative-hyperprior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#choosing-a-standard-parameterization-and-setting-up-a-noninformative-hyperprior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Because we have no immediately available information about the distribution of tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribution for $(\alpha, \beta)$.&lt;/p&gt;
&lt;p&gt;By reparameterizing the hyperparameters, we transform them into a space that may have more intuitive or meaningful interpretations. In this case, $logit(\frac{\alpha}{\alpha + \beta}) = \log(\frac{\alpha}{\beta})$ represents the log-odds of $\alpha$ relative to the total of $\alpha$ and $\beta$, providing a clear interpretation of the prior mean in the beta distribution for $\theta$. Similarly, $\log(\alpha + \beta)$ captures the logarithm of the &amp;ldquo;sample size,&amp;rdquo; influencing the precision or spread of the distribution.&lt;/p&gt;
&lt;p&gt;Also the logit transformation helps stabilize the numerical computations, especially when dealing with probabilities or proportions that are bounded between 0 and 1. By working in the logit space, we avoid issues related to extreme values or boundaries that can arise in the original parameter space. And transforming the hyperparameters can facilitate the specification of appropriate prior distributions.&lt;/p&gt;
&lt;p&gt;One reasonable choice of diffuse hyperprior density is uniform on $(\frac{\alpha}{\alpha + \beta}, (\alpha + \beta)^{−1/2})$, which when multiplied by the appropriate Jacobian yields the following densities on the original scale,&lt;/p&gt;
$$
\begin{aligned}
p(\alpha, \beta) \propto (\alpha + \beta)^{-5/2}
\end{aligned}
$$&lt;p&gt;and on the natural transformed scale:&lt;/p&gt;
$$
\begin{aligned}
p(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)) \propto \alpha\beta(\alpha + \beta)^{-5/2}
\end{aligned}
$$&lt;h4&gt;Computing the marginal posterior density of the hyperparameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;computing-the-marginal-posterior-density-of-the-hyperparameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#computing-the-marginal-posterior-density-of-the-hyperparameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Now that we have established a full probability model for data and parameters, we compute the marginal posterior distribution of the hyperparameters.&lt;/p&gt;
&lt;p&gt;The next figure shows a contour plot of the unnormalized marginal posterior density on a grid of values of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/marginal_contour_plot.png&#34; alt=&#34;Contour plot of the marginal posterior distribution&#34; loading=&#34;lazy&#34; /&gt;
To create the plot, we first compute the logarithm of the density function of $p(\alpha, \beta|y)$ with prior density $p(\alpha, \beta) \propto (\alpha + \beta)^{-5/2}$, multiplying by the Jacobian to obtain the density $p(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)|y)$&lt;/p&gt;
&lt;p&gt;The most obvious features of the contour plot are (1) the mode is not far from the point estimate (as we would expect), and (2) important parts of the marginal posterior distribution lie outside the range of the graph.&lt;/p&gt;
&lt;p&gt;We recompute the previous pdf in a different range $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)) \in [-2.3, -1.3] \times [1, 5]$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/marginal_contour_plot_1.png&#34; alt=&#34;Contour plot of the marginal posterior distribution&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure $5.3b$ displays $1000$ random draws from the numerically computed posterior distribution.
The graphs show that the marginal posterior distribution of the hyperparameters, under this transformation, is approximately symmetric about the mode, roughly $(−1.75, 2.8)$. This corresponds to approximate values of $(\alpha, \beta) = (2.4, 14.0)$, which differs somewhat from the crude estimate obtained earlier.&lt;/p&gt;
&lt;p&gt;Having computed the relative posterior density at a grid that covers the effective range of $(\alpha, \beta)$, we normalize by approximating the distribution as a step function over the grid
and setting the total probability in the grid to $1$.&lt;/p&gt;
&lt;h4&gt;Sampling from the joint posterior distribution of parameters and hyperparameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;sampling-from-the-joint-posterior-distribution-of-parameters-and-hyperparameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#sampling-from-the-joint-posterior-distribution-of-parameters-and-hyperparameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We draw $1000$ random samples from the joint posterior distribution of $(\alpha, \beta, \theta_1, \cdots, \theta_J)$, as follows.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simulate $1000$ draws of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$ from their posterior distribution using the same discrete-grid sampling procedure used to draw $(\alpha, \beta)$ for Figure $3.3b$.&lt;/li&gt;
&lt;li&gt;For $l = 1, \cdots, 1000$:&lt;/li&gt;
&lt;li&gt;Transform the $l$th draw of $(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta))$ to the scale $(\alpha, \beta)$ to yield a draw of the hyperparameters from their marginal posterior distribution.&lt;/li&gt;
&lt;li&gt;For each $j = 1, \cdots, J$, sample $\theta_j$ from its conditional posterior distribution, $\theta_j|\alpha, \beta, y \sim Beta(\alpha + y_j, \beta + n_j − y_j)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Displaying the results&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;displaying-the-results&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#displaying-the-results&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Figure $5.4$ shows posterior medians and $95%$ intervals for the $\theta_j$’s, computed by simulation.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/posterior_values_simulation.png&#34; alt=&#34;Posterior Values Simulation&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are superficially similar to what would be obtained based on a point estimate of the hyperparameters, which makes sense in this example, because of the fairly large number of experiments. But key differences remain, notably that posterior variability is higher in the full Bayesian analysis, reflecting posterior uncertainty in the hyperparameters.&lt;/p&gt;
&lt;h2&gt;Normal model with exchangeable parameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;normal-model-with-exchangeable-parameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#normal-model-with-exchangeable-parameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Model definition&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;model-definition&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#model-definition&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We now present a full treatment of a simple hierarchical model based on the normal distribution, with different means for each &amp;ldquo;group&amp;rdquo; but with known observation variance and a normal population distribution for the group means.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consider $J$ independent experiments. The &lt;strong&gt;likelihood (sampling distribution)&lt;/strong&gt; is defined as:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
y_{ij} | \theta_j \sim \text{N}(\theta_j, \sigma^2), \text{ for } i = 1, \cdots, n_j; j = 1, \cdots, J.
\end{aligned}
$$&lt;p&gt;where we label the sample mean of each group $j$ as:&lt;/p&gt;
$$
\begin{aligned}
\overline{y}_j = \frac{1}{n_j} \sum_{i = 1}^{n_j} y_{ij}
\end{aligned}
$$&lt;p&gt;and the sampling variance as:&lt;/p&gt;
$$
\begin{aligned}
\sigma^2_j = \frac{\sigma^2}{n_j}
\end{aligned}
$$&lt;p&gt;Here we assume that $\sigma$ is a know value.&lt;/p&gt;
&lt;p&gt;We can then write the likelihood for each $\theta_j$ using the sufficient statistics, $\overline{y}_j$:&lt;/p&gt;
$$
\begin{aligned}
\overline{y}_j | \theta_j \sim \text{N}(\theta_j, \sigma_j^2)
\end{aligned}
$$&lt;p&gt;Sufficient statistics are summary statistics of the data that capture all the information about the parameter of interest. In this case, the sufficient statistic $\overline{y}_j$ represents the data summary for experiment $j$ that is used to estimate the parameter $\theta_j$. By using the sufficient statistic $\overline{y}_j$, the likelihood function for each $\theta_j$ is constructed based on the observed data in experiment $j$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;prior distribution&lt;/strong&gt; over $\theta_j$, assuming the prior to be normal for the sake of conjugacy is defined as:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\theta_j|\mu, \tau \sim \text{N}(\theta_j|\mu, \tau^2)
\end{aligned}
$$&lt;p&gt;Assuming each $\theta_j$ to be independent we obtain the following joint distribution:&lt;/p&gt;
$$
\begin{aligned}
p(\theta_1, \cdots, \theta_J|\mu, \tau) = \prod_{j=1}^J \text{N}(\theta_j|\mu, \tau^2)
\end{aligned}
$$&lt;p&gt;and by process of marginalization:&lt;/p&gt;
$$
\begin{aligned}
p(\theta_1, \cdots, \theta_J) = \int \left[\prod_{j=1}^J \text{N}(\theta_j|\mu, \tau^2)\right]p(\mu, \tau) d(\mu, \tau)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;hyperprior over the parameters&lt;/strong&gt; $\mu$ and $\tau$ is defined as a non-informative distribution (i.e. uniform density), such that:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p(\mu, \tau) = p(\mu|\tau)p(\tau) \propto p(\tau)
\end{aligned}
$$&lt;p&gt;We define a prior distribution over $\tau$. For our illustrative analysis, we use the uniform prior distribution $p(\tau) \propto 1$. Once an initial analysis is performed using the noninformative
&amp;lsquo;uniform&amp;rsquo; prior density, a sensitivity analysis with a more realistic prior distribution is often desirable.&lt;/p&gt;
&lt;h3&gt;Inference&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inference&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inference&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Joint posterior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;joint-posterior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#joint-posterior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This distribution combines prior information (hyperprior distribution $p(\mu, \tau)$) the population distribution $p(\theta_j|\mu, \tau)$ and the likelihood function $p(y_{ij}|\theta_j)$. We define it as follows:&lt;/p&gt;
$$
\begin{aligned}
p(\theta, \mu, \tau|y)
\end{aligned}
$$&lt;p&gt;By Bayes Theorem (ignoring the normalization term):&lt;/p&gt;
$$
\begin{aligned}
\propto p(y|\theta) p(\theta|\mu, \tau) p(\mu, \tau)
\end{aligned}
$$&lt;p&gt;Here $p(y|\theta)$ is the likelihood function previously defined in terms of the sufficient statistics $\overline{y}_j$&lt;/p&gt;
$$
\begin{aligned}
\propto \left[\prod_{j=1}^J \text{N}(\overline{y}_j|\theta_j, \sigma_j^2)\right] p(\theta|\mu, \tau) p(\mu, \tau)
\end{aligned}
$$&lt;p&gt;and $p(\theta|\mu, \tau)$ is the prior, also previouly defined, such that:&lt;/p&gt;
$$
\begin{aligned}
\propto \left[\prod_{j=1}^J \text{N}(\overline{y}_j|\theta_j, \sigma_j^2)\right] \left[\prod_{j=1}^J \text{N}(\theta_j|\mu, \tau^2) p(y|\theta)\right] p(\mu, \tau)
\end{aligned}
$$&lt;p&gt;where we can ignore factors that depend only on $y$ and the parameters $\sigma_j$, which are assumed known for this analysis.&lt;/p&gt;
&lt;h4&gt;The conditional posterior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-conditional-posterior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-conditional-posterior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The conditional posterior distribution calculates the posterior distribution of $\theta_j$ given the hyperparameters $\mu, \tau$. It allows us to understand how parameters intereact and influence each other. We define them for each $\theta_j$ as follows:&lt;/p&gt;
$$
\begin{aligned}
\theta_j | \mu, \tau, y \sim \text{N}(\hat{\theta}_j, V_j)
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\hat{\theta}_j = \frac{\frac{1}{\sigma_j^2}\overline{y}_j + \frac{1}{\tau^2}\mu}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}}
\end{aligned}
$$&lt;p&gt;and:&lt;/p&gt;
$$
\begin{aligned}
V_j = \frac{1}{\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}}
\end{aligned}
$$&lt;h4&gt;The marginal posterior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-marginal-posterior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-marginal-posterior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This distribution allows us to estimate the hyperparameters $\mu$ and $\tau$ through the Bayesian paradigm. By the conditional rule we obtain:&lt;/p&gt;
$$
\begin{aligned}
p(\mu, \tau|y) \propto p(\mu, \tau)p(y|\mu, \tau)
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\overline{y}_j | \mu, \tau \sim \text{N}(\mu, \sigma_j^2 + \tau^2)
\end{aligned}
$$&lt;p&gt;such that:&lt;/p&gt;
$$
\begin{aligned}
p(\mu, \tau|y) \propto p(\mu, \tau) \prod_{j=1}^J \text{N}(\overline{y}_j|\mu, \sigma^2_j + \tau^2)
\end{aligned}
$$&lt;h6&gt;Posterior distribution of $\mu$ given $\tau$&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;posterior-distribution-of-mu-given-tau&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#posterior-distribution-of-mu-given-tau&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h6&gt;&lt;p&gt;We can further simplify by integrating over $\mu$, leaving a simple univariate numerical computation of $p(\tau|y)$, by the conditional rule:&lt;/p&gt;
$$
\begin{aligned}
p(\mu, \tau|y) = p(\mu | \tau, y) p(\tau|y)
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\mu | \tau, y \sim \text{N}(\hat{\mu}, V_\mu)
\end{aligned}
$$&lt;p&gt;with:&lt;/p&gt;
$$
\begin{aligned}
\hat{\mu} = \frac{\sum_{j=1}^J \frac{1}{\sigma_j^2 + \tau^2}\overline{y}_j}{\sum_{j=1}^J \frac{1}{\sigma_j^2 + \tau^2}}
\end{aligned}
$$&lt;p&gt;and&lt;/p&gt;
$$
\begin{aligned}
V_{\mu}^{-1} = \sum_{j=1}^J \frac{1}{\sigma_j^2 + \tau^2}
\end{aligned}
$$&lt;h6&gt;Posterior distribution $\tau$&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;posterior-distribution-tau&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#posterior-distribution-tau&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h6&gt;&lt;p&gt;We know from the previous section that:&lt;/p&gt;
$$
\begin{aligned}
p(\mu, \tau|y) = p(\mu | \tau, y) p(\tau|y)
\end{aligned}
$$&lt;p&gt;such that:&lt;/p&gt;
$$
\begin{aligned}
p(\tau|y) = \frac{p(\mu, \tau|y)}{p(\mu | \tau, y)}
\end{aligned}
$$&lt;p&gt;We previously defined $\mu | \tau, y \sim \text{N}(\hat{\mu}, V_\mu)$, therefore:&lt;/p&gt;
$$
\begin{aligned}
p(\tau|y) = \frac{p(\mu, \tau|y)}{\text{N}(\hat{\mu}, V_\mu)}
\end{aligned}
$$&lt;p&gt;We also defined $p(\mu, \tau|y) \propto p(\mu, \tau) \prod_{j=1}^J \text{N}(\overline{y}_j|\mu, \sigma^2_j + \tau^2)$:&lt;/p&gt;
$$
\begin{aligned}
\propto \frac{p(\tau) \prod_{j=1}^J \text{N}(\overline{y}_j|\mu, \sigma^2_j + \tau^2)}{\text{N}(\hat{\mu}, V_\mu)}
\end{aligned}
$$&lt;h3&gt;Simulation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;simulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#simulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For this model, computation of the posterior distribution of θ is most conveniently performed via simulation, following the factorization:&lt;/p&gt;
$$
\begin{aligned}
p(\theta, \mu, \tau|y) = p(\theta|\mu, \tau, y) p(\mu|\tau, y) p(\tau, y)
\end{aligned}
$$&lt;h3&gt;Posterior predictive distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;posterior-predictive-distributions-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#posterior-predictive-distributions-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To obtain a draw from the posterior predictive distribution of new data $\tilde{y}$ from the current batch of parameters, $\theta$, first obtain a draw from $p(\theta, \mu, \tau|y)$ and then draw the predictive data $\tilde{y}$ from the sampling distribution:&lt;/p&gt;
$$
\begin{aligned}
y_{ij} | \theta_j \sim \text{N}(\theta_j, \sigma^2), \text{ for } i = 1, \cdots, n_j; j = 1, \cdots, J.
\end{aligned}
$$&lt;p&gt;To obtain posterior predictive simulations of new data $\tilde{y}$ for $\tilde{J}$ new groups, perform the following three steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw $(\mu, \tau)$ from their posterior distribution $p(\mu, \tau|y)$&lt;/li&gt;
&lt;li&gt;Draw $\tilde{J}$ new parameters $\tilde{\theta} = \tilde{\theta}&lt;em&gt;1, \cdots, \tilde{\theta}&lt;/em&gt;{\tilde{J}}$ from the population distribution $p(\tilde{\theta}|\mu, \tau)$.&lt;/li&gt;
&lt;li&gt;Draw $\tilde{y}$ given $\tilde{\theta}$ from the sampling distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Example: parallel experiments in eight schools&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-parallel-experiments-in-eight-schools&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-parallel-experiments-in-eight-schools&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Inferences based on nonhierarchical models and their problems&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inferences-based-on-nonhierarchical-models-and-their-problems&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inferences-based-on-nonhierarchical-models-and-their-problems&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Before fitting the hierarchical Bayesian model, we first consider two simpler nonhierarchical methods—estimating the effects from the eight experiments independently (separate estimates), and complete pooling—and discuss why neither of these approaches is adequate for this example.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_hierarchical_modeling_comparison.png&#34; alt=&#34;Model Comparison&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Consider $\theta_1$, the effect in school $A$. The effect in school $A$ is estimated as $28.4$ with a standard error of $14.9$ under the separate analysis, versus a pooled estimate of $7.7$ with a standard error of $4.1$ under
the common-effect model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: given a Normal distribution (symmetrical with respect to it mean) the probability that an estimate takes a value under the mean is $\frac{1}{2}$ (cumulative density function), as the $\mu$ serves as the midpoint of a Normal distribution such that half the area for the normal curve is contained under $[0, \mu]$.&lt;/p&gt;
&lt;p&gt;The separate analyses of the eight schools imply the following posterior statement: &amp;rsquo;the probability is $\frac{1}{2}$ that the true effect in $A$ is more than $28.4$&amp;rsquo; a doubtful statement, considering the results for the other seven schools. On the other hand, the pooled model implies the following statement: &amp;rsquo;the probability is $\frac{1}{2}$ that the true effect in A is less than $7.7$,&amp;rsquo; which seems an inaccurate summary of our knowledge. As in the theoretical discussion of the previous section, neither estimate is fully satisfactory, and we would like a compromise that combines information from all eight experiments without assuming all the $\theta_j$&amp;rsquo;s to be equal. The Bayesian analysis under the hierarchical model provides exactly that.&lt;/p&gt;
&lt;h3&gt;Posterior simulation under the hierarchical model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;posterior-simulation-under-the-hierarchical-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#posterior-simulation-under-the-hierarchical-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Consequently, we compute the posterior distribution of $\theta_1, \cdots, \theta_8$, based on the normal model presented in &lt;a href=&#34;#hierarchical-normal-modeling&#34; &gt;Section 4&lt;/a&gt;. We draw from the posterior distribution for the Bayesian model by simulating the random variables $\tau$, $\mu$, and $\theta$, in that order, from their posterior distribution, as discussed at the end of the previous section. The sampling standard deviations, $\sigma_j$, are assumed known and equal to the values in Table 5.2, and we assume independent uniform prior densities on $\mu$ and $\tau$.&lt;/p&gt;
&lt;p&gt;The marginal posterior density function, $p(\tau|y)$ from, is plotted in the next figure:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_example_margina_posterior_density.png&#34; alt=&#34;Normal Posterior Density&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Values of $\tau$ near zero are mos plausible. In the normal hierarchical model, however, we learn a great deal by considering the conditional posterior distributions given $\tau$ (and averaged over $\mu$), that is $\mathbb{E}[\theta_j|\tau, y]$, averaging over $\mu$. This is displayed on the following image:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_example_conditional_posterior.png&#34; alt=&#34;Conditional Posterior Density&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Comparing with the previous figure, which has the same scale on the horizontal axis, we see that for most of the likely values of $\tau$, that is for $\tau \approx 0$ the estimated effects for all the groups are relatively close together (when $\tau = 0$ you would guess they are clustered on the same point). However, as $\tau$ becomes larger, corresponding to more variability among schools, the estimates become more like the raw values shown on the first figure of this section.&lt;/p&gt;
&lt;p&gt;The lines in the following figure show the conditional standard deviations, $sd(\theta_j|\tau, y)$, as a function of $\tau$. As $\tau$ increases, the population distribution allows the eight effects to be more different from each other, and hence the posterior uncertainty in each individual $\tau_j$ increases, approaching the standard deviations shown in the raw data in the limit of $\tau \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_example_conditional_parameter_std.png&#34; alt=&#34;Conditional Parameters Standard Deviations&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Contrary to what we saw with separate estimates and pooled estimates, for the likely values of $\tau$ (see figure for $p(\tau|y)$), the estimates in all schools are substantially less than $28$ points. For example, even at $\tau = 0$, the probability that the effect in school A is less than $28$ points is $\Phi[(28 − 14.5)/9.1] = 93%$, where $\Phi$ is the standard normal cumulative distribution function.&lt;/p&gt;
&lt;p&gt;Of substantial importance, we do not obtain an accurate summary of the data if we condition on the posterior mode of $\tau$ as it ignores the uncertainty associated with $\tau$ as conveyed by the full posterior distribution. In Bayesian statistics, the posterior distribution encapsulates both the most likely values of parameters as well as the uncertainty or variability in those estimates.&lt;/p&gt;
&lt;p&gt;By only considering the mode (the peak or maximum) of the posterior distribution and neglecting its shape and spread, we may miss out on valuable information about the range of plausible values for τ and the associated uncertainty.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;discussion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#discussion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Table 5.3 summarizes the $200$ simulated effect estimates for all eight schools.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_example_simulation.png&#34; alt=&#34;Simlated Effects (theta parameter)&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Bayesian probability that the effect in school A is as large as $28$ points is less than $10%$, which is substantially less than the $50%$ probability based on the separate estimate for school A.&lt;/p&gt;
&lt;p&gt;As an illustration of the simulation-based posterior results, $200$ simulations of school A&amp;rsquo;s effect are shown in Figure 5.8a.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/normal_example_effects_simulation.png&#34; alt=&#34;Simlated Effects&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Having simulated the parameter $\theta$, it is easy to ask more complicated questions of this model. For example, what is the posterior distribution of $\max(\theta_j)$, the effect of the most successful of the eight coaching programs? Figure 5.8b displays a histogram of $200$ values from this posterior distribution and shows that only $22$ draws are larger than $28.4$. For another example, we can estimate $Pr(\theta_1 &amp;gt; \theta_3|y)$, the posterior probability that the coaching program is more effective in school A than in school C, by the proportion of simulated draws of $\theta$ for which $\theta_1 &amp;gt; \theta_3$; the result is $\frac{141}{200} = 0.705$.&lt;/p&gt;
&lt;p&gt;To sum up, the Bayesian analysis of this example not only allows straightforward inferences about many parameters that may be of interest, but the hierarchical model is flexible enough to adapt to the data, thereby providing posterior inferences that account for the partial pooling as well as the uncertainty in the hyperparameters.&lt;/p&gt;
&lt;h2&gt;Hierarchical modeling applied to a meta-analysis&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-modeling-applied-to-a-meta-analysis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-modeling-applied-to-a-meta-analysis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;A normal approximation to the likelihood&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;a-normal-approximation-to-the-likelihood&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-normal-approximation-to-the-likelihood&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;If clinical trial $j$ (in the series to be considered for meta-analysis) involves the use of $n_{0j}$ subjects in the control group and $n_{1j}$ in the treatment group, giving rise to $y_{0j}$ and $y_{1j}$ deaths in control and treatment groups, respectively, then the usual sampling model involves two independent binomial distributions with probabilities of death $p_{0j}$ and $p_{1j}$, respectively.&lt;/p&gt;
&lt;p&gt;For each study $j$, one can estimate $\theta_j$ by:&lt;/p&gt;
$$
\begin{aligned}
y_j = \log\left(\frac{y_{1j}}{n_{1j} - y_{1j}}\right) - \log\left(\frac{y_{0j}}{n_{0j} - y_{0j}}\right)
\end{aligned}
$$&lt;p&gt;with approximate sampling variance&lt;/p&gt;
$$
\begin{aligned}
\sigma_j^2 = \frac{1}{y_{1j}} + \frac{1}{n_{1j} - y_{1j}} + \frac{1}{y_{0j}} + \frac{1}{n_{0j} - y_{0j}}
\end{aligned}
$$&lt;p&gt;based on empirical logits. The estimated log-odds ratios $y_j$ and their estimated standard errors $\sigma_j^2$ are displayed as the fourth and fifth columns of Table 5.4.&lt;/p&gt;
&lt;h3&gt;Goals of inference in meta-analysis&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;goals-of-inference-in-meta-analysis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#goals-of-inference-in-meta-analysis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Complete pooling: we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on.&lt;/li&gt;
&lt;li&gt;Separate estimates: the studies are so different that the results of any one study provide no information about the results of any of the others.&lt;/li&gt;
&lt;li&gt;Bayesian analysis: we regard the studies as exchangeable but not necessarily either identical or completely unrelated.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study.&lt;/p&gt;
&lt;h3&gt;What if exchangeability is inappropriate?&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;what-if-exchangeability-is-inappropriate&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#what-if-exchangeability-is-inappropriate&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;What if other information (in addition to the data $(n, y)$) is available to distinguish among the $J$ studies in a meta-analysis, so that an exchangeable model is inappropriate? In this situation, we can expand the framework of the model to be exchangeable in the observed data and covariates.&lt;/p&gt;
&lt;h3&gt;A hierarchical model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;a-hierarchical-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-hierarchical-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let $y_j$ represent generically the point estimate of the effect $\theta_j$ in the $j$th study, the sampling distribution is defined as:&lt;/p&gt;
$$
\begin{aligned}
y_j|\theta_j, \sigma_j \sim \text{N}(\theta_j, \sigma_j^2)
\end{aligned}
$$&lt;p&gt;where $\sigma_j$ represents the corresponding estimated standard error, which is assumed known without error.&lt;/p&gt;
&lt;p&gt;At the second stage of the hierarchy, we again use an exchangeable normal &lt;strong&gt;prior distribution&lt;/strong&gt;, with mean $\mu$ and standard deviation $\tau$, which are unknown hyperparameters.&lt;/p&gt;
$$
\begin{aligned}
\theta|\mu, \tau \sim \text{N}(\mu, \tau)
\end{aligned}
$$&lt;p&gt;Finally, a &lt;strong&gt;hyperprior distribution&lt;/strong&gt; is required for $\mu$ and $\tau$. For this problem, it is reasonable to assume a noninformative or locally uniform prior density for $\mu$. We also assume a locally uniform prior density for $\tau$.&lt;/p&gt;
&lt;h2&gt;Weakly Informative Priors&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;weakly-informative-priors&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#weakly-informative-priors&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Concepts relating to the choice of prior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;concepts-relating-to-the-choice-of-prior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#concepts-relating-to-the-choice-of-prior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Improper limit of a prior distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;improper-limit-of-a-prior-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#improper-limit-of-a-prior-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Improper prior densities can, but do not necessarily, lead to proper posterior distributions.&lt;/p&gt;
&lt;h4&gt;Calibration&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;calibration&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#calibration&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Posterior inferences can be evaluated using the concept of calibration of the posterior mean. For any parameter $\theta$, if we label the posterior mean as $\hat{\theta} = \mathbb{E}[\theta|y]$, we can define the miscalibration of the posterior mean as $\mathbb{E}[\theta|\hat{\theta}] - \hat{\theta}$.&lt;/p&gt;
&lt;p&gt;We can judge the accuracy of our conclusions from Bayesian analysis by checking how close the average value we predict (the posterior mean) is to the true value.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how it works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we call our predicted average value $\hat{\theta}$, and we calculate how far off it is from the true value $\theta$, that&amp;rsquo;s what we call the miscalibration of the prediction.&lt;/li&gt;
&lt;li&gt;If our initial guesses (prior distribution) are accurate and our data matches those guesses, then our predictions will be right on target, meaning the miscalibration will be zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are models where the probabilities don&amp;rsquo;t add up to $1$, which makes it impossible to draw a parameter $\theta$ from them. So, we need to expand our theory to deal with this. To see if our predictions are accurate in these cases, we need to imagine a &amp;ldquo;true&amp;rdquo; prior distribution where $\theta$ comes from, and compare it to the &amp;ldquo;inferential&amp;rdquo; prior distribution we actually use for our Bayesian analysis.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take the example of the 8 schools model. Here, we consider an improper uniform distribution on $\tau$ (a parameter in the model) as a limit of uniform distributions on a range (from $0$ to a really large number $A$, which is getting bigger and bigger).&lt;/p&gt;
&lt;p&gt;When we use this improper uniform distribution, our inferences tend to overestimate $\tau$. Let&amp;rsquo;s see why:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If both the &amp;ldquo;true&amp;rdquo; and &amp;ldquo;inferential&amp;rdquo; prior distributions are uniform on $(0, A)$, our miscalibration is zero. This means our predictions are accurate.&lt;/li&gt;
&lt;li&gt;Now, if we keep the &amp;ldquo;true&amp;rdquo; prior distribution as $U(0, A)$ but let the &amp;ldquo;inferential&amp;rdquo; prior distribution go to $U(0, \infty)$, our predictions tend to increase (because now we&amp;rsquo;re including more and more extreme values of $\theta$), which leads to a positive miscalibration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Classes of noninformative and weakly informative prior distributions for hierarchical variance parameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;classes-of-noninformative-and-weakly-informative-prior-distributions-for-hierarchical-variance-parameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#classes-of-noninformative-and-weakly-informative-prior-distributions-for-hierarchical-variance-parameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;General considerations&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;general-considerations&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#general-considerations&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We view any noninformative or weakly informative prior distribution as inherently provisional—after the model has been fit, one should look at the posterior distribution and see if it makes sense. If the posterior distribution does not make sense, this implies that additional prior knowledge is available that has not been included in the model, and that contradicts the assumptions of the prior distribution that has been used&lt;/p&gt;
&lt;h4&gt;Uniform prior distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;uniform-prior-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#uniform-prior-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;When we&amp;rsquo;re setting up our model, we often start with uniform priors. But we have to be careful about how we define the scale of this uniform distribution. One common situation is when we&amp;rsquo;re dealing with parameters that must be positive, like variance parameters. Using a uniform prior on the logarithm of these parameters ($\log \tau$) might seem like a good idea, but it can lead to problems because the resulting posterior distribution becomes improper (doesn&amp;rsquo;t add up to 1).&lt;/p&gt;
&lt;p&gt;An alternative is trying to set up a range for our prior distribution, like $[-A, A]$ where $A$ is a really large number. This seems like a good idea to keep things in check, but there&amp;rsquo;s a catch: the posterior distribution (our updated belief after looking at the data) can end up heavily influenced by the lower bound, $-A$, of our range. When we calculate the marginal likelihood $p(y|\tau)$ of our data given a certain parameter ($\tau$), it ends up approaching a fixed, non-zero value as $\tau$ gets really close to $0$. Because when we calculate the likelihood of our data given a parameter ($\tau$), it&amp;rsquo;s like asking, &amp;ldquo;How likely is it that we&amp;rsquo;d see this data if our parameter τ were true?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Now, imagine $\tau$ is getting really close to $0$. In many situations, this means we&amp;rsquo;re saying there&amp;rsquo;s almost no variability in our data. But even if $\tau$ is very close to $0$, the likelihood of observing our data isn&amp;rsquo;t exactly $0$. There&amp;rsquo;s still some chance, even if it&amp;rsquo;s tiny, that we&amp;rsquo;d see our data just by random chance, even with very little variability. So, as $\tau$ approaches $0$, the likelihood doesn&amp;rsquo;t drop to $0$ as well. Instead, it approaches a fixed, non-zero value.&lt;/p&gt;
&lt;p&gt;Another option we can consider is using a uniform prior distribution directly on the parameter $\tau$ itself. This helps avoid some of the problems we discussed earlier because it keeps the total probability finite, especially near $\tau = 0$. However, there&amp;rsquo;s a drawback to this approach. It tends to lean slightly towards positive values, because it allows for the possibility of very large values of $\tau$ as well.&lt;/p&gt;
&lt;p&gt;When we&amp;rsquo;re dealing with just one or two groups ($J = 1$ or $2$), using this uniform prior actually results in an improper posterior density. This means that our analysis essentially concludes that $\tau$ is infinite, and it doesn&amp;rsquo;t do any pooling of data from different groups. In a way, this makes sense because it&amp;rsquo;s hard to decide from just a few groups how much we should pool their data together. But from a Bayesian perspective, it&amp;rsquo;s a bit awkward because we&amp;rsquo;re making this decision before even looking at the data.&lt;/p&gt;
&lt;p&gt;When we&amp;rsquo;re dealing with these improper uniform prior distributions, we can think of them as being like the limit of certain types of weakly informative priors. For example, the uniform prior distribution on the logarithm of $\tau$ is basically like saying that $\tau$ follows a distribution where the probability decreases as $\tau$ gets bigger.&lt;/p&gt;
&lt;p&gt;Sometimes, in Bayesian statistics, people suggest using a uniform prior distribution directly on $\tau^2$. This means that every possible value for $\tau^2$ is considered equally likely. However, we don&amp;rsquo;t recommend this approach. It tends to have a bigger issue with miscalibration towards higher values compared to the other approaches we discussed earlier. Plus, using this uniform prior on τ squared requires us to have at least 4 groups for the analysis to work properly and give us a reasonable posterior distribution.&lt;/p&gt;
&lt;h4&gt;Inverse-gamma$(\epsilon, \epsilon)$ prior distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inverse-gammaepsilon-epsilon-prior-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inverse-gammaepsilon-epsilon-prior-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In the schools model, the parameter $\tau$ doesn&amp;rsquo;t have any simple family of prior distributions that work well because its likelihood depends on all the data from all the groups in a complex way. However, there&amp;rsquo;s a kind of distribution called the inverse-gamma family that works well in this situation. This means that if we use an inverse-gamma distribution as a prior for $\tau^2$, then after we collect our data and update our beliefs, the conditional distribution for $\tau^2$, $p(\tau^2|\mu, \theta, y)$ that we get is still an inverse-gamma distribution.&lt;/p&gt;
&lt;p&gt;The inverse-gamma prior distribution is a way to set up our beliefs in a noninformative (or weakly informative) manner when we&amp;rsquo;re dealing with certain types of data. We choose a parameter called alpha ($\alpha$) to control how informative the prior is. Now, here&amp;rsquo;s the thing: If we set alpha to a very low value, like $1$ or $0.01$ or $0.001$, it&amp;rsquo;s supposed to mean we&amp;rsquo;re not putting much prior information into our model. But there&amp;rsquo;s a problem: When we make alpha too small, the posterior distribution (our updated beliefs after looking at the data) can end up being improper, which means it doesn&amp;rsquo;t add up to $1$. To avoid this, we need to set alpha to a reasonable value, not too small.&lt;/p&gt;
&lt;h4&gt;Half-Cauchy prior distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;half-cauchy-prior-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#half-cauchy-prior-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We&amp;rsquo;re going to look at another type of distribution called the $t$ family, specifically the $\text{half}-t$ because our scale parameter ($\tau$) has to be positive. Now, we&amp;rsquo;re interested in the $t$ family for this problem because it&amp;rsquo;s pretty flexible and can cover a wide range of situations. Plus, we can use a neat trick called reparameterization to express it as a prior distribution for our scale parameter ($\tau$) in a way that works well with our model.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s why it&amp;rsquo;s helpful: The half-Cauchy distribution has a wide peak around zero and just one parameter that we can adjust, which we&amp;rsquo;ll call $A$. We can set $A$ to be a large value, and as it gets bigger and bigger (approaching infinity), the half-Cauchy distribution starts to look more like a uniform distribution on our parameter $\tau$.&lt;/p&gt;
&lt;p&gt;When we set $A$ to a large but finite value, it means we&amp;rsquo;re using a slightly informative prior distribution. Even though it&amp;rsquo;s not completely flat, it&amp;rsquo;s still pretty gentle, especially in the tails. This means that even if we have some prior beliefs, the data we collect can still have a big influence on our final results, especially if the data is strong.&lt;/p&gt;
&lt;p&gt;So, we&amp;rsquo;re going to use the half-Cauchy distribution for situations where we&amp;rsquo;re estimating variance parameters from just a few groups. In these cases, our choices about our prior beliefs can really affect our results, so we want to use a prior distribution that&amp;rsquo;s flexible and doesn&amp;rsquo;t have a strong influence unless the data really supports it.&lt;/p&gt;
&lt;h3&gt;Application to the 8-schools example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;application-to-the-8-schools-example&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#application-to-the-8-schools-example&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We demonstrate the properties of some proposed noninformative prior densities on the eight-schools example of &lt;a href=&#34;./05_example_normal.md&#34; &gt;Section 5&lt;/a&gt;. Figure 5.9 displays the posterior distributions for the 8-schools model resulting from three different choices of prior distributions that are intended to be noninformative.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/inverse_gamma.png&#34; alt=&#34;Inverse Gamma&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first histogram (on the left) shows what we think about the parameter $\tau$ when we use a uniform prior distribution. The data suggest that $\tau$ could be anywhere below $20$, but there&amp;rsquo;s a small chance it could be even larger. This makes sense because we only have data from $8$ groups, and it&amp;rsquo;s hard to be sure about large values of $\tau$ with that little data.&lt;/p&gt;
&lt;p&gt;Now, look at the second histogram (in the middle). Here, we&amp;rsquo;ve changed our prior to something called an $\text{inverse-gamma}(1, 1)$ distribution. This changes our conclusions. Now, our estimate for $\tau$ is lower, and we&amp;rsquo;re more confident in our estimates for the individual group parameters ($\theta_j$&amp;rsquo;s). To understand why this happens, let&amp;rsquo;s think about the shape of our prior distribution. With the inverse-gamma prior, it&amp;rsquo;s concentrated in a narrow range, from $0.5$ to $5$. This means it&amp;rsquo;s not giving much weight to really large or really small values of $\tau$. In comparison, the uniform prior seemed less informative, meaning it didn&amp;rsquo;t strongly influence our conclusions.&lt;/p&gt;
&lt;p&gt;The last histogram (on the right) in Figure 5.9 shows what happens when we use a different kind of prior distribution called $\text{inverse-gamma}(0.001, 0.001)$ for $\tau$ squared. This prior is very sharply peaked near zero, meaning it puts a lot of emphasis on very small values of $\tau$. Because of this, our conclusions from the data get distorted. Even though the data might suggest that $\tau$ could be larger, the prior is pulling our estimates towards smaller values. The reason this happens is because the likelihood for $\tau$, stays high near zero. So even though our data might suggest that larger values of $\tau$ are possible, the strong influence of the prior near zero pulls our estimates towards smaller values.&lt;/p&gt;
&lt;p&gt;In this example, we&amp;rsquo;re not considering two other options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using a uniform prior distribution on the logarithm of $\tau$, which would result in an improper posterior density with a spike at $\tau = 0$, similar to the last histogram but even more pronounced.&lt;/li&gt;
&lt;li&gt;Using a uniform prior distribution directly on $\tau^2$, which would result in a posterior distribution similar to the first histogram, but with a slightly higher tail on the right side.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Application for the 3-schools problem&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;application-for-the-3-schools-problem&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#application-for-the-3-schools-problem&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The uniform prior distribution seems fine for the 8-school analysis, but problems arise if the number of groups $J$ is much smaller, in which case the data supply little information about the group-level variance, and a noninformative prior distribution can lead to a posterior distribution that is improper or is proper but unrealistically broad.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/3_school_prior_distribution.png&#34; alt=&#34;Posterior Distribution over the hyperparameters&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 5.10 displays the inferences for $\tau$ based on two different priors. We start with a default uniform distribution, which means we&amp;rsquo;re not favoring any particular values for our parameter $\tau$. This worked well when we had data from $8$ groups (as seen in Figure 5.9). But now, we&amp;rsquo;re looking at a new dataset with only $3$ groups. Unfortunately, the resulting histogram (the left one in Figure 5.10) shows that the posterior distribution for $\tau$ has a really long tail on the right side. This means it&amp;rsquo;s suggesting values of $\tau$ that are way too high to be reasonable.&lt;/p&gt;
&lt;p&gt;This long tail is expected because we have such a small number of groups (if we had even fewer groups, the tail would be even longer, going on forever). Using this kind of posterior distribution can be a problem because it means we&amp;rsquo;re not pooling the estimates of the school effects ($\theta_j$) as much as we should be.&lt;/p&gt;
&lt;p&gt;The last histogram (on the right) in Figure 5.10 shows what happens when we use a different kind of prior distribution called a half-Cauchy. We set the scale parameter ($A$) of this prior to $25$. We chose this value to be a bit higher than what we expect for the standard deviation of the underlying $\theta_j$&amp;rsquo;s in our educational testing example. This way, our model only weakly constrains the parameter $\tau$. On the graph, you&amp;rsquo;ll see a line that represents this prior distribution. It&amp;rsquo;s highest for values of $\tau$ less than $50$ and gradually falls off beyond that. This means the prior puts more weight on smaller values of $\tau$ but still allows for larger values.&lt;/p&gt;
&lt;p&gt;This half-Cauchy prior distribution would also perform well in the 8-schools problem; however it was unnecessary because the default uniform prior gave reasonable results. With only 3 schools, we went to the trouble of using a weakly informative prior, a distribution that was not intended to represent our actual prior state of knowledge about $\tau$ but rather to constrain the posterior distribution, to an extent allowed by the data.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Evaluación y Comparación de Modelos</title>
      <link>//localhost:1313/notes/datascience/master/mbj/03_evaluation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/mbj/03_evaluation/</guid>
      <description>
        
        
        &lt;h2&gt;Measures of Predictive Accuracy&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;measures-of-predictive-accuracy&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#measures-of-predictive-accuracy&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Point Prediction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;point-prediction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#point-prediction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In &lt;strong&gt;point prediction&lt;/strong&gt; (predictive point estimation or point forecasting) a single value is reported as a prediction of the unknown future observation. Measures of predictive accuracy for point prediction are called &lt;strong&gt;scoring functions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, the &lt;strong&gt;mean squared error&lt;/strong&gt;:&lt;/p&gt;
$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n (y_i - \mathbb{E}[y_i|\theta])^2
\end{aligned}
$$&lt;p&gt;or its weighted version:&lt;/p&gt;
$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n \frac{(y_i - \mathbb{E}[y_i|\theta])^2}{\mathbb{V}[y_i|\theta]}
\end{aligned}
$$&lt;p&gt;These are easy to compute but they are less appropiated for models that are far from the normal distribution.&lt;/p&gt;
&lt;h4&gt;Probabilistic Prediction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;probabilistic-prediction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#probabilistic-prediction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In &lt;strong&gt;probabilistic prediction&lt;/strong&gt; (probabilistic forecasting) the aim is to report inferences about
$\hat{y}$ in such a way that the full uncertainty over $\hat{y}$ is taken into account. These are called &lt;strong&gt;scoring rules&lt;/strong&gt;. Examples include the quadratic, logarithmic, and zero-one scores&lt;/p&gt;
&lt;p&gt;Good scoring rules for prediction are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proper: the scoring rule encourages the decision maker to be honest when reporting their beliefs.&lt;/li&gt;
&lt;li&gt;Local: the scoring rule takes into account the fact that some predictions may be worse than others, and it adjusts accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example the &lt;strong&gt;log predictive density&lt;/strong&gt; or &lt;strong&gt;log-likelihood&lt;/strong&gt;, $p(y|\theta)$, which is proportional to the mean squared error if the model is normal with constant variance.&lt;/p&gt;
&lt;p&gt;Why not use the log posterior? The answer is that we are interested here in summarizing the fit of model to data, and for this purpose the prior is relevant in estimating the parameters but not inassessing a model&amp;rsquo;s accuracy. We are not saying that the prior cannot be used in assessing a model&amp;rsquo;s fit to data; rather we say that the prior density is not relevant in computing predictive accuracy.&lt;/p&gt;
&lt;h3&gt;Predictive accuracy for a single data point&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;predictive-accuracy-for-a-single-data-point&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#predictive-accuracy-for-a-single-data-point&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The best way to measure how well a model fits is by seeing how accurately it predicts outcomes in new data that it hasn&amp;rsquo;t seen before (out-of-sample predictive performance), but that comes from the same process as the original data.&lt;/p&gt;
&lt;p&gt;We label $f$ as the true model, $y$ as the observed data and $\tilde{y}$ as future data. The out-of-sample predictive fit for a new data point $\tilde{y}_i$ using logarithmic score is:&lt;/p&gt;
$$
\begin{aligned}
\log p_{\text{post}}(\tilde{y}_i) = \log \mathbb{E}_{\text{post}}[p(\tilde{y}_i|\theta)] =
\end{aligned}
$$&lt;p&gt;By the definition of the expected value for a random variable:&lt;/p&gt;
$$
\begin{aligned}
= \log \int p(\tilde{y}_i|\theta) p_{\text{post}}(\theta)d\theta
\end{aligned}
$$&lt;p&gt;where $p_{\text{post}}(\tilde{y}_i)$ is the predictive density for $\tilde{y}&lt;em&gt;i$ induced by the posterior distribution $p&lt;/em&gt;{\text{post}}(\theta)$.&lt;/p&gt;
&lt;p&gt;Note that we use $p_{\text{post}}$ and $\mathbb{E}_{\text{post}}$ to denote any probability or expectation that averages over the posterior distribution of $\theta$.&lt;/p&gt;
&lt;h3&gt;Averaging over the distribution of future data&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;averaging-over-the-distribution-of-future-data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#averaging-over-the-distribution-of-future-data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The future data $\tilde{y}_i$ are themselves unknown and thus we define the expected out-of-sample log predictive density. By the definition of expected value of the function $\log (x)$ over $\tilde{y}$ with respect to a function $f$ that describes the distribution of the data, we compute the &lt;strong&gt;expected log predictive density&lt;/strong&gt; or elpd for a new data point as follows:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}_f[\log p_{\text{post}}(\tilde{y}_i)] = \int \log (p_{\text{post}}(\tilde{y}_i)) f(\tilde{y}_i) d\tilde{y}
\end{aligned}
$$&lt;p&gt;In general we do not know the data distribution $f$. A natural way to estimate the expected out-of-sample log predictive density would be to plug in an estimate for $f$, but this will tend to imply too good a fit. For now we consider the estimation of predictive accuracy in a Bayesian context. One can define a measure of predictive accuracy for the $n$ data points taken one at a time:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i=1}^n \mathbb{E}_f[\log(p_{\text{post}}(\tilde{y}_i))]
\end{aligned}
$$&lt;p&gt;This gives us the &lt;strong&gt;expected log pointwise predictive density&lt;/strong&gt; for a new dataset.&lt;/p&gt;
&lt;p&gt;Using a single-point measure instead of dealing with the entire set of predictions (the joint distribution $p_{\text{post}}(\tilde{y})$) allows us to connect it to cross-validation, which helps us approximate how well our model performs on new data based on the data we already have.&lt;/p&gt;
&lt;p&gt;It is sometimes useful to consider predictive accuracy given a point estimate $\theta(\tilde{y})$ (sampled data point given the parameter $\theta$?). This gives us the &lt;strong&gt;expected log predictive density&lt;/strong&gt; given $\hat{\theta}$:&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}_f[\log(p(\tilde{y}|\theta))]
\end{aligned}
$$&lt;h3&gt;Evaluating predictive accuracy for a fitted model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;evaluating-predictive-accuracy-for-a-fitted-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evaluating-predictive-accuracy-for-a-fitted-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In practice the parameter $\theta$ is not known, so we cannot know the log predictive density $\log p(y|\theta)$, which tells us how well our model predicts new data based on $\theta$. So, instead of using $\theta$ directly, we use something called the posterior distribution, denoted as $p_{\text{post}}(\theta) = p(\theta|y)$. This distribution gives us a range of possible values for $\theta$ based on the data we have. From this distribution, we can summarize how accurately our model predicts new data. So we define the &lt;strong&gt;log pointwise predictive density&lt;/strong&gt; or lppd as:&lt;/p&gt;
$$
\begin{aligned}
\log \prod_{i=1}^n p_{\text{post}}(y_i) = \sum_{i=1}^n \log \int p(y_i|\theta)p_{\text{post}}(\theta)d\theta
\end{aligned}
$$&lt;p&gt;To calculate this predictive density, we can use samples drawn from the posterior distribution $p_{\text{post}}(\theta)$ using simulation. These samples are labeled as $\theta_s$, where $s$ ranges from $1$ to $S$. So we define the &lt;strong&gt;computed log pointwise predictive density&lt;/strong&gt; or computed lppd as:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i=1}^n \log \left(\frac{1}{S}\sum_{s=1}^S p(y_i|\theta^s)\right)
\end{aligned}
$$&lt;p&gt;We basically compute the sample mean of the likelihood $p(y_i|\theta)$ for over all the ${\theta^s}_{s=1}^S$ We typically assume that the number of simulation draws $S$ is large enough to fully capture the posterior distribution.&lt;/p&gt;
&lt;p&gt;The lppd of observed data y is an overestimate of the elppd for future data. Hence the plan is to start with lppd and then apply some sort of bias correction to get a reasonable estimate of elppd.&lt;/p&gt;
&lt;h2&gt;Information criteria and cross-validation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;information-criteria-and-cross-validation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#information-criteria-and-cross-validation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Estimating out-of-sample predictive accuracy using available data&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;estimating-out-of-sample-predictive-accuracy-using-available-data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#estimating-out-of-sample-predictive-accuracy-using-available-data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Several methods are available to estimate the expected predictive accuracy without waiting for out-of-sample data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Within-sample predictive accuracy&lt;/strong&gt;: A naive estimate of the expected log predictive density for new data is the log predictive density for existing data using the computed lppd. This summary is in general an overestimate of elppd because it is evaluated on the data from which the model was fit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adjusted within-sample predictive accuracy&lt;/strong&gt;: Given that lppd is a biased estimate of elppd, the next logical step is to correct that bias. Formulas such as AIC, DIC, and WAIC (all discussed below) give approximately unbiased estimates of elppd.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt;: One can attempt to capture out-of-sample prediction error by fitting the model to training data and then evaluating this predictive accuracy on a holdout set. Cross-validation can be computationally expensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Akaike information criterion (AIC)&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;akaike-information-criterion-aic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#akaike-information-criterion-aic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In much of the statistical literature on predictive accuracy, inference for $\theta$ is summarized not by a posterior distribution $p_{\text{post}}$ but by a point estimate $\hat{\theta}$, typically the maximum
likelihood estimate. Out-of-sample predictive accuracy is then defined not by the expected log posterior predictive density (elppd) but by $\text{elpd}_{\hat{\theta}} = \mathbb{E}_f[\log p(\tilde{y}|\tilde{\theta}(y))]$.&lt;/p&gt;
&lt;p&gt;Let $k$ be the number of parameters estimated in the model. AIC is defined as follows:&lt;/p&gt;
$$
\begin{aligned}
\hat{\text{elpd}}_{\text{AIC}} = -2 \log p(y|\hat{\theta}_{\text{mle}}) + 2k
\end{aligned}
$$&lt;p&gt;Subtracting $k$ from the log predictive density given the maximum likelihood estimate $\theta_{\text{mle}}$ is a correction to account for how much the fitting of $k$ parameters will increase predictive accuracy, purely by chance.&lt;/p&gt;
&lt;p&gt;When we move beyond linear models with simple priors, just adding the number of fitted parameters $k$ to adjust the deviance isn&amp;rsquo;t accurate. Informative priors and hierarchical structures typically decrease overfitting compared to simple estimation methods like least squares or maximum likelihood. In models with informative priors or hierarchical setups, the actual number of parameters depends heavily on the variance of the group-level parameter.&lt;/p&gt;
&lt;h3&gt;Deviance Information Criterion (DIC) and Effective Number of Parameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;deviance-information-criterion-dic-and-effective-number-of-parameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#deviance-information-criterion-dic-and-effective-number-of-parameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;DIC is a somewhat Bayesian version of AIC making two changes, replacing the maximum likelihood estimate with the posterior mean $\hat{\theta}_{\text{Bayes}} = \mathbb{E}[\theta|y]$ and replacing $k$ with a data-based bias correction. The new measure of predictive accuracy is:&lt;/p&gt;
$$
\begin{aligned}
\hat{\text{elpd}}_{DIC} = \log p(y|\hat{\theta}_{\text{Bayes}}) - p_{\text{DIC}}
\end{aligned}
$$&lt;p&gt;where $p_{\text{DIC}}$ is the effective number of parameters, defined as:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{DIC}} = 2 \left(\log p(y|\hat{\theta}_{\text{Bayes}}) - \mathbb{E}_{post}[\log p(y|\theta)]\right)
\end{aligned}
$$&lt;p&gt;where $\mathbb{E}_{post}[\log p(y|\theta)]$ is an average of $\theta$ over its posterior distribution. This is computed using simulation $\theta^s, s= 1, \cdots, S$ as:&lt;/p&gt;
$$
\begin{aligned}
\text{computed } p_{\text{DIC}} = 2 \left(\log p(y|\hat{\theta}_{\text{Bayes}}) - \frac{1}{S} \sum_{s=1}^S \log p(y|\theta^s)\right)
\end{aligned}
$$&lt;p&gt;When the average value of $\theta$ in the posterior distribution matches the highest point (mode), it leads to the maximum log predictive density. However, if the average value is significantly different from the mode, it can result in a negative value for $p_{\text{DIC}}$.&lt;/p&gt;
&lt;p&gt;An alternative version of DIC uses a slightly different effective number of parameters:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{DIC}_{\text{alt}}} = 2 \mathbb{V}_{\text{post}}[\log p(y|\theta)]
\end{aligned}
$$&lt;p&gt;Of these two measures, $p_{\text{DIC}}$ is more numerically stable but $p_{\text{DIC}_{\text{alt}}}$ has the advantage of always being positive. The actual quantity called DIC is defined in terms of the deviance rather than the log predictive density; thus:&lt;/p&gt;
$$
\begin{aligned}
\text{DIC} = -2 \log p(y|\hat{\theta}_{\text{Bayes}}) + 2p_{DIC}
\end{aligned}
$$&lt;h3&gt;Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;watanabe-akaike-or-widely-applicable-information-criterion-waic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#watanabe-akaike-or-widely-applicable-information-criterion-waic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;WAIC&lt;/strong&gt; is a more fully Bayesian approach for estimating the out-of-sample expectation. Starting with the computed lppd and then adding a correction for effective number of parameters to adjust for overfitting. Two adjustments have been proposed:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{WAIC}1} = 2 \sum_{n=1}^n \left(\log(\mathbb{E}_{\text{post}}[p(y_i|\theta)]) - \mathbb{E}_{\text{post}}[\log p(y_i|\theta)] \right)
\end{aligned}
$$&lt;p&gt;computed by replacing the expectations by averages over the $S$ posterior draws $\theta^s$:&lt;/p&gt;
$$
\begin{aligned}
\text{computed } p_{\text{WAIC}1} = 2 \sum_{n=1}^n \left(\log\left(\frac{1}{S}\sum_{s=1}^S p(y_i|\theta^s)\right) - \frac{1}{S}\sum_{s=1}^S \log p(y_i|\theta^s) \right)
\end{aligned}
$$&lt;p&gt;The other measure uses the variance of individual terms:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{WAIC}2} = \sum_{i=1}^n \mathbb{V}_{\text{post}}[\log p(y_i|\theta)]
\end{aligned}
$$&lt;p&gt;To compute it we compute the posterior sample variance ($\mathbb{V}_{s=1}^S$) of the log predictive density for each data point $y_i$ and we sum over all the data points:&lt;/p&gt;
$$
\begin{aligned}
\text{computed } p_{\text{WAIC}2} = \sum_{i=1}^n \mathbb{V}_{s=1}^S[\log p(y_i|\theta^s)]
\end{aligned}
$$&lt;p&gt;We can then use either $p_{\text{WAIC}1}$ or $p_{\text{WAIC}2}$ as a bias correction:&lt;/p&gt;
$$
\begin{aligned}
\hat{\text{elppd}}_{\text{WAIC}} = \text{lppd} - p_{\text{WAIC}}
\end{aligned}
$$&lt;p&gt;As with $\text{AIC}$ and $\text{DIC}$, we define $\text{WAIC}$ so as to be on the deviance scale:&lt;/p&gt;
$$
\begin{aligned}
\text{WAIC} = -2\text{lppd} + 2p_{\text{WAIC}2}
\end{aligned}
$$&lt;p&gt;For a normal linear model with a large sample size, known variance, and a uniform prior distribution on the coefficients, $p_{\text{WAIC}1}$ and $p_{\text{WAIC}2}$ are roughly equal to the number of parameters in the model. In general, this adjustment approximates the number of &amp;ldquo;unconstrained&amp;rdquo; parameters in the model. A parameter is counted as $1$ if it&amp;rsquo;s estimated without constraints or prior information, $0$ if it&amp;rsquo;s fully constrained, or if all the information comes from the prior distribution, or a value in between if both the data and prior distributions provide information.&lt;/p&gt;
&lt;p&gt;WAIC stands out because it averages over the whole posterior distribution rather than relying on a single point estimate, which is what AIC and DIC do. This makes WAIC more relevant when it comes to predicting new data in a Bayesian framework.&lt;/p&gt;
&lt;p&gt;However, using WAIC requires dividing the data into $n$ parts, which can be challenging in certain data setups like time series or spatial data. AIC and DIC don&amp;rsquo;t require this explicit partition, but they assume that residuals are independent given a point estimate $\hat{\theta}$, which may not fully capture posterior uncertainty.&lt;/p&gt;
&lt;h3&gt;Effective Number of Parameters as a Random Variable&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;effective-number-of-parameters-as-a-random-variable&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#effective-number-of-parameters-as-a-random-variable&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The number of parameters estimated in a model, as measured by $p_{\text{DIC}}$ and $p_{\text{WAIC}}$, can vary depending on the observed data.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a simple example: imagine a model where the data $y_1, \cdots, y_n$ follow a normal distribution with a mean parameter $\theta$ and a known standard deviation of $1$. The parameter $\theta$ is drawn from a uniform distribution between $0$ and infinity, meaning it&amp;rsquo;s positive but otherwise not informative.&lt;/p&gt;
&lt;p&gt;Now, consider two scenarios:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Imagine you have a bunch of data points, but they are all very close to zero. In this case, the model has to consider that the parameter $\theta$ could be anywhere from very small positive values up to infinity. However, since the data are all close to zero, they don&amp;rsquo;t provide much information about where $\theta$ might lie. The only constraint is that $\theta$ has to be positive. Because the data don&amp;rsquo;t give a strong indication of where $\theta$ might be, we say the effective number of parameters is roughly half. This is because half of the information about $\theta$ comes from the data, and the other half comes from the prior constraint that $\theta$ must be positive.&lt;/li&gt;
&lt;li&gt;Now, imagine your data points are all large and positive. In this case, the constraint that $\theta$ must be positive doesn&amp;rsquo;t really affect things much because the data already tell us that $\theta$ needs to be positive to explain those large positive values. Since the data provide most of the information about where $\theta$ might lie, we say the effective number of parameters is approximately $1$. This means that the data have a stronger influence on determining $\theta$ in this scenario.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This example shows that even with the same model and true parameters, the effective number of parameters can change depending on the observed data.&lt;/p&gt;
&lt;h3&gt;Bayesian information criterion (BIC)&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bayesian-information-criterion-bic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bayesian-information-criterion-bic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;BIC is a way to decide between different models by considering both how well the model fits the data and how complex the model is. The formula for BIC is:&lt;/p&gt;
$$
\begin{aligned}
-2 \log (p(y|\hat{\theta})) + k \log(n)
\end{aligned}
$$&lt;p&gt;where $p(y|\hat{\theta})$ is the likelihood of the data given the estimated parameters, $k$ is the number of parameters in the model and $n$ is the sample size.&lt;/p&gt;
&lt;p&gt;BIC aims to approximate the marginal probability density of the data under the model, which can be used for comparing models and estimating relative posterior probabilities. BIC tends to favor simpler models for large datasets because it penalizes complexity more, so a complicated model may perform well in predicting data but still have a high BIC due to the penalty for complexity.&lt;/p&gt;
&lt;p&gt;Unlike AIC, which doesn&amp;rsquo;t take the sample size into account, BIC penalizes complex models more as the sample size increases. Unlike AIC, DIC, and WAIC, BIC doesn&amp;rsquo;t focus on predicting future data but rather on estimating the probability of the observed data under the model.&lt;/p&gt;
&lt;h3&gt;Leave-one-out cross-validation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;leave-one-out-cross-validation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#leave-one-out-cross-validation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In Bayesian cross-validation, we split the data into two parts: a training set ($y_{\text{ytrain}}$) and a holdout set ($y_{\text{holdout}}$). We repeat this process multiple times, such that for each split:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We train the model using the training set ($\text{ytrain}$). This gives us a distribution of possible parameter values called $p_{\text{train}}(\theta) = p(\theta|y_{\text{train}})$.&lt;/li&gt;
&lt;li&gt;Then, we use this trained model to make predictions on the holdout set ($y_\text{holdout}$).&lt;/li&gt;
&lt;li&gt;We evaluate the performance of our predictions using the log predictive density:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\log p_{\text{train}}(y_{\text{holdout}}) = \log \mathbb{E}_{\text{post}}[p_{\text{train}}(y_{\text{holdout}})] = \log \int p_{\text{pred}}(y_{\text{holdout}}|\theta)p_{\text{train}}(\theta)d\theta
\end{aligned}
$$&lt;p&gt;Assuming the posterior distribution $p(\theta|y_{\text{train}})$ is summarized by $S$ simulation draws $\theta^s$, we calculate the log predictive density as:&lt;/p&gt;
$$
\begin{aligned}
\log \left(\frac{1}{S} \sum_{s=1}^S p(y_{\text{holdout}}|\theta^s)\right)
\end{aligned}
$$&lt;p&gt;In LOOCV, we split the data into $n$ partitions, where each partition represents a single data point. Performing the analysis for each of the $n$ data points yields n different inferences $p_{\text{post}(-i)}$, each summarized by $S$ posterior simulations, $\theta^{is}$.&lt;/p&gt;
&lt;p&gt;The Bayesian LOO-CV estimate of out-of-sample predictive fit is:&lt;/p&gt;
$$
\begin{aligned}
\text{lppd}_{\text{loo-cv}} = \sum_{i=1}^n \log (p_{\text{post}(-i)}(y_i))
\end{aligned}
$$&lt;p&gt;computed as:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i=1}^n \log \left(\frac{1}{S} \sum_{s=1}^S p(y_i|\theta^{is})\right)
\end{aligned}
$$&lt;p&gt;Where $\theta^s$ represents the $S$ simulations under the posterior distribution $p(\theta|y_{-1})$.&lt;/p&gt;
&lt;p&gt;Each prediction is conditioned on $n − 1$ data points, which causes underestimation of the predictive fit. For large $n$ the difference is negligible, but for small $n$ (or when using $k$-fold cross-validation) we can use a first order bias correction b by estimating how much better predictions would be obtained if conditioning on $n$ data points:&lt;/p&gt;
$$
\begin{aligned}
b = \text{lppd} - \overline{\text{lppd}}_{-i}
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\overline{\text{lppd}}_{-i} = \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \log p_{\text{post}(-i)}(y_j)
\end{aligned}
$$&lt;p&gt;computed as:&lt;/p&gt;
$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n \log \left(\frac{1}{S} \sum_{s=1}^S p(y_j|\theta^{is})\right)
\end{aligned}
$$&lt;p&gt;The bias-corrected Bayesian LOO-CV is then:&lt;/p&gt;
$$
\begin{aligned}
\text{lppd}_{\text{cloo-cv}} = \text{lppd}_{\text{loo-cv}} + b
\end{aligned}
$$&lt;p&gt;The bias correction $b$ is rarely used as it is usually small, but we include it for completeness.&lt;/p&gt;
&lt;p&gt;We compute an estimate of the effective number of parameters as:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{loo-cv}} = \text{lppd} - \text{lppd}_{\text{loo-cv}}
\end{aligned}
$$&lt;p&gt;or, using bias-corrected LOO-CV:&lt;/p&gt;
$$
\begin{aligned}
p_{\text{cloo-cv}} = \text{lppd} - \text{lppd}_{\text{cloo-cv}}
\end{aligned}
$$$$
\begin{aligned}
= \overline{\text{lppd}}_{-i} - \text{lppd}_{\text{loo-cv}}
\end{aligned}
$$&lt;p&gt;CV, like WAIC, requires the data to be split into distinct and ideally independent pieces. This can be challenging for structured models where the data isn&amp;rsquo;t easily divided. Additionally, CV can be computationally expensive, especially if the model needs to be re-fit for each fold. However, there are some shortcuts available, such as Leave-One-Out Cross-Validation (LOO-CV), which can efficiently approximate predictions using the full posterior distribution.&lt;/p&gt;
&lt;p&gt;Under certain conditions, different information criteria (like AIC, DIC, and WAIC) have been shown to be equivalent to LOO-CV as the size of the dataset becomes very large. AIC is equivalent to LOO-CV when using maximum likelihood estimates. DIC is a variation of regularized information criteria that approximates LOO-CV using plug-in predictive densities. WAIC has been shown to be equivalent to Bayesian LOO-CV.&lt;/p&gt;
&lt;p&gt;LOO-CV predicts the outcome for one data point using all other data points except that one. WAIC predicts the outcome for a data point using all observed data points. This difference becomes noticeable when dealing with small datasets or complex models, like hierarchical models. In regression or hierarchical models, LOO-CV focuses on predicting specific data points, while WAIC predicts outcomes based on all observed data. This distinction can be important in models where predictions at one point are only weakly influenced by other data points.&lt;/p&gt;
&lt;h3&gt;Summary&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;summary&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#summary&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;All the different measures discussed above are based on adjusting the log predictive density of the observed data by subtracting an approximate bias correction. The measures differ both in their baseline measures of fit and in their adjustments.&lt;/p&gt;
&lt;p&gt;AIC starts with the log predictive density of the data conditional on the maximum likelihood estimate $\hat{\theta}$, DIC conditions on the posterior mean $\mathbb{E}[\theta|y]$, and WAIC starts with the log predictive density, averaging over $p_{\text{post}}(\theta) = p(\theta|y)$. Of these three approaches, only WAIC is fully Bayesian and so it is our preference when using a bias correction formula. Cross-validation can be applied to any measure of fit; we use the log pointwise posterior predictive density as with WAIC.&lt;/p&gt;
&lt;h2&gt;Model Comparison Based on Predictive Performance&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;model-comparison-based-on-predictive-performance&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#model-comparison-based-on-predictive-performance&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;On the &lt;a href=&#34;../02_models#example-parallel-experiments-in-eight-schools&#34; &gt;eight schools example&lt;/a&gt; we defined three separate models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;No pooling&lt;/strong&gt;: Separate estimates for each of the eight schools, reflecting that the experiments were performed independently. This model has eight parameters: an estimate for each school.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete pooling&lt;/strong&gt;: A combined estimate averaging the data from all schools into a single number, reflecting that the eight schools come from the same population. This model has only one, shared, parameter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical model&lt;/strong&gt;: A Bayesian meta-analysis, partially pooling the eight estimates toward a common mean. This model has eight parameters but they are constrained through their hierarchical distribution and are not estimated independently; thus the effective number of parameters should be some number less than 8.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the following table we show the performance metrics for each of the models using predivtive log densities and information criteria.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/example_evaluation_metrics.png&#34; alt=&#34;Evaluation Metrics&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;AIC&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;aic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#aic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The log predictive density is higher—that is, a better fit—for the &lt;strong&gt;no pooling model&lt;/strong&gt;. This makes sense: with no pooling, the maximum likelihood estimate is right at the data, whereas with complete pooling there is only one number to fit all $8$ schools. However, the ranking of the models changes after adjusting for the fitted parameters ($8$ for no pooling, $1$ for complete pooling), and the expected log predictive density is estimated to be the best (that is, AIC is lowest) for &lt;strong&gt;complete pooling&lt;/strong&gt;. The last column of the table is blank for AIC, as this procedure is defined based on maximum likelihood estimation which is meaningless for the hierarchical model.&lt;/p&gt;
&lt;h4&gt;DIC&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;dic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For both the &lt;strong&gt;no-pooling&lt;/strong&gt; and &lt;strong&gt;complete-pooling&lt;/strong&gt; models with their flat priors, DIC provides results similar to AIC. However, for the &lt;strong&gt;hierarchical model&lt;/strong&gt;, DIC falls in between the two extremes: it fits the data better than complete pooling but not as well as no pooling, and it suggests an effective number of parameters closer to $1$ than to $8$. This indicates that the estimated school effects are mostly pooled back to their common mean. When considering the correction for fitting, complete pooling emerges as the winner, which aligns with the idea that the data support very little variation between groups.&lt;/p&gt;
&lt;h4&gt;WAIC&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;waic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#waic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This Bayesian measure, similar to DIC, indicates slightly worse fit to observed data for each model. This is because the posterior predictive density has a wider distribution, resulting in lower density values at the mode compared to the predictive density conditional on the point estimate. However, the correction for the effective number of parameters is lower with WAIC compared to DIC. For models with &lt;strong&gt;no pooling&lt;/strong&gt; and &lt;strong&gt;hierarchical models&lt;/strong&gt;, the effective number of parameters ($p_{\text{WAIC}}$) is about half of what&amp;rsquo;s estimated by DIC, suggesting that WAIC behaves as expected when there&amp;rsquo;s only one data point per parameter. Conversely, for &lt;strong&gt;complete pooling&lt;/strong&gt;, $p_{\text{WAIC}}$ is only slightly less than $1$, which aligns with expectations given the sample size of $8$. Overall, $p_{\text{WAIC}}$ is much less than pDIC for all three models, mainly because the WAIC already considers much of the uncertainty stemming from parameter estimation.&lt;/p&gt;
&lt;h4&gt;Cross Validation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;cross-validation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#cross-validation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For this example, it&amp;rsquo;s impossible to cross-validate the &lt;strong&gt;no-pooling&lt;/strong&gt; model because it would mean predicting the performance of one school using data from the other seven, which isn&amp;rsquo;t feasible. This highlights a key difference from information criteria, which assume predictions for the same schools and can work even in the absence of pooling.&lt;/p&gt;
&lt;p&gt;However, for the &lt;strong&gt;complete pooling&lt;/strong&gt; and &lt;strong&gt;hierarchical models&lt;/strong&gt;, we can directly perform leave-one-out cross-validation. In this setup, cross-validation predicts based only on information from other schools, while WAIC considers both the local observation and information from other schools. Although both methods predict unknown future data, they differ in the amount of information used. As the hierarchical prior becomes less informative (or more vague), the predictive performance estimates diverge further, with the difference approaching infinity when the hierarchical prior becomes uninformative, effectively yielding the no-pooling model.&lt;/p&gt;
&lt;h4&gt;Comparing the Three Models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;comparing-the-three-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#comparing-the-three-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this dataset, the complete pooling model performs best in predicting new data. Surprisingly, setting the hierarchical variance $\tau$ to zero results in a better fit to the data compared to both no pooling and complete pooling models. However, despite this result, we still prefer the hierarchical model because we don&amp;rsquo;t believe $\tau$ is truly zero.&lt;/p&gt;
&lt;p&gt;For instance, the estimated effects in schools A and C show some differences, although they are not statistically significant. The data suggest that there might be no variation in effects between schools, but we are not entirely confident in this conclusion. Therefore, while the model with $\tau = 0$ performs well, we might consider using a more informative prior distribution for $\tau$ to better capture the uncertainty and avoid implausible scenarios.&lt;/p&gt;
&lt;p&gt;In general, predictive accuracy measures are useful in parallel with posterior predictive checks to see if there are important patterns in the data that are not captured by each model.&lt;/p&gt;
&lt;h3&gt;Evaluating Predictive Error Comparisons&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;evaluating-predictive-error-comparisons&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evaluating-predictive-error-comparisons&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When comparing models for their predictive accuracy, we face two main challenges: &lt;strong&gt;statistical significance&lt;/strong&gt; and &lt;strong&gt;practical significance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Statistical significance arises from the uncertainty in estimating how well a model predicts new data. This uncertainty is due to variation in individual prediction errors, which can affect the averages we calculate from any finite dataset. A practical estimate of related sampling uncertainty can be obtained by analyzing the variation in the expected log predictive densities $\hat{\text{elppd}}$ using parametric or nonparametric approaches.&lt;/p&gt;
&lt;p&gt;In some cases, we can use scoring functions that are familiar to experts in a particular field to understand the significance of differences in predictive accuracy. However, in situations where there are no established measures like AUC, it can be challenging to interpret the significance of differences in log predictive probability between two models. One way to gauge the importance of such differences is by comparing them to simpler models.&lt;/p&gt;
&lt;p&gt;Consider a scenario where we have two models for a survey of voters in an election: one model predicts a $50$/$50$ chance for each voter to support either party, while the other model correctly assigns probabilities of $0.4$ and $0.6$ to the voters. In this case, the improvement in log predictive probability from using the better model can be calculated. For instance, if we have $1000$ voters, the improvement would be $20$, but for only $10$ voters, the improvement would be just $2$. This aligns with our intuition: a clear improvement in prediction is more noticeable in a larger dataset than in a smaller one where noise might overshadow the improvement.&lt;/p&gt;
&lt;h3&gt;Bias Induced by Model Selection&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bias-induced-by-model-selection&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bias-induced-by-model-selection&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Cross-validation and information criteria are methods that adjust for using the data twice—once for building the model and again for evaluating its performance. They aim to provide unbiased estimates of how well a model predicts new data. However, when these methods are used to select a model from multiple options, the estimate of predictive performance for the chosen model can be biased because of the selection process.&lt;/p&gt;
&lt;p&gt;When there are only a few models to compare, any bias introduced by the selection process is usually small. However, if there are many models to choose from, especially as the number of observations or predictors increases, the selection process can lead to significant overfitting. While it&amp;rsquo;s possible to estimate and correct for this bias using additional cross-validation, it doesn&amp;rsquo;t guarantee that the selected model will have the best predictive performance. Therefore, cross-validation and information criteria are better suited for understanding models rather than selecting the best one among many options.&lt;/p&gt;
&lt;h3&gt;Challenges&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;challenges&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#challenges&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The methods we have for measuring how well predictive models fit still have their flaws. AIC, DIC, and WAIC don&amp;rsquo;t always work perfectly: AIC struggles with strong prior information, DIC gives odd results when the average of the posterior distribution isn&amp;rsquo;t reliable, and WAIC can be tricky to use with structured data like spatial or network data. Cross-validation seems like a good alternative, but it can be slow to compute and doesn&amp;rsquo;t always work well with dependent data.&lt;/p&gt;
&lt;p&gt;Bayesian statisticians often don&amp;rsquo;t rely solely on predictive error comparisons in their work because of various limitations. However, there are situations where comparing very different models is necessary, and in those cases, predictive comparisons can be valuable. Additionally, measures of effective numbers of parameters are useful for understanding statistical methods.&lt;/p&gt;
&lt;p&gt;Currently, we prefer cross-validation because it&amp;rsquo;s similar to WAIC in large samples. However, in finite cases with weak priors or strong outliers, Pareto-smoothed importance sampling LOO-CV is both computationally efficient and robust.&lt;/p&gt;
&lt;h2&gt;Model Comparison Using Bayes Factors&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;model-comparison-using-bayes-factors&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#model-comparison-using-bayes-factors&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Example: A discrete example in which Bayes factors are helpful&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-a-discrete-example-in-which-bayes-factors-are-helpful&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-a-discrete-example-in-which-bayes-factors-are-helpful&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In the genetics example we talked about earlier, we can use Bayes factors to help us make sense of things. Imagine we have two possible scenarios: $H_1$, where the woman is affected, and $H_2$, where she&amp;rsquo;s not affected. We can represent these scenarios using some numbers. For example, let&amp;rsquo;s say $\theta = 1$ means she&amp;rsquo;s affected, and $\theta = 0$ means she&amp;rsquo;s not.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s say before we look at any data, we&amp;rsquo;re equally likely to believe either scenario. So, the odds of H2 compared to H1 are 1 to 1, that is $\frac{p(H_2)}{p(H_1)} = 1$.&lt;/p&gt;
&lt;p&gt;Then, when we look at the data and find out the woman has two unaffected sons, the data is $4$ times more likely under $H_2$ than under $H_1$. That is $\frac{p(y|H_2)}{p(y|H_1)} = \frac{1.0}{0.25}$. The posterior odds are thus $\frac{p(H_2|y)}{p(H_1|y)} = 4$&lt;/p&gt;
&lt;p&gt;This example is helpful for Bayes factors because the scenarios we&amp;rsquo;re comparing make sense scientifically, and there are no other possible scenarios in between. Also, the way the data fits with each scenario makes sense and gives us clear results.&lt;/p&gt;
&lt;p&gt;Bayes factors don&amp;rsquo;t work as well for models that are continuous. For instance, if we&amp;rsquo;re looking at something like the effectiveness of a treatment, which can vary along a scale, it doesn&amp;rsquo;t make sense to assign a probability to it being exactly zero.&lt;/p&gt;
&lt;p&gt;Similarly, if we&amp;rsquo;re comparing different models in regression, like deciding which variables to include, it&amp;rsquo;s better to have all the possible variables in our consideration. We can then use a prior distribution to decide how much to trust each variable, even if we think some might not have much impact. To show why Bayes factors struggle with continuous models, let&amp;rsquo;s consider the example of the 8 schools problem, comparing the no-pooling and complete-pooling models.&lt;/p&gt;
&lt;p&gt;== Example. A continuous example where Bayes factors are a distraction ==&lt;/p&gt;
&lt;p&gt;Suppose we had analyzed the data from the 8 schools using Bayes factors for the discrete collection of previously proposed standard models, no pooling ($H_1$) and complete pooling ($H_2$):&lt;/p&gt;
$$
\begin{aligned}
H_1: p(y|\theta_1, \cdots, \theta_J) = \prod_{j=1}^J text{N}(y_j|\theta_j, \sigma_j^2), p(\theta_1, \cdots, \theta_J) \propto 1
\end{aligned}
$$$$
\begin{aligned}
H_2: p(y|\theta_1, \cdots, \theta_J) = \prod_{j=1}^J text{N}(y_j|\theta_j, \sigma_j^2), \theta_1 = \cdots = \theta_J = \theta \propto 1
\end{aligned}
$$&lt;p&gt;If we try to use Bayes factors to pick or combine these models, we run into a problem. The Bayes factor, which is the ratio of how likely the data is under one model compared to another, isn&amp;rsquo;t defined here. That&amp;rsquo;s because the prior distributions we&amp;rsquo;re using are improper, which means they don&amp;rsquo;t behave properly in the calculations. Specifically, when we try to divide one function by another, we end up with $\frac{0}{0}$, which doesn&amp;rsquo;t give us a clear answer.&lt;/p&gt;
&lt;p&gt;So, if we want to stick with the idea of assigning probabilities to these two specific models, we have two options: either use proper prior distributions or carefully construct improper ones in a way that makes sense. However, no matter which route we take, the results won&amp;rsquo;t be very satisfying.&lt;/p&gt;
&lt;p&gt;More explicitly, suppose we replace the flat prior distributions in $H_1$ and $H_2$ by independent normal prior distributions, $\text{N}(0, A^2)$, for some large $A$. The resulting posterior distribution for the effect in school $j$ is:&lt;/p&gt;
$$
\begin{aligned}
p(\theta_j|y) = (1 - \lambda)p(\theta_j|y, H_1) + \lambda p(\theta_j|y, H_2)
\end{aligned}
$$&lt;p&gt;The Bayes factor, which compares how likely the data is under different models, is very sensitive to the prior variance, which is represented by $A^2$. As we increase $A$ (while keeping the data and prior odds fixed), the results tend to favor one model over the other more strongly. This means that Bayes factors can&amp;rsquo;t be reliably used with non-informative prior densities, even if we carefully define them in certain ways.&lt;/p&gt;
&lt;p&gt;Another problem with Bayes factors in this example is that they behave differently as we change the number of schools in the model. The results can vary significantly depending on how many schools are included, which doesn&amp;rsquo;t make much sense from a scientific perspective.&lt;/p&gt;
&lt;p&gt;So, if we were to use Bayes factors here, we&amp;rsquo;d likely run into issues during the model-checking stage, where we compare the model&amp;rsquo;s predictions to what we know from real-world knowledge. Instead, it might be better to use a smoother, continuous family of models that bridges the gap between the extreme models. This continuous model doesn&amp;rsquo;t assign discrete probabilities to extreme values that don&amp;rsquo;t make scientific sense.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Aspectos Computacionales de la Inferencia Bayesiana</title>
      <link>//localhost:1313/notes/datascience/master/mbj/04_computation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/mbj/04_computation/</guid>
      <description>
        
        
        &lt;h2&gt;Gibbs Sampler&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gibbs-sampler&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gibbs-sampler&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Introduction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;introduction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#introduction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Many smart techniques have been developed to create and sample from different types of posterior distributions. One common method is called Markov chain simulation, also known as &lt;strong&gt;Markov chain Monte Carlo&lt;/strong&gt; (MCMC). It works by first drawing values of a parameter (usually denoted as $\theta$) from rough estimates of the distribution, and then adjusting those draws to better match the actual distribution we&amp;rsquo;re interested in, called the posterior distribution, denoted as $p(\theta|y)$. The drawing process is done step by step, with each draw depending on the previous one, forming what&amp;rsquo;s called a Markov chain.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/markov_chain_example.png&#34; alt=&#34;Markov Chain Exmaple&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Figure 11.1 shows a simple example of Markov chain simulation, using what&amp;rsquo;s called a Metropolis algorithm. Imagine we have a parameter called $\theta$ that is a vector with two components, such that $\theta \sim \text{N}(0, I)$. Now, let&amp;rsquo;s look at Figure 11.1a, which shows the early steps of this simulation. The picture represents all the possible values that $\theta$ can take, and each of the five squiggly lines shows the path taken by a random walk. These random walks start either near the center or at the edges of the distribution and move around based on a series of random steps.&lt;/p&gt;
&lt;p&gt;In Figure 11.1b, we see the later stages of the same simulation. Each of the random walks has now traced a path throughout the entire space of possible $\theta$ values. They&amp;rsquo;ve settled into a common pattern, which matches the target distribution we&amp;rsquo;re interested in.&lt;/p&gt;
&lt;p&gt;Finally, in Figure 11.1c, we can use the information gathered from the second halves of these simulated random walks to make inferences about $\theta$.&lt;/p&gt;
&lt;p&gt;In our use of Markov chain simulation, we create multiple separate sequences. Each sequence starts from a particular point, like $\theta_0$, and then we move step by step, drawing a new value $\theta_t$ from a transition distribution $T_t(\theta_t|\theta_{t−1})$, which depends on the previous draw. Markov chain simulation is used when it is not possible to sample $\theta$ directly from $p(\theta|y)$ instead we sample iteratively in such a way that at each step of the process we expect to draw from a distribution that becomes closer to $p(\theta|y)$.&lt;/p&gt;
&lt;p&gt;Once the simulation algorithm has been implemented and the simulations drawn, it is absolutely necessary to check the convergence of the simulated sequences; for example, the simulations of Figure 11.1a are far from convergence and are not close to the target distribution.&lt;/p&gt;
&lt;h3&gt;Gibbs Sampler&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gibbs-sampler-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gibbs-sampler-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Imagine you have a $d$-dimensional parameter vector $\theta$ that&amp;rsquo;s been split into smaller parts, such that $\theta = (\theta_1, \cdots, \theta_d)$. Each time the Gibbs sampler runs, it goes through each of the dimensions, one at a time, while keeping the rest fixed. So, if there are $d$ dimensions, there are $d$ steps in each iteration $t$.&lt;/p&gt;
&lt;p&gt;At each iteration, we pick an order for the $d$ parts of $\theta$. Then, we go through each part one by one and we sample a new value for each part based on the current values of all the other parts. This continues for each part until we&amp;rsquo;ve updated all of them once, and then we start the process over again for the next iteration.&lt;/p&gt;
$$
\begin{aligned}
p(\theta_j|\theta_{-j}^{t-1}, y)
\end{aligned}
$$&lt;p&gt;where $\theta_{-j}^{t-1}$ represents all the components of $\theta$ except for $\theta_j$ at their current values:&lt;/p&gt;
$$
\begin{aligned}
\theta_{-j}^{t - 1} = (\theta_1^t, \cdots, \theta^t_{j - 1}, \theta_{j + 1}^{t - 1}, \theta_{d}^{t - 1})
\end{aligned}
$$&lt;p&gt;Thus, each subvector $\theta_j$ is updated conditional on the latest values of the other components of $\theta$, which are the iteration $t$ values for the components already updated and the iteration $t − 1$ values for the others.&lt;/p&gt;
&lt;p&gt;Here, we illustrate the workings of the Gibbs sampler with a simple example.&lt;/p&gt;
&lt;h4&gt;Example: Bivariate Normal Distribution&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-bivariate-normal-distribution&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-bivariate-normal-distribution&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Consider a single observation $(y_1, y_2)$ from a bivariate normally distributed population with unknown mean $\theta = (\theta_1, \theta_2)$ and known covariance matrix $\begin{bmatrix}1 &amp;amp; \rho \ \rho &amp;amp; 1\end{bmatrix}$. With a uniform prior distribution on $\theta$, the posterior distribution is:&lt;/p&gt;
$$
\begin{aligned}
\begin{bmatrix}
\theta_1 \\
\theta_2 \\
\end{bmatrix} | y \sim \text{N}(\begin{bmatrix}
y_1 \\
y_2 \\
\end{bmatrix}, \begin{bmatrix}
1 &amp; \rho \\
\rho &amp; 1 \\
\end{bmatrix})
\end{aligned}
$$&lt;p&gt;We need the conditional posterior distributions, which, from the properties of the multivariate normal distribution, are:&lt;/p&gt;
$$
\begin{aligned}
\theta_1 |\theta_2, y \sim \text{N}(y_1 + \rho(\theta_2 - y_2), 1 - \rho^2)
\end{aligned}
$$$$
\begin{aligned}
\theta_2 |\theta_1, y \sim \text{N}(y_2 + \rho(\theta_1 - y_1), 1 - \rho^2)
\end{aligned}
$$&lt;p&gt;The Gibbs sampler proceeds by alternately sampling from these two normal distributions. Figure 11.2 illustrates for the case $\rho = 0.8$, data $(y_1, y_2) = (0, 0)$, and four independent sequences started at $(\pm 2.5, \pm 2.5)$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/gibb_sampler_example.png&#34; alt=&#34;Gibb Sampler Exampler&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Metropolis and Metropolis-Hastings Algorithms&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;metropolis-and-metropolis-hastings-algorithms&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#metropolis-and-metropolis-hastings-algorithms&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;The Metropolis algorithm&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-metropolis-algorithm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-metropolis-algorithm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The Metropolis algorithm is an adaptation of a random walk with an acceptance/rejection rule to converge to the specified target distribution. The algorithm proceeds as follows.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Draw a starting point $\theta_0$, for which $p(\theta_0|y) &amp;gt; 0$, from a starting distribution $p_0(\theta)$. The starting distribution might be based on an approximation or we may simply choose starting values dispersed around a crude approximate estimate.&lt;/li&gt;
&lt;li&gt;For $t = 1, 2, \cdots$:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Sample a proposal $\theta^\ast$ from a jumping distribution (or proposal distribution) at time $t$, $J_t(\theta^\ast|\theta^{t-1})$. For the Metropolis algorithm (but not the Metropolis-Hastings algorithm, as discussed later in this section), the jumping distribution must be symmetric.&lt;/li&gt;
&lt;li&gt;Calculate the ratio of the densities:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
r = \frac{p(\theta^*|y)}{p(\theta^{t- 1}|y)}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Set:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\theta^t = \begin{cases}
\theta^* &amp; \text{ with probability } \min(r, 1) \\
\theta^{t-1} \text{ otherwise }
\end{cases}
\end{aligned}
$$&lt;p&gt;The acceptance/rejection rule of the Metropolis algorithm can be stated as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the jump increases the posterior density, set $\theta^t = \theta^\ast$;&lt;/li&gt;
&lt;li&gt;If the jump decreases the posterior density, set $\theta^t = \theta^\ast$ with probability equal to the density ratio, $r$, otherwise set $\theta_t = \theta^{t - 1}$ (with probability $1 - r$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Metropolis algorithm can thus be viewed as a stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density but only sometimes accepting downward steps.&lt;/p&gt;
&lt;p&gt;To use the algorithm, we need to calculate the ratio $r$ for every pair of $(\theta, \theta^\ast)$, and we also need to choose $\theta$ from the jumping distribution $J_t(\theta^\ast|\theta)$ for all $\theta$ and $t$. Additionally, we need to generate a random number for step ($c$) in the process.&lt;/p&gt;
&lt;p&gt;Even if the jump isn&amp;rsquo;t accepted and $\theta_t$ equals $\theta_{t-1}$, it still counts as a step in the algorithm.&lt;/p&gt;
&lt;h4&gt;Example: Bivariate Unit Normal Density with Normal Jumping Kernel&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-bivariate-unit-normal-density-with-normal-jumping-kernel&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-bivariate-unit-normal-density-with-normal-jumping-kernel&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For simplicity, we illustrate the Metropolis algorithm with the simple example of the bivariate unit normal distribution. The target density is the bivariate unit normal, $p(\theta|y) = \text{N}(\theta|0, I)$. The jumping distribution is also bivariate normal, centered at the current iteration and scaled to $\frac{1}{5}$ the size: $J_t(\theta^\ast|\theta^{t−1}) = \text{N}(\theta^\ast|\theta^{t−1}, 0.22\cdot I)$.&lt;/p&gt;
&lt;p&gt;At each step, it is easy to calculate the density ratio:&lt;/p&gt;
$$
\begin{aligned}
r = \frac{\text{N}(\theta^\ast|0, I)}{\text{N}(\theta^{t-1}|0, I)}
\end{aligned}
$$&lt;p&gt;It is clear from the form of the normal distribution that the jumping rule is symmetric. &lt;a href=&#34;#introduction&#34; &gt;Figure 11.1&lt;/a&gt; displays five simulation runs starting from different points. We have purposely set the scale of this jumping algorithm to be too small, relative to the target distribution, so that the algorithm will run inefficiently and its random-walk aspect will be obvious in the figure.&lt;/p&gt;
&lt;h3&gt;Why does the Metropolis Algorithm Work?&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;why-does-the-metropolis-algorithm-work&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#why-does-the-metropolis-algorithm-work&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The proof that the sequence of iterations $\theta_1, \theta_2, \cdots$ converges to the target distribution has two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It is shown that the simulated sequence is a Markov chain with a unique stationary distribution.&lt;/li&gt;
&lt;li&gt;It is shown that the stationary distribution equals the target distribution.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Except for trivial exceptions, the latter two conditions hold for a random walk on any proper distribution, and irreducibility holds as long as the jumping distributions Jt is eventually be able to jump to all states with positive probability.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To show (1) consider starting the algorithm at time $t − 1$ with a draw $\theta^{t−1}$ from the target distribution $p(\theta|y)$. Now consider any two such points $\theta_a$ and $\theta_b$, drawn from $p(\theta|y)$ and labeled so that $p(\theta_b|y) \geq p(\theta_a|y)$. The unconditional probability density of a transition from $\theta_a$ to $\theta_b$ is:&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{aligned}
p(\theta^{t - 1} = \theta_a, \theta^t = \theta_b) = p(\theta_a|y)J_t(\theta_b|\theta_a)
\end{aligned}
$$&lt;p&gt;where the acceptance probability is $1$ because of our labeling of $a$ and $b$, and the unconditional probability density of a transition from $\theta_b$ to $\theta_a$ is:&lt;/p&gt;
$$
\begin{aligned}
p(\theta^t = \theta_a, \theta^{t-1} = \theta_b) = p(\theta_b|y)J_t(\theta_a|\theta_b) \left(\frac{p(\theta_a|y)}{p(\theta_b|y)}\right)
\end{aligned}
$$$$
\begin{aligned}
= p(\theta_b|y)J_t(\theta_a|\theta_b)
\end{aligned}
$$&lt;p&gt;which is the same as the probability of a transition from $\theta_a$ to $\theta_b$, since we have required that $J_t(\cdot|\cdot)$ be symmetric.&lt;/p&gt;
&lt;p&gt;Since their joint distribution is symmetric, $\theta^t$ and $\theta^{t−1}$ have the same marginal distributions, and so $p(\theta|y)$ is the stationary distribution of the Markov chain of $\theta$.&lt;/p&gt;
&lt;h3&gt;The Metropolis-Hastings Algorithm&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;the-metropolis-hastings-algorithm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-metropolis-hastings-algorithm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The Metropolis-Hastings algorithm generalizes the basic Metropolis algorithm presented above in two ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The jumping rules $J_t$ need no longer be symmetric.&lt;/li&gt;
&lt;li&gt;To correct for the asymmetry in the jumping rule the ratio $r$ is replaced by a ratio of ratios:&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{aligned}
r = \frac{\frac{p(\theta^\ast|y)}{J_t(\theta^\ast|\theta^{t-1})}}{\frac{p(\theta^{t-1}|y)}{J_t(\theta^{t-1}|\theta^\ast)}}
\end{aligned}
$$&lt;p&gt;Allowing asymmetric jumping rules can be useful in increasing the speed of the random walk.&lt;/p&gt;
&lt;h3&gt;Relation Between the Jumping Rule and Efficiency of Simulations&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;relation-between-the-jumping-rule-and-efficiency-of-simulations&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#relation-between-the-jumping-rule-and-efficiency-of-simulations&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The ideal Metropolis-Hastings jumping rule is simply to sample the proposal, $\theta^\ast$, from the target distribution; such that our jumping distribution is equal to the target distribution, $J(\theta^\ast|\theta) ≡ p(\theta^\ast|y)$. Then the ratio $r$ is always exactly $1$, so we always choose the new sampled $\theta^\ast$ to update $\theta^t$ instead of remaining with $\theta^{t-1}$.&lt;/p&gt;
&lt;p&gt;A good jumping distribution has the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For any $\theta$, it is easy to sample from $J(\theta^\ast|\theta)$&lt;/li&gt;
&lt;li&gt;It is easy to compute the ratio $r$&lt;/li&gt;
&lt;li&gt;Each jump goes a reasonable distance in the parameter space (otherwise the random walk moves too slowly).&lt;/li&gt;
&lt;li&gt;The jumps are not rejected too frequently (otherwise the random walk wastes too much time standing still).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Using Gibbs and Metropolis as Building Blocks&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;using-gibbs-and-metropolis-as-building-blocks&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#using-gibbs-and-metropolis-as-building-blocks&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Interpretation of the Gibbs Sampler as a Special Case of the Metropolis-Hastings Algorithm&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;interpretation-of-the-gibbs-sampler-as-a-special-case-of-the-metropolis-hastings-algorithm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#interpretation-of-the-gibbs-sampler-as-a-special-case-of-the-metropolis-hastings-algorithm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We first define iteration $t$ to consist of a series of $d$ steps, with step $j$ of iteration $t$ corresponding to an update of the subvector $\theta_j$ conditional on all the other elements of $\theta$. Then the jumping distribution, $J_{j,t}(\cdot|\cdot)$ is defined as follows:&lt;/p&gt;
$$
\begin{aligned}
J_{j, t}^{\text{Gibbs}}(\theta^*|\theta^{t-1}) = \begin{cases}
p(\theta_j^**|\theta_{-j}^{t-1}, y) &amp; \text{ if } \theta_{-j}^** = \theta_{-j}^{t-1} \\
0 &amp; \text{ otherwise}
\end{cases}
\end{aligned}
$$&lt;p&gt;Such that at step $j$ of iteration $t$ it only jumps along the $j$th subvector, and does so with the conditional posterior density of $\theta_j$ given $\theta_{-j}^{t - 1}$.&lt;/p&gt;
&lt;p&gt;The only possible jumps are to parameter vectors $\theta^\ast$ that match $\theta^{t-1}$ on all components other than the $j$th. Under this jumping distribution, the ratio at the $j$th step of iteration t is:&lt;/p&gt;
$$
\begin{aligned}
r = \frac{\frac{p(\theta^\ast|y)}{J_{j, t}^{\text{Gibbs}}(\theta^\ast|\theta^{t-1})}}{\frac{p(\theta^{t-1}|y)}{J_{j,t}^{\text{Gibbs}}(\theta^{t-1}|\theta^\ast)}}
\end{aligned}
$$$$
\begin{aligned}
= \frac{\frac{p(\theta^\ast|y)}{p(\theta_j^\ast|\theta^{t-1}_{-j}, y)}}{\frac{p(\theta^{t-1}|y)}{p(\theta^{t-1}_j|\theta^{t-1}_{-j}, y)}}
\end{aligned}
$$$$
\begin{aligned}
= \frac{p(\theta^{t-1}_{-j}, y)}{p(\theta^{t-1}_{-j}, y)}
\end{aligned}
$$$$
\begin{aligned}
= 1
\end{aligned}
$$&lt;p&gt;and thus every jump is accepted. The second line above follows from the first because, under this jumping rule, $\theta^\ast$ differs from $\theta^{t−1}$ only in the $j$th component. The third line follows from the second by applying the rules of conditional probability to $\theta = (\theta_j, \theta_{−j})$ and noting that $\theta^\ast_{-j} = \theta^{t-1}_{-j}$.&lt;/p&gt;
&lt;p&gt;It is possible to define Gibbs sampling without the restriction that each component be updated in each iteration, as long as each component is updated periodically.&lt;/p&gt;
&lt;h3&gt;Gibbs Sampler with Approximations&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gibbs-sampler-with-approximations&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gibbs-sampler-with-approximations&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For some problems, sampling from some, or all, of the conditional distributions $p(\theta_j|\theta_{−j}, y)$ is impossible, but one can construct approximations, which we label $g(\theta_j|\theta_{−j})$, from which sampling is possible. The jumping function at the jth Metropolis step at iteration $t$ is then:&lt;/p&gt;
$$
\begin{aligned}
J_{j, t}(\theta^\ast|\theta^{t-1}) = \begin{cases}
g(\theta^\ast_j|\theta_{-j}^{t-1}) &amp; \text{ if } \theta^\ast_{-j} = \theta^{t-1}_{-j} \\
0 &amp; \text{ otherwise }
\end{cases}
\end{aligned}
$$&lt;p&gt;and the ratio $r$ must be computed and the acceptance or rejection of $\theta^\ast$ decided.&lt;/p&gt;
&lt;h2&gt;Inference and Assessing Convergence&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inference-and-assessing-convergence&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inference-and-assessing-convergence&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Difficulties of Inference from Iterative Simulation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;difficulties-of-inference-from-iterative-simulation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#difficulties-of-inference-from-iterative-simulation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Iterative simulation adds two challenges to simulation inference:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the iterations have not proceeded long enough the simulations may be unrepresentative of the target distribution (&lt;a href=&#34;#introduction&#34; &gt;Figure 11.1a&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The iterative simulation draws present within-sequence correlation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Serial correlation in the simulations is not necessarily a problem because, at convergence, the draws are identically distributed as $p(\theta|y)$. But such correlation can cause inefficiencies in simulations.&lt;/p&gt;
&lt;p&gt;We handle these problems as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We design the simulation runs to allow effective monitoring of convergence by simulating multiple sequences with starting points dispersed throughout parameter space.&lt;/li&gt;
&lt;li&gt;We monitor the convergence of all quantities of interest by comparing variation between and within simulated sequences until &amp;ldquo;within&amp;rdquo; variation roughly equals &amp;ldquo;between&amp;rdquo; variation. Only when the distribution of each simulated sequence is close to the distribution of all the sequences mixed together can they all be approximating the target distribution.&lt;/li&gt;
&lt;li&gt;If the simulation efficiency is low, the algorithm may be altered.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Discarding Early iterations of the Simulation Runs&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;discarding-early-iterations-of-the-simulation-runs&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#discarding-early-iterations-of-the-simulation-runs&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To diminish the influence of the starting values, we discard the first half of each sequence and focus attention on the second half. So our inferences will be based on the assumption that the distributions of the simulated values $\theta_t$, for large enough $t$, are close to the target distribution, $p(\theta|y)$.&lt;/p&gt;
&lt;p&gt;We refer to the practice of discarding early iterations in Markov chain simulation as warm-up. Depending on the context, different warm-up fractions (number of elements on the sequence to discard) can be appropriate.&lt;/p&gt;
&lt;h3&gt;Dependence of the Iterations in each Sequence&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;dependence-of-the-iterations-in-each-sequence&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dependence-of-the-iterations-in-each-sequence&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Once approximate convergence has been reached, is whether to thin the sequences by keeping every $k$th simulation draw from each sequence and discarding the rest. Whether or not the sequences are thinned, if the sequences have reached approximate convergence, they can be directly used for inferences about the parameters $\theta$ and any other quantities of interest.&lt;/p&gt;
&lt;h3&gt;Multiple Sequences with Overdispersed Starting Points&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;multiple-sequences-with-overdispersed-starting-points&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#multiple-sequences-with-overdispersed-starting-points&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Our recommended approach to assessing convergence of iterative simulation is based on comparing different simulated sequences, as illustrated in &lt;a href=&#34;#introduction&#34; &gt;Figure 11.1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Figure 11.1a, the multiple sequences clearly have not converged; the variance within each sequence is much less than the variance between sequences. Later, in Figure 11.1b, the sequences have mixed, and the two variance components are essentially equal.&lt;/p&gt;
&lt;h3&gt;Monitoring Scalar Estimands&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;monitoring-scalar-estimands&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#monitoring-scalar-estimands&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We monitor each scalar estimand or other scalar quantities of interest separately. Estimands include all the parameters of interest in the model and any other quantities of interest (for example, the ratio of two parameters or the value of a predicted future observation). It is often useful also to monitor the value of the logarithm of the posterior density, which has probably already been computed if we are using a version of the Metropolis algorithm.&lt;/p&gt;
&lt;h3&gt;Challenges of Monitoring Convergence: Mixing and Stationarity&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;challenges-of-monitoring-convergence-mixing-and-stationarity&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#challenges-of-monitoring-convergence-mixing-and-stationarity&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Figure 11.3a illustrates that, to achieve convergence, the sequences must together have mixed. The second graph in Figure 11.3 shows two chains that have mixed, in the sense that they have traced out a common distribution, but they do not appear to have converged. Figure 11.3b illustrates that, to achieve convergence, each individual sequence must reach stationarity.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/convergence_monitorization.png&#34; alt=&#34;Convergence Monitorization&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So to check convergence we have to simultaneously tests mixing (if all the chains have mixed well, the separate parts of the different chains should also mix) and stationarity (at stationarity, the first and second half of each sequence should be traversing the same distribution).&lt;/p&gt;
&lt;h3&gt;Splitting each Saved Sequence into Two Parts&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;splitting-each-saved-sequence-into-two-parts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#splitting-each-saved-sequence-into-two-parts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We diagnose convergence (as noted above, separately for each scalar quantity of interest) by checking mixing and stationarity. Our approach consists on splitting each chain in half and check that all the resulting halfsequences have mixed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We start with some number of simulated sequences in which the warm-up period has already been discarded.&lt;/li&gt;
&lt;li&gt;We then take each of these chains and split into the first and second half.&lt;/li&gt;
&lt;li&gt;Let $m$ be the number of chains (after splitting) and $n$ be the length of each chain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, suppose we simulate $5$ chains, each of length $1000$, and then discard the first half of each as warm-up. We are then left with $5$ chains, each of length $500$, and we split each into two parts: iterations $1–250$ (originally iterations $501–750$) and iterations $251–500$ (originally iterations $751–1000$). We now have $m = 10$ chains, each of length $n = 250$.&lt;/p&gt;
&lt;h3&gt;Assessing Mixing using Between- and Within-Sequence Variances&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;assessing-mixing-using-between--and-within-sequence-variances&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#assessing-mixing-using-between--and-within-sequence-variances&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For each scalar estimand $\psi$, we label the simulations as $\psi_{ij}, (i = 1, \cdots, n; j = 1, \cdots, m)$, and we compute $B$ and $W$, the between- and within-sequence variances:&lt;/p&gt;
$$
\begin{aligned}
B = \frac{n}{m - 1} \sum_{j=1}^m (\overline{\psi}_{.j} - \overline{\psi}_{..})^2
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\overline{\psi}_{.j} = \frac{1}{n} \sum_{i=1}^n \psi_{ij}
\end{aligned}
$$$$
\begin{aligned}
\overline{\psi}_{..} = \frac{1}{m} \sum_{j=1}^m \overline{\psi}_{.j}
\end{aligned}
$$&lt;p&gt;and&lt;/p&gt;
$$
\begin{aligned}
W = \frac{1}{m} \sum_{j=1}^m s_{j}^2
\end{aligned}
$$&lt;p&gt;where&lt;/p&gt;
$$
\begin{aligned}
s^2_j = \frac{1}{n - 1} \sum_{i = 1}^n (\psi_{ij} - \overline{\psi}_{.j})^2
\end{aligned}
$$&lt;p&gt;We can estimate $\mathbb{V}[\psi|y]$, the marginal posterior variance of the estimand, by a weighted average of $W$ and $B$, namely:&lt;/p&gt;
$$
\begin{aligned}
\hat{\mathbb{V}}^+[\psi|y] = \frac{n - 1}{n}W + \frac{1}{n} B
\end{aligned}
$$&lt;p&gt;This quantity overestimates the marginal posterior variance assuming the starting distribution is appropriately overdispersed, but is unbiased under stationarity.&lt;/p&gt;
&lt;p&gt;Meanwhile, for any finite $n$, the &amp;ldquo;within&amp;rdquo; variance $W$ should be an underestimate of $\mathbb{V}[\psi|y]$ because the individual sequences have not had time to range over all of the target distribution and, as a result, will have less variability; in the limit as $n \rightarrow \infty$, the expectation of $W$ approaches $\mathbb{V}[\psi|y]$.&lt;/p&gt;
&lt;p&gt;We monitor convergence of the iterative simulation by estimating the factor by which the scale of the current distribution for $\psi$ might be reduced if the simulations were continued in the limit $n \rightarrow \infty$. This potential scale reduction is estimated by:&lt;/p&gt;
$$
\begin{aligned}
\hat{R} = \sqrt{\frac{\hat{\mathbb{V}}[\psi|y]}{W}}
\end{aligned}
$$&lt;p&gt;which declines to $1$ as $n \rightarrow 1$. If the potential scale reduction is high, then we have reason to believe that proceeding with further simulations may improve our inference about the target distribution of the associated scalar estimand.&lt;/p&gt;
&lt;h4&gt;Example. Bivariate Unit Normal Density with Bivariate Normal Jumping Kernel (continued)&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-bivariate-unit-normal-density-with-bivariate-normal-jumping-kernel-continued&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-bivariate-unit-normal-density-with-bivariate-normal-jumping-kernel-continued&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Table 11.1 displays posterior inference for the two parameters of the distribution as well as the log posterior density.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/simulation_convergence_evaluation.png&#34; alt=&#34;Simulation Convergence Evaluation&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;After $50$ iterations, the variance between the five sequences is much greater than the variance within, for all three univariate summaries considered. However, the five simulated sequences have converged adequately after $2000$ or certainly $5000$ iterations for the quantities of interest.&lt;/p&gt;
&lt;p&gt;The comparison with the true target distribution shows how some variability remains in the posterior inferences even after the Markov chains have converged.&lt;/p&gt;
&lt;p&gt;The method of monitoring convergence presented here has the key advantage of not requiring the user to examine time series graphs of simulated sequences. Inspection of such plots is a notoriously unreliable method.&lt;/p&gt;
&lt;h2&gt;Effective Number of Simulation Draws&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;effective-number-of-simulation-draws&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#effective-number-of-simulation-draws&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Bounded or Long-Tailed Distributions&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bounded-or-long-tailed-distributions&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bounded-or-long-tailed-distributions&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The above convergence diagnostics are based on means and variances, and they will not work so well for parameters or scalar summaries for which the posterior distribution, $p(\phi|y)$, is far from Gaussian.&lt;/p&gt;
&lt;p&gt;For summaries $\phi$ whose distributions are constrained or otherwise far from normal, we can preprocess simulations using transformations before computing the potential scale reduction factor $\hat{R}$ and the effective sample size $\hat{n}_{eff}$.&lt;/p&gt;
&lt;h3&gt;Stopping the Simulations&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;stopping-the-simulations&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#stopping-the-simulations&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We monitor convergence for the entire multivariate distribution, $p(\theta|y)$, by computing the potential scale reduction factor $\hat{R}$ and the effective sample size $\hat{n}_{eff}$ for each scalar summary of interest.&lt;/p&gt;
&lt;p&gt;We recommend computing the potential scale reduction for all scalar estimands of interest; if $\hat{R}$ is not near $1$ for all of them, continue the simulation runs. We can use effective sample size $\hat{n}_{eff}$ to give us a sense of the precision obtained from our simulations.&lt;/p&gt;
&lt;p&gt;Once $\hat{R}$ is near $1$ and $\hat{n}_{eff}$ is more than $10$ per chain for all scalar estimands of interest, just collect the $mn$ simulations (with warm-up iterations already excluded) and treat them as a sample from the target distribution.&lt;/p&gt;
&lt;p&gt;As a default rule, we suggest running the simulation until $\hat{n}_{eff}$ is at least $5m$, that is, until there are the equivalent of at least $10$ independent draws per sequence. For some purposes, more precision will be desired, and then a higher effective sample size threshold can be used.&lt;/p&gt;
&lt;p&gt;Even if an iterative simulation appears to converge and has passed all tests of convergence, it still may actually be far from convergence if important areas of the target distribution were not captured by the starting distribution and are not easily reachable by the simulation algorithm. When we declare approximate convergence, we are actually concluding that each individual sequence appears stationary and that the observed sequences have mixed well with each other. These checks are not hypothesis tests. There is no p-value and no statistical significance.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Appendix</title>
      <link>//localhost:1313/notes/datascience/master/mbj/05_appendix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/mbj/05_appendix/</guid>
      <description>
        
        
        &lt;p&gt;Thanks to how little I understand the source book I will use other sources in order to properly understand Bayesian Hierarchical Modeling, the contents on the following chapter explains how hierarchical Bayesian models came to be and its appeal. Then on section 2 it lays out Hierarchical Nolmal Modeling followed by an explanation on hierarchical Beta-Binomial modeling on section 3.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bayesball.github.io/BOOK/bayesian-hierarchical-modeling.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Hierarchical Modeling&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;introduction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#introduction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Separate estimates?&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;separate-estimates&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#separate-estimates&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;One approach for handling this group estimation problem is find separate estimates for each school. One focuses on the observations in school $j$, ${Y_{1j}, \cdots, Y_{n_jj}}$, choose a prior distribution $\pi(\mu_j, \sigma_j)$ for the mean and the standard deviation parameters.&lt;/p&gt;
&lt;p&gt;This &amp;ldquo;separate estimates&amp;rdquo; approach may be reasonable, especially if the researcher thinks the means and the standard deviations from the five Normal models are completely unrelated to each other. That is, one’s prior beliefs about the parameters of the SAT score distribution in one school are unrelated to the prior beliefs about the distribution parameters in another school.&lt;/p&gt;
&lt;h3&gt;Combined estimates?&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;combined-estimates&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#combined-estimates&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another way to handle this group estimation problem is to ignore the fact that there is a grouping variable and estimate the parameters in the combined sample. In our school example, one ignores the school variable and simply assumes that the SAT scores $Y_i$&amp;rsquo;s are distributed from a single Normal population with mean $\mu$ and standard deviation $\sigma$ where $i = 1, \cdots, n$ is the total number of students from all five schools. Using this approach, one is effectively ignoring any differences between the five schools.&lt;/p&gt;
&lt;h3&gt;A two-stage prior&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;a-two-stage-prior&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-two-stage-prior&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Is there an alternative approach that compromises between the separate and combined estimate methods?&lt;/p&gt;
&lt;p&gt;For simplicity of discussion it is assumed the standard deviation $\sigma_j$ of the $j$th school is known. Consider the collection of five mean parameters, ${\mu_1, \mu_2, \mu_3, \mu_4, \mu_5}$ representing the means of the five schools&amp;rsquo; SAT scores. One believes that the $\mu_j$&amp;rsquo;s are distinct, because each $\mu_j$ depends on the characteristics of school $j$. One wishes to construct a prior distribution for the five mean parameters that reflects the belief that ${\mu_1, \mu_2, \mu_3, \mu_4, \mu_5}$ are related or similar in size.&lt;/p&gt;
&lt;p&gt;The prior belief in similarity of the means is constructed in two stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Stage 1] The prior distribution for the $j$th mean $\mu_j$ is Normal, where the mean and standard deviation parameters are shared among all $\mu_j$:&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{aligned}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau), j = 1, \cdots, 5
\end{aligned}
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;[Stage 2] In Stage 1, the parameters $\mu$ and $\tau$ are unknown. So this stage assigns the parameters a prior density $\pi$ (hyperprior):&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{aligned}
\mu, \tau \sim \pi(\mu, \tau)
\end{aligned}
$$&lt;p&gt;Stage 1 indicates that the $\mu_j$&amp;rsquo;s a priori are related and thus come from the same distribution.&lt;/p&gt;
&lt;p&gt;If one considers the limit of the Stage 1 prior as the standard deviation $\tau$ approaches zero, the group means $\mu_j$ will be identical. Then one is in the combined groups&amp;rsquo; situation where one is pooling the SAT data to learn about a single population.&lt;/p&gt;
&lt;p&gt;At the other extreme, if one allows the standard deviation $\tau$ of the Stage 1 prior to approach infinity, then one is saying that the group means are unrelated and that leads to the separate estimates situation.&lt;/p&gt;
&lt;p&gt;Since $\mu$ and $\tau$ are parameters in the prior distribution, they are called hyperparameters. Learning about $\mu$ and $\tau$ provides information about the population of $\mu_j$. In Bayesian inference, one learns about $\mu_j$ and $\tau$ by specifying a hyperprior distribution and performing inference based on the posterior distribution.&lt;/p&gt;
&lt;p&gt;It will be seen that the hierarchical model posterior estimates for one school borrows information from other schools. This process is often called partial pooling information among groups.&lt;/p&gt;
&lt;p&gt;From the structural point of view, due to the two stages of the model, this approach is called hierarchical or multilevel modeling. In essence, hierarchical modeling takes into account information from multiple levels, acknowledging differences and similarities among groups. In the posterior analysis, one learns simultaneously about each group and learns about the population of groups by pooling information across groups.&lt;/p&gt;
&lt;h2&gt;Hierarchical Normal Modeling&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-normal-modeling&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-normal-modeling&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Example: ratings of animation movies&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-ratings-of-animation-movies&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-ratings-of-animation-movies&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;MovieLens is a website which provides personalized movie recommendations from users who create accounts and rate movies that they have seen. Based on such information, MovieLens works to build a custom preference profile for each user and provide movie recommendations.&lt;/p&gt;
&lt;h3&gt;A hierarchical Normal model with random $\sigma$&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;a-hierarchical-normal-model-with-random-sigma&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-hierarchical-normal-model-with-random-sigma&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In this situation it is reasonable to develop a model for the movie ratings where the grouping variable is the movie title. We index a rating by two subscripts, where $Y_{ij}$ denotes the $i$th rating for the $j$th movie title, with $j = 1, \cdots, 8$.&lt;/p&gt;
&lt;p&gt;Since the ratings are continuous, it is reasonable to use the Normal data model. For simplicity and ease of illustration, a common and shared unknown standard deviation $\sigma$ is assumed for all Normal models (however it could also be modeled). Therefore we define the sampling distribution as:&lt;/p&gt;
$$
\begin{aligned}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{aligned}
$$&lt;p&gt;Since these movies are all animations, it is reasonable to believe that the mean ratings are similar across movies. So one assigns each mean rating the same Normal prior distribution at the first stage:&lt;/p&gt;
$$
\begin{aligned}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{aligned}
$$&lt;p&gt;The hyperparameters $\mu$ and $\tau$ are treated as random since we are unsure about the degree of pooling of the eight sets of ratings. After observing data, inference is performed about $\mu$ and $\tau$ based on their posterior distributions.&lt;/p&gt;
&lt;p&gt;Treating $\mu$ and $\tau$ as random, one arrives at the following hierarchical model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sampling&lt;/strong&gt; for $j = 1, \cdots, 8$ and $i = 1, \cdots, n_j$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt; for $\mu_j$, Stage 1, $j = 1, \cdots, 8$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt; for $\mu_j$, Stage 2, the hyperprior:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu, \tau \sim \pi(\mu, \tau)
\end{aligned}
$$&lt;p&gt;To complete the model, one needs to specify a prior distribution for the standard deviation parameter, $\sigma$:&lt;/p&gt;
$$
\begin{aligned}
\frac{1}{\sigma^2} | a_{\sigma}, b_{\sigma} \sim \text{Gamma}(a_{\sigma}, b_{\sigma})
\end{aligned}
$$&lt;p&gt;One assigns a known Gamma prior distribution for $\frac{1}{\sigma^2}$, with fixed hyperparameter values $a_{\sigma}$ and $b_{\sigma}$. In some situations, one may consider the situation where $a_{\sigma}$ and $b_{\sigma}$ are random and assign hyperprior distributions for these unknown hyperparameters.&lt;/p&gt;
&lt;p&gt;It is helpful to contrast the two-stage prior distribution for ${\mu_j}$ and the one-stage prior distribution for $\sigma$.&lt;/p&gt;
&lt;p&gt;For the means ${\mu_j}$, we have discussed that specifying a common prior distribution for different $j$ pools information across the movies. One is simultaneously estimating both a mean for each movie (the $\mu_j$&amp;rsquo;s) and the variation among the movies ($\mu$ and $\tau$). For the standard deviation, the hierarchical model also pools information across movies. However, all of the observations are combined in the estimation of $\sigma$. Since separate values of $\sigma_j$, one cannot learn about the differences and similarities among the $\sigma_j$&amp;rsquo;s.&lt;/p&gt;
&lt;h4&gt;Graphical representation of the hierarchical model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;graphical-representation-of-the-hierarchical-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#graphical-representation-of-the-hierarchical-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;An alternative way of expressing this hierarchical model uses the following graphical representation.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/normal_hierarchical_model.png&#34; alt=&#34;Normal Hierarchical Model&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the middle section of the graph, $Y_{ij}$ represents the collection of random variables for all ratings of movie $j$. The upper section of the graph focuses on the $\mu_j$&amp;rsquo;s. All means follow the same prior, a Normal distribution with mean $\mu$ and standard deviation $\sigma$.&lt;/p&gt;
&lt;p&gt;Since $\mu$ and $\tau$ are random, these second-stage parameters are associated with the prior label $\pi(\mu, \tau)$.&lt;/p&gt;
&lt;h4&gt;Second-stage prior&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;second-stage-prior&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#second-stage-prior&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The hierarchical Normal model presented in Equations (10.6) through (10.9) has not specified the hyperprior distribution $\pi(\mu, \tau)$. How does one construct a prior on these second-stage hyperparameters?&lt;/p&gt;
&lt;p&gt;A typical approach for Normal models is to assign two independent prior distributions — a Normal distribution for the mean $\mu$ and a Gamma distribution for the precision $\frac{1}{\tau^2}$. Such a specification facilitates the use of the Gibbs sampling. Using this approach, the density $\pi(\mu, \tau)$ is replaced by the two hyperprior distributions below:&lt;/p&gt;
$$
\begin{aligned}
\mu | \mu_0, \gamma_0 \sim \text{Normal}(\mu_0, \gamma_0)
\end{aligned}
$$$$
\begin{aligned}
\frac{1}{\tau^2} | a, b \sim \text{Gamma}(a_{\tau}, b_{\tau})
\end{aligned}
$$&lt;p&gt;The task of choosing a prior for $(\mu, \tau)$ reduces to the problem of choosing values for the four hyperparameters $\mu_0, \gamma_0, a_{\tau}$ and $b_{\tau}$. If one believes that $mu$ is located around the value of $3$ and she is not very confident of this choice, the set of values $\mu_0 = 3$ and $\gamma_0 = 1$ could be chosen. As for $\tau$, one chooses a weakly informative prior with $a_{\tau} = b_{\tau} = 1$ as $\text{Gamma}(1, 1)$. Moreover, to choose a prior for $\sigma$, let $a_{\sigma} = b_{\sigma} = 1$ to have the weakly informative $\text{Gamma}(1, 1)$ prior.&lt;/p&gt;
&lt;h3&gt;Inference through MCMC&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inference-through-mcmc&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inference-through-mcmc&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;With the specification of the prior, the complete hierarchical model is described as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sampling&lt;/strong&gt; for $j = 1, \cdots, 8$ and $i = 1, \cdots, n_j$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt; for $\mu_j$, Stage 1, $j = 1, \cdots, 8$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt; for $\mu_j$, Stage 2: the hyperpriors:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu \sim \text{Normal}(3, 1)
\end{aligned}
$$$$
\begin{aligned}
\frac{1}{\tau^2} \sim \text{Gamma}(1, 1)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior&lt;/strong&gt; for $\sigma$&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\frac{1}{\sigma^2} \sim \text{Gamma}(1, 1)
\end{aligned}
$$&lt;h4&gt;Describe the model by a script&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;describe-the-model-by-a-script&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#describe-the-model-by-a-script&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The first step in using the &lt;a href=&#34;https://mcmc-jags.sourceforge.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JAGS&lt;/a&gt; software is to write the following script defining the hierarchical model. The model is saved in the character string modelString.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;modelString&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;model {
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;### sampling
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;for (i in 1:N){
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;   y[i] ~ dnorm(mu_j[MovieIndex[i]], invsigma2)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;### priors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;for (j in 1:J){
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;   mu_j[j] ~ dnorm(mu, invtau2)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;invsigma2 ~ dgamma(a_s, b_s)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;sigma &amp;lt;- sqrt(pow(invsigma2, -1))
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;### hyperpriors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;mu ~ dnorm(mu0, g0)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;invtau2 ~ dgamma(a_t, b_t)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;tau &amp;lt;- sqrt(pow(invtau2, -1))
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;In the sampling part of the script, note that the loop goes from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;N&lt;/code&gt;, where &lt;code&gt;N&lt;/code&gt; is the number of observations with index &lt;code&gt;i&lt;/code&gt;. However, because now &lt;code&gt;N&lt;/code&gt; observations are grouped according to movies, indicated by &lt;code&gt;j&lt;/code&gt;, one needs to create one vector, &lt;code&gt;mu_j&lt;/code&gt; of length eight, and use &lt;code&gt;MovieIndex[i]&lt;/code&gt; to grab the corresponding &lt;code&gt;mu_j&lt;/code&gt; based on the movie index.&lt;/p&gt;
&lt;p&gt;In the priors part of the script, the loop goes from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;J&lt;/code&gt;, and &lt;code&gt;J = 8&lt;/code&gt; in the current example. Inside the loop, the first line corresponds to the prior distribution for &lt;code&gt;mu_j&lt;/code&gt;. Due to a commonly shared &lt;code&gt;sigma&lt;/code&gt;, &lt;code&gt;invsigma2&lt;/code&gt; follows &lt;code&gt;dgamma(a_g, b_g)&lt;/code&gt; outside of the loop. In addition, &lt;code&gt;sigma &amp;lt;- sqrt(pow(invsigma2, -1))&lt;/code&gt; is added to help tracksigma directly.&lt;/p&gt;
&lt;p&gt;Finally in the hyperpriors section of the script, one specifies the Normal hyperprior for &lt;code&gt;mu&lt;/code&gt;, a Gamma hyperprior for &lt;code&gt;invtau2&lt;/code&gt;. Keep in mind that the arguments in the &lt;code&gt;dnorm&lt;/code&gt; in JAGS are the mean and the precision (std). If one is interested instead in the standard deviation parameter &lt;code&gt;tau&lt;/code&gt;, one could return it in the script by using &lt;code&gt;tau &amp;lt;- sqrt(pow(invtau2, -1))&lt;/code&gt;, enabling the tracking of its MCMC chain in the posterior inferences.&lt;/p&gt;
&lt;h4&gt;Define the data and prior parameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;define-the-data-and-prior-parameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#define-the-data-and-prior-parameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;After one has defined the model script, the next step is to provide the data and values for parameters of the prior.&lt;/p&gt;
&lt;p&gt;In the R script below, a list &lt;code&gt;the_data&lt;/code&gt; contains the vector of observations, the vector of movie indices, the number of observations, and the number of movies. It also contains the Normal hyperparameters &lt;code&gt;mu0&lt;/code&gt; and &lt;code&gt;g0&lt;/code&gt;, and two sets of Gamma hyperparameters (&lt;code&gt;a_t&lt;/code&gt; and &lt;code&gt;b_t&lt;/code&gt;) for &lt;code&gt;invtau2&lt;/code&gt;, and (&lt;code&gt;a_s&lt;/code&gt; and &lt;code&gt;b_s&lt;/code&gt;) for &lt;code&gt;invsigma2&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MovieRatings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rating&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;MovieIndex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MovieRatings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Group_Number&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;N&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;J&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;unique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MovieIndex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;the_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;MovieIndex&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MovieIndex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 &lt;span class=&#34;s&#34;&gt;&amp;#34;N&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;J&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 &lt;span class=&#34;s&#34;&gt;&amp;#34;mu0&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;g0&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 &lt;span class=&#34;s&#34;&gt;&amp;#34;a_t&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;b_t&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 &lt;span class=&#34;s&#34;&gt;&amp;#34;a_s&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;b_s&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;One uses the &lt;code&gt;run.jags()&lt;/code&gt; function in the runjags R package to generate posterior samples by using the MCMC algorithms in JAGS.&lt;/p&gt;
&lt;p&gt;The script below runs one MCMC chain with $1000$ iterations in the adapt period (preparing for MCMC), $5000$ iterations of burn-in and an additional set of $5000$ iterations to be run and collected for inference. By using &lt;code&gt;monitor = c(&amp;quot;mu&amp;quot;, &amp;quot;tau&amp;quot;, &amp;quot;mu_j&amp;quot;, &amp;quot;sigma&amp;quot;)&lt;/code&gt;, one collects the values of all parameters in the model. In the end, the output variable &lt;code&gt;posterior&lt;/code&gt; contains a matrix of simulated draws.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;posterior&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;run.jags&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;modelString&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;n.chains&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;monitor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;mu&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;tau&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;mu_j&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;sigma&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;adapt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;burnin&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;5000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;5000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;h4&gt;MCMC diagnostics and summarization&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;mcmc-diagnostics-and-summarization&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#mcmc-diagnostics-and-summarization&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;To perform some MCMC diagnostics in our example, one uses the &lt;code&gt;plot()&lt;/code&gt; function, specifying the variable to be checked by the vars argument. For example, the script below returns four diagnostic plots (trace plot, empirical PDF, histogram, and autocorrelation plot) for the hyperparameter $\tau$.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;posterior&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vars&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;tau&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/tau_posterior_diagnostics.png&#34; alt=&#34;Tau Posterior Diagnostics&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In practice MCMC diagnostics should be performed for all parameters to justify the overall MCMC convergence. In our example, the above diagnostics should be implemented for each of the eleven parameters in the model: $\mu, \tau, \mu_1, \cdots, \mu_8$ and $\sigma$.&lt;/p&gt;
&lt;p&gt;Once diagnostics are done, one reports posterior summaries of the parameters using &lt;code&gt;print()&lt;/code&gt;. Note that these summaries are based on the 5000 iterations from the sample period, excluding the adapt and burn-in iterations.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-R&#34; data-lang=&#34;R&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;posterior&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;digits&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;Lower95&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Median&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Upper95&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;Mean&lt;/span&gt;     &lt;span class=&#34;n&#34;&gt;SD&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Mode&lt;/span&gt;   &lt;span class=&#34;n&#34;&gt;MCerr&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;         &lt;span class=&#34;m&#34;&gt;3.19&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.78&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.34&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.77&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.286&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00542&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tau&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;0.357&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.638&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;1.08&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.677&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;0.2&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00365&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[1]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;2.96&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.47&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.99&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.47&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.262&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00376&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[2]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.38&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.81&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.25&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.82&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.221&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00313&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[3]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.07&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.91&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.75&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.91&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.425&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00677&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[4]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.21&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.74&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.31&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.74&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.285&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00428&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[5]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.09&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;4.15&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;5.43&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;4.18&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.588&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.0115&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[6]&lt;/span&gt;     &lt;span class=&#34;m&#34;&gt;2.7&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.84&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.99&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.85&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.576&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00915&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[7]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;2.74&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;3.53&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.27&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;3.51&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.388&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00595&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu_j[8]&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;3.58&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;4.12&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;4.66&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;4.12&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.276&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00423&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;     &lt;span class=&#34;m&#34;&gt;0.763&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;0.92&lt;/span&gt;    &lt;span class=&#34;m&#34;&gt;1.12&lt;/span&gt;  &lt;span class=&#34;m&#34;&gt;0.93&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.0923&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;--&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;0.00142&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;For example, the movies &amp;ldquo;How to Train Your Dragon&amp;rdquo; (corresponding to $\mu_1$) and &amp;ldquo;Megamind&amp;rdquo; (corresponding to $\mu_7$) have the lowest average ratings with short $90%$ credible intervals, $(2.96, 3.99)$ and $(2.74, 4.27)$ respectively, whereas &amp;ldquo;Legend of the Guardians: The Owls of Ga’Hoole&amp;rdquo; (corresponding to $μ_6$) also has a low average rating but with a wider $90%$ credible interval $(2.70, 4.99)$. The differences in the width of the credible intervals stem from the sample sizes: there are eleven ratings for &amp;ldquo;How to Train Your Dragon&amp;rdquo;, four ratings for &amp;ldquo;Megamind&amp;rdquo;, and only a single rating for &amp;ldquo;Legend of the Guardians: The Owls of Ga’Hoole&amp;rdquo;. The smaller the sample size, the larger the variability in the inference, even if one pools information across groups.&lt;/p&gt;
&lt;h4&gt;Shrinkage&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;shrinkage&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#shrinkage&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Recall that the two-stage prior specifies a shared prior Normal $(\mu, \tau)$ for all $\mu_j$&amp;rsquo;s which facilitates simultaneous estimation of the movie mean ratings (the $\mu_j$&amp;rsquo;s), and estimation of the variation among the movie mean ratings through the parameters $\mu$ and $\tau$. The posterior mean of the rating for a particular movie $\mu_j$ shrinks the observed mean rating towards an average rating. The following figure displays a shrinkage plot which illustrates the movement of the observed sample mean ratings towards an average rating.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/mean_shrinkage.png&#34; alt=&#34;Shrinkage&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The left side plots the sample movie rating means and lines connect the sample means to the corresponding posterior means (i.e. means of the posterior draws of $\mu_j$). The shrinkage effect is obvious for the movie &amp;ldquo;Batman: Under the Red Hood&amp;rdquo; which corresponds to the dot at the value $5.0$ on the left. This movie only received one rating of $5.0$ and its mean rating $\mu_5$ shrinks to the value $4.178$ on the right, which is still the highest posterior mean among the nine movie posterior means.&lt;/p&gt;
&lt;p&gt;A large shrinkage is desirable for a movie with a small number of ratings such as &amp;ldquo;Batman: Under the Red Hood&amp;rdquo;. For a movie with a small sample size, information about other ratings of similar movies helps to produce a more reasonable estimate at the true average movie rating. The amount of shrinkage is more modest for movies with larger sample sizes.&lt;/p&gt;
&lt;h4&gt;Sources of variability&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;sources-of-variability&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#sources-of-variability&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We know that the prior distribution $\text{Normal}(\mu, \tau)$ is shared among the means $\mu_j$&amp;rsquo;s of all groups in a hierarchical Normal model, and the hyperparameters $\mu$ and $\tau$ provide information about the population of $\mu_j$&amp;rsquo;s. Specifically, the standard deviation $\tau$ measures the variability among the $\mu_j$&amp;rsquo;s. When the hierarchical model is estimated through MCMC, summaries from the simulation draws from the posterior of $\tau$ provide information about this source of variation after analyzing the data.&lt;/p&gt;
&lt;p&gt;There are actually two sources for the variability among the observed $Y_{ij}$&amp;rsquo;s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sampling level&lt;/strong&gt;: within-group variability:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_{ij} \sim \text{Normal}(\mu_j, \sigma)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Group level&lt;/strong&gt; between-group variability:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu_{j} | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{aligned}
$$&lt;p&gt;When the hierarchical model is fit through MCMC, summaries from the marginal posterior distributions of $\sigma$ and $\tau$ provide information about the two sources of variability.&lt;/p&gt;
&lt;p&gt;The Bayesian posterior inference in the hierarchical model is able to compare these two sources of variability, taking into account the prior belief and the information from the data. One initially provides prior beliefs about the values of the standard deviations $\sigma$ and $\tau$ through Gamma distributions.&lt;/p&gt;
&lt;p&gt;What can be said about these two sources of variability after the estimation of the hierarchical model? As seen in the output of &lt;code&gt;print(posterior, digits = 3)&lt;/code&gt;, the $90%$ credible interval for $\sigma$ is $(0.763, 1.12)$ and the $90%$ credible interval for $\tau$ is $(0.357, 1.08)$. After observing the data, the within-group variability in the measurements is estimated to be larger than the between-group variability.&lt;/p&gt;
&lt;p&gt;To compare both variability sources we compute:&lt;/p&gt;
$$
\begin{aligned}
R = \frac{\tau^2}{\tau^2 + \sigma^2}
\end{aligned}
$$&lt;p&gt;It represents the fraction of the total variability in the movie ratings due to the differences between groups. If the value of $R$ is close to $1$, most of the total variability is attributed to the between-group variability. On the other side, if $R$ is close to $0$, most of the variation is within groups and there is little significant differences between groups.&lt;/p&gt;
&lt;p&gt;A $95%$ credible interval for $R$ is $(0.149, 0.630)$. Since much of the posterior probability of $R$ is located below the value $0.5$, this confirms that the variation between the mean movie rating titles is smaller than the variation of the ratings within the movie titles in this example.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/IC_variability_comparison.png&#34; alt=&#34;Variability Comparison&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Hierarchical Beta-Binomial Modeling&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-beta-binomial-modeling&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-beta-binomial-modeling&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Example: Deaths after heart attack&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example-deaths-after-heart-attack&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example-deaths-after-heart-attack&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The New York State (NYS) Department of Health collects and releases data on mortality after a heart attack. We focus on 13 hospitals in Manhattan, New York City, with the goal of learning about the percentages of resulted deaths from heart attack for hospitals in this sample.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/heart_attack_data.png&#34; alt=&#34;Data&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;A Hierarchical Beta-Binomial Model&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;a-hierarchical-beta-binomial-model&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#a-hierarchical-beta-binomial-model&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Treating “cases” as trials and “deaths” as successes, the Binomial sampling model is a natural choice for this data, and the objective is to learn about the death probability $p$ of the hospitals.&lt;/p&gt;
&lt;p&gt;If one creates thirteen separate Binomial sampling models, one for each hospital, and conducts separate inferences, one loses the ability to use potential information about the death rate from hospital $i$ when making inference about that of a different hospital $j$. Since these are all hospitals in Manhattan, New York City, they may share attributes in common related to death rates from heart attack.&lt;/p&gt;
&lt;p&gt;Therefore, one builds a hierarchical model based on a common Beta distribution that generalizes the Beta-Binomial conjugate model described in &lt;a href=&#34;https://bayesball.github.io/BOOK/proportion.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 7&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let $Y_i$ denote the number of resulted deaths from heart attack, $n_i$ the number of heart attack cases, and $p_i$ the death rate for hospital $i$. So the sampling and first stage of the prior of our model is written as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling for $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_i \sim \text{Binomial}(n_i, p_i)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Prior for $p_i$, $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p_i \sim \text{Beta}(a, b)
\end{aligned}
$$&lt;p&gt;Note that the hyperparameters $a$ and $b$ are shared among all hospitals. If $a$ and $b$ are known values, then the posterior inference for $p_i$ of hospital $i$ is simply another Beta distribution by conjugacy (refer to &lt;a href=&#34;https://compcogsci-3016.djnavarro.net/technote_betabinomial.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beta-binomial conjugate prior (page 7)&lt;/a&gt;):&lt;/p&gt;
$$
\begin{aligned}
p_i | y_i \sim \text{Beta}(a + y_i, b + n_i - y_i)
\end{aligned}
$$&lt;p&gt;In the general situation where the hyperparameters $a$ and $b$ are unknown, a second stage of the prior $\pi(a, b)$ needs to specified for these hyperparameters, such that the model is now defined as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling for $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_i \sim \text{Binomial}(n_i, p_i)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Prior for $p_i$, Stage 1: $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p_i \sim \text{Beta}(a, b)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Prior for $p_i$, Stage 2: the hyperprior:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
a, b \sim \pi(a, b)
\end{aligned}
$$&lt;p&gt;When we start analyzing the New York State heart attack death rate dataset, the specification of this hyperprior distribution $\pi(a, b)$ will be described.&lt;/p&gt;
&lt;h4&gt;Graphical Representation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;graphical-representation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#graphical-representation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One sees that the upper section of the graph represents the sampling density, with the arrow directing from $p_i$ to $Y_i$. Here the start of the arrow is the parameter and the end of the arrow is the random variable. The lower section of the graph represents the prior, with arrows directing from $a$ and $b$ to $p_i$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/beta_binomial_hierarchical_model.png&#34; alt=&#34;Graphical Representation of the Beta-Binomial Hierarchical Model&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Inference through MCMC&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;inference-through-mcmc-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#inference-through-mcmc-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Second-stage prior&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;second-stage-prior-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#second-stage-prior-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;For a $\text{Beta}(a, b)$ prior distribution for a proportion $p$, one considers the parameter $a$ as the prior count of “successes”, the parameter $b$ as the prior count of &amp;ldquo;failures&amp;rdquo;, and the sum $a + b$ represents the prior sample size. Also the expectation is given by $\frac{a}{a + b}$. From these facts a more natural parametrization of the hyperprior distribution $\pi(a, b)$ is $\pi(\mu, \eta)$ where $\mu = \frac{a}{a + b}$ is the hyperprior mean and $\eta = a + b$ is the hyperprior sample size. Therefore:&lt;/p&gt;
$$
\begin{aligned}
\mu, \eta \sim \pi(\mu, \eta)
\end{aligned}
$$&lt;p&gt;where $a = \mu\eta$ and $b = (1 - \mu)\eta$.&lt;/p&gt;
&lt;p&gt;Assume $\mu$ and $\eta$ are independent which means that one&amp;rsquo;s beliefs about the prior mean are independent of the beliefs about the prior sample size. The hyperprior expectation $\mu$ is the mean measure for $p_i$, the average death rate across $13$ hospitals. If one has little prior knowledge about the expectation $\mu$, one assigns this parameter a Uniform prior which is equivalent to a $\text{Beta}(1, 1)$ prior.&lt;/p&gt;
&lt;p&gt;To motivate the prior choice for the hyperparameter sample size $\eta$, consider the case where the hyperparameter values are known. If $y^\ast$ and $n^\ast$ are respectively the number of deaths and number of cases for one hospital, then the posterior mean of death rate parameter $p^\ast$ is given by (refer to the Beta-binomial conjugate definition):&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[p^\ast|y^\ast] = \frac{y^\ast + \mu\eta}{n^\ast + \eta}
\end{aligned}
$$&lt;p&gt;With a little algebra, the posterior mean is rewritten as&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[p^\ast|y^\ast] = (1 - \lambda)\frac{y^\ast}{n^\ast} + \lambda\mu
\end{aligned}
$$&lt;p&gt;where $\lambda$ is the shrinkage fraction:&lt;/p&gt;
$$
\begin{aligned}
\lambda = \frac{\eta}{n^* + \eta}
\end{aligned}
$$&lt;p&gt;The parameter $\lambda$ falls in the interval $(0, 1)$ and represents the degree of shrinkage of the posterior mean away from the sample proportion $\frac{y^\ast}{n^\ast}$ towards the prior mean $\mu$.&lt;/p&gt;
&lt;p&gt;Suppose one believes a priori that, for a representative sample size $n^\ast$, the shrinkage $\lambda$ is Uniformly distributed on $(0, 1)$. By performing a transformation, this implies that the prior density for the prior sample size $\eta$ has the form:&lt;/p&gt;
$$
\begin{aligned}
\pi(\eta) = \frac{n^\ast}{(n^\ast + \eta)^2}, \eta &gt; 0
\end{aligned}
$$&lt;p&gt;Equivalently, the logarithm of $\eta$, $\theta = \log(\eta)$, has a Logistic distribution with location $\log(n^\ast)$ and scale $1$. We represent this distribution as $Logistic(\log(n^\ast), 1)$, with pdf:&lt;/p&gt;
$$
\begin{aligned}
\pi(\theta) = \frac{e^{-(\theta - \log(n^\ast))}}{(1 + e^{-(\theta - \log(n^\ast))})^2}
\end{aligned}
$$&lt;p&gt;With this specification of the hyperparameter distribution, one writes down the complete hierarchical model as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling for $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
Y_i \sim \text{Binomial}(n_i, p_i)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Prior for $p_i$, Stage 1: $i = 1, \cdots, 13$:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
p_i \sim \text{Beta}(a, b)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Prior for $p_i$, Stage 2: the hyperpriors:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\mu, \eta \sim \pi(\mu, \eta)
\end{aligned}
$$$$
\begin{aligned}
\log \eta \sim \text{Logistic}(\log n^*, 1)
\end{aligned}
$$&lt;p&gt;where $a = \mu\eta$ and $b = (1 - \mu)\eta$&lt;/p&gt;
&lt;h4&gt;MCMC diagnostics and summarization&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;mcmc-diagnostics-and-summarization-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#mcmc-diagnostics-and-summarization-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;After the diagnostics are performed, one reports posterior summaries of the parameters:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/beta_binomial_MCMC_summary.png&#34; alt=&#34;MCMC Summary&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the posterior output, one evaluates the effect of information pooling in the hierarchical model. See Figure 10.6 displays a shrinkage plot showing how the sample proportions are shrunk towards the overall death rate.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/beta_binomial_shrinkage.png&#34; alt=&#34;MCMC Shrinkage&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To compare the posterior densities of the different $p_i$, one displays the density estimates in a single graph as in the following figure:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/beta_binomial_posterior_densities.png&#34; alt=&#34;Posterior Density Comparison&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
