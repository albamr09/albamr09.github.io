<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alba&#39;s Notes – Aprendizaje Automático II</title>
    <link>//localhost:1313/notes/datascience/master/aaii/</link>
    <description>Recent content in Aprendizaje Automático II on Alba&#39;s Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    
	  <atom:link href="//localhost:1313/notes/datascience/master/aaii/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Tema 1. Random Forests</title>
      <link>//localhost:1313/notes/datascience/master/aaii/01_random_forests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/aaii/01_random_forests/</guid>
      <description>
        
        
        &lt;h2&gt;Bias/Variance Tradeoff&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;biasvariance-tradeoff&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#biasvariance-tradeoff&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Test error, also referred to as &lt;strong&gt;generalization error&lt;/strong&gt;, is the prediction error over an independent test sample:&lt;/p&gt;
$$
\begin{aligned}
Err_{\mathcal{T}} = \mathbb{E}[L(Y | \hat{f}(X)) | \mathcal{T}]
\end{aligned}
$$&lt;p&gt;Where $L$ is the loss function. A related quantity is the &lt;strong&gt;expected prediction error&lt;/strong&gt; (or &lt;strong&gt;expected test error&lt;/strong&gt;):&lt;/p&gt;
$$
\begin{aligned}
Err = \mathbb{E}[Err_{\mathcal{T}}]
\end{aligned}
$$&lt;p&gt;where $Err_{\mathcal{T}}$ is the test error.&lt;/p&gt;
&lt;p&gt;The error can always be decomposed into the sum of three fundamental quantities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The variance of $\hat{f}(x_0)$&lt;/li&gt;
&lt;li&gt;The squared bias of $\hat{f}(x_0)$&lt;/li&gt;
&lt;li&gt;The variance of the error terms $\epsilon$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is,&lt;/p&gt;
$$
\begin{aligned}
\mathbb{E}[\left(y_0 - \hat{f}(x_0)\right)^2] = \mathbb{V}[\hat{f}(x_0)] + [Bias(\hat{f}(x_0))]^2 + \mathbb{V}[\epsilon]
\end{aligned}
$$&lt;p&gt;This amount is derived from:&lt;/p&gt;
$$
\begin{aligned}
Err(x_0) = \mathbb{E}[(Y - \hat{f}(x_0))^2] = \mathbb{E}[Y^2 + \hat{f}(x_0)^2 - 2Y\hat{f}(x_0)]
\end{aligned}
$$$$
\begin{aligned}
= \mathbb{E}[Y^2] + \mathbb{E}[\hat{f}(x_0)^2] -2\mathbb{E}[Y]\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$&lt;p&gt;We know that $\mathbb{V}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$, such that:&lt;/p&gt;
$$
\begin{aligned}
= \mathbb{V}[Y] + \mathbb{E}[Y]^2 + \mathbb{V}[\hat{f}(x_0)] + \mathbb{E}[\hat{f}(x_0)]^2 -2\mathbb{E}[Y]\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$$$
\begin{aligned}
= \mathbb{E}[(Y - \mathbb{E}[Y])^2] + \mathbb{E}[Y]^2 + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + \mathbb{E}[\hat{f}(x_0)]^2 -2\mathbb{E}[Y]\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$&lt;p&gt;Note that, $Y = f(x_0) + \epsilon[/$], donde [$]\mathbb{E}[\epsilon] = 0$, thus it follows:&lt;/p&gt;
$$
\begin{aligned}
= \mathbb{E}[(Y - \mathbb{E}[Y])^2] + (\mathbb{E}[f(x_0)] + \mathbb{E}[\epsilon])^2 + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + \mathbb{E}[\hat{f}(x_0)]^2 -2(\mathbb{E}[f(x_0)] + \mathbb{E}[\epsilon])\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$$$
\begin{aligned}
= \mathbb{E}[(Y - \mathbb{E}[Y])^2] + \mathbb{E}[f(x_0)]^2 + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + \mathbb{E}[\hat{f}(x_0)]^2 -2\mathbb{E}[f(x_0)]\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$&lt;p&gt;We know that $(a + b)^2 = a^2 + b^2 + 2ab$ and that $\mathbb{E}[f(x_0)] = f(x_0)$, such that:&lt;/p&gt;
$$
\begin{aligned}
= \mathbb{E}[(Y - \mathbb{E}[Y])^2] + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + f(x_0)^2 + \mathbb{E}[\hat{f}(x_0)]^2 -2f(x_0)\mathbb{E}[\hat{f}(x_0)]
\end{aligned}
$$$$
\begin{aligned}
= \mathbb{E}[(Y - \mathbb{E}[Y])^2] + \mathbb{E}[(\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)])^2] + \left(\mathbb{E}[\hat{f}(x_0)] - f(x_0)\right)^2
\end{aligned}
$$&lt;p&gt;Here the notation $\mathbb{E}[\left(y_0 - \hat{f}(x_0)\right)^2]$ defines the &lt;strong&gt;expected test MSE&lt;/strong&gt;, and refers expected to the average test MSE that we would obtain if we repeatedly $f$ using a large number of training sets, and tested each at $x_0$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;overall expected test MSE&lt;/strong&gt; can be computed by averaging $\mathbb{E}[\left(y_0 - \hat{f}(x_0)\right)^2]$ over all possible values of $x_0$ in the test set.&lt;/p&gt;
&lt;p&gt;The previous equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves &lt;strong&gt;low variance&lt;/strong&gt; and &lt;strong&gt;low bias&lt;/strong&gt;. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that &lt;strong&gt;the expected test MSE can never lie below&lt;/strong&gt; $\mathbb{V}[\epsilon]$&lt;strong&gt;, the irreducible error&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;variance of the error terms&lt;/strong&gt;, $\mathbb{V}[\epsilon]$, is the variance of the target around its true mean $f(x_0)$, and &lt;strong&gt;cannot be avoided&lt;/strong&gt; no matter how well we estimate $f(x_0)$, unless $\sigma^2 = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Ideally the estimate for $f$ should not vary too much between training sets. This is computed as the expected squared deviation of $\hat{f}(x_0)$ around its mean.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; refers to the error that is introduced by approximating a real-life problem by a simpler model. This quentifies the amount by which the average of our estimate differs from the true mean.&lt;/p&gt;
&lt;p&gt;As a general rule, as we use &lt;strong&gt;more flexible methods&lt;/strong&gt;, the &lt;strong&gt;variance will increase and the bias will decrease&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As we &lt;strong&gt;increase the flexibility&lt;/strong&gt;, the &lt;strong&gt;bias&lt;/strong&gt; tends to initially &lt;strong&gt;decrease faster&lt;/strong&gt; than the variance increases. Consequently, the &lt;strong&gt;expected test MSE declines&lt;/strong&gt;. At some point &lt;strong&gt;increasing flexibility&lt;/strong&gt; has little impact on the bias but starts to &lt;strong&gt;significantly increase the variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In a real-life situation in which $f$ is unobserved, it is &lt;strong&gt;generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training error&lt;/strong&gt; consistently &lt;strong&gt;decreases&lt;/strong&gt; with model complexity, typically &lt;strong&gt;dropping to zero&lt;/strong&gt; if we increase the model complexity enough. A model with &lt;strong&gt;zero training error is overfit&lt;/strong&gt; to the training dat and will typically &lt;strong&gt;generalize poorly&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It is important to note that there are in fact two separate goals:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Model selection&lt;/strong&gt;: &lt;strong&gt;estimating the performance&lt;/strong&gt; of different models in order to &lt;strong&gt;choose the best model&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model assessment&lt;/strong&gt;: having chosen a &lt;strong&gt;final model&lt;/strong&gt;, estimating its prediction error (&lt;strong&gt;generalization error&lt;/strong&gt;) on new data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;strong&gt;training set&lt;/strong&gt; is used to fit the models. The &lt;strong&gt;validation set&lt;/strong&gt; is used to estimate prediction error for model selection. The &lt;strong&gt;test set&lt;/strong&gt; is used for assessment of the generalization error&lt;/p&gt;
&lt;h2&gt;Bootstrapping&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bootstrapping&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bootstrapping&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Bootstrapping is a resampling technique used in statistics to estimate the sampling distribution of a statistic by sampling with replacement from the original dataset. The method is particularly useful when analytical methods for deriving the sampling distribution are complex or unavailable.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a breakdown of the bootstrapping process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sampling with Replacement&lt;/strong&gt;: From the original dataset, &lt;strong&gt;randomly draw&lt;/strong&gt; $n$ &lt;strong&gt;samples with replacement&lt;/strong&gt;. This means that each observation has an equal chance of being selected for the sample, and an observation may be selected multiple times.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sample Statistics&lt;/strong&gt;: Calculate the statistic of interest (e.g., mean, median, standard deviation, regression coefficient) on each bootstrapped sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt;: Repeat steps $2$ and $3$ a large number of times to generate multiple bootstrap samples and their corresponding statistics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Estimate Sampling Distribution&lt;/strong&gt;: With the collection of bootstrap statistics, you can estimate the sampling distribution of the statistic of interest. This empirical distribution approximates the true sampling distribution of the statistic, providing information about its variability and uncertainty.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Use the estimated sampling distribution to make inferences about the population parameter or to construct confidence intervals.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Bagging&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bagging&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bagging&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Introduction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;introduction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#introduction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The decision trees discussed suffer from &lt;strong&gt;high variance&lt;/strong&gt;. In contrast, a procedure with &lt;strong&gt;low variance&lt;/strong&gt; will yield &lt;strong&gt;similar results&lt;/strong&gt; if applied repeatedly to &lt;strong&gt;distinct data sets&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bootstrap aggregation&lt;/strong&gt;, or &lt;strong&gt;bagging&lt;/strong&gt;, is a procedure for reducing the &lt;strong&gt;variance&lt;/strong&gt; of a &lt;strong&gt;statistical learning method&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Recall that given a set of $n$ independent observations $Z_1, \cdots, Z_n$ each with variance $\sigma^2$, the variance of the mean $\overline{Z}$ of the observations is given by $\frac{\sigma^2}{n}$.&lt;/p&gt;
&lt;p&gt;So, &lt;strong&gt;averaging&lt;/strong&gt; a set of observations &lt;strong&gt;reduces variance&lt;/strong&gt;. Thus, to &lt;strong&gt;reduce the variance&lt;/strong&gt; and &lt;strong&gt;increase the prediction accuracy&lt;/strong&gt; of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and &lt;strong&gt;average the resulting predictions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Using $B$ separate &lt;strong&gt;training sets&lt;/strong&gt;, and average them in order to obtain a &lt;strong&gt;single low-variance statistical learning model&lt;/strong&gt;, given by:&lt;/p&gt;
$$
\begin{aligned}
\hat{f}_{avg}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}^b(x)
\end{aligned}
$$&lt;p&gt;For each bootstrap sample $Z^{**b}, b = 1, 2, \cdots, B$, we fit our model, giving prediction $\hat{f}^{**b}(x)$. The bagging estimate is defined by:&lt;/p&gt;
$$
\begin{aligned}
\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^B \hat{f}^{*b}(x)
\end{aligned}
$$&lt;h3&gt;Bagging on Regression Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bagging-on-regression-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bagging-on-regression-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To apply bagging to regression trees, we simply construct $B$ regression trees using $B$ bootstrapped training sets, and average the resulting predictions.&lt;/p&gt;
&lt;p&gt;These trees are grown &lt;strong&gt;deep&lt;/strong&gt;, and are &lt;strong&gt;not pruned&lt;/strong&gt;. Hence each &lt;strong&gt;individual tree has high variance, but low bias&lt;/strong&gt;. &lt;strong&gt;Averaging&lt;/strong&gt; these B trees &lt;strong&gt;reduces the variance&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Bagging on Decision Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bagging-on-decision-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bagging-on-decision-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;How can bagging be extended to a classification problem where Y is qualitative? For a given test observation, we can record the class predicted by each of the $B$ trees, and take a &lt;strong&gt;majority vote&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Suppose our tree produces a classifier $\hat{G}(x)$ for a $K$-class response. Then the bagged estimate $\hat{f}_{bag}(x)$ is a $K$-vector $[p_1(x), p_2(x), \cdots, p_K(x)]$, with $p_k(x)$ equal to the proportion of trees predicting class $k$ at $x$.&lt;/p&gt;
&lt;p&gt;The bagged classifier selects the class with the most votes from the $B$ trees, $\hat{G}&lt;em&gt;{bag}(x) = \arg \max_k \hat{f}&lt;/em&gt;{bag}(x)$.&lt;/p&gt;
&lt;p&gt;Often we require the &lt;strong&gt;class-probability&lt;/strong&gt; estimates at $x$. For many classifiers $\hat{G}(x)$ there is already an underlying function $\hat{f}(x)$ that &lt;strong&gt;estimates the class probabilities at&lt;/strong&gt; $x$ (for trees, the class proportions in the terminal node). An alternative bagging strategy is to &lt;strong&gt;average these&lt;/strong&gt; instead.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;number of trees&lt;/strong&gt; $B$ is &lt;strong&gt;not a critical parameter&lt;/strong&gt; with bagging; using a very &lt;strong&gt;large value&lt;/strong&gt; of $B$ will &lt;strong&gt;not lead to overfitting&lt;/strong&gt;. In practice we use a value of $B$ sufficiently large that the &lt;strong&gt;error has settled down&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that bagging a good classifier can make it better, but bagging a bad classifier can make it worse.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Out-of-Bag Error Estimation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;out-of-bag-error-estimation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#out-of-bag-error-estimation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;There is a very straightforward way to estimate the test error of a bagged model&lt;/p&gt;
&lt;p&gt;One can show that on average, &lt;strong&gt;each bagged tree makes use of around two-thirds of the observations&lt;/strong&gt;. The observations not used to fit a given bagged tree are referred to as the &lt;strong&gt;out-of-bag (OOB) observations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;OOB prediction&lt;/strong&gt; can be obtained for each of the $n$ observations on the OOB observation set, from which the overall &lt;strong&gt;OOB MSE&lt;/strong&gt; (for a regression problem) or classification error (for a classification problem) can be computed.&lt;/p&gt;
&lt;p&gt;The resulting &lt;strong&gt;OOB error&lt;/strong&gt; is a valid &lt;strong&gt;estimate&lt;/strong&gt; of the &lt;strong&gt;test error&lt;/strong&gt; for the bagged model.&lt;/p&gt;
&lt;h3&gt;Variable Importance Measures&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;variable-importance-measures&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#variable-importance-measures&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Although the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall &lt;strong&gt;summary of the importance of each predictor&lt;/strong&gt; using the &lt;strong&gt;RSS&lt;/strong&gt; (for bagging regression trees) or the &lt;strong&gt;Gini index&lt;/strong&gt; (for bagging classification trees)&lt;/p&gt;
&lt;p&gt;In the case of bagging regression trees, we can record the total amount that the &lt;strong&gt;RSS is decreased due to splits over a given predictor&lt;/strong&gt;, averaged over all $B$ trees.&lt;/p&gt;
&lt;p&gt;For classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all $B$ trees.&lt;/p&gt;
&lt;h3&gt;Advantages of ensemble models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;advantages-of-ensemble-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#advantages-of-ensemble-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: it improves single models&amp;rsquo; perfomance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robustness&lt;/strong&gt;: reduces predictions&amp;rsquo; variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So it also improves the equilibrium between bias and variance.&lt;/p&gt;
&lt;h3&gt;How to generate diversity&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;how-to-generate-diversity&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#how-to-generate-diversity&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Manipulating &lt;strong&gt;instances&lt;/strong&gt;: selecting a different subset of instances for each model.&lt;/li&gt;
&lt;li&gt;Manipulating &lt;strong&gt;features&lt;/strong&gt;: selecting a different subset of features for each model.&lt;/li&gt;
&lt;li&gt;Manipulating &lt;strong&gt;models&amp;rsquo; definition&lt;/strong&gt;: selecting different hyperparameters, optimization algorithm for ach model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Hybridation&lt;/strong&gt;: Mix any of the previous practices.&lt;/p&gt;
&lt;h3&gt;Ensemble algorithms&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;ensemble-algorithms&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ensemble-algorithms&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Bagging&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bagging-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bagging-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Models are trained concurrently with different data sets generated using bootstrapping.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/bagging.png&#34; alt=&#34;Bagging&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;boosting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#boosting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Construye múltiples modelos (típicamente modelos del mismo tipo) secuenciales, cada uno de los cuales aprende a corregir los errores de predicción de un modelo anterior en la cadena.&lt;/p&gt;
&lt;p&gt;El objetivo es desarrollar un modelo fuerte a partir de muchos &lt;strong&gt;modelos débiles&lt;/strong&gt; especialmente diseñados que se combinan mediante votación simple o promediando.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/bootstrapping.png&#34; alt=&#34;Bootstrapping&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Staking&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;staking&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#staking&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Construye múltiples modelos &lt;strong&gt;sobre el mismo conjunto de datos&lt;/strong&gt;, típicamente modelos de diferentes tipos (modelos de nivel 0); y un modelo supervisado o meta modelo (modelo de nivel 1) que aprende cómo combinar mejor las predicciones de los modelos primarios.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/stacking.png&#34; alt=&#34;Stacking&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Model of Experts&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;model-of-experts&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#model-of-experts&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Podemos dividir el espacio de características de entrada en subespacios según algún conocimiento de dominio del problema.&lt;/p&gt;
&lt;p&gt;Luego se puede entrenar un modelo en cada subespacio del problema, convirtiéndose de hecho en un experto en el subproblema específico.&lt;/p&gt;
&lt;p&gt;Luego, un modelo aprende a qué experto recurrir para predecir nuevos ejemplos en el futuro.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/model_of_experts.png&#34; alt=&#34;Model Of Experts&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Random Forests&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;random-forests&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#random-forests&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Definition&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;definition&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#definition&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea in random forests (ilustrated on the image below) is to &lt;strong&gt;improve the variance reduction of bagging by reducing the correlation between the trees&lt;/strong&gt;, without increasing the variance too much. This is achieved in the tree-growing process through &lt;strong&gt;random selection of the input variables&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;An average of $B$ i.i.d. random variables, each with variance $\sigma^2$, has variance $\frac{1}{B}\sigma^2$.&lt;/p&gt;
&lt;p&gt;If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation ρ, the variance of the average is:&lt;/p&gt;
$$
\begin{aligned}
\rho\sigma^2 + \frac{1 - \rho}{B}\sigma^2
\end{aligned}
$$&lt;p&gt;As $B$ increases, the second term disappears, but the first remains, and hence the size of &lt;strong&gt;the correlation of pairs of bagged trees limits the benefits of averaging&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/random_forests_algorithm.png&#34; alt=&#34;Random Forests Algorithm&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When growing a tree on a bootstrapped dataset before each split, &lt;strong&gt;select&lt;/strong&gt; $m \leq p$ &lt;strong&gt;of the input variables at random&lt;/strong&gt; as candidates for splitting.&lt;/p&gt;
&lt;p&gt;After $B$ such trees ${T(x; \Theta_b)}_1^B$ are grown, the random forest (regression) predictor is:&lt;/p&gt;
$$
\begin{aligned}
\hat{f}_{rf}^B(x) = \frac{1}{B}\sum_{b=1}^B T(x; \Theta_b)
\end{aligned}
$$&lt;p&gt;Where $\Theta_b$ characterizes the bth random forest tree in terms of split variables, cutpoints at each node, and terminal-node values.&lt;/p&gt;
&lt;p&gt;Intuitively, &lt;strong&gt;reducing&lt;/strong&gt; $m$ will &lt;strong&gt;reduce the correlation between any pair of trees in the ensemble&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Details of Random Forests&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;details-of-random-forests&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#details-of-random-forests&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For &lt;strong&gt;classification&lt;/strong&gt;, the default value for $m$ is $\lfloor \sqrt{p} \rfloor$ and the minimum node size is one.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;regression&lt;/strong&gt;, the default value for $m$ is $\lfloor \frac{p}{3} \rfloor$ and the minimum node size is five.&lt;/p&gt;
&lt;p&gt;In practice the best values for these parameters will depend on the problem, and they should be treated as &lt;strong&gt;tuning parameters&lt;/strong&gt; (hyperparamters).&lt;/p&gt;
&lt;h4&gt;Variable Importance&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;variable-importance&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#variable-importance&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;At each split in each tree, the &lt;strong&gt;improvement in the split-criterion&lt;/strong&gt; is the &lt;strong&gt;importance measure attributed to the splitting variable&lt;/strong&gt;, and is &lt;strong&gt;accumulated over all the trees&lt;/strong&gt; in the forest separately for each variable.&lt;/p&gt;
&lt;p&gt;Random forests also use the OOB samples to construct a different variable importance measure.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When the bth tree is grown, the &lt;strong&gt;OOB samples&lt;/strong&gt; are passed down the tree, and the &lt;strong&gt;prediction accuracy is recorded&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Then the values for the jth variable are &lt;strong&gt;randomly permuted in the OOB samples&lt;/strong&gt;, and the accuracy is again computed.&lt;/li&gt;
&lt;li&gt;The decrease in accuracy as a result of this permuting is &lt;strong&gt;averaged over all trees&lt;/strong&gt;, and is used as a &lt;strong&gt;measure of the importance of variable&lt;/strong&gt; $j$ in the random forest.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Proximity Plots&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;proximity-plots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#proximity-plots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In growing a random forest, an $N \times N$ proximity matrix is accumulated for the training data. Such that the entry $ij$ contains the number of trees for which the OOB sample $x_i$ and the OOB sample $x_j$ are on the same terminal node.&lt;/p&gt;
&lt;p&gt;This proximity matrix is then represented in two dimensions using multidimensional scaling like the following example:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/proximity_plot.png&#34; alt=&#34;Proximity Plot&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proximity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier.&lt;/p&gt;
&lt;h4&gt;Random Forests and Overfitting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;random-forests-and-overfitting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#random-forests-and-overfitting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;When the number of variables $p$ is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small $m$. At each split the chance can be small that the relevant variables will be selected.&lt;/p&gt;
&lt;p&gt;Another claim is that random forests “cannot overfit” the data.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tema 2. Intensificación (boosting)</title>
      <link>//localhost:1313/notes/datascience/master/aaii/02_boosting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/aaii/02_boosting/</guid>
      <description>
        
        
        &lt;h2&gt;Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;boosting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#boosting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Regression Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;regression-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#regression-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Boosting regression trees, often referred to as gradient boosting machines:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/boosting_regression_trees.png&#34; alt=&#34;Boosting on Regression Trees&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the idea behind this procedure? Given the current model, we fit a decision tree to the residuals $r_i$ from the model rather than the outcome $Y$.&lt;/p&gt;
&lt;p&gt;Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter $d$.&lt;/p&gt;
&lt;p&gt;By fitting small trees to the residuals, we slowly improve $\hat{f}$ in areas where it does not perform well.&lt;/p&gt;
&lt;p&gt;The shrinkage parameter $\lambda$ slows the process down even further.&lt;/p&gt;
&lt;p&gt;Boosting has three tuning parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The number of trees $B$: boosting can overfit if $B$ is too large.&lt;/li&gt;
&lt;li&gt;The shrinkage parameter $\lambda$: this controls the rate at which boosting learns.&lt;/li&gt;
&lt;li&gt;The number $d$ of splits in each tree, which controls the complexity of the ensemble.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Ada Boost&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;ada-boost&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ada-boost&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Definition&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;definition&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#definition&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;AdaBoost, also known as Adaptive Boosting, is a machine learning algorithm that combines multiple weak classifiers to create a strong classifier. Let&amp;rsquo;s break down the AdaBoost algorithm using the pseudocode shown in:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/ada_boost_algorithm.png&#34; alt=&#34;Ada Boost Algorithm&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: AdaBoost starts by initializing the weights of all training examples equally $D_1(i) = \frac{1}{N}$ for $i = 1, 2, \cdots, N$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Process&lt;/strong&gt; for $t = 1, \cdots, T$:&lt;/li&gt;
&lt;li&gt;AdaBoost iterates through rounds, where each round involves training a weak classifier $h_t: \mathcal{X} \rightarrow {-1, +1}$ on the data $D_t$.&lt;/li&gt;
&lt;li&gt;The algorithm adjusts the weights of the training examples based on the performance of the weak classifier: $D_{t+1} = \frac{D_t(i) \cdot e^{-\alpha_ty_th_t(x_t)}}{Z_t}$, where $Z_t$ is a regularization term and $\alpha_t$ is the weight of $h_t$ on the final ensemble model based on its accuracy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combining Classifiers&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;After multiple rounds, AdaBoost combines all the weak classifiers into a final strong classifier.&lt;/li&gt;
&lt;li&gt;The final classifier makes predictions based on a weighted voting system using the predictions of the individual weak classifiers: $H(x) = sign(\sum_{t=1}^T \alpha_t h_t(x))$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt;: When making predictions on new data, AdaBoost uses the combined classifier to predict the class label based on the weighted votes of the weak classifiers&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Characteristics&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;characteristics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#characteristics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The weak learning assumption means that we assume each weak classifier makes errors that are not too close to random guessing. So the the error $\epsilon_t$ is at most $\frac{1}{2} - \gamma$ for some small positive constant $\gamma$.&lt;/p&gt;
&lt;p&gt;It can be proven that the training error of the c ombined classifier drops exponentially fas as a function of the number of weak classifiers combined, but it says nothing about the behaviour of its generalization error computed over the test data.&lt;/p&gt;
&lt;h3&gt;Toy Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;toy-example&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#toy-example&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;To illustrate how AdaBoost works, let us look at the tiny toy learning problem shown in the following picture:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/ada_boost_example.png&#34; alt=&#34;Ada Boost Example&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h5&gt;Round 1&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;round-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#round-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;On round $1$, we assign equal weights to all the examples. So $D1_(i) = \frac{1}{n} = \frac{1}{10}$.&lt;/p&gt;
&lt;p&gt;The hypothesis $h_1$ classifies incorrectly three points, so its error is $\epsilon_1 = 0.3$, so it follows that the weight assigned to this first model is $\alpha_1 = 0.42$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/ada_boost_example_1.png&#34; alt=&#34;Ada Boost Example&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h5&gt;Round 2&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;round-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#round-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;When constructing $D_2$ we increment the weight of the three points misclassified by $h_1$. And we define a new hypothesis $h_2$ over this data, where we can see that it classifies correctly the three points misclassified by $h_1$ however it still classifies incorrectly three other points.&lt;/p&gt;
&lt;p&gt;The error of this model is $\epsilon_2 = 0.21$ and thus the weight of this model is defined as $\alpha_2 = 0.65$.&lt;/p&gt;
&lt;h5&gt;Round 3&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;round-3&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#round-3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;We modify the weights of the data taking into account the three points previously misclassified, augmenting their weight and decresing the weight of those correctly classified.&lt;/p&gt;
&lt;p&gt;This classifier misses none of the points misclassified by $h_1$ and $h_2$.&lt;/p&gt;
&lt;h5&gt;Final Round&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;final-round&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#final-round&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The combined classifier $H$ is defiend as a weigthed vote between $h_1$, $h_2$ and $h_3$ where the weights are given by $\alpha_1$, $\alpha_2$ and $\alpha_3$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/ada_boost_example_2.png&#34; alt=&#34;Ada Boost Example&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;Boosting Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;boosting-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#boosting-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Variable Space Partitioning&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;variable-space-partitioning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#variable-space-partitioning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Partitioning the predictor variable space into regions in boosting trees involves recursively splitting the data based on predictor variables to create distinct regions with associated constant values for making predictions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Starting Point&lt;/strong&gt;: entire predictor variable space is considered as one large region, $R_1$.&lt;/li&gt;
&lt;li&gt;Decision Making: At each step, a decision tree algorithm finds the best split based on a predictor variable $x_j$ and a split point $s$ that minimizes a certain criterion. This split divides region $R_j$ into two subregions $R_{left}$ and $R_{right}$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Splitting Criteria&lt;/strong&gt;: it can be represented as $(j, s) = arg \min_{j, s} [\sum_{x_i \in R_{left}} L(y_i, f(x_i)) + \sum_{x_i \in R_{right}} L(y_i, f(x_i)]$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive Process&lt;/strong&gt;: This process is repeated recursively for each resulting subregion until a stopping criterion is met.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminal Nodes&lt;/strong&gt;: The final regions, or terminal nodes, are denoted as $R_J$ and are assigned constant values $\gamma_j$ representing the prediction for data points falling into that region.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constant Assigment&lt;/strong&gt;: Each terminal node is associated with a constant value, resulting in a piecewise constant function, such that each tree can be denoted as: $T(x; \Theta) = \sum_{j=1}^J \gamma_j I(x \in R_j)$ where $I(\cdot)$ is the indication function and $\Theta = {R_j, \gamma_j}_1^J$ are the parameters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;When making predictions for new data points, the algorithm determines the region $R_j$ that the data point belongs to based on the predictor variables.&lt;/li&gt;
&lt;li&gt;The prediction for that data point is then based on the constant value $\gamma_j$ assigned to the corresponding region.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Optimization Problem&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;optimization-problem&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#optimization-problem&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So as we have seen the optimization problem is defined, on a simplified manner, as follows:&lt;/p&gt;
$$
\begin{aligned}
\hat{\Theta} = \arg \min_{\Theta} \sum_{j=1}^J \sum_{x_i \in R_j} L(y_i, \gamma_j)
\end{aligned}
$$&lt;p&gt;This is a combinatorial problem that we usually aproximate using suboptimal solutions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finding $\gamma_j$ given $R_j$: we usually define $\hat{\gamma}_j = \overline{y}_j$ for regression problems.&lt;/li&gt;
&lt;li&gt;Finding $R_j$: this is the difficult part. We usually resort to a greedy, top-down recursive partitioning algorithm to find $R_j$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Boosting Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;boosting-trees-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#boosting-trees-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In boosting trees, terminal-node trees refer to the individual decision trees that make up the ensemble model. Each terminal-node tree is denoted as $T(x; \Theta_m)$ where $\Theta_m = {R_{jm}, \gamma_{jm}}_1^{J_m}$.&lt;/p&gt;
&lt;p&gt;The boosted tree model is an additive model, where each tree is added sequentially to improve the overall prediction. The model can be expressed as&lt;/p&gt;
$$
\begin{aligned}
f_M(x) = \sum_{m=1}^M T(x; \Theta_m)
\end{aligned}
$$&lt;p&gt;where $M$ represents the total number of trees in the ensemble.&lt;/p&gt;
&lt;h4&gt;Optimization Problem&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;optimization-problem-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#optimization-problem-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The optimization problem for boosting trees involves finding the optimal regions and constants for each tree in the ensemble model.&lt;/p&gt;
&lt;h5&gt;Optimization Objective&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;optimization-objective&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#optimization-objective&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The goal is to minimize the empirical risk:&lt;/p&gt;
$$
\begin{aligned}
\hat{\Theta} = arg \min_{\Theta} \sum_{x_i \in R_j} L(y_i, \gamma_j)
\end{aligned}
$$&lt;p&gt;Where $L(y_i, \gamma_i)$ represents the loss incurred for pedicting the target value $y_i$ with constant $\gamma_j$ in region $R_j$.&lt;/p&gt;
&lt;h5&gt;Finding Optimal Consants&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;finding-optimal-consants&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#finding-optimal-consants&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Given the regions $R_{jm}$ finding the optimal constant in $\gamma_j$ in each regions involves minimizing the loss function for the data points within that region:&lt;/p&gt;
$$
\begin{aligned}
\hat{\gamma}_{jm} = arg \min_{\gamma_{jm}} \sum_{x_i \in R_{jm}} L(y_i, f_{m - 1}(x_i) + \gamma_{jm})
\end{aligned}
$$$$
\begin{aligned}
\hat{\gamma}_{jm} = arg \min_{\gamma_{jm}} \sum_{x \in R_{jm}} L(y_i, f_{m - 1}(x_i) + T(x_i; \Theta_m))
\end{aligned}
$$&lt;p&gt;Finding the regions is difficult, and even more difficult than for a single tree.&lt;/p&gt;
&lt;h5&gt;Solution for Regression Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;solution-for-regression-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#solution-for-regression-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;For regressions trees the solution for boosted trees consists on choosing the regression tree that best predicts the current residuals $y_i - f_{m-1}(x_i)$ and $\hat{\gamma}_{jm}$&lt;/p&gt;
&lt;h5&gt;Solution for Classification Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;solution-for-classification-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#solution-for-classification-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;For two-class classification and exponential loss, it gives rise to the AdaBoost method. It can be showin that given $R_{jm}$ the solution is:&lt;/p&gt;
$$
\begin{aligned}
\hat{\gamma}_{jm} = \log \frac{\sum_{x_i \in R_{jm}} w_i^{(m)} I(y_i = 1)}{\sum_{x_i \in R_{jm}} w_i^{(m)} I(y_i = -1)}
\end{aligned}
$$&lt;h3&gt;Appendix&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;appendix&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#appendix&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Greedy Top-Down Recurisve Partitioning Algorithm&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;greedy-top-down-recurisve-partitioning-algorithm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#greedy-top-down-recurisve-partitioning-algorithm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A greedy, top-down recursive partitioning algorithm is a method used in decision tree construction to recursively split the predictor variable space into regions in a step-by-step manner.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Greedy Approach&lt;/strong&gt;: at each step, it makes the best split based on the available data without considering the impact of future splits.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Top Down Process&lt;/strong&gt;: starts at the top with the entire predictor variable space considered as one region. It then recursively divides this space into smaller regions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recursive Partitioning&lt;/strong&gt;: At each step the predictor variable space is divided into two or more subregions based on a splitting criterion. This process continues recursively for each resulting subregion until a stopping criterion is met.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Gradient Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gradient-boosting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gradient-boosting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Numerical Optimization via Gradient&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;numerical-optimization-via-gradient&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#numerical-optimization-via-gradient&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Imagine you have a machine learning model that makes predictions, but it&amp;rsquo;s not perfect. Gradient boosting is like a smart way to teach this model to make better predictions over time.&lt;/p&gt;
&lt;p&gt;Instead of trying to fix all the prediction errors at once, gradient boosting focuses on correcting one error at a time. It does this by looking at the direction where the error is the steepest and making adjustments to improve the prediction in that direction.&lt;/p&gt;
&lt;p&gt;By repeating this process step by step, the model gradually gets better at making predictions, leading to more accurate results.&lt;/p&gt;
&lt;p&gt;So if you have the following function you want to optimize:&lt;/p&gt;
$$
\begin{aligned}
L(f) =  \sum_{i=1}^N L(y_i, f(x_i))
\end{aligned}
$$&lt;p&gt;where $f \in \mathbb{R}^N$ is the prediction function and its evaluation at each instance $x_i$ are the parameteres we want to optimize:&lt;/p&gt;
$$
\begin{aligned}
f = \{f(x_1), \cdots, f(x_i), \cdots, f(x_n)\}
\end{aligned}
$$&lt;p&gt;Therefore the optimization problem with respect to $f$ can be summarized as follows:&lt;/p&gt;
$$
\begin{aligned}
\hat{f} = \arg \min_f L(f)
\end{aligned}
$$&lt;p&gt;Solving this entire problem at once may be challenging. To make it easier, numerical optimization procedures break down this big problem into smaller pieces, represented by component vectors. Each component vector addresses a specific aspect of the problem. So:&lt;/p&gt;
$$
\begin{aligned}
f_{M} = \sum_{m = 0}^M h_m, h_m \in \mathbb{R}^N
\end{aligned}
$$&lt;p&gt;where $f_M$ represents the final model or prediction function obtained after M iterations or steps of the boosting algorithm.&lt;/p&gt;
&lt;p&gt;Here $f_m$ represents the model at iteration $m$, whereas $h_m$ represents the increment to the model at iteration $m$ It is the component vector added to the current model to move towards the optimized solution. Each $h_m$ is induced based on the current parameter vector $f_{m-1}$ and contributes to the overall model improvement.&lt;/p&gt;
&lt;p&gt;Here is a simple layout of how the algorithm optimizes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At the beginning of the gradient boosting process, the initial model $f_0$ is set to an initial guess.&lt;/li&gt;
&lt;li&gt;As the algorithm progresses through iterations ($m = 1, 2, \cdots, M$), each step involves updating the model based on the gradient information to reduce errors in predictions. Numerical optimization methods differ in their prescriptions for computing each increment vector $h_m$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Steepest Descent&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;steepest-descent&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#steepest-descent&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Steepest descent is a method used in optimization to find the minimum value of a function. This method chooses $h_m = \rho_m g_m$ where $\rho_m$ is a scalar and $g_m$ is the gradient of $L(f_{m-1})$, that is, the cost function evaluated at values predicted by the &amp;ldquo;previous model&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The components of the gradient $g_m$ are defined as follows:&lt;/p&gt;
$$
\begin{aligned}
g_{im} = \left[\frac{\delta L(y_i, f(x_i))}{\delta f(x_i)}\right]_{f_m(x_i) = f_{m-1}(x_i)}
\end{aligned}
$$&lt;p&gt;The step length (kinda like the learning rate):&lt;/p&gt;
$$
\begin{aligned}
\rho_m = \arg \min_{\rho} L(f_{m-1} - \rho g_m)
\end{aligned}
$$&lt;p&gt;Thus, at each step, the predictor is updated as follows:&lt;/p&gt;
$$
\begin{aligned}
f_m = f_{m - 1} - \rho_m g_m \in \mathbb{R}^N
\end{aligned}
$$&lt;p&gt;This updates $f_m$ towards the direction of maximum descent at $L(f_{m-1})$, which is why this is often interpreted as a greedy algorithm.&lt;/p&gt;
&lt;h3&gt;Gradient Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gradient-boosting-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gradient-boosting-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Gradient Boosting aims to create a strong predictive model by combining multiple weak models. It starts with a simple model and gradually enhances it to minimize errors.&lt;/p&gt;
&lt;p&gt;At each iteration, a new tree model is fit to the negative gradient of the loss function. The predictions from these trees guide the model towards better predictions. Using squared error to measure closeness, this leads us to:&lt;/p&gt;
$$
\begin{aligned}
\tilde{\Theta}_m = \arg \min_{\Theta} \sum_{i=1}^N (-g_{im} - T(x_i; \Theta))^2
\end{aligned}
$$&lt;p&gt;This measures how close each prediction $T(x_i; \Theta)$ is to the gradient $-g_{im}$.&lt;/p&gt;
&lt;p&gt;The negative gradient of the loss function represents the direction in which the model&amp;rsquo;s predictions need to be adjusted to reduce errors. By fitting a new tree to this negative gradient, the model learns how to correct its predictions to move closer to the actual target values. That is at each iteration, the new tree model focuses on capturing the errors or residuals of the current ensemble model.&lt;/p&gt;
&lt;p&gt;While the exact regions where the new tree makes corrections may not match perfectly with the original model&amp;rsquo;s regions, they are close enough to serve the same purpose of improving the model&amp;rsquo;s accuracy. Here the original model is the ensemble model.&lt;/p&gt;
&lt;p&gt;The following figure summarizes the gradients for commonly used loss functions:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/gradients_gradient_boosting.png&#34; alt=&#34;Common Gradients&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Implementations of Gradient Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;implementations-of-gradient-boosting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#implementations-of-gradient-boosting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Algorithm $10.3$ presents the generic gradient tree-boosting algorithm for regression. Specific algorithms are obtained by inserting different loss criteria $L(y,f(x))$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/gradient_boosting_algorithms.png&#34; alt=&#34;Gradient Boosting Algorithm&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with an initial model $f_0(x)$ that minimizes the loss function $L(y, f(x))$.&lt;/li&gt;
&lt;li&gt;For each boosting round $m = 1, \cdots, M$:
&lt;ul&gt;
&lt;li&gt;Calculate the negative gradient for each data point&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
r_{im} = -\left[\frac{\delta L(y_i, f(x_i))}{\delta f(x_i)}\right]_{f(x_i) = f_{m-1}(x_i)}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Fit a regression tree to the gradients $r_{im}$, which gives us the regions $R_{jm}, j = 1, 2, \cdots, J_m$&lt;/li&gt;
&lt;li&gt;The step length $\gamma$ is determined by minimizing the loss using the previous model ($f_{m-1}$):&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
\gamma_{jm} = \arg \min_{\gamma} \sum_{x_i \in R_{jm}} L(y_i, f_{m-1}(x_i) + \gamma)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Update the model by adding a new tree to the ensemble $f_m(x) = f_{m-1}(x) + \gamma T(x; \Theta_m)$, where $T(x; \Theta)$ is the new tree model with parameters $\Theta_m$ that corrects the errors in the previous model.&lt;/li&gt;
&lt;li&gt;The output of the ensemble model is defined as $\hat{f}(x) = f_M(x)$, that is as the sum of the weaker models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Step Length&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;step-length&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#step-length&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The step length $\gamma$ is crucial in determining how much each new tree should contribute to the ensemble model. It controls the impact of the new tree on the overall model&amp;rsquo;s predictions.&lt;/p&gt;
&lt;p&gt;The line search aims to find the value of γ that minimizes the loss function:&lt;/p&gt;
$$
\begin{aligned}
L(f_{m-1} - \gamma g_m)
\end{aligned}
$$&lt;p&gt;This means finding the optimal step length that results in the smallest possible loss when updating the model with the new tree. By minimizing the loss function with respect to $\gamma$, the algorithm is essentially performing a form of gradient descent.&lt;/p&gt;
&lt;h4&gt;Characteristics&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;characteristics-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#characteristics-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Two basic tuning parameters are the number of iterations $M$ and the sizes of each of the constituent trees
$J_m, m = 1, 2, \cdots, M$.&lt;/p&gt;
&lt;p&gt;The original implementation of this algorithm was called MART for &amp;ldquo;multiple additive regression trees&amp;rdquo;.&lt;/p&gt;
&lt;h2&gt;Interpretability&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;interpretability&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#interpretability&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Importance of Predictor Variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;importance-of-predictor-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#importance-of-predictor-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We attempt to discuss the relevance of predictor variables in a statistical modeling technique called boosting.&lt;/p&gt;
&lt;h4&gt;Decision Trees&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;decision-trees&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#decision-trees&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;We define the following as a measure of relevance for each predictor variable $X_{\mathcal{l}}$:&lt;/p&gt;
$$
\begin{aligned}
I_{\mathcal{l}}^2(T) = \sum_{t = 1}^{J - 1} \hat{i}^2_t I(v(t) = \mathcal{l})
\end{aligned}
$$&lt;p&gt;This equation calculates the relevance of each predictor variable based on the squared improvements in error risk within the internal nodes of the tree. Let&amp;rsquo;s split each part of the equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The term $\hat{i}^2_t$ represents the squared improvement in error risk at node $t$ when splitting on the $X_{\mathcal{l}}$ predictor variable.&lt;/li&gt;
&lt;li&gt;The variable $v(t)$ indicates which predictor variable is used for the split at node $t$.&lt;/li&gt;
&lt;li&gt;Each improvement is weighted by the indicator function $I(v(t) = \mathcal{l})$, which checks if the splitting variable at node $t$ is the predictor variable of interest $X_{\mathcal{l}}$. This weighting ensures that only the improvements related to the predictor variable $X_{\mathcal{l}}$ are considered in the calculation.&lt;/li&gt;
&lt;li&gt;We sum these improvements over the $J - 1$ internal nodes on the tree, which are not terminal nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Additive Models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;additive-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#additive-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This importance measure is easily generalized to additive tree expansions:&lt;/p&gt;
$$
\begin{aligned}
I_{\mathcal{l}}^2 = \frac{1}{M}\sum_{m = 1}^{M} I_{\mathcal{l}}^2(T_m)
\end{aligned}
$$&lt;p&gt;Due to the stabilizing effect of averaging, this measure turns out to be more reliable than the measure computed only over one tree.&lt;/p&gt;
&lt;h3&gt;Partial Dependence Plots&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;partial-dependence-plots&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#partial-dependence-plots&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Partial dependence functions, by isolating the effects of selected variables, provides an interpretable analysis of their impact on the model&amp;rsquo;s predictions, overcoming the challenges of information overload in high-dimensional spaces. Let&amp;rsquo;s define the partial dependency of $f(X)$ on $X_S$,&lt;/p&gt;
$$
\begin{aligned}
f_S(X_S) = \mathbb{E}_{X_C}[f(X_S, X_C)]
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X_S$ is the subset of variables we want to study.&lt;/li&gt;
&lt;li&gt;$X_C$ is the complement set, that is the rest of variables.&lt;/li&gt;
&lt;li&gt;$E_{X_C}$ denotes the expectation operator with respect to the variables in the complement set $X_C$. It averages the model&amp;rsquo;s output over the variables in the complement set.&lt;/li&gt;
&lt;li&gt;$f$ represents the model.&lt;/li&gt;
&lt;li&gt;$f_S(X_S)$ represents the relationship between the subset of variables $X_S$ and the model&amp;rsquo;s output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The average can be estimated as follows:&lt;/p&gt;
$$
\begin{aligned}
\overline{f}_S(X_S) = \frac{1}{N} \sum_{i=1}^N f(X_S, x_{i\mathcal{C}})
\end{aligned}
$$&lt;p&gt;We simply iterate over every data point on the training set, such that $x_{i\mathcal{C}}$ refers to the values of the variables in the complement set $X_C$ for the $i$th data point.&lt;/p&gt;
&lt;p&gt;Previously we measured the effects of $X_S$ after accounting for the effects of the other variables $X_C$ on $F(X)$, they were &lt;strong&gt;not&lt;/strong&gt; the effect of $X_S$ on $f(X)$ ignoring the effects of $X_C$, that is given by:&lt;/p&gt;
$$
\begin{aligned}
\tilde{f}_S(X_S) = \mathbb{E}(f(X_S, X_C)|X_S)
\end{aligned}
$$&lt;p&gt;Thus $\tilde{f}_S(X_S) = \overline{f}_S(X_S)$ only if $X_S$ and $X_C$ are independent.&lt;/p&gt;
&lt;h4&gt;Example&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;example&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#example&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;If we assume a purely additive effect, where the overall prediction is the sum of two components:&lt;/p&gt;
$$
\begin{aligned}
f(X) = h_1(X_S) + h_2(X_C)
\end{aligned}
$$&lt;p&gt;This implies that the effect of $X_S$ on the prediction is independent of the other variables in $X_C$. However if the prediction is defined as:&lt;/p&gt;
$$
\begin{aligned}
f(X) = h_1(X_S) \cdot h_2(X_C)
\end{aligned}
$$&lt;p&gt;Then this implies that the effect of $X_S$ on the prediction is dependent on the values of the variables in $X_C$.&lt;/p&gt;
&lt;h4&gt;Representation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;representation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#representation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Owing to the limitations of computer graphics, and human perception, the size of the subsets $X_S$ must be small ($l \approx 1, 2, 3$).&lt;/p&gt;
&lt;h2&gt;Examples&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;examples&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#examples&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;California Housing&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;california-housing&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#california-housing&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The dataset consists of information from $20,460$ neighborhoods in California. The target variable ($Y$) is the median house value in each neighborhood.&lt;/p&gt;
&lt;p&gt;Predictor variables include demographic factors like median income (MedInc), housing density (House), average occupancy (AveOccup), location coordinates (longitude and latitude), and house attributes like average number of rooms (AveRooms) and bedrooms (AveBedrms). There are thus a total of eight predictors, all numeric.&lt;/p&gt;
&lt;p&gt;We fit a gradient boosting model using the MART procedure, with $J = 6$ terminal nodes.&lt;/p&gt;
&lt;p&gt;The test error is seen to decrease monotonically with increasing $M$, more rapidly during the early stages and then leveling off to being nearly constant as iterations increase. Thus, the choice of a particular value of $M$ is not critical, as long as it is not too small.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/example_gradient_boosting_error.png&#34; alt=&#34;Gradient Boost Error&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The next figure displays the relative variable importances for each of the eight predictor variables.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/example_gradient_boosting_var_importance.png&#34; alt=&#34;Gradient Boost Variable Importance&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, median income in the neighborhood is the most relevant predictor. Longitude, latitude, and average occupancy all have roughly half the relevance of income, whereas the others are somewhat less influential&lt;/p&gt;
&lt;p&gt;On the following graphs we show single-variable partial dependence plots on the most relevant nonlocation predictors.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/example_gradient_boosting_dependency_plots.png&#34; alt=&#34;Gradient Boost Depedency Plots&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the plots are not strictly smooth as a consequence of using tree-based models.&lt;/p&gt;
&lt;p&gt;The hash marks at the base of each plot delineate the deciles of the data distribution of the corresponding variables. So for example, $90%$ of the data have a &lt;code&gt;MedInc&lt;/code&gt; value of less than $6$.&lt;/p&gt;
&lt;p&gt;The partial dependence of median house value on median income is monotonic increasing. House value is generally monotonic decreasing with increasing average occupancy, except perhaps for average occupancy rates less than one.&lt;/p&gt;
&lt;p&gt;Median house value is seen to have a very weak partial dependence on house age that is inconsistent with its importance ranking. This suggests that this weak main effect may be masking stronger interaction effects with other variables.&lt;/p&gt;
&lt;p&gt;The next graph shows the two-variable partial dependence of the fitted model on joint values of longitude and latitude:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/example_gradient_boosting_dependency_plots_lat_lng.png&#34; alt=&#34;Gradient Boost Depedency Plots for Latitude and Longitude&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a very strong dependence of median house value on the neighborhood location in California. It can be viewed as representing an extra premium one pays for location. This premium is seen to be relatively large near the Pacific coast especially in the Bay Area and Los Angeles–San Diego. In the northern, central valley, and southeastern desert regions of California, location costs considerably less.&lt;/p&gt;
&lt;h2&gt;Practice&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;practice&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#practice&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Boosting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;boosting-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#boosting-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Just like random forest, GBM is an ensemble method.&lt;/p&gt;
&lt;p&gt;Imagine a data set just $10$ examples and two numeric predictor variables, and we are trying to learn to distinguish between two possible classes: circle or cross.&lt;/p&gt;
&lt;p&gt;The very simplest decision tree we can make has just one node; I will represent it with a straight line in the following diagrams.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/simple_decision_tree_example.png&#34; alt=&#34;Decision Tree&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It scored $60%$: six right, four wrong.&lt;/p&gt;
&lt;p&gt;What we do now is train another very simple tree, but first we modify the training data to give the four rows it got wrong a higher weight. How much of a higher weight? That is where the &amp;ldquo;gradient&amp;rdquo; bit of GBM comes in.&lt;/p&gt;
&lt;p&gt;In the next figure the circles and crosses for the wrong items are bigger, and our next tree pays more attention to them.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/simple_decision_tree_2_example.png&#34; alt=&#34;Decision Tree&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It got a different three items wrong so it still scores $60%$.&lt;/p&gt;
&lt;p&gt;So, for our third tree, we tell it those four are more important; the one it has got wrong twice in a row is the biggest of all.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/simple_decision_tree_3_example.png&#34; alt=&#34;Decision Tree&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we stop training here, we end up with three weak models that scored $60%$, $60%$, and $80%$, respectively. However, at least one of each of those three trees got every training row correct. You can see how they can work together to cover each other&amp;rsquo;s weaknesses.&lt;/p&gt;
&lt;p&gt;GBM naturally focuses attention on the difficult rows in your training data, the ones that are hard to learn. That is good, but it can also be bad. If there is one outlier that each tree keeps getting wrong it is going to get boosted and boosted until it is bigger than the whole universe. This is bad when the data is a mistake instead of an outlier, as it distorts the model&amp;rsquo;s accuracy.&lt;/p&gt;
&lt;p&gt;The mysterious? Well, unlike (simple) decision trees, which can be really good at explaining their thinking, it becomes a bit of a black box.&lt;/p&gt;
&lt;h3&gt;Parameters&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;parameters&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#parameters&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_trees&lt;/code&gt;: how many trees to make.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_depth&lt;/code&gt;: how deep are the trees allowed to be.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learn_rate&lt;/code&gt;: controls the speed at which the model learns&lt;/li&gt;
&lt;li&gt;&lt;code&gt;learn_rate_annealing&lt;/code&gt;: allows you to have the learn_rate start high, then gradually get lower as trees are added.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_rows&lt;/code&gt;: how many examples are needed to make a leaf node. Low number might lead to overfitting.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_split_improvement&lt;/code&gt;: controls how much error improvement must be to perform a split.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;histogram_type&lt;/code&gt;: what type of histogram to use for finding optimal split points.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nbins&lt;/code&gt;: For numerical columns, build a histogram of (at least) this many bins, then split at the best point.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nbins_cat&lt;/code&gt;: For categorical columns, build a histogram of (at most) this many bins, then split at the best point.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;build_tree_one_node&lt;/code&gt;: Run on one node only.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Building Energy Efficiency: Default GBM&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;building-energy-efficiency-default-gbm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#building-energy-efficiency-default-gbm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;This data set deals with the heating/cooling costs of various house designs.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;h2o.estimators.gbm&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H2OGradientBoostingEstimator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;H2OGradientBoostingEstimator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;GBM_defaults&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nfolds&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;Fifty trees were made, each of depth $5$. On cross-validation data, the MSE (mean squared error) is $2.462$, and $R^2$ is $0.962$. Under “Variable Importances” (shown next), which can be seen with &lt;code&gt;h2o.varimp(m)&lt;/code&gt; you will see it is giving &lt;code&gt;X5&lt;/code&gt; way more importance than any of the others; this is typical for GBM models.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;.././assets/GBM_variable_importance.png&#34; alt=&#34;GBM Varible Importance&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How about on the unseen data? &lt;code&gt;m.model_performance(test)&lt;/code&gt; is saying MSE is $2.318$, better than on the training data.&lt;/p&gt;
&lt;h3&gt;Building Energy Efficiency: Tuned GBM&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;building-energy-efficiency-tuned-gbm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#building-energy-efficiency-tuned-gbm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;I decided to start, this time, with a big random grid search. The hyperparameters tuned are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;max_depth&lt;/code&gt;: The default is $5$, and we tried $5,10,15,20,25,30,40,50,60,75,90$. The ninth best model was &lt;code&gt;max_depth=75&lt;/code&gt;, so high values may not be bad, as such, but they don’t appear to help.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;min_rows&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sample_rate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;col_sample_rate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nbins&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What about ntrees? Instead of trying to tune it, we set it high ($1000$) and used early stopping.&lt;/p&gt;
&lt;p&gt;More model results just confirmed the first impression: &lt;code&gt;min_rows&lt;/code&gt; of $1$ (or $2$) is effective with max_depth of $5$, but really poor with higher values. &lt;code&gt;min_rows&lt;/code&gt; of $10$ is effective with any value of &lt;code&gt;max_depth&lt;/code&gt;, but possibly $10$ to $20$ is best. Curiously &lt;code&gt;min_rows&lt;/code&gt; of $5$ is mediocre. A &lt;code&gt;sample_rate&lt;/code&gt; of $0.9$ or $0.95$ looks best, while there is still no clarity for &lt;code&gt;col_sample_rate&lt;/code&gt; or &lt;code&gt;nbins&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how it does on the test data, we obtain a MSE of $1.640$. This is way better than the default GBM’s
2.462, and also way better than the best tuned random forest model from the previous chapter.&lt;/p&gt;
&lt;h3&gt;MNIST: Default GBM&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;mnist-default-gbm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#mnist-default-gbm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;It is a multinomial classification, trying to look at the $784$ pixels of a handwritten digit, and say which of $0$ to $9$ it is.&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h2o&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;estimators&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;H2OGradientBoostingEstimator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;GBM_defaults&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;validation_frame&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;valid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;The confusion matrix on the training data (h2o.confusionMatrix(m)) shows an error rate of $2.08%$, while on the validation data it is a bit higher at $4.82%$. MSE is $0.028$ and $0.044$, respectively. So we have a bit of overfitting on the training data, but not too much.&lt;/p&gt;
&lt;p&gt;On the test data the error this time is $4.44%$ (MSE is $0.048$); in other words, the validation and test sets are giving us similar numbers, which is good.&lt;/p&gt;
&lt;h3&gt;MNIST: Tuned GBM&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;mnist-tuned-gbm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#mnist-tuned-gbm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As usual, the first thing I want to do is switch to using early stopping, so I can then give it lots of trees to work with, with the following parameters:&lt;/p&gt;
&lt;div class=&#34;hextra-code-block hx-relative hx-mt-6 first:hx-mt-0 hx-group/code&#34;&gt;
  

&lt;pre&gt;&lt;code&gt;stopping_tolerance = 0.001,
stopping_rounds = 3,
score_tree_interval = 10,
ntrees = 400&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;hextra-code-copy-btn-container hx-opacity-0 hx-transition group-hover/code:hx-opacity-100 hx-flex hx-gap-1 hx-absolute hx-m-[11px] hx-right-0 hx-top-0&#34;&gt;
  &lt;button
    class=&#34;hextra-code-copy-btn hx-group/copybtn hx-transition-all active:hx-opacity-50 hx-bg-primary-700/5 hx-border hx-border-black/5 hx-text-gray-600 hover:hx-text-gray-900 hx-rounded-md hx-p-1.5 dark:hx-bg-primary-300/10 dark:hx-border-white/10 dark:hx-text-gray-400 dark:hover:hx-text-gray-50&#34;
    title=&#34;Copy code&#34;
  &gt;
    &lt;div class=&#34;copy-icon group-[.copied]/copybtn:hx-hidden hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
    &lt;div class=&#34;success-icon hx-hidden group-[.copied]/copybtn:hx-block hx-pointer-events-none hx-h-4 hx-w-4&#34;&gt;&lt;/div&gt;
  &lt;/button&gt;
&lt;/div&gt;

  
&lt;/div&gt;
&lt;p&gt;Just using this, with all other default settings, had some interesting properties&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training classification score was perfect after 140 trees.&lt;/li&gt;
&lt;li&gt;Validation score was down to $2.83%$.&lt;/li&gt;
&lt;li&gt;The MSE and logloss of both the training data and validation data continued to fall, and so did the validation classification score.&lt;/li&gt;
&lt;li&gt;Relative runtime kept increasing. That is, each new tree is taking longer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It finished up with 360 trees, with a very respectable $2.17%$ error on the validation data.&lt;/p&gt;
&lt;p&gt;How can we improve that further? We know there is a lot of examples and variables, so we expect that lower sample ratios will be more effective.&lt;/p&gt;
&lt;p&gt;In terms of the &lt;code&gt;learn_rate&lt;/code&gt; we know low is slower, but better… and we have a lot of data. So we use a high (quick) &lt;code&gt;learn_rate&lt;/code&gt; for the first grid or two, then lower it later on, once we start to home in on the best parameters.&lt;/p&gt;
&lt;p&gt;This is going to be a random grid search, because we are going to use a lot of parameters.&lt;/p&gt;
&lt;p&gt;The first discovery was that a high &lt;code&gt;max_depth&lt;/code&gt; was very slow and no better than a shallow one. Also &lt;code&gt;min_rows=1&lt;/code&gt; seemed poor. We also found that &lt;code&gt;max_depth=20&lt;/code&gt; was distinctly worse than &lt;code&gt;max_depth=5&lt;/code&gt;. We also noticed that &lt;code&gt;min_rows=10&lt;/code&gt; seemed to be doing best, though it was less clear. Reducing the three sample rates to $1$ did seem to help, though there was not enough data to draw a confident conclusion.&lt;/p&gt;
&lt;p&gt;So, another try. We&amp;rsquo;ll leave &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_rows&lt;/code&gt; at their defaults, and just concentrate on testing sampling rates.&lt;/p&gt;
&lt;p&gt;There was not that much clarity in the parameters, but the best two had &lt;code&gt;col_sample_rate&lt;/code&gt; of $0.8$ and &lt;code&gt;sample_rate&lt;/code&gt; of $0.95$, whereas &lt;code&gt;sample_rate=0.5&lt;/code&gt; was only chosen once, but was the worst of the nine. The default model with just early stopping added, would have come second best in the grid measured on classification error, but fourth on MSE, and seventh on logloss, whereas the “tuned” model is top on all metrics, so we have more confidence in selecting it.&lt;/p&gt;
&lt;p&gt;As a final step, we ran the chosen model on the test data and got an error rate of $2.33%$. This compares to $4.44%$ with the default settings.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tema 3. Otras Combinaciones de Modelos</title>
      <link>//localhost:1313/notes/datascience/master/aaii/03_combination/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/aaii/03_combination/</guid>
      <description>
        
        
        &lt;h2&gt;Introduccion&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;introduccion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#introduccion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Bibliography&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;bibliography&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#bibliography&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/8335271&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Classifiers Combination Techniques: A Comprehensive Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[10] S. Chitroub, &amp;ldquo;Classifier combination and score level fusion: concepts and practical aspects,&amp;rdquo; Int. J. Image Data Fusion, vol. 1, no. 2, pp. 113–135, Jun. 2010.&lt;/li&gt;
&lt;li&gt;[11] H. He and Y. Cao, &amp;ldquo;SSC: a classifier combination method based on signal strength.,&amp;rdquo; IEEE Trans. neural networks Learn. Syst., vol. 23, no. 7, pp. 1100–17, Jul. 2012.&lt;/li&gt;
&lt;li&gt;[12] C. De Stefano, F. Fontanella, and A. S. di Freca, &amp;ldquo;A Novel Naive Bayes Voting Strategy for Combining Classifiers.,&amp;rdquo; in ICFHR, 2012, pp. 467–472.&lt;/li&gt;
&lt;li&gt;[13] J. Hou, Z.-S. Feng, and B.-P. Zhang, &amp;ldquo;A graph-theoretic approach to classifier combination,&amp;rdquo; in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, 2012, pp. 1017–1020.&lt;/li&gt;
&lt;li&gt;[14] Y.-D. Lan and L. Gao, &amp;ldquo;A New Model of Combining Multiple Classifiers Based on Neural Network,&amp;rdquo; 2013 Fourth Int. Conf. Emerg. Intell. Data Web Technol., no. 2, pp. 154–159, Sep. 2013.&lt;/li&gt;
&lt;li&gt;[15] H. Kuang, X. Zhang, Y.-J. Li, L. L. H. Chan, and H. Yan, &amp;ldquo;Nighttime Vehicle Detection Based on Bio-Inspired Image Enhancement and Weighted Score-Level Feature Fusion,&amp;rdquo; IEEE Trans. Intell. Transp. Syst., vol. 18, no. 4, pp. 927–936, Apr. 2017.&lt;/li&gt;
&lt;li&gt;[16] C. Senaras, M. Ozay, and F. T. Yarman Vural, &amp;ldquo;Building detection with decision fusion,&amp;rdquo; 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Marco general para la combinacion de clasificadores&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;marco-general-para-la-combinacion-de-clasificadores&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#marco-general-para-la-combinacion-de-clasificadores&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The task is seen as a problem of finding a combination function which accepts $N$-dimensional score vectors from each of the $M$ classifiers, then producing a single final classification score representing the selected class.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_combination.png&#34; alt=&#34;Classifier Combination&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given the $N$ possible classes ${\omega_1, \omega_2, \cdots, \omega_N}$ and a pattern $Z$ (that is a data sample), let $x_k$ be the measurement vector (the numerical attributes of $Z$) used by the $k$th classifier (different classifiers may use different attributes to discriminate). The probability density function of the measurement vector is represented by:&lt;/p&gt;
$$
\begin{aligned}
p(x_k|\omega_n)
\end{aligned}
$$&lt;p&gt;while the prior probability of the occurrence of the class is denoted by $p(\omega_n)$. The Bayesian framework aims to determine the class label for the pattern $Z$ by considering the information provided by all $M$ classifiers. The final decision is based on the aposteriori probability, which is the probability of the pattern belonging to a specific class $j$ given the measurement vectors from all classifiers $x_1, x_2, \cdots, x_M$.&lt;/p&gt;
$$
\begin{aligned}
p(\theta = \omega_j|x_1, x_2, \cdots, x_M) = \max_{k} p(\theta = \omega_k | x_1, x_2, \cdots, x_M)
\end{aligned}
$$&lt;p&gt;So the pattern $Z$ is assigned to class $\omega_j$ which produces the maximum a posterior probability. Where the aposteriori distribution is computed as follows:&lt;/p&gt;
$$
\begin{aligned}
p(\theta = \omega_j|x_1, x_2, \cdots, x_M) = \frac{p(x_1, x_2, \cdots, x_M|\theta = \omega_j)p(\theta=\omega_j)}{p(x_1, x_2, \cdots, x_M)}
\end{aligned}
$$&lt;p&gt;Provided that each classifier provides independently a decision support obtained from $x_k$, where $p(x_1, x_2, \cdots, x_M)$ is the joint pdf of the observations independently of class label.&lt;/p&gt;
&lt;p&gt;The important issue is that the individual classifiers should not make identical erroneous decisions on the same observation instances, they should provide complementary information.&lt;/p&gt;
&lt;h2&gt;Estrategias para la combinación de clasificadores&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;estrategias-para-la-combinación-de-clasificadores&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#estrategias-para-la-combinaci%c3%b3n-de-clasificadores&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Levels of Classifiers Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;levels-of-classifiers-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#levels-of-classifiers-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Classifiers combination can be carried out at three different levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Early combination at sensor data level&lt;/strong&gt;: combination of data collected from two or more sensors before feature selection technique is applied.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Combination at feature level&lt;/strong&gt;: it may simply involve basic concatenation of feature vectors with equal or different weights (might result in high dimensonal vectors, whose dimension has to be reduced).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Late combination at the decision level&lt;/strong&gt;: they are based on one of three approaches: abstract, rank, and score:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Abstract-based&lt;/strong&gt;: a single output label from each individual classifier is used as input to the combination scheme.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rank-based&lt;/strong&gt;: each classifier yields several labels ranked from the most likely to the least likely. This information is then used by the combination scheme to reach the final decision.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Score-based&lt;/strong&gt;: each classifier outputs the $n$ best labels together with their confidence scores. The combination can be &lt;strong&gt;density-based&lt;/strong&gt;, &lt;strong&gt;transformation-based&lt;/strong&gt; or &lt;strong&gt;classifier-based&lt;/strong&gt; score fusion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_combination_levels.png&#34; alt=&#34;Classifier Combination Levels&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Hard and Soft Level Classifier Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hard-and-soft-level-classifier-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hard-and-soft-level-classifier-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Another way to categorize combination algorithms is whether hard thresholding or soft scoring is used with each of the classifiers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hard-level combination&lt;/strong&gt;: uses the output of the classifier after it is hard thresholded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Soft-level combination&lt;/strong&gt;: uses estimates of the aposteriori probability of the class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_hard_soft_combination.png&#34; alt=&#34;Classifier Hard/Soft Combination&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The sum, product, max, min rules, etc., fall under the soft level combiners as they use the output aposteriori probability of the classifier or a score.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sum rule&lt;/strong&gt;: The class with the highest sum of probabilities is chosen as the final prediction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Product rule&lt;/strong&gt;: The class with the highest product of probabilities is chosen as the final prediction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max rule&lt;/strong&gt;: the class with the highest posterior probability among all classifications made by individual classifiers is selected as the final prediction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Min rule&lt;/strong&gt;: the class with the lowest posterior probability among all classifications made by individual classifiers is selected as the final prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Majority voting is a typical example of hard-level combiners and has found widespread use in the literature. There are three different versions of voting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unanimous voting.&lt;/li&gt;
&lt;li&gt;More than half voting.&lt;/li&gt;
&lt;li&gt;Highest number of votes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considering the output label vector of the $i$th classifier as:&lt;/p&gt;
$$
\begin{aligned}
[d_{i, 1}, \cdots, d_{i, N}]^T \in [0, 1]^N
\end{aligned}
$$&lt;p&gt;where $i = 1, 2, \cdots, M$ and $d_{i, j} = 1$ if the classifier $D_i$ labels the $i$th instance as class $\omega_j$ and $0$ otherwise. The majority vote results in a decision for class $\omega_k$ if:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i = 1}^M d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M d_{i, j}
\end{aligned}
$$&lt;p&gt;Where $M$ is the total number of classifiers and $N$ is total number of classes. Such that class $\omega_k$ is the most &amp;ldquo;selected&amp;rdquo; on all the classifier.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_combination_majority_voting.png&#34; alt=&#34;Majority Voting&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The accuracy of the combination scheme is given as:&lt;/p&gt;
$$
\begin{aligned}
P_{maj} = \sum_{m = \frac{M}{2} + 1}^M \binom{M}{m} p^m (1-p)^{M - m}
\end{aligned}
$$&lt;p&gt;where $p$ is the probability of correct classification.&lt;/p&gt;
&lt;p&gt;Majority voting provides an accurate class label when at least $\frac{M}{2} + 1$ classifiers give correct classifications, it also requires participating classifiers to have comparable accuracies.&lt;/p&gt;
&lt;h5&gt;Weighted Majority Voting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;weighted-majority-voting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#weighted-majority-voting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Weighted majority voting is used when the classifiers&amp;rsquo; accuracies are not similar, so it is reasonable to assign more weight to the most accurate classifier. Now the decision rule becomes:&lt;/p&gt;
$$
\begin{aligned}
\sum_{i = 1}^M b_i d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M b_i d_{i, j}
\end{aligned}
$$&lt;p&gt;where $b_i$ is the weight associated with classifier $D_i$. For the sake of convenience, it is a good practice to normalize the weights such that the sum is one.&lt;/p&gt;
&lt;p&gt;The weight selection is very important in determining the overall accuracy of the classifier combinations. Therefore, to minimize the classification error of the combination, the weights are assigned as follows:&lt;/p&gt;
$$
\begin{aligned}
b_i \propto \log\left(\frac{p_i}{1 - p_i}\right)
\end{aligned}
$$&lt;p&gt;where $p_i, \cdots, p_M$ are the individual accuracies for each independent classifier.&lt;/p&gt;
&lt;h5&gt;Dynamic Weighted Consult-and-vote&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;dynamic-weighted-consult-and-vote&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dynamic-weighted-consult-and-vote&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [30], Muhlbaier et al. introduced a method for combining ensembles of classifiers using a dynamic weighted consult-and-vote approach for incremental learning of new classes. The proposed technique focuses on incremental learning, specifically for incorporating new classes into the classification system. The consult-and-vote strategy involves a dynamic process where individual classifiers within the ensemble consult with each other to determine their respective voting weights for classifying test instances.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/dynamic_weighted_consult_and_vote.png&#34; alt=&#34;Dynamic Weighted Consult and Vote&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The voting weights assigned to each classifier are determined based on their relative performance on the training data.&lt;/p&gt;
&lt;p&gt;The method proposed by Muhlbaier et al. represents an enhancement over a previous approach developed by the authors. The previous approach may have encountered the &amp;ldquo;out-voting&amp;rdquo; problem, where certain classifiers dominate the decision-making process, potentially leading to biased or inaccurate results.&lt;/p&gt;
&lt;h5&gt;Divide and Conquer&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;divide-and-conquer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#divide-and-conquer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Another modification of majority voting, involving a divide and conquer strategy, is described in [31]. It aims to enhance the majority voting approach by breaking down the classification task into smaller, more manageable sub-problems. Each sub-problem is then addressed independently, with dedicated classifiers or classification algorithms focusing on solving the specific challenges within that subset of data. After solving each of the smaller sub-problems individually, the results or decisions from these segments are combined using a majority voting scheme. Majority voting involves aggregating the outputs of the classifiers or algorithms involved in solving the sub-problems and selecting the class label that receives the most votes as the final decision.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_divide_and_conquer.png&#34; alt=&#34;Divide and Conquer&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h5&gt;Quality Based Combination Techniques&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;quality-based-combination-techniques&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#quality-based-combination-techniques&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The quality-based combination approach, as referenced in [32], focuses on assigning higher weights to more reliable classifiers based on specific quality measures. Quality measures used to assess the performance of classifiers may vary depending on the application domain and specific requirements.&lt;/p&gt;
&lt;h5&gt;Feature Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;feature-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#feature-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The challenge of increasing dimensionality resulting from the simple combination of features from different datasets is addressed through innovative approaches to decision-level combination, as discussed in [33]. Combining features from diverse datasets can lead to a significant increase in the dimensionality of the data. The study proposed an effective approach for decision-level combination by leveraging spectral reflectance and its higher-order derivatives to classify hyperspectral land images. Spectral reflectance and its derivatives provide valuable information about the characteristics of the land surface, which can aid in classification tasks. The study conducted experiments under two scenarios to address the curse of dimensionality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scenario 1 - LDA-Based Dimensionality Reduction&lt;/strong&gt;: Linear Discriminant Analysis (LDA) was employed for dimensionality reduction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scenario 2 - Multiple Classifiers Decision Fusion&lt;/strong&gt; (MCDF): multiple classifiers were utilized for decision fusion to enhance classification performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/classifier_combination_spectral.png&#34; alt=&#34;MCDF System&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h5&gt;Critic Classifier&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;critic-classifier&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#critic-classifier&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [34], the authors introduced an innovative approach that focuses specifically on addressing two-class classification problems. The key feature of this approach is the incorporation of a classifier critic associated with each individual classifier, aimed at predicting the error rate of the classifier. The role of the classifier critic is to forecast the potential error or misclassification rate of its associated classifier. The approach relies on classical standard voting techniques for combining the outputs of multiple classifiers.&lt;/p&gt;
&lt;h5&gt;Adaptative Voting Technique&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;adaptative-voting-technique&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#adaptative-voting-technique&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [35], the authors introduced an adaptive approach to voting techniques. The key innovation in this approach is the dynamic weighting of classifiers based on their estimated recognition performance.&lt;/p&gt;
&lt;h5&gt;Different Model Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;different-model-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#different-model-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [36], the authors explored the combination of three distinct classifiers - Naive Bayes, J48 Decision Tree, and Decision Table - using various voting techniques to enhance classification accuracy. The classifiers were combined using different voting strategies, including simple voting (where each classifier has equal weight), weighted voting (assigning different weights to classifiers based on their performance), and probability-based voting (considering the confidence or probability estimates of classifiers). The study reported that the ensemble of classifiers using weighted and probability-based voting techniques outperformed simple majority voting. In addition to classifier combination and voting strategies, the study applied a supervised dimensionality reduction algorithm to further enhance performance. Dimensionality reduction techniques aim to reduce the complexity of the feature space while preserving relevant information, thereby improving classification accuracy and efficiency.&lt;/p&gt;
&lt;p&gt;In [38], a voting strategy was employed for land cover classification of remotely sensed images by utilizing an ensemble of six different classifiers. Common voting techniques include simple majority voting, weighted voting, or probability-based voting, where the combined decision is based on the collective predictions of the ensemble members.&lt;/p&gt;
&lt;p&gt;In the study referenced as [39], researchers proposed an approach that combines Artificial Neural Network (ANN) and K-Nearest Neighbors (KNN) based classifiers using a majority voting scheme. This method was specifically designed to enhance accuracy in scenarios where sensor data is prone to drift. The researchers employed a majority voting scheme to combine the predictions of the ANN and KNN classifiers. In this approach, each classifier in the ensemble provides a prediction, and the final decision is made based on the majority vote of the individual classifier outputs.&lt;/p&gt;
&lt;p&gt;The primary goal of the study was to leverage the complementary strengths of ANN and KNN classifiers to address the challenge of sensor data drift. ANN models are known for their ability to learn complex patterns from data, while KNN is a non-parametric method based on instance-based learning that can be effective in classification tasks.&lt;/p&gt;
&lt;p&gt;By combining multiple KNN classifiers using majority voting and employing median voting for the ANN classifiers, the researchers observed a substantial improvement in classification performance. The ensemble of ANN and KNN classifiers demonstrated enhanced accuracy in classifying sensor data, showcasing the effectiveness of the majority voting scheme in mitigating the impact of data drift on classification outcomes.&lt;/p&gt;
&lt;h5&gt;Dynamic Entropy Based Combination Technique&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;dynamic-entropy-based-combination-technique&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dynamic-entropy-based-combination-technique&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [37], the authors introduced a novel dynamic entropy-based technique for combining classifiers. The key idea behind the combination scheme is to assign weights to individual classifiers based on their confidence levels in making decisions. Classifiers that exhibit high confidence in their predictions are assigned larger weights, while those with lower confidence receive smaller weights.&lt;/p&gt;
&lt;h5&gt;Assuming Linear Dependency Between Predictors&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;assuming-linear-dependency-between-predictors&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#assuming-linear-dependency-between-predictors&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [40], the authors considered linear dependency of both classifiers and features. To address this, they proposed a new approach that models the dependencies between features without making any assumptions about the distribution of features (independency) or classifiers. The researchers introduced two key models as part of their framework:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Classifier Dependency Modelling&lt;/strong&gt; (LCDM): This model focuses on capturing dependencies between the classifiers themselves, exploring how the outputs of different classifiers may be linearly related.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear Feature Dependency Modelling&lt;/strong&gt; (LFDM) : This model is designed to identify and model dependencies between the features used by the classifiers, allowing for a more comprehensive understanding of the relationships within the feature space.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The results of the study demonstrated that the proposed approach outperformed existing methods in scenarios where linear dependencies between features and classifiers play a significant role. By explicitly modeling these dependencies, the framework was able to capture more complex relationships within the data and improve classification accuracy.&lt;/p&gt;
&lt;h5&gt;Runtime Weighted Opinion Pool&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;runtime-weighted-opinion-pool&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#runtime-weighted-opinion-pool&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [45], introduces a novel classifiers combination approach known as the Runtime Weighted Opinion Pool (RWOP). This approach dynamically assigns weights to the classifiers during runtime based on their local performance, leading to an adaptive and context-aware combination strategy. Unlike traditional weighted sum-based approaches, RWOP utilizes an intuitive runtime strategy to determine the weights for combining classifier outputs. The dynamic weight assignment in RWOP allows the system to adapt to changing conditions and varying input patterns, leading to more robust and accurate classification outcomes.&lt;/p&gt;
&lt;h5&gt;Using Hidden Markov Models&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;using-hidden-markov-models&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#using-hidden-markov-models&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [46], the authors introduce a novel approach for combining classifiers specifically designed for Hidden Markov Model (HMM) based classifiers. Unlike traditional methods where the combination typically occurs at the decision level, this new approach operates at a more elementary level within the HMM framework.&lt;/p&gt;
&lt;h5&gt;Assigning Weights to Classes&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;assigning-weights-to-classes&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#assigning-weights-to-classes&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [12], a novel weighted majority voting approach was proposed for classifiers combination. This approach differs from traditional methods by assigning weights to different classes rather than individual base classifiers. The weights are determined by estimating the joint probability distribution of each class using the scores provided by all classifiers in the combination pool. The joint probability distribution is computed using the Naïve Bayes probabilistic model.&lt;/p&gt;
&lt;h3&gt;Combination Rules&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;combination-rules&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#combination-rules&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In the context of multiple classifiers combination, the way individual classifiers handle input patterns can vary. Some classifiers may use the same representation of the input pattern, while others may employ their own unique representations. The effectiveness of combining these classifiers using different strategies has been explored in various studies, including those referenced as [41], [42], and [43].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple Sum Rule: Despite being developed under restrictive assumptions, the simple sum rule was found to outperform other combination rules in certain scenarios.&lt;/li&gt;
&lt;li&gt;Majority Voting: The experimental results indicated that the majority voting approach was the most effective combination rule for the specific dataset used in the study.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Assesment&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;assesment&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#assesment&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Fixed rule-based combination techniques in the context of multiple classifiers do not require a training stage and rely on class labels, distances, or confidences provided by individual classifiers. The efficiency of fixed rule techniques is influenced by various factors, as discussed in reference [44].&lt;/p&gt;
&lt;p&gt;According to reference [44], fixed rule techniques are most efficient under specific conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Availability of Large Training Sets: Adequate training data is essential for reliable performance.&lt;/li&gt;
&lt;li&gt;Generation of Reliable Confidences: Individual classifiers should provide accurate and trustworthy confidence values.&lt;/li&gt;
&lt;li&gt;Training on Different Feature Spaces: Base classifiers should be trained on diverse feature representations to capture varied aspects of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In scenarios where the strict conditions for fixed rule techniques are not met, a trained combination rule may yield better results.&lt;/p&gt;
&lt;h3&gt;Adaptative and Non-Adaptative Combiners&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;adaptative-and-non-adaptative-combiners&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#adaptative-and-non-adaptative-combiners&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;h4&gt;Adaptative Combiners&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;adaptative-combiners&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#adaptative-combiners&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Adaptive techniques for classifiers combination are mainly based on evolution or artificial intelligence algorithms. They include neural networks combination strategies and genetic algorithms as well as fuzzy set theory. Techniques under these categories are summarized on the following figure:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/adaptative_non_adaptative_combiners.png&#34; alt=&#34;Adaptative and Non-Adaptative Combiners&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Artificial Neural Networks are usually used as a base classifier [29], however, it has also found wide use in combination of classifiers.&lt;/p&gt;
&lt;h5&gt;AD and CM Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;ad-and-cm-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ad-and-cm-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [48], the author introduces two innovative approaches for combining classifiers to enhance robustness and fault tolerance: the Attractor Dynamics (AD) algorithm and the Classifier Masking (CM) algorithm. The CM algorithm is described as a non-neural version of the AD algorithm, inspired by modeling properties of sensory integration in the central nervous system. Both approaches are designed to promote consensus among individual classifiers and improve the overall performance of the combined system by discarding corrupted classifier outputs.&lt;/p&gt;
&lt;h5&gt;ANN vs SVM Combination&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;ann-vs-svm-combination&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ann-vs-svm-combination&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [49], the authors conducted a comparative analysis of combining the outputs of an ensemble of Artificial Neural Networks (ANNs) with Support Vector Machine (SVM) classifiers for processing remotely sensed data. They employed an Multi-Layer Perceptron (MLP) module to facilitate the non-linear combination of the outputs generated by the networks. The researchers explored two distinct approaches for optimizing coefficient selection: the Bayesian method and the error correlation matrix. Through experimental evaluation, the authors found that the MLP-based combination scheme yielded the most favorable results compared to the SVM classifiers.&lt;/p&gt;
&lt;h5&gt;Combination through ANN&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;combination-through-ann&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#combination-through-ann&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [50], the authors proposed a novel approach where they utilized an Artificial Neural Network (ANN) as a model for combining classifiers. Instead of combining the outputs of different classifiers, they integrated various training sets with distinct classifiers to train a unified combination rule within a three-layer ANN architecture. In this setup, each classifier represented a unit in the hidden layer of the ANN.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/nn_based_classifier.png&#34; alt=&#34;NN Based Classifier&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h5&gt;Incorporating A Priori Knowledge&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;incorporating-a-priori-knowledge&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#incorporating-a-priori-knowledge&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [51], the authors delved into the significance of leveraging a priori knowledge within existing classifiers combination techniques, specifically exploring the application of the Behavior Knowledge Space and the Dempster-Shafer (D-S) theory. This investigation aimed to elucidate how incorporating prior knowledge can enhance the performance of classifiers combination, particularly when dealing with strongly correlated classifiers. The study also highlighted the utilization of adaptive combiners, encompassing strategies such as adaptive weighting, associative switching, Mixture of Local Experts (MLE), and Hierarchical MLE&lt;/p&gt;
&lt;h5&gt;Combining Combination Strategies Dynamically&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;combining-combination-strategies-dynamically&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#combining-combination-strategies-dynamically&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [57] an adaptive approach to combining classifiers was introduced. The proposed approach dynamically selects between two different combination strategies based on the belief values obtained from each strategy. Specifically, the study compared the performance of a Bayesian classifiers combination approach and product and max rule combination strategy. In the Bayesian method, the combination of classifiers is based on probabilistic principles, where the posterior probabilities of class labels are calculated using Bayes&amp;rsquo; theorem. The product rule combines the outputs of individual classifiers by multiplying their probabilities or scores for each class. The max rule operates by selecting the class label that receives the highest score or confidence level among all the individual classifiers in the ensemble.&lt;/p&gt;
&lt;h5&gt;Modifying Majority Voting&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;modifying-majority-voting&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#modifying-majority-voting&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In [58] the authors introduced various modifications to the traditional majority voting rule by incorporating a Bayesian framework and a Genetic Algorithm (GA) to determine the weights assigned to different classifiers in the ensemble. The Bayesian framework allowed for the probabilistic modeling of the weights, while the GA provided an optimization technique to search for the best combination of weights that maximized the ensemble performance. The results of the study indicated that the modified majority voting rule, when combined with the Bayesian framework and GA for weight optimization, achieved significant improvements in accuracy. Specifically, the optimal accuracies obtained were $94.3%$ for the majority vote, $95.4%$ for the genetic algorithm, and 95.95% for the Bayesian approach.&lt;/p&gt;
&lt;h5&gt;Based on Genetic Theory&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;based-on-genetic-theory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#based-on-genetic-theory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [22], the authors introduced an innovative approach that involved the simultaneous extraction and selection of features and classifiers to improve the performance of gender and age classification using speech signals collected from a typical Korean home environment.&lt;/p&gt;
&lt;p&gt;The authors employed a Genetic Algorithm to simultaneously select features and classifiers. GA is a metaheuristic optimization technique inspired by the process of natural selection and genetics, used to search for the optimal combination of features and classifiers for the classification task. The outputs of the selected classifiers were combined using the Dempster-Shafer theory, a mathematical theory for combining evidence from different sources to make decisions under uncertainty.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/ga_classifier.png&#34; alt=&#34;GA Classifier&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In [59] a novel approach based on Genetic Algorithm (GA) with self-configuration capabilities was developed for classifier combination. The researchers employed a pool of twelve expert classifiers that were already trained on the task of character recognition, including both printed and handwritten characters. These expert classifiers likely had different strengths and weaknesses, making them suitable candidates for ensemble learning. The GA was integrated into the system to optimize the combination of outputs from the expert classifiers. By using the evolutionary principles of genetic algorithms, the system could iteratively adjust the weights assigned to each classifier in the ensemble to maximize the overall accuracy of the system.&lt;/p&gt;
&lt;h5&gt;Fuzzy Based&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;fuzzy-based&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#fuzzy-based&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;In the study referenced as [16], the authors introduced a novel approach known as Fuzzy Stacked Generalization (FSG) to combine the outputs of multiple classifiers. FSG operates within a hierarchical framework where multiple base-layer classifiers are utilized to make individual predictions on the input data. In FSG, the decisions made by the base-layer classifiers are aggregated to form a decision vector. This decision vector is then fed into a meta-layer classifier, which combines the outputs of the base-layer classifiers to make the final decision.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/fsg_classifier.png&#34; alt=&#34;FSG Classifier&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In [53], the authors proposed a classifiers combination technique using fuzzy templates (FT). An object is labeled with the class whose fuzzy template is closest to the objects&amp;rsquo; decision profile. the authors obtained an improved performance over majority, min, max and product rules, and unweighted average combination techniques&lt;/p&gt;
&lt;p&gt;In [54], an adaptive fuzzy integral was used to combine multiple classifiers. The parameter $\lambda$-fuzzy, which measures performance, is adaptively adjusted depending upon the interaction among the classifiers. The essence of the parameter is to search for the maximum degree of agreement between the conflicting and complementary sources of evidence.&lt;/p&gt;
&lt;p&gt;In [55], a fuzzy decision rule was employed to combine the outputs of multiple classifiers without the need for a training stage. Each classifier was independently applied to the input data, but no final decision was made based on their outputs at this stage. These classifiers are pre-existing models that have been trained on labeled data to make predictions or classifications. The results from the classifiers were aggregated using a fuzzy decision rule. This rule considered the membership degrees of the classes assigned by each classifier and selected the class with the highest membership degree (the confidence or certainty with which each classifier assigns a data point to a specific class) as the correct class. Two measures of accuracy, namely information reliability and global accuracy, were utilized in the combination rule to assess the performance of the combined classifiers.&lt;/p&gt;
&lt;p&gt;In [56] the authors introduced a first-order Takagi-Sugeno-Kang (TSK) fuzzy model for combining multiple classifiers. Unlike conventional linear combination methods that assign different weights to pairs of classifiers and classes, the proposed TSK fuzzy model assigns weights to each individual classifier, class, and region of the classifier output space (decision boundary). This finer granularity in weight assignment allows for a more nuanced and adaptive combination of classifier outputs. The TSK fuzzy model is utilized to integrate the outputs of multiple classifiers. This model leverages fuzzy logic to combine the predictions of individual classifiers in a way that considers the uncertainty and variability in the classifier outputs. The study demonstrated improved accuracy compared to using individual classifiers alone. While the TSK fuzzy model showed promising results in enhancing classification accuracy, the authors did not explicitly address the potential bias and variance reduction that could arise from using a linear model for combining classifiers.&lt;/p&gt;
&lt;h4&gt;Non-Adaptative Combiners&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;non-adaptative-combiners&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#non-adaptative-combiners&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The highest confidence approach is an example of nonadaptive combination techniques. It involves ranking the individual classifiers based on their confidence then selecting the decision of the top ranked one.&lt;/p&gt;
&lt;p&gt;The Borda count technique is also an example of nonadaptive methods. It is based on the principle of single winner classifier in which the individual classifiers provide a ranked list of the classes. It is a more sophisticated alternative to majority voting [60] based on ranking level [9]. It does not require training, just like averaging, sum, and voting rules [52].&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;conclusion&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#conclusion&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In summary, adaptive combiners tend to do better than the non-adaptive types. This is due to the fact that adaptive combiners update the weights given to the individual classifier dynamically before making the final decision. Given the fact that the performance of the individual classifiers can vary over input patterns, such a dynamic combination provides an edge over its non-adaptive counterpart especially when the data space is wide and diverse.&lt;/p&gt;
&lt;h3&gt;Classification Based on the Number of Classifiers&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;classification-based-on-the-number-of-classifiers&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#classification-based-on-the-number-of-classifiers&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The most commonly used techniques for ensemble based combinations are displayed in the following figure:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/multiclass_ensemble_techniques.png&#34; alt=&#34;Multiclass Ensemble Techniques&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bagging is one of the most intuitive and simple techniques used for ensemble based combination. However, unlike bagging, in boosting, the individual classifiers are trained hierarchically to discriminate more complex regions in the feature space. AdaBoost is a variation of the boosting technique. It is an adaptive boosting meta-algorithm that combines outputs of weak classifiers into a weighted sum that represents the final decision. However, the technique is sensitive to noisy data and outliers.&lt;/p&gt;
&lt;p&gt;In [66], the authors used AdaBoost to enhance the performance of a hybrid Hidden Markov Model (HMM) and Neural Network (NN) speech recognition system. HMMs are commonly used for modeling sequential data like speech signals, while NNs are effective in capturing complex patterns in data. The researchers evaluated the performance of the hybrid HMM/NN system with and without the AdaBoost algorithm under noisy environments. Noise in speech signals can introduce distortions and affect the accuracy of the recognition system. By applying AdaBoost, the system was expected to adapt better to noisy conditions and enhance its robustness. The results of the study demonstrated that incorporating AdaBoost into the hybrid HMM/NN speech recognition system led to improved performance, even in the presence of noise. AdaBoost&amp;rsquo;s ability to focus on difficult instances and adjust the weights of the classifiers based on their performance contributed to the system&amp;rsquo;s ability to handle noisy environments and enhance overall recognition accuracy.&lt;/p&gt;
&lt;p&gt;In [67] the researchers explored a combination approach at the feature level using Support Vector Machine (SVM) classifiers and a Global AdaBoost classifier. The study focused on combining features extracted from different datasets at the feature level. By utilizing SVM classifiers and a Global AdaBoost classifier, the researchers aimed to leverage the strengths of both classifiers in integrating information from multiple datasets to improve classification performance. One significant drawback identified in the study regarding feature-level combination is the issue of high dimensionality. Combining features from multiple datasets can result in a large number of features, which can lead to challenges such as increased computational complexity, overfitting, and reduced interpretability of the model.&lt;/p&gt;
&lt;h3&gt;Other Combination Techniques&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;other-combination-techniques&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#other-combination-techniques&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In [68] the researchers introduced a novel classifiers combination technique based on an SVM active learning algorithm. The study proposed a method that leverages Support Vector Machine (SVM) classifiers in conjunction with an active learning algorithm. Active learning refers to a machine learning approach where the algorithm can select the most informative data points for labeling, thereby improving the learning process iteratively. The researchers developed a strategy where an initial classifier, likely an SVM model, is used to generate class aposteriori probabilities. These probabilities serve as inputs to the classifiers-combiner, which is based on the SVM active learning algorithm. The approach outperforms traditional classifiers combination rules when considering class labeling cost and classification accuracy.&lt;/p&gt;
&lt;p&gt;In [70] and [71] the researchers introduced a novel approach using eigenclassifiers for combining correlated classifiers. The proposed method involves utilizing Principal Component Analysis (PCA) projection to create eigenclassifiers from a set of initially correlated classifiers. By applying PCA, the goal is to transform the correlated classifiers into uncorrelated eigen-classifiers. This transformation process aims to enhance the diversity and independence of the classifiers, enabling them to complement each other effectively during the combination stage. The results of the study indicated that the PCA-based eigenclassifiers technique provided better or comparable accuracy with a reduced number of classifiers compared to Bagging and AdaBoost. This suggests that the uncorrelation process facilitated by PCA enhanced the performance of the combined classifiers, leading to improved classification results with fewer individual classifiers.&lt;/p&gt;
&lt;p&gt;Similarly, in [72] the researchers explored methods to address linear and non-linear correlations among the outputs of individual classifiers by leveraging Principal Component Analysis (PCA) and a generalized kernel-based PCA approach. Initially, the authors identified linear correlations among the outputs of individual classifiers. To mitigate these linear correlations, the researchers applied a simple PCA approach. Building on the success of addressing linear correlations, the authors extended their approach to consider non-linear dependencies among the outputs of individual classifiers. o handle these non-linear dependencies, the researchers proposed a generalized kernel-based PCA approach. The results of the experiments demonstrated that the generalized kernel-based PCA approach outperformed alternative methods in improving classification accuracy.&lt;/p&gt;
&lt;p&gt;In [73] the researchers introduced a novel classifier combination technique that focused on extracting class boundaries and utilizing a set of local linear combination rules. The proposed technique involved extracting class boundaries, which are the decision boundaries that separate different classes in the dataset. The researchers employed a set of local linear combination rules to combine the outputs of individual classifiers. These rules likely involved linear combinations of classifier outputs within specific regions of the feature space, allowing for adaptive and context-aware decision-making. The experimental results demonstrated that the classifier combination technique based on class boundaries and local linear combination rules achieved better accuracy compared to other methods such as linear combination, voting, and decision templates.&lt;/p&gt;
&lt;p&gt;In [13], the researchers proposed a weighted averaging approach that incorporated graph-theoretical clustering and a Support Vector Machine (SVM) classifier for classifier combination. The researchers utilized graph-theoretical clustering techniques as part of the weighted averaging approach. Graph theory provides a framework for analyzing relationships between data points, and clustering algorithms can group similar data points together based on certain criteria. By incorporating graph-theoretical clustering, the approach likely aimed to identify clusters of data points with similar characteristics for more effective combination of classifier outputs. In addition to clustering, the approach involved the use of an SVM classifier. SVMs are powerful machine learning models commonly used for classification tasks. By integrating an SVM classifier into the weighted averaging process, the researchers likely leveraged its ability to create optimal decision boundaries between classes in the feature space. The results obtained from the experiments indicated that the proposed approach, despite its simplicity and intuitive nature, performed comparably to more sophisticated methods.&lt;/p&gt;
&lt;p&gt;In [74], the researchers employed three different techniques - Highest Rank (HR), Borda Count (BC), and Logistic Regression (LR) - for combining decisions in a multi-classifier system. The decisions produced by each individual classifier were ranked based on their confidence or accuracy. The HR, BC, and LR techniques were then applied to either reduce the set of possible classes or re-rank them during the combination process. The results obtained from the experiments demonstrated a substantial improvement in the performance of the multi-classifier system.&lt;/p&gt;
&lt;p&gt;Similarly, in [75] a new combination technique called Mixed Group Rank (MGR) was introduced as a novel approach to balancing between preference and confidence in a multi-classifier system. This technique aimed to generalize the principles of Highest Rank (HR), Borda Count (BC), and Logistic Regression (LR) by incorporating elements of both preference-based ranking and confidence-based decision-making.&lt;/p&gt;
&lt;p&gt;In [76], the authors introduced an innovative approach that involves dynamically switching between classifier combination and classifier selection based on the characteristics of different regions in the feature space. The authors further introduced a hybrid combination scheme that integrates clustering-and-selection (CS) techniques with decision template (DT) methods. This hybrid approach likely combines the benefits of clustering for identifying regions of dominance and selection of the most appropriate classifier, along with decision templates for combining classifier outputs in a structured manner. The authors discussed the tradeoff between selecting the best classifier and combining classifiers. This tradeoff likely involves considerations of the strengths and weaknesses of individual classifiers versus the potential benefits of combining multiple classifiers.&lt;/p&gt;
&lt;p&gt;In [77], the authors introduced a method based on classifier selection that focused on identifying the most suitable candidate through confidence evaluation of distance-based classifiers. The method aimed to select the most precise candidate from a set of distance-based classifiers by evaluating their confidence levels. This process likely involved assessing the certainty or reliability of each classifier&amp;rsquo;s decision-making based on the distances between data points in the feature space. The authors likely defined specific rules or criteria for selecting the precise candidate based on the confidence evaluations of the distance-based classifiers. These rules may have considered factors such as the proximity of data points to decision boundaries, the consistency of classifier outputs, or the overall confidence levels of individual classifiers. The experiments conducted in the study likely utilized distance metrics such as Euclidean distance and city block distance for recognizing handwritten characters.&lt;/p&gt;
&lt;p&gt;In [78], the author used information from the confusion matrix to merge multiple classifiers using a class ranking Borda type reconciliation method. The class ranking Borda type reconciliation method is a technique that combines the outputs of multiple classifiers by ranking the classes based on their performance and then using a Borda count approach to reconcile the rankings. The results obtained from this method were compared with three other classifier combination techniques: majority voting, sum rule, and median rule. The comparison was done using three types of confusion matrices: deterministic, uniform, and stochastic. The APBorda (aposteriori Borda count) and sum rule gave the overall best improvement, except in the case of a stochastic confusion matrix and disparate combination (where classifiers had a $10%$ accuracy difference from each other). This means that in most cases, the APBorda and sum rule performed better in combining the classifiers, but there were specific scenarios where they did not perform as well.&lt;/p&gt;
&lt;p&gt;In [79] a combination technique based on the F-measure was proposed for recognizing human emotions using an SVM (Support Vector Machine) classifier. In this technique, the F-measure was used to form a decision matrix to determine the final emotion.&lt;/p&gt;
&lt;p&gt;In [80], the authors proposed an approach for detecting vacant parking spaces by combining two different systems. The first system was based on analyzing image data, while the second system relied on sensor data. The experiments conducted by the authors demonstrated that combining the outputs of these two different systems resulted in a reduced error in detecting vacant parking spaces.&lt;/p&gt;
&lt;p&gt;In summary, several classifiers combination techniques have been proposed in the literature with each technique having its own strengths and weaknesses. Recent techniques mostly involve hybridization or modification of previous techniques to achieve better accuracy or to remove an associated constraint on which a particular technique was built on. Some of these constraints include the issue of correlated classifiers, Gaussian distribution, and IID. There is still a need to develop classifiers combination strategies which are not constrained to specific distributions.&lt;/p&gt;
&lt;h2&gt;References&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;references&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#references&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;[12] C. De Stefano, F. Fontanella, and A. S. di Freca, &amp;ldquo;A Novel Naive Bayes Voting Strategy for Combining Classifiers.,&amp;rdquo; in ICFHR, 2012, pp. 467–472.&lt;/p&gt;
&lt;p&gt;[16] C. Senaras, M. Ozay, and F. T. Yarman Vural, “Building detection with decision fusion,” 2013.&lt;/p&gt;
&lt;p&gt;[22] Y. Zhan, H. Leung, K.-C. Kwak, and H. Yoon, “Automated speaker recognition for home service robots using genetic algorithm and Dempster&amp;ndash;Shafer fusion technique,” Instrum. Meas. IEEE Trans., vol. 58, no. 9, pp. 3058–3068, 2009.&lt;/p&gt;
&lt;p&gt;[29] L. I. Kuncheva, Combining pattern classifiers: methods and algorithms. John Wiley &amp;amp; Sons, 2004.&lt;/p&gt;
&lt;p&gt;[32] N. Poh and J. Kittler, &amp;ldquo;A unified framework for biometric expert fusion incorporating quality measures,&amp;rdquo; Pattern Anal. Mach. Intell. IEEE Trans., vol. 34, no. 1, pp. 3–18, 2012.&lt;/p&gt;
&lt;p&gt;[33] H. R. Kalluri, S. Prasad, and L. M. Bruce, &amp;ldquo;Decision-level fusion of spectral reflectance and derivative information for robust hyperspectral land cover classification,&amp;rdquo; Geosci. Remote Sensing, IEEE Trans., vol. 48, no. 11, pp. 4047–4058, 2010.&lt;/p&gt;
&lt;p&gt;[34] D. J. Miller and L. Yan, &amp;ldquo;Ensemble classification by critic-driven combining,&amp;rdquo; in Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, 1999, vol. 2, pp. 1029–1032&lt;/p&gt;
&lt;p&gt;[35] F. Mattern, T. Rohlfing, and J. Denzler, &amp;ldquo;Adaptive performancebased classifier combination for generic object recognition,&amp;rdquo; in Proc. of International Fall Workshop Vision, Modeling and Visualization (VMV), 2005, pp. 139–146&lt;/p&gt;
&lt;p&gt;[36] G. Jain, A. Ginwala, and Y. A. Aslandogan, &amp;ldquo;An approach to text classification using dimensionality reduction and combination of classifiers,&amp;rdquo; in Information Reuse and Integration, 2004. IRI 2004. Proceedings of the 2004 IEEE International Conference on, 2004, pp. 564–569.&lt;/p&gt;
&lt;p&gt;[37] M. Magimai-Doss, D. Hakkani-Tur, O. Cetin, E. Shriberg, J. Fung, and N. Mirghafori, &amp;ldquo;Entropy based classifier combination for sentence segmentation,&amp;rdquo; in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, 2007, vol. 4, p. IV&amp;ndash;189&lt;/p&gt;
&lt;p&gt;[39] S. Adhikari and S. Saha, &amp;ldquo;Multiple classifier combination technique for sensor drift compensation using ANN &amp;amp; KNN,&amp;rdquo; in Advance Computing Conference (IACC), 2014 IEEE International, 2014, pp. 1184–1189.&lt;/p&gt;
&lt;p&gt;[40] A. J. Ma, P. C. Yuen, and J.-H. Lai, &amp;ldquo;Linear dependency modeling for classifier fusion and feature combination,&amp;rdquo; Pattern Anal. Mach. Intell. IEEE Trans., vol. 35, no. 5, pp. 1135–1148, 2013.&lt;/p&gt;
&lt;p&gt;[43] Z. Wu, C.-H. Li, and V. Cheng, &amp;ldquo;Large margin maximum entropy machines for classifier combination,&amp;rdquo; in Wavelet Analysis and Pattern Recognition, 2008. ICWAPR’08. International Conference on, 2008, vol. 1, pp. 378–383&lt;/p&gt;
&lt;p&gt;[44] R. P. W. Duin, &amp;ldquo;The combining classifier: to train or not to train?,&amp;rdquo; in Pattern Recognition, 2002. Proceedings. 16th International Conference on, 2002, vol. 2, pp. 765–770.&lt;/p&gt;
&lt;p&gt;[45] W. Wang, A. Brakensiek, and G. Rigoll, &amp;ldquo;Combination of multiple classifiers for handwritten word recognition,&amp;rdquo; in Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International Workshop on, 2002, pp. 117–122.&lt;/p&gt;
&lt;p&gt;[48] A. V Bogdanov, &amp;ldquo;Neuroinspired architecture for robust classifier fusion of multisensor imagery,&amp;rdquo; Geosci. Remote Sensing, IEEE Trans., vol. 46, no. 5, pp. 1467–1487, 2008.&lt;/p&gt;
&lt;p&gt;[49] G. Pasquariello, N. Ancona, P. Blonda, C. Tarantino, G. Satalino, and A. D’Addabbo, &amp;ldquo;Neural network ensemble and support vector machine classifiers for the analysis of remotely sensed data: a comparison,&amp;rdquo; in Geoscience and Remote Sensing Symposium, 2002. IGARSS’02. 2002 IEEE International, 2002, vol. 1, pp. 509–511.&lt;/p&gt;
&lt;p&gt;[50] Y.-D. Lan and L. Gao, &amp;ldquo;A New Model of Combining Multiple Classifiers Based on Neural Network,&amp;rdquo; in Emerging Intelligent Data and Web Technologies (EIDWT), 2013 Fourth International Conference on, 2013, pp. 154–159.&lt;/p&gt;
&lt;p&gt;[51] V. Di Lecce, G. Dimauro, A. Guerriero, S. Impedovo, G. Pirlo, and A. Salzo, &amp;ldquo;Knowledge-based methods for classifier combination: an experimental investigation,&amp;rdquo; in Image Analysis and Processing, 1999. Proceedings. International Conference on, 1999, pp. 562–565.&lt;/p&gt;
&lt;p&gt;[52] A. K. Jain, R. P. W. Duin, and J. Mao, “Statistical pattern recognition: A review,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 22, no. 1, pp. 4–37, 2000.&lt;/p&gt;
&lt;p&gt;[53] L. I. Kuncheva, J. C. Bezdek, and M. A. Sutton, “On combining multiple classifiers by fuzzy templates,” in Fuzzy Information Processing Society-NAFIPS, 1998 Conference of the North American, 1998, pp. 193–197&lt;/p&gt;
&lt;p&gt;[54] T. D. Pham, “Combination of multiple classifiers using adaptive fuzzy integral,” in Artificial Intelligence Systems, 2002.(ICAIS 2002). 2002 IEEE International Conference on, 2002, pp. 50–55.&lt;/p&gt;
&lt;p&gt;[55] M. Fauvel, J. Chanussot, and J. A. Benediktsson, “Decision fusion for the classification of urban remote sensing images,” Geosci. Remote Sensing, IEEE Trans., vol. 44, no. 10, pp. 2828–2838, 2006.&lt;/p&gt;
&lt;p&gt;[56] M. Cococcioni, B. Lazzerini, and F. Marcelloni, “A TSK fuzzy model for combining outputs of multiple classifiers,” in Fuzzy Information, 2004. Processing NAFIPS’04. IEEE Annual Meeting of the, 2004, vol. 2, pp. 871–876.&lt;/p&gt;
&lt;p&gt;[57] Y. Yaslan and Z. Cataltepe, “Co-training with adaptive bayesian classifier combination,” in Computer and Information Sciences, 2008. ISCIS’08. 23rd International Symposium on, 2008, pp. 1–4.&lt;/p&gt;
&lt;p&gt;[58] L. Lam and C. Y. Suen, “Optimal combinations of pattern classifiers,” Pattern Recognit. Lett., vol. 16, no. 9, pp. 945–954, 1995.&lt;/p&gt;
&lt;p&gt;[59] K. Sirlantzis and M. C. Fairhurst, “Optimisation of multiple classifier systems using genetic algorithms,” in Image Processing, 2001. Proceedings. 2001 International Conference on, 2001, vol. 1, pp. 1094–1097&lt;/p&gt;
&lt;p&gt;[66] H. Schwenk, “Using boosting to improve a hybrid HMM/neural network speech recognizer,” in Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, 1999, vol. 2, pp. 1009–1012.&lt;/p&gt;
&lt;p&gt;[67] J. Hu and Y. Chen, “Offline Signature Verification Using Real Adaboost Classifier Combination of Pseudo-dynamic Features,” in Document Analysis and Recognition (ICDAR), 2013 12th International Conference on, 2013, pp. 1345–1349&lt;/p&gt;
&lt;p&gt;[68] X. Yi, Z. Kou, and C. Zhang, “Classifier combination based on active learning,” in Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on, 2004, vol. 1, pp. 184–187.&lt;/p&gt;
&lt;p&gt;[69] J. Kremer, K. Steenstrup Pedersen, and C. Igel, “Active learning with support vector machines,” Wiley Interdiscip. Rev. Data Min. Knowl. Discov., vol. 4, no. 4, pp. 313–326, Jul. 2014.&lt;/p&gt;
&lt;p&gt;[70] A. Ulaş, O. T. Yıldız, and E. Alpaydın, “Eigenclassifiers for combining correlated classifiers,” Inf. Sci. (Ny)., vol. 187, pp. 109– 120, 2012.&lt;/p&gt;
&lt;p&gt;[71] E. Ulaş, A., Semerci, M., Yıldız, O. T., &amp;amp; Alpaydın, “Incremental construction of classifier and discriminant ensembles,” Inf. Sci. (Ny)., vol. 179, no. 9, pp. 1298–1318, 2009&lt;/p&gt;
&lt;p&gt;[72] U. Ekmekci and Z. Cataltepe, “Classifier combination with kernelized eigenclassifiers,” in Information Fusion (FUSION), 2013 16th International Conference on, 2013, pp. 743–749.&lt;/p&gt;
&lt;p&gt;[73] M. Liu, K. Li, and R. Zhao, “A boundary based classifier combination method,” in Control and Decision Conference, 2009. CCDC’09. Chinese, 2009, pp. 3777–3782&lt;/p&gt;
&lt;p&gt;[74] T. K. Ho, J. J. Hull, and S. N. Srihari, “Decision combination in multiple classifier systems,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 16, no. 1, pp. 66–75, 1994.&lt;/p&gt;
&lt;p&gt;[75] O. Melnik, Y. Vardi, and C.-H. Zhang, “Mixed group ranks: Preference and confidence in classifier combination,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 26, no. 8, pp. 973–981, 2004.&lt;/p&gt;
&lt;p&gt;[76] L. I. Kuncheva, “Switching between selection and fusion in combining classifiers: An experiment,” Syst. Man, Cybern. Part B Cybern. IEEE Trans., vol. 32, no. 2, pp. 146–156, 2002.&lt;/p&gt;
&lt;p&gt;[77] C.-L. Liu and M. Nakagawa, “Precise candidate selection for large character set recognition by confidence evaluation,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 22, no. 6, pp. 636–641, 2000.&lt;/p&gt;
&lt;p&gt;[78] J. R. Parker, “Combining multiple non-homogeneous classifiers: an empirical approach,” in Cognitive Informatics, IEEE International Conference on, 2002, p. 288.&lt;/p&gt;
&lt;p&gt;[79] A. Agrawal and N. K. Mishra, “Fusion Based Emotion Recognition System,” in 2016 International Conference on Computational Science and Computational Intelligence (CSCI), 2016, pp. 727–732&lt;/p&gt;
&lt;p&gt;[80] Junzhao, L., Mohandes, M., Deriche, M., “A Multi-Classifier Image Based Vacant Parking Detection System”, IEEE International Conference on Electronics, Circuits, and Systems ICESC, pp. 933-936, Abu Dhabi, UAE, DEC 8-11, 2013&lt;/p&gt;
&lt;h2&gt;Conclusiones&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;conclusiones&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#conclusiones&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;While a large number of combination techniques have been proposed, the literature still lacks a comprehensive performance analysis of such techniques for a given application. The review showed that while one strategy (e.g. fusion at decision level) may outperform others for a given application, the results from such a strategy may not be the best for another application.&lt;/p&gt;
&lt;p&gt;However overall, it was shown that classifiers combinations in general improve performance significantly over individual classifiers for most problems.&lt;/p&gt;
&lt;p&gt;An important research direction relies on adding an enhancement stage (post processing) to the classifiers output before applying combination rules. This would improve the performance of individual classifiers before the combination stage. Many classifiers combination techniques have performed well under certain restrictions which include independence assumption, Gaussian distribution, linear process, limited class problem (mostly 2-class problem) and low dimensional feature space. Thus, future work can reconsider relaxing some of these constraints&lt;/p&gt;
&lt;p&gt;Another issue that needs to be further investigated is to explore the advantages of using different strategies for the fusion including probabilistic, learning, decision based, or evidence based techniques.&lt;/p&gt;
&lt;p&gt;The discussion on voting based approaches has shown that there is a scope for improving classification accuracy. This issue also offers numerous opportunities for developing optimization techniques to determine the weights. Some of the approaches including GA, PSO, and Ant optimization techniques, among others, can be investigated.&lt;/p&gt;
&lt;p&gt;Neural networks as well as similar models such as fuzzy networks, deep neural networks; SVM, etc. also offer an excellent opportunity for developing adaptive techniques to combine individual classifiers outputs. For example, can individual classifiers be considered as layers of more general architectures.&lt;/p&gt;
&lt;p&gt;Computational complexity is an important issue that needs further research especially when the different algorithms are deployed over mobile or low power platform&lt;/p&gt;
&lt;p&gt;An important issue which is still open is that of finding the optimal number of classifiers to be combined for a given application. Additionally, for a given number of features, is there a way to distribute these among the different classifiers to be combined.&lt;/p&gt;
&lt;p&gt;Finally, the use of hybrid approaches to integrate results from different combination techniques, offers further opportunities for solving more involved applications.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Tema 4. Aprendizaje No Supervisado</title>
      <link>//localhost:1313/notes/datascience/master/aaii/04_unsupervised_ml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/notes/datascience/master/aaii/04_unsupervised_ml/</guid>
      <description>
        
        
        &lt;h2&gt;Introduction&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;introduction&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#introduction&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In this chapter we address unsupervised learning. In this case one has a set of $N$ observations $(x_1,x_2, \cdots ,x_N)$ of a random vector $X$ having joint density $\text{Pr}(X)$. The goal is to directly infer the properties of this probability density without the help of a supervisor or
teacher providing correct answers or degree-of-error for each observation.&lt;/p&gt;
&lt;p&gt;Principal components, multidimensional scaling, self-organizing maps, and principal curves try to find simpler patterns in complex data. They look for lower-dimensional structures in the data that capture where most of the data points lie. By doing this, they help us understand how variables are related to each other and if they can be thought of as being controlled by a smaller group of underlying factors.&lt;/p&gt;
&lt;p&gt;Cluster analysis looks for groups or clusters in the data that are like little &amp;ldquo;bumps&amp;rdquo; or &amp;ldquo;peaks&amp;rdquo; where the data is most concentrated, that is convex regios of the $X$-space that contain modes of $\text{Pr}(X)$. It helps us see if the data can be divided into different types or categories. Mixture modeling aims for the same thing. Association rules try to find simple rules or patterns that describe where the data is most concentrated, especially when dealing with data that has many features and is either present or absent (binary-valued).&lt;/p&gt;
&lt;p&gt;In unsupervised learning, where we don&amp;rsquo;t have clear outcomes to compare against. We often have to rely on guesswork and intuition to decide if the results make sense or not. This uncertainty has led to many different methods being proposed, but ultimately, it&amp;rsquo;s hard to know for sure which one is the best since there&amp;rsquo;s no straightforward way to check their effectiveness.&lt;/p&gt;
&lt;h2&gt;Cluster Analysis&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;cluster-analysis&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#cluster-analysis&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Proximity Matrices&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;proximity-matrices&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#proximity-matrices&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Sometimes the data is represented directly in terms of the proximity (alikeness or affinity) between pairs of objects. This type of data can be represented by an $N \times N$ matrix $D$, where $N$ is the number of objects, and each element $d_{ij}$ records the proximity between the $j$th and $j$th objects. This matrix is then provided as input to the clustering algorithm.&lt;/p&gt;
&lt;p&gt;Most algorithms assume symmetric dissimilarity matrices, so if the original matrix $D$ is not symmetric it must be replaced by $\frac{(D + D^T)}{2}$.&lt;/p&gt;
&lt;h3&gt;Dissimilarities Based on Attributes&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;dissimilarities-based-on-attributes&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#dissimilarities-based-on-attributes&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Since most of the popular clustering algorithms take a dissimilarity matrix as their input, we must first construct pairwise dissimilarities between the observations. By far the most common
choice is squared distance:&lt;/p&gt;
$$
\begin{aligned}
d_j(x_{ij}, x_{i&#39;j}) = (x_{ij} - x_{i&#39;j})^2
\end{aligned}
$$&lt;p&gt;where $j$ denotes the attribute and $i, i&amp;rsquo;$ denotes the instance.&lt;/p&gt;
&lt;p&gt;We first discuss alternatives in terms of the attribute type:&lt;/p&gt;
&lt;h4&gt;Quantitative variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;quantitative-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#quantitative-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Measurements of this type of variable or attribute are represented by continuous real-valued numbers. One way to do this is by looking at the absolute difference between them:&lt;/p&gt;
$$
\begin{aligned}
d(x_i, x_{i&#39;}) = l(|x_i - x_{i&#39;}|)
\end{aligned}
$$&lt;p&gt;Alternatively, clustering can be based on the correlation&lt;/p&gt;
$$
\begin{aligned}
\rho (x_i, x_{i&#39;}) = \frac{\sum_{j} (x_{ij} - \overline{x}_i)(x_{ij} - \overline{x}_i)}{\sqrt{\sum_{j} (x_{ij} - \overline{x}_i)^2 \sum_j(x_{ij} - \overline{x}_i)^2}}
\end{aligned}
$$&lt;h4&gt;Ordinal Variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;ordinal-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#ordinal-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Error measures for ordinal variables are generally defined by replacing their $M$ original values with:&lt;/p&gt;
$$
\begin{aligned}
\frac{i - \frac{1}{2}}{M}, i = 1, \cdots, M
\end{aligned}
$$&lt;p&gt;in the prescribed order of their original values. They are then treated as quantitative variables on this scale.&lt;/p&gt;
&lt;h4&gt;Categorical variables&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;categorical-variables&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#categorical-variables&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;If the variable assumes $M$ distinct values, these can be arranged in a symmetric $M \times M$ matrix with elements:&lt;/p&gt;
$$
\begin{aligned}
m_{ij} = \begin{cases}
0 &amp; x_{i} = x_{j} \\
1 &amp; x_{i} \neq x_{j} \\
\end{cases}
\end{aligned}
$$&lt;h3&gt;Object Dissimilarity&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;object-dissimilarity&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#object-dissimilarity&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Next we define a procedure for combining the $p$-individual attribute dissimilarities $d_j(x_{ij},x_{i&amp;rsquo;j}), j = 1,2, \cdots, p$ into a single overall measure of dissimilarity $D(x_i, x_i&amp;rsquo;)$.&lt;/p&gt;
&lt;p&gt;This is nearly always done by means of a weighted average:&lt;/p&gt;
$$
\begin{aligned}
D(x_i, x_{i&#39;}) = \sum_{j=1}^p w_j d_j(x_{ij}, x_{i&#39;j})
\end{aligned}
$$&lt;p&gt;where:&lt;/p&gt;
$$
\begin{aligned}
\sum_{j=1}^p w_j = 1
\end{aligned}
$$&lt;p&gt;Here $w_j$ is a weight assigned to the $j$th attribute regulating the relative influence of that variable in determining the overall dissimilarity between objects.&lt;/p&gt;
&lt;p&gt;If the goal is to discover natural groupings in the data, some attributes may exhibit more of a grouping tendency than others. Variables that are more relevant in separating the groups should be assigned a higher influence in defining object dissimilarity. Giving all attributes equal influence
in this case will tend to obscure the groups to the point where a clustering algorithm cannot uncover them.&lt;/p&gt;
&lt;p&gt;Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering algorithm.&lt;/p&gt;
&lt;h3&gt;Clustering Algorithms&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;clustering-algorithms&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#clustering-algorithms&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Clustering algorithms fall into three distinct types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Combinatorial algorithms&lt;/strong&gt;: work directly on the observed data with no direct reference to an underlying probability model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixture modeling&lt;/strong&gt;: supposes that the data is an i.i.d sample from some population described by a probability density function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mode seeking&lt;/strong&gt;: take a nonparametric perspective, attempting to directly estimate distinct modes of the probability density function&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Combinatorial Algorithms&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;combinatorial-algorithms&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#combinatorial-algorithms&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Each observation is uniquely labeled by an integer $i \in {1, \cdots, N}$. One seeks the particular encoder $C^*(i)$ that assigns the $i$th observation to the $k$th cluster that satisfies the required goal based on the dissimilarities $d(x_i, x_{i&amp;rsquo;})$. The &amp;ldquo;parameters&amp;rdquo; of the procedure are the individual cluster assignments for each of the $N$ observations. These are adjusted so as to minimize a &amp;ldquo;loss&amp;rdquo; function.&lt;/p&gt;
&lt;p&gt;Since the goal is to assign close points to the same cluster, a natural loss function would be:&lt;/p&gt;
$$
\begin{aligned}
W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k}\sum_{C(i&#39;) = k} d(x_i, d_{i&#39;})
\end{aligned}
$$&lt;p&gt;This measure tells us how close together the things in the same group are. It is sometimes referred to as the &amp;ldquo;within cluster&amp;rdquo; point scatter. The total point scatter is given by:&lt;/p&gt;
$$
\begin{aligned}
T = \frac{1}{2} \sum_{i=1}^N\sum_{i&#39;=1}^N d_{ii&#39;} = \frac{1}{2}\sum_{k=1}^K\sum_{C(i) = k} \left(\sum_{C(i&#39;) = k} d_{ii&#39;} +  \sum_{C(i&#39;) \neq k} d_{ii&#39;}\right)
\end{aligned}
$$&lt;p&gt;This basically divides, for each $i$th instance on cluster $k$, so $C(i) = k$, the distances into two categories, distances to instances on the same cluster $\sum_{C(i&amp;rsquo;) = k} d_{ii&amp;rsquo;}$, and distances to instances on a different cluster $\sum_{C(i&amp;rsquo;) \neq k} d_{ii&amp;rsquo;}$. That is:&lt;/p&gt;
$$
\begin{aligned}
T = W(C) + B(C)
\end{aligned}
$$&lt;p&gt;where $B(C)$ is the between-cluster point scatter:&lt;/p&gt;
$$
\begin{aligned}
B(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k} \sum_{C(i&#39;)\neq k} d_{ii&#39;}
\end{aligned}
$$&lt;p&gt;So, minimizing $W(C)$ is equivalent to maximizing $B(C)$ given $W(C) = T - B(C)$.&lt;/p&gt;
&lt;p&gt;Cluster analysis by combinatorial optimization is straightforward in principle. One simply minimizes $W$ or equivalently maximizes $B$ over all possible assignments of the $N$ data points to $K$ clusters. Unfortunately, such optimization by complete enumeration is feasible only for very small data sets.&lt;/p&gt;
&lt;p&gt;For this reason, practical clustering algorithms are able to examine only a very small fraction of all possible encoders $k = C(i)$. The goal is to identify a small subset that is likely to contain the optimal one, or at least a good suboptimal partition. Such feasible strategies are based on iterative greedy descent. An initial partition is specified. At each iterative step, the cluster assignments are changed in such a way that the value of the criterion is improved from its previous value. Clustering algorithms of this type differ in their prescriptions for modifying the cluster assignments at each iteration. When the prescription is unable to provide an improvement, the algorithm terminates with the current assignments as its solution. However, these algorithms converge to local optima which may be highly suboptimal when compared to the global optimum.&lt;/p&gt;
&lt;h4&gt;K-Means&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;k-means&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#k-means&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The K-means algorithm is one of the most popular iterative descent clustering methods, which uses the squared Euclidean distance. So the within point-scatter can be written as:&lt;/p&gt;
$$
\begin{aligned}
W(C) = \frac{1}{2} \sum_{k=1}^K\sum_{C(i) = k}\sum_{C(i&#39;)=k} ||x_i - x_{i&#39;}||^2
\end{aligned}
$$$$
\begin{aligned}
= \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - \overline{x}_k||^2
\end{aligned}
$$&lt;p&gt;where $\overline{x}&lt;em&gt;k$ is the mean vector associated with the $k$th cluster and $N_k = \sum&lt;/em&gt;{i=1}^N I(C(i) = k)$ is the number of instances on the $k$th cluster. Therefore the goal is to group the $N$ observations into $K$ clusters in a way that minimizes the average difference between each observation and the mean of its cluster.&lt;/p&gt;
$$
\begin{aligned}
C^* \min_{C} \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - \overline{x}_k||^2
\end{aligned}
$$&lt;p&gt;can be obtained by noting that for any set of observations $S$:&lt;/p&gt;
$$
\begin{aligned}
\overline{x}_S = \arg \min_{m} \sum_{i \in S} ||x_i - m||^2
\end{aligned}
$$&lt;p&gt;That is we defined the centroid for $S$ as the point $m$ that minimizes the sum of distances for each instance on $S$. Hence we can obtain $C^*$ by solving the enlarged optimization problem:&lt;/p&gt;
$$
\begin{aligned}
\min_{\{C, m_k\}^K_1} \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - m_k||^2
\end{aligned}
$$&lt;p&gt;Where for each cluster $k$, we search for the encoder $C$ and the optimal centroid $m_k$. Thus the optmimization process can be performed in two steps as seen in Algorithm 14.1. On (1) we obtain the optimal centroids $m_i, i = 1, \cdots, K$, and on (2) we try to find the best encoder $C$ given the set of centroids ${m_1, \cdots, m_K}$.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/k_means_algorithm.png&#34; alt=&#34;KMeans Algorithm&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Gaussian Mixtures as Soft K-means Clustering&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;gaussian-mixtures-as-soft-k-means-clustering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#gaussian-mixtures-as-soft-k-means-clustering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The K-means clustering procedure is closely related to the EM algorithm for estimating a certain Gaussian mixture model.&lt;/p&gt;
&lt;p&gt;The E-step of the EM algorithm assigns &amp;ldquo;responsibilities&amp;rdquo; for each data point based in its relative density under each mixture component (cluster). While the M-step recomputes the component density parameters based on the current responsibilities. Where the relative density is a monotone function of the euclidean distance between the data point and the mixture center. Hence in this setup EM is a &amp;ldquo;soft&amp;rdquo; version of K-means clustering, making probabilistic (rather than deterministic) assignments of points to cluster centers.&lt;/p&gt;
&lt;p&gt;As $\sigma^2$ tends to $1$, these probabilities become $0$ and $1$, and the two methods coincide:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/k_means_vs_gaussian_mixtures.png&#34; alt=&#34;KMeans and Gaussian Mixtures&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Vector Quantization&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;vector-quantization&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#vector-quantization&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Vector quantization (VQ) is a technique used in data compression, particularly in the compression of digital signals or images. It involves representing a large set of data points (vectors) by a smaller set of representative values, called codewords or centroids. These centroids are selected from the original data set and are used to approximate the original data. By replacing groups of similar data points with these representative values, vector quantization can significantly reduce the amount of data needed to represent the information while minimizing the loss of quality.&lt;/p&gt;
&lt;p&gt;In this example of vector quantization, given an image of $1024\times 1024$ pixels we start by dividing the image into small blocks of $2\times 2$. Each block, which contains four pixels, is treated like a tiny picture of its own. Each of the $512 \times 512$ blocks of four numbers is regarded as vector in $\mathbb{R}^4$. Then, using K-means clustering, we group these blocks together based on their similarity.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/vector_quantization.png&#34; alt=&#34;Vector Quantization Example&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The clustering process is called the &lt;strong&gt;encoding step&lt;/strong&gt;, and the collection of centroids is called the &lt;strong&gt;codebook&lt;/strong&gt;. Why do we expect VQ to work at all? The reason is that for typical everyday images like photographs, many of the blocks look the same. What we have described is known as lossy compression, since our images are degraded versions of the original.&lt;/p&gt;
&lt;h4&gt;K-Medoids&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;k-medoids&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#k-medoids&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The K-Means algorithm can be generalized for use with arbitrarily defined dissimilarities $D(x_i, x_{i&amp;rsquo;})$. The process of finding the centers of the clusters stays similar, but the way we measure similarity can change. This makes the algorithm versatile and applicable to various types of data. This gives way to the K-medoids algorithm:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/k_medoids_algorithm.png&#34; alt=&#34;K Medoids Algorithm&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Practical Issues&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;practical-issues&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#practical-issues&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In order to apply K-means or K-medoids one must select the number of clusters $K^*$ and an initialization.&lt;/p&gt;
&lt;p&gt;The latter can be defined by specifying an initial set of centers ${m_1,\cdots,m_K}$ or ${i_1, \cdots,i_K}$ or an initial encoder $C(i)$. A choice for the number of clusters $K$ depends on the goal. For data segmentation $K$ is usually defined as part of the problem.&lt;/p&gt;
&lt;p&gt;Data-based methods for estimating $K^*$ typically examine the withincluster dissimilarity $W_K$ as a function of the number of clusters $K$. The corresponding values generally decrease with increasing $K$. Thus cross-validation techniques, so useful for model selection in supervised learning, cannot be utilized in this context.&lt;/p&gt;
&lt;p&gt;As we increase the number of clusters, the solution quality will improve because more natural groups will be captured separately. So, if we keep adding more clusters beyond the true number ($K &amp;gt; K^*$), some estimated clusters will start to split the real groups. However, splitting a group that&amp;rsquo;s already close together won&amp;rsquo;t improve the solution as much as properly separating two distinct groups.&lt;/p&gt;
&lt;p&gt;To the extent this scenario is realized, there will be a sharp decrease in successive differences in criterion value, $W_K − W_{K+1}$, at $K = K^&lt;strong&gt;$. That is, ${W_K − W_{K+1} |K &amp;lt; K^&lt;/strong&gt;} \geq {W_K − W_{K+1} |K \geq K^&lt;strong&gt;}$. An estimate $\hat{K}&lt;/strong&gt;$ for $K^*$ is then obtained by identifying a “kink” in the plot of $W_K$ as a function of $K$.&lt;/p&gt;
&lt;p&gt;The recently proposed Gap statistic compares the shape of a curve based on our data to a curve we&amp;rsquo;d get if the data were spread out evenly. We&amp;rsquo;re looking for a point where there&amp;rsquo;s a big gap between these two curves. This point tells us the best number of clusters for our data. If $G(K)$ is the Gap curve at $K$ clusters, the formal rule for estimating $K^*$ is:&lt;/p&gt;
$$
\begin{aligned}
K^* = \text{argmin}_{K} \{K | G(K) \geq G(K + 1 - s&#39;_{K + 1}) \}
\end{aligned}
$$&lt;p&gt;where $s_K$ is the standard deviation. The following figure shows and example on how to choose the optimal number of clusters:&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/gap_statistic.png&#34; alt=&#34;Gap Statistic&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h4&gt;Hierarchical Clustering&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-clustering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-clustering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;As the name suggests, they produce hierarchical representations in which the clusters at each level of the hierarchy are created by merging clusters at the next lower level. At the lowest level, each cluster contains a single observation. At the highest level there is only one cluster containing all of the data.&lt;/p&gt;
&lt;p&gt;Strategies for hierarchical clustering divide into two basic paradigms: agglomerative (bottom-up) and divisive (top-down). Each level of the hierarchy represents a particular grouping of the data into disjoint clusters of observations. The entire hierarchy represents an ordered sequence of such groupings. It is up to the user to decide which level (if any) actually represents a &amp;ldquo;natural&amp;rdquo; clustering&lt;/p&gt;
&lt;p&gt;A dendrogram provides a highly interpretable complete description of the hierarchical clustering in a graphical format. Cutting the dendrogram horizontally at a particular height partitions the data into disjoint clusters represented by the vertical lines that intersect it. The height at which we cut represents the level of similarity required to form a cluster. Generally, groups that merge at higher levels in the dendrogram are considered more significant clusters. However, it&amp;rsquo;s essential to be cautious when interpreting dendrograms because different clustering methods or slight changes in the data can lead to different dendrogram structures.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/dendogram_example.png&#34; alt=&#34;Dendogram Example&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also, dendrogram interpretations are only valid if the data truly exhibits the hierarchical structure imposed by the clustering algorithm. This can be assesed using the &lt;em&gt;cophenetic correlation coefficient&lt;/em&gt;, it measures the correlation between the &lt;em&gt;cophenetic dissimilarities&lt;/em&gt; $C_{ii&amp;rsquo;}$ and the distances between observations in the original data $d_{ii&amp;rsquo;}$. The &lt;em&gt;cophenetic dissimilarity&lt;/em&gt; $C_{ii&amp;rsquo;}$ between two observations $(i, i&amp;rsquo;)$ is the intergroup dissimilarity at which observations $i$ and $i&amp;rsquo;$ are first joined together in the same cluster.&lt;/p&gt;
&lt;h5&gt;Agglomerative Clustering&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;agglomerative-clustering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#agglomerative-clustering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Agglomerative clustering starts with each observation as its own cluster. Then, at each step, it merges the two closest clusters together until there&amp;rsquo;s only one big cluster left. To measure dissimilarity between two clusters, let&amp;rsquo;s call them $G$ and $H$. We look at all the pairs of observations, one from $G$ and one from $H$, and find the dissimilarity between them.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Single linkage&lt;/em&gt; (SL) or nearest neighbour agglomerative clustering chooses the closest pair of observations between the two clusters as the measure of dissimilarity between the clusters:&lt;/p&gt;
$$
\begin{aligned}
d_{SL}(G, H) = \min_{i \in G, i&#39; \in H} d_{ii&#39;}
\end{aligned}
$$&lt;p&gt;&lt;em&gt;Complete linkage&lt;/em&gt; (CL) or furthest-neighbor technique agglomerative clustering measures the dissimilarity between two clusters as the distance the pair of observations that are the farthest apart.&lt;/p&gt;
$$
\begin{aligned}
d_{CL}(G, H) = \max_{i \in G, i&#39; \in H} d_{ii&#39;}
\end{aligned}
$$&lt;p&gt;&lt;em&gt;Group average&lt;/em&gt; (GA) clustering uses the average dissimilarity between the groups:&lt;/p&gt;
$$
\begin{aligned}
d_{GA}(G, H) = \frac{1}{N_G N_H} \sum_{i \in G} \sum_{i \in H} d_{ii&#39;}
\end{aligned}
$$&lt;p&gt;where $N_G$ and $N_H$ are the respective number of observations in each group.&lt;/p&gt;
&lt;p&gt;
    &lt;img src=&#34;../assets/agglomerative_clustering_dissimilarity_comparison.png&#34; alt=&#34;Agglomerative Clustering Dissimilarity Comparison&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If the data shows clear clusters that are close together and distinct from each other, all three methods—single linkage, complete linkage, and average linkage—will give similar results. However, if the data doesn&amp;rsquo;t exhibit this pattern, the results of the three methods will differ. Single linkage tends to join clusters even if just one pair of observations is close together, which can lead to long chains of connections between clusters. This phenomenon is known as &lt;em&gt;chaining&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Complete linkage will tend to produce compact clusters with small diameters because it considers two groups close only if all the observations in their combined set are similar. Sometimes it may violate the rule that observations within a cluster should be closer to each other than to observations in other clusters.&lt;/p&gt;
&lt;p&gt;Group average clustering strikes a balance between single and complete linkage. It tries to make clusters compact while keeping them relatively far apart. However, its outcome can be affected by how the dissimilarities between observations are measured. Changing the measurement scale can change the clustering result. In contrast, single and complete linkage methods are not affected by such changes in scale.&lt;/p&gt;
&lt;h5&gt;Divisive Clustering&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;divisive-clustering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#divisive-clustering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;Divisive clustering starts with the entire dataset as one cluster and then splits it into smaller clusters step by step. This method isn&amp;rsquo;t as widely studied as agglomerative clustering, but it has been explored, especially in engineering contexts like compression.&lt;/p&gt;
&lt;p&gt;One potential advantage of divisive clustering is when you want to divide the data into only a few clusters. Divisive methods can be used recursively with techniques like K-means or K-medoids to split clusters into smaller ones. However the way you begin the splitting process at each step can influence the final outcome.&lt;/p&gt;
&lt;p&gt;A method has been developed to overcome this limitations. The divisive algorithm, proposed by Macnaughton Smith et al. (1965), is defined as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Puts all observations in one big cluster called $G$.&lt;/li&gt;
&lt;li&gt;It picks the observation that&amp;rsquo;s farthest on average from all the others and makes it the first member of a new cluster called $H$.&lt;/li&gt;
&lt;li&gt;At each successive step that observation in $G$ whose average distance from those in $H$, minus that for the remaining observations in $G$ is largest, is transferred to $H$.&lt;/li&gt;
&lt;li&gt;This continues until there are no more observations in $G$ that are closer to those in $H$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This splitting procedure is repeated for each new cluster formed at the previous level, creating a hierarchical structure. Kaufman and Rousseeuw (1990) suggest choosing the cluster with the largest diameter for splitting at each level, but another option is to pick the one with the largest average dissimilarity among its members.&lt;/p&gt;
&lt;h2&gt;Evaluation Metrics&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;evaluation-metrics&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#evaluation-metrics&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Formal Limitations of Clustering&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;formal-limitations-of-clustering&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#formal-limitations-of-clustering&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Jon Kleinberg proposes three axioms that highlight the characteristics that a grouping problem should exhibit and can be considered &amp;ldquo;good&amp;rdquo;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Scale Invariance&lt;/strong&gt;: indicates that a clustering algorithm should not modify its results when all distances between points are scaled by the factor determined by a constant $\alpha$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Richness&lt;/strong&gt;: the clustering function must be flexible enough to produce any arbitrary partition/clustering of the input data set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: the clustering results do not change if the distances within clusters decrease and/or the distances between clusters increase.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given the above three axioms, Kleinberg proves the following theorem: For every $n \geq 2$, there is no clustering function $f$ that satisfies scale invariance, richness, and consistency. Since the three axioms cannot hold simultaneously, clustering algorithms can be designed to violate one of the axioms while sarisfying the other two.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$k$-cluster stopping condition: Stop merging clusters when we have $k$ clusters (violates the richness axiom).&lt;/li&gt;
&lt;li&gt;Distance $r$ stopping condition: Stop merging clusters when the nearest pair of clusters are farther than $r $ (violates scale invariance).&lt;/li&gt;
&lt;li&gt;Scale-$\epsilon$ stopping condition: Stop merging clusters when the nearest pair of clusters are farther than a fraction $\epsilon$ the maximum pairwise distance $\Delta$. (consistency is violated).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Methods for Clustering Evaluation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;methods-for-clustering-evaluation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#methods-for-clustering-evaluation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When analyzing clustering results, several aspects must be taken into account for the validation of the algorithm results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determining the clustering tendency in the data (i.e. whether non-random structure really exists).&lt;/li&gt;
&lt;li&gt;Determining the correct number of clusters.&lt;/li&gt;
&lt;li&gt;Assessing the quality of the clustering results without external information.&lt;/li&gt;
&lt;li&gt;Comparing the results obtained with external information.&lt;/li&gt;
&lt;li&gt;Comparing two sets of clusters to determine which one is better.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first three issues are addressed by &lt;strong&gt;internal or unsupervised validation&lt;/strong&gt;, because there is no use of external information. The fourth issue is resolved by &lt;strong&gt;external or supervised validation&lt;/strong&gt;. Finally, the last issue can be addressed by both supervised and unsupervised validation techniques.&lt;/p&gt;
&lt;h4&gt;Null Hypothesis Testing&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;null-hypothesis-testing&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#null-hypothesis-testing&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;One of the desirable characteristics of a clustering process is to show whether data exhibits some tendency to form actual clusters. In this case, the null hypothesis $H_0$ is the randomness of data and, when the null hypothesis is rejected, we assume that the data is significantly unlikely to be random.&lt;/p&gt;
&lt;p&gt;One of the difficulties of null hypothesis testing in this context is determining the statistical distribution under which the randomness hypothesis can be rejected. Jain and Dubes propose three alternatives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random plot hypothesis $H_0$: all proximity matrices of order $n \times n$ are equally likely.&lt;/li&gt;
&lt;li&gt;Random label hypothesis $H_0$: all permutations of labels of $n$ objects are equally likely.&lt;/li&gt;
&lt;li&gt;Randon position hypothesis $H_0$: all sets of $n$ locations is some region of a $d$-dimensional space are equally likely.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Internal Validation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;internal-validation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#internal-validation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Internal validation methods (or internal indices) make it possible to establish the quality of the clustering structure without having access to external information. In general, two types of internal validation metrics can be combined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cohesion measures: evaluates how closely the elements of the same cluster are to each other.&lt;/li&gt;
&lt;li&gt;Separation measures: quantify the level of separation between clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Internal indices are usually employed in conjunction with two clustering algorithm families: hierarchical clustering algorithms and partitional algorithms. For partitional algorithms, metrics based on the proximity matrix, as well as metrics of cohesion and separation, such as the silhouette coefficient, are often used. For hierarchical algorithms, the cophenetic coefficient is the most common.&lt;/p&gt;
&lt;h4&gt;Partitional Methods&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;partitional-methods&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#partitional-methods&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In general, the internal validation value of a set of $K$ clusters can be decomposed as the sum of the validation values for each cluster:&lt;/p&gt;
$$
\begin{aligned}
\text{general validity} = \sum_{i=1}^K w_i \text{validity}(C_i)
\end{aligned}
$$&lt;p&gt;This measure of validity can be cohesion, separation, or some combination of both. Quite often, the weights that appear in the previous expression correspond to cluster size. The individual measures of cohesion and separation are defined as follows:&lt;/p&gt;
$$
\begin{aligned}
\text{cohesion}(C_i) = \sum_{x \in C_i, y \in C_i} \text{proximity}(x, y)
\end{aligned}
$$$$
\begin{aligned}
\text{separation}(C_i, C_j) = \sum_{x \in C_i, y \in C_j} \text{proximity}(x, y)
\end{aligned}
$$&lt;p&gt;It should be noted that the cohesion metric defined above is equivalent to the cluster SSE [Sum of Squared Errors]:&lt;/p&gt;
$$
\begin{aligned}
SSE(C_i) = \sum_{x \in C_i} d(c_i, x)^2 = \frac{1}{2m_i} \sum_{x \in C_i} \sum_{y \in C_i} d(x, y)^2
\end{aligned}
$$&lt;p&gt;Likewise, we can maximize the distance between clusters using a separation metric. This approach leads to the between group sum of squares, or SSB:&lt;/p&gt;
$$
\begin{aligned}
SSB = \sum_{i = 1}^K m_i d(c_i, c)^2 = \frac{1}{2K} \sum_{i=1}^K \sum_{j = 1}^K \frac{m}{K} d(c_i, c_j)^2
\end{aligned}
$$&lt;p&gt;where $c_i$ is the mean of the $i$th cluster and $c$ is the overall mean.&lt;/p&gt;
&lt;p&gt;Instead of dealing with separate metrics for cohesion and separation, there are several metrics that try to quantify the level of separation and cohesion in a single measure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Calisnki-Harabasz coefficient: it is a measure based on the internal dispersion of clusters and the dispersion between clusters. We would choose the number of clusters that maximizes the CH.&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
CH = \frac{\frac{SSB_M}{M - 1}}{\frac{SSE_M}{M}}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The Dunn index is the ratio of the smallest distance between data from different clusters and the largest distance between clusters. Again, this ratio should be maximized:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
D = \min_{1 &lt; i &lt; k} \left\{\min_{1 &lt; j &lt; k, i\neq j} \left\{\frac{\delta (C_i, C_j)}{\max_{1 &lt; l &lt; k} \{\Delta (C_l)\}}\right\}\right\}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The Xie-Beni score was designed for fuzzy clustering, but it can applied to hard clustering. It is a ratio whose numerator estimates the level of compaction of the data within the same cluster and whose denominator estimates the level of separation of the data from different clusters:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
XB = \frac{\sum_{i=1}^N \sum_{k=1}^M u^2_{ik} ||x_i - C_k||^2}{N_{t \neq s} \min (||C_t - C_s||^2)}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The Ball-Hall index is a dispersion measure based on the quadratic distances of the cluster points with respect to their centroid&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
BH = \frac{SSE_M}{M}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The Hartigan index is based on the logarithmic relationship between the sum of squares within the cluster and
the sum of squares between clusters:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
H = \log \left(\frac{SSB_M}{SSE_M}\right)
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The Xu coefficient takes into account the dimensionality $D$ of the data, the number $N$ of data examples, and the sum of squared errors $SSE_M$ form $M$ clusters:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
X_u = D \log_2 \left(\sqrt{\frac{SSE_M}{DN^2}}\right) + \log M
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;The silhouette coefficient is the most common way to combine the metrics of cohesion and separation in a single measure. Its computation is divided into four steps:
&lt;ol&gt;
&lt;li&gt;Compute the average intracluster distance for each example $i$: $a(i) = \frac{1}{|C_a|} \sum_{j \in C_a, i \neq j} d(i, j)$&lt;/li&gt;
&lt;li&gt;Compute the minimum intercluster distance for each example $i$: $b(i) = \min_{C_b \neq C_a} \frac{1}{|C_b|} \sum_{j \in C_b} d(i, j)$&lt;/li&gt;
&lt;li&gt;Compute the silhouette coefficient for each example $i$: $s(i) = \frac{b(i) - a(i)}{max(a(i), b(i))}$&lt;/li&gt;
&lt;li&gt;Compute the silhouette puntuation as the average of the silhouette coefficients: $S = \frac{1}{n} \sum_{i = 1}^n s(i)$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The silhouette is defined in the interval $[-1, 1]$. Positive values indicate a high separation between clusters, negative values are an indication that the clusters are mixed with each other. When the silhouette
coefficient is zero, it is an an indication that the data are uniformly distributed throughout the Euclidean space.&lt;/p&gt;
&lt;p&gt;Unfortunately, one of the main drawbacks of the silhouette coefficient is its high computational complexity.&lt;/p&gt;
&lt;p&gt;Cohesion and separation metrics are not the only validation method available for partitional clustering techniques. In fact, cohesion and separation metrics do not perform well when it comes to analyzing results obtained by algorithms based on density analysis.&lt;/p&gt;
&lt;p&gt;One way to validate clustering is by comparing the actual proximity matrix with an ideal version based on the provided clustering by the algorithm. If we reorder rows and columns so that all examples of the same cluster appear together, the ideal proximity matrix has a block diagonal structure. High correlation between the actual and ideal proximity matrices indicates that examples in the same cluster are close to each other, although it may not be a good measure for density-based clustering.&lt;/p&gt;
&lt;p&gt;Imagine you have a table where each row and column represents a data point, and the cells contain numbers indicating how similar or close those data points are to each other. Now, if you group similar data points together into clusters, you can rearrange the rows and columns of the table so that all the data points within each cluster are together. When you do this, the table will have a diagonal pattern where each cluster forms a block of closely related data points. This diagonal pattern is what we mean by a &amp;ldquo;block diagonal structure&amp;rdquo; in the context of a proximity matrix.&lt;/p&gt;
&lt;p&gt;Unfortunately, the mere construction of the whole proximity matrix is computationally expensive.&lt;/p&gt;
&lt;h4&gt;Hierarchical Methods&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hierarchical-methods&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hierarchical-methods&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;h5&gt;Cophenetic Correlation Coefficient&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;cophenetic-correlation-coefficient&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#cophenetic-correlation-coefficient&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;The cophenetic distance between two examples is the proximity at which an agglomerative hierarchical clustering algorithm puts the examples in the same cluster for the first time. The cophenetic correlation coefficient (CPCC) is defined as he correlation between the entries of the cophenetic matrix $P_c$ containing cophenetic distances, and the proximity matrix $P$, containing similarities. The cophenetic correlation coefficient is then defined as:&lt;/p&gt;
$$
\begin{aligned}
\text{CPCC} = \frac{\sum_{i &lt; j} (d_{ij} - \overline{d})(d_{ij}^** - \overline{d}^**)}{\sqrt{\sum_{i &lt; j} (d_{ij} - \overline{d})^2 \sum_{i &lt; j}(d_{ij}^** - \overline{d}^**)}}
\end{aligned}
$$&lt;p&gt;where $d_{ij}$ is the distance between the example pair $(i, j)$, $d_{ij}^&lt;strong&gt;$ is their cophenetic distance, $\overline{d}$ is the average of the distances in the proximity matrix and $d_{ij}^&lt;/strong&gt;$ is the average of the cophenetic distances in the cophenetic matrix.&lt;/p&gt;
$$
\begin{aligned}
\overline{d} = \frac{\sum_{i &lt; j} d_{ij}}{2(n^2 - n)}
\end{aligned}
$$$$
\begin{aligned}
\overline{d}^** = \sqrt{\frac{\sum_{i &lt; j} (d_{ij} - d_{ij}^**)^2}{\sum_{i &lt; j} (d_{ij}^*)^2}}
\end{aligned}
$$&lt;p&gt;The cophenetic correlation coefficient is a value in the interval $[−1, 1]$. High CPCC values indicate a high level of similarity between the two matrices, an indication that the clustering algorithm has been able to identify the underlying structure of its input data.&lt;/p&gt;
&lt;h5&gt;Hubert Statistic&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hubert-statistic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hubert-statistic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h5&gt;&lt;p&gt;First, concordance are discordance are defined for pairs of examples. A pair $(i, j)$ is concordant when $((v_{p_i} &amp;lt; v_{c_i}) \&amp;amp; (v_{p_j} &amp;lt; v_{c_j}))$ or $((v_{p_i} &amp;gt; v_{c_i}) \&amp;amp; (v_{p_j} &amp;gt; v_{c_j}))$. And it is said to be discordant when $((v_{p_i} &amp;lt; v_{c_i}) \&amp;amp; (v_{p_j} &amp;gt; v_{c_j}))$ or $((v_{p_i} &amp;gt; v_{c_i}) \&amp;amp; (v_{p_j} &amp;lt; v_{c_j}))$. Therefore, a pair is neither concordant nor discordant if $v_{p_i} = v_{c_i}$ or $v_{p_j} = v_{c_j}$.&lt;/p&gt;
&lt;p&gt;Let $S_+$ and $S_-$ be the number of concordant and discordant pairs, respectively. Then, the Hubert coefficient is defined as:&lt;/p&gt;
$$
\begin{aligned}
\gamma = \frac{S_+ - S_-}{S_+ + S_-}
\end{aligned}
$$&lt;p&gt;The Hubert statistic is between $-1$ and $1$. It has been mainly used to compare the results of two hierarchical clustering algorithms. A higher Hubert $\gamma$ value corresponds to a better clustering of data.&lt;/p&gt;
&lt;h3&gt;External Validation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;external-validation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#external-validation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;External validation proceeds by incorporating additional information in the clustering validation process, i.e. external class labels for the training examples. We want to compare the result of a clustering algorithm
$C = {C_1, C_2, \cdots, C_m}$ to a potentially different partition of data $P = {P_1, P_2, \cdots, P_s}$ which might represent the expert knowledge of the analyst (his experience or intuition), prior knowledge of the data in the form of class labels, the results obtained by another clustering algorithm, or simply a grouping considered to be &amp;ldquo;correct&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;In order to carry out this analysis, a contingency matrix must be built to evaluate the clusters detected by the algorithm that encompasses the following data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$TP$: The number of data pairs found in the same cluster, both in $C$ and in $P$.&lt;/li&gt;
&lt;li&gt;$FP$: The number of data pairs found in the same cluster in $C$ but in different clusters in $P$.&lt;/li&gt;
&lt;li&gt;$FN$: The number of data pairs found in different clusters in $C$ but in the same cluster in $P$.&lt;/li&gt;
&lt;li&gt;$TN$: The number of data pairs found in different clusters, both in $C$ and in $P$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Matching Sets&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;matching-sets&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#matching-sets&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Several measures can be defined to measure the similarity between the clusters in $C$, obtained by the clustering algorithm, and the clusters if $P$, corresponding to our prior (external) knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Precision: $Pr = \frac{TP}{TP + FP}$&lt;/li&gt;
&lt;li&gt;Recall: $R = \frac{TP}{TP + FN}$&lt;/li&gt;
&lt;li&gt;F-measure: $F_{\alpha} = \frac{1 + \alpha}{\frac{1}{Pr} + \frac{\alpha}{R}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quite often, precision and recall are evenly combined with an unweighted harmonic mean ($\alpha = 1$):&lt;/p&gt;
$$
\begin{aligned}
F = \frac{2 \cdot Pr \cdot R}{Pr + R}
\end{aligned}
$$&lt;ul&gt;
&lt;li&gt;Purity: evaluates whether each cluster contains only examples from the same class:&lt;/li&gt;
&lt;/ul&gt;
$$
\begin{aligned}
U = \sum_{i} p_i (\max_j \frac{p_{ij}}{p_i})
\end{aligned}
$$&lt;p&gt;where $p_i = \frac{n_i}{n}$, $p_j = \frac{n_j}{n}$ and $p_{ij} = \frac{n_{ij}}{n}$. Where $n_{ij}$ are the number of examples belonging to the class $i$ found in the cluster $j$ and $n_i$ is the number of examples in the cluster $i$.&lt;/p&gt;
&lt;h4&gt;Peer-to-peer Correlation&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;peer-to-peer-correlation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#peer-to-peer-correlation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A second family of measures for external validation are based on the correlation between pairs, i.e. they seek to measure the similarity between two partitions under equal conditions, such as the result of a grouping process for the same set, but by means of two different methods $C$ and $P$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Jaccard Coefficient: $J = \frac{TP}{TP + FP + FN}$&lt;/li&gt;
&lt;li&gt;The Rand Coefficient: $Rand = \frac{TP + TN}{M}$&lt;/li&gt;
&lt;li&gt;The Folkes and Mallows coefficient: $FM = \sqrt{\frac{TP}{TP + FP} \cdot \frac{TP}{TP + FN}}$&lt;/li&gt;
&lt;li&gt;The Hubert statistical coefficient: $\Gamma = \frac{1}{M} \sum_{i=1}^{n - 1} \sum_{j = i + 1}^{n} X_{ij} Y_{ij}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where $n_{ij}$ are the number of examples belonging to the class $i$ found in the cluster $j$ and $n_i$ is the number of examples in the cluster $i$.&lt;/p&gt;
&lt;h4&gt;Measures Based on Information Theory&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;measures-based-on-information-theory&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#measures-based-on-information-theory&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This family includes basic measures such as entropy and mutual information, as well as their respective normalized variants.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy: $H = - \sum_{i} p_i \left(\sum_{j} \frac{p_{ij}}{p_i} \log \frac{p_{ij}}{p_i}\right)$&lt;/li&gt;
&lt;li&gt;Mutual Information: $MI = \sum_{i} \sum_{j} p_{ij} \log \frac{p_{ij}}{p_i p_j}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where $p_{ij} = \frac{n_{ij}}{n}$ and $p_i = \frac{n_i}{n}$.&lt;/p&gt;
&lt;h3&gt;Hyperparameter Tuning&lt;span class=&#34;hx-absolute -hx-mt-20&#34; id=&#34;hyperparameter-tuning&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hyperparameter-tuning&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Even though external validation metrics can help us evaluate whether the obtained clusters closely match the underlying categories in the training data, which the clustering algorithm tries to identify without externally-provided class labels, those metrics cannot address other issues such as the right number of clusters for our current data set.&lt;/p&gt;
&lt;p&gt;Hyperparameter tuning tries to determine, for the different possible values of the parameters in $P_{alg}$ , which set of parameter values is the most suitable for our particular clustering problem. We could proceed in the following way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the algorithm does not include the number of clusters $n_c$ among its parameters, we run the algorithm with different values for its parameters so that we can determine their largest range for which $n_c$ remains constant. Later, we choose as parameter values the values in the middle of this range.&lt;/li&gt;
&lt;li&gt;When the algorithm parameters Palg include the desired number of clusters $n_c$, we run the algorithm for a range of values for $n_c$. For each value of $n_c$, we run the algorithm multiple times using different sets of values (i.e. starting from different initial conditions) and choose the value that optimizes our desired validation metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When we just want to determine the “right” number of clusters, $n_c$, plotting the validation results for different values of $n_c$ can sometimes show a relevant change in the validation metric, commonly referred to as a &amp;ldquo;knee&amp;rdquo; or &amp;ldquo;elbow&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Hyperparameter tuning can then be seen as a combinatorial optimization problem using different strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grid Search: is based on a systematic exploration of the hyperparameter space.&lt;/li&gt;
&lt;li&gt;Random Search: chooses parameter configurations at random.&lt;/li&gt;
&lt;li&gt;Smart Search techniques try to optimize the problem of searching for hyperparameter values. Different strategies can be implemented, such as Bayesian optimization using Gaussian processes and evolutionary optimization using genetic algorithms or evolution strategies.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
