<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../src/style/custom.css">
    <title>Colors, Depth Testing, and Alpha Blending</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Colors, Depth Testing, and Alpha Blending"><h1 id="Colors, Depth Testing, and Alpha Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending">Colors, Depth Testing, and Alpha Blending</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="06.html#Using%20Colors%20in%20WeblGL">Using Colors in WeblGL</a>

</li><li>
<a href="06.html#Using%20Colors%20in%20Objects">Using Colors in Objects</a>

<ul>
<li>
<a href="06.html#Constant%20Coloring">Constant Coloring</a>

</li><li>
<a href="06.html#Per-Vertex%20Coloring">Per-Vertex Coloring</a>

</li><li>
<a href="06.html#Per-Fragment%20Coloring">Per-Fragment Coloring</a>

</li></ul>
</li><li>
<a href="06.html#Use%20of%20Color%20in%20Lights">Use of Color in Lights</a>

<ul>
<li>
<a href="06.html#Scalability%20Problem">Scalability Problem</a>

</li><li>
<a href="06.html#How%20Many%20Uniforms%20Can%20We%20Use">How Many Uniforms Can We Use</a>

</li><li>
<a href="06.html#Simplifying%20the%20Problem">Simplifying the Problem</a>

</li><li>
<a href="06.html#Using%20Uniform%20Arrays%20to%20Handle%20Multiple%20Lights">Using Uniform Arrays to Handle Multiple Lights</a>

</li><li>
<a href="06.html#Directional%20Point%20Lights">Directional Point Lights</a>

<ul>
<li>
<a href="06.html#Attenuation%20Factor">Attenuation Factor</a>

</li></ul>
</li></ul>
</li><li>
<a href="06.html#Use%20of%20Color%20in%20the%20Scene">Use of Color in the Scene</a>

<ul>
<li>
<a href="06.html#Transparency">Transparency</a>

</li><li>
<a href="06.html#Updated%20Rendering%20Pipeline">Updated Rendering Pipeline</a>

</li></ul>
</li><li>
<a href="06.html#Depth%20Testing">Depth Testing</a>

<ul>
<li>
<a href="06.html#Depth%20Function">Depth Function</a>

</li></ul>
</li><li>
<a href="06.html#Alpha%20Blending">Alpha Blending</a>

<ul>
<li>
<a href="06.html#The%20Blending%20Function">The Blending Function</a>

</li><li>
<a href="06.html#Separate%20Blending%20Functions">Separate Blending Functions</a>

</li><li>
<a href="06.html#The%20Blend%20Equation">The Blend Equation</a>

</li><li>
<a href="06.html#WebGL%20Alpha-Blending%20API">WebGL Alpha-Blending API</a>

</li><li>
<a href="06.html#The%20Blend%20Color">The Blend Color</a>

</li><li>
<a href="06.html#Alpha%20Blending%20Modes">Alpha Blending Modes</a>

<ul>
<li>
<a href="06.html#Additive%20Blending">Additive Blending</a>

</li><li>
<a href="06.html#Substractive%20Blending">Substractive Blending</a>

</li><li>
<a href="06.html#Multiplicative%20Blending">Multiplicative Blending</a>

</li><li>
<a href="06.html#Interpolative%20Blending">Interpolative Blending</a>

</li></ul>
</li></ul>
</li><li>
<a href="06.html#Creating%20Transparent%20Objects">Creating Transparent Objects</a>

</li></ul>
<hr>

<div id="Colors, Depth Testing, and Alpha Blending-Using Colors in WeblGL"><h2 id="Using Colors in WeblGL" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Using Colors in WeblGL">Using Colors in WeblGL</a></h2></div>

<p>
WebGL supplies a fourth attribute to the RGB model. This attribute is called the alpha channel. The extended model then is known as the RGBA model, where A stands for alpha. The alpha channel contains a value between the range of \(0.0\) to \(1.0\).
</p>

<p>
A completely opaque color will have an alpha value of \(1.0\), whereas a completely transparent color will have an alpha value of \(0.0\). This is the general case, but as we will see, we need to take other factors into account when we obtain translucent colors.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Using Colors in Objects"><h2 id="Using Colors in Objects" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Using Colors in Objects">Using Colors in Objects</a></h2></div>

<div id="Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Constant Coloring"><h3 id="Constant Coloring" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Constant Coloring">Constant Coloring</a></h3></div>

<p>
To obtain a constant color, we store the desired color in a uniform that is passed to the fragment shader. This uniform is usually called the object's diffuse material property. We can also combine object normals and light-source information to obtain a Lambert coefficient. We can use the Lambert coefficient to proportionally change the reflecting color depending on the angle on which the light hits the object.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Per-Vertex Coloring"><h3 id="Per-Vertex Coloring" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Per-Vertex Coloring">Per-Vertex Coloring</a></h3></div>

<p>
To implement per-vertex coloring, we need to define an attribute that stores the color for the vertex in the vertex shader:
</p>

<pre>in vec4 aVertexColor;
</pre>

<p>
The next step is to assign the aVertexColor attribute to a varying so that it can be passed to the fragment shader. Remember that varyings are automatically interpolated. Therefore, each fragment will have a color that is the weighted result of its contributing vertices.
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/constant_vs_per_vertex_color.png" alt="Constant vs Per Vertex Color">
</p>

<p>
If we want our color map to be sensitive to lighting conditions, we can multiply each vertex color by the diffuse component of the light. The result is then assigned to the varying that will transfer the result to the fragment shader.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Per-Fragment Coloring"><h3 id="Per-Fragment Coloring" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Using Colors in Objects-Per-Fragment Coloring">Per-Fragment Coloring</a></h3></div>

<p>
We can also assign a random color to each pixel of the object we are rendering.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights"><h2 id="Use of Color in Lights" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights">Use of Color in Lights</a></h2></div>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Scalability Problem"><h3 id="Scalability Problem" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Scalability Problem">Scalability Problem</a></h3></div>

<p>
Given the desire to use more than one light in our scene, we need to define and map the number of appropriate uniforms of the lighting model of choice. If we have four properties per light (ambient, diffuse, specular, and location), we need to define four uniforms for each light. If we want to have three lights, we need to write, use, and map twelve uniforms! We need to resolve this complexity before it gets out of hand.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-How Many Uniforms Can We Use"><h3 id="How Many Uniforms Can We Use" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-How Many Uniforms Can We Use">How Many Uniforms Can We Use</a></h3></div>

<p>
To find out the limit for your WebGL implementation, you can query WebGL using the <code>gl.getParameter</code> function with these constants:
</p>

<pre>gl.MAX_VERTEX_UNIFORM_VECTORS
gl.MAX_FRAGMENT_UNIFORM_VECTORS
</pre>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Simplifying the Problem"><h3 id="Simplifying the Problem" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Simplifying the Problem">Simplifying the Problem</a></h3></div>

<p>
In order to simplify the problem, we can assume that the ambient component is the same for all of the lights. This will reduce the number of uniformsâ€”one fewer uniform for each light.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Using Uniform Arrays to Handle Multiple Lights"><h3 id="Using Uniform Arrays to Handle Multiple Lights" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Using Uniform Arrays to Handle Multiple Lights">Using Uniform Arrays to Handle Multiple Lights</a></h3></div>

<p>
As we've seen, handling light properties with individual uniforms makes the code verbose and difficult to maintain. Fortunately, ESSL provides several mechanisms we can use to solve the problem of handling multiple lights. One of them is <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Using Uniform Arrays to Handle Multiple Lights-uniform arrays"></span><strong id="uniform arrays">uniform arrays</strong>.
</p>

<p>
This technique allows us to handle multiple lights by introducing enumerable arrays of vectors in the shaders. This allows us to calculate light contributions by iterating through the light arrays in the shaders.
</p>

<pre c="">uniform vec3 uPositionLight[3];
</pre>

<p>
Itâ€™s important to note that ESSL does not support dynamic initialization of uniform arrays.
We could try something such as this, but will not work:
</p>

<pre c="">uniform int numLights;
uniform vec3 uPositionLight[numLights];
</pre>

<p>
However, this construct is valid:
</p>

<pre c="">const int numLights = 3;
uniform vec3 uPositionLight[numLights];
</pre>

<p>
To map these variables on javascript:
</p>

<pre javascript="">const lightPosition1 = [0, 7, 3];
const lightPosition2 = [2.5, 3, 3];
const lightPosition3 = [-2.5, 3, 3];

const location = gl.getUniformLocation(program, 'uPositionLight');

// The values are concatenated on a single flat array
gl.uniform3fv(location, [...lightPosition1, ...lightPosition2, ...lightPosition3]);
</pre>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights"><h3 id="Directional Point Lights" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights">Directional Point Lights</a></h3></div>

<p>
In this section, we will combine directional and positional lights creating a <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights-directional point light"></span><strong id="directional point light">directional point light</strong>, commonly referred to as a <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights-spot light"></span><strong id="spot light">spot light</strong>.
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/directional_point_ligth.png" alt="Directional Point Light">
</p>

<p>
The trick to creating these lights is to subtract the light-direction vector from the normal for each vertex. The resulting vector will create a different Lambert coefficient that will reflect into the cone generated by the light source. On the following excerpts we show a practical example of how a sportlight could be implemented. On the first place we have the vertex shader whose responsability it is to compute:
</p>

<ul>
<li>
The rays, <code>vRay</code>, for each light source. This is the vector between the position of the light and the position of the vertex (after is has been transformed by the model view).

</li><li>
The "modified" normal, <code>vTransformedNormals</code>. This stores the normals after the ligth direction vector has been substracted from it.

</li></ul>
<p>
We will use both of these vectors to compute the lambert term on the fragment shader.
</p>

<pre c="">#version 300 es

const int numLights = 3;

uniform mat4 uModelViewMatrix;
uniform mat4 uNormalMatrix;
uniform mat4 uProjectionMatrix;

uniform vec3 uLightPositions[numLights];
uniform vec3 uLightDirections[numLights];

in vec3 aPos;
in vec3 aNormal;

out vec3 vRay[numLights];
out vec3 vTransformedNormals[numLights];

void main(void) {
  vec4 vertex = uModelViewMatrix * vec4(aPos, 1.0);

  
  vec3 normal = vec3(uNormalMatrix * vec4(aNormal, 1.0));

  // Iterate over each light
  for(int i = 0; i &lt; numLights; i++) {
    // Define each ray as the vector berween the light and the vertex
    vec4 lightPosition = uModelViewMatrix * vec4(uLightPositions[i], 1.0);
    vRay[i] = vertex.xyz - lightPosition.xyz;
    // Transform the direction of the light
    vec3 directionLight = vec3(uNormalMatrix * vec4(uLightDirections[i], 1.0));
    // Transform the normal by substracting the direction of each light
    vTransformedNormals[i] = normal - directionLight;
  }

  gl_Position = uProjectionMatrix * vertex;
}
</pre>

<p>
Here we have the fragment shader where we compute the final color. This color is made up from two main sources:
</p>

<ul>
<li>
Ambient light

</li><li>
Diffuse light: these light is computed taking into account a number of lights given <code>numLights</code>. For each one we incrementally modify the final diffuse light, <code>Id</code>.

</li></ul>
<p>
Note that for each of these diffuse lights we apply the <a href="03.html#Lambertian Reflection Model">Lambertian Reflection Model</a>, where we compute the final color as the product of the color of the light, the color of the material and finally the cosine of the angle between the light source (<code>vRay</code>) and the normal of the surface (that is the <code>lamberTerm</code>). In this case, instead of the normal of the surface we use our transformed normal, <code>vTransformedNormals</code>.
</p>

<pre c="">#version 300 es

precision mediump float;

const int numLights = 3;

uniform vec4 uLightColors[numLights];
uniform vec4 uMaterialAmbient;
uniform vec4 uMaterialDiffuse;

uniform vec4 uLightAmbient;
uniform float uLightCutOff;

in vec3 vRay[numLights];
in vec3 vTransformedNormals[numLights];

out vec4 fragColor;

void main(void) {

  vec4 Ia = uLightAmbient * uMaterialAmbient;
  vec4 Id = vec4(0.0);
  
  // Iterate over each light
  for(int i = 0; i &lt; numLights; i++) {
    // Define the normalized transformed normal per each light, as we 
    // have modified the surface normal with the light's direction
    vec3 N = normalize(vTransformedNormals[i]);
    vec3 L = normalize(vRay[i]);
    // Cosine of angle between light and surface
    float lambertTerm = dot(N, -L);
    // If cosine is bigger than cutoff (the angle is less than an implicit
    // threhsold imposed but that cutoff) then we update the 
    // sum of the diffuse color
    if (lambertTerm &gt; uLightCutOff) {
      Id += uLightColors[i] * uMaterialDiffuse * lambertTerm;
    }
  }

  fragColor = vec4(vec3(Ia + Id), 1.0);
}
</pre>

<p>
One other thing to note is the <code>uLightCutOff</code>. This variable allows us to create a spotlight, it basically defines the minimum value of the cosine of the angle between the light source and the normal. This cosine is maximized when the light is perpendicular to the surface, and minimized when the light is perpendicular. So with the <code>uLightCutOff</code> we are kind of saying what is the maximum angle we allow between the light and the surface. 
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights-Attenuation Factor"><h4 id="Attenuation Factor" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights-Attenuation Factor">Attenuation Factor</a></h4></div>

<p>
However we can use this threshold as the variable to define an <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in Lights-Directional Point Lights-Attenuation Factor-attenuation"></span><strong id="attenuation">attenuation</strong>. For example by computing the final color as follows:
</p>

<pre>if (lambertTerm &gt; uLightCutOff) {
  Id += uLightColors[i] * uMaterialDiffuse * pow(lamberTerm, 10.0 * uLightCutOff);
}
</pre>

<p>
So now the effect of lamberTerm is not as straight forward, and it does not increasig "linearly" but by the means of a power function:
</p>

\[
f(x) = \text{lambert term}^{10 \text{cutoff}}
\]

<p>
This is illustrated of the following figure:
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/attenuation_factor_function.png" alt="Attenuation Factor">
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene"><h2 id="Use of Color in the Scene" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene">Use of Color in the Scene</a></h2></div>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Transparency"><h3 id="Transparency" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Transparency">Transparency</a></h3></div>

<p>
The first approach to render transparent objects is to use <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Transparency-polygon stippling"></span><strong id="polygon stippling">polygon stippling</strong>. This technique consists of discarding some fragments so that you can see through the object. OpenGL supports polygon stippling through the glPolygonStipple function. This function is not available in WebGL. You could try to replicate this functionality by dropping some fragments in the fragment shader using the ESSL discard command. More commonly, we can use the alpha channel information to obtain translucent objects. However, modifying the alpha values does not produce transparency automatically.
</p>

<p>
Creating transparency corresponds to altering the fragments that weâ€™ve already written to the framebuffer. On a scene where there is one translucent object in front of an opaque object (from our camera view) we need to be able to see the opaque object through the translucent object. Therefore, the fragments that overlap between the far and near objects need to be combined somehow to create the transparency effect.
</p>

<p>
To properly render transparent surfaces, we need to learn about two important WebGL concepts: <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Transparency-depth testing"></span><strong id="depth testing">depth testing</strong> and <span id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Transparency-alpha blending"></span><strong id="alpha blending">alpha blending</strong>.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Updated Rendering Pipeline"><h3 id="Updated Rendering Pipeline" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Use of Color in the Scene-Updated Rendering Pipeline">Updated Rendering Pipeline</a></h3></div>

<p>
Depth testing and alpha blending are two optional stages for fragments once they've been processed by the fragment shader.
</p>

<p>
If the depth test is not activated, all the fragments are automatically available for alpha blending. If the depth test is enabled, those fragments that fail the test will automatically be discarded by the pipeline and will no longer be available for any other operation. This means that discarded fragments will not be rendered.
</p>

<p>
The following diagram shows the order in which depth testing and alpha blending are performed:
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/updated_rendering_pipeline.png" alt="Updated Rendering Pipeline">
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Depth Testing"><h2 id="Depth Testing" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Depth Testing">Depth Testing</a></h2></div>

<p>
Each fragment that has been processed by the fragment shader carries an associated depth value. Though fragments are two-dimensional since they're rendered on the screen, the depth value keeps the information of how far the fragment is from the camera (screen).
</p>

<p>
Depth values are stored in a special WebGL buffer named <span id="Colors, Depth Testing, and Alpha Blending-Depth Testing-depth buffer"></span><strong id="depth buffer">depth buffer</strong> or <span id="Colors, Depth Testing, and Alpha Blending-Depth Testing-z-buffer"></span><strong id="z-buffer">z-buffer</strong>. The \(z\) comes from the fact that \(x\) and \(y\) values correspond to the screen coordinates of the fragment, while the \(z\) value measures distance perpendicular to the screen.
</p>

<p>
After the fragment has been calculated by the fragment shader, it becomes available for depth testing. This only occurs if the depth test is enabled.
</p>

<pre javascript="">gl.enable(gl.DEPTH_TEST);
</pre>

<p>
The depth test takes the depth value of a fragment into consideration and compares it to the depth value for the same fragment coordinates already stored in the depth buffer. The depth test determines whether that fragment is accepted for further processing in the rendering pipeline.
</p>

<p>
In normal circumstances, when the depth test is enabled, only those fragments with a lower depth value than the corresponding fragments present in the depth buffer will be accepted.
</p>

<p>
Depth testing is a commutative operation with respect to the rendering order. This means that no matter which object gets rendered first, as long as depth testing is enabled, we will always have a consistent scene.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Depth Testing-Depth Function"><h3 id="Depth Function" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Depth Testing-Depth Function">Depth Function</a></h3></div>

<p>
In some applications, we may be interested in changing the default behavior of depth testing, which discards fragments with a higher depth value than those fragments in the depth buffer. For that purpose, WebGL provides the <code>gl.depthFunc(function)</code> method.
</p>

<p>
This method has only one parameter, the <code>function</code> to use:
</p>

<table>
<thead>
<tr>
<th>
Parameter
</th>
<th>
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>gl.NEVER</code>
</td>
<td>
The depth test always fails.
</td>
</tr>
<tr>
<td>
<code>gl.LESS</code>
</td>
<td>
Only fragments with a depth lower than current fragments on the depth buffer will pass the test
</td>
</tr>
<tr>
<td>
<code>gl.LEQUAL</code>
</td>
<td>
Fragments with a depth less than or equal to corresponding current fragments in the depth buffer will pass the test.
</td>
</tr>
<tr>
<td>
<code>gl.EQUAL</code>
</td>
<td>
Only fragments with the same depth as current fragments on the depth buffer will pass the test
</td>
</tr>
<tr>
<td>
<code>gl.NOTEQUAL</code>
</td>
<td>
Only fragments that do not have the same depth value as fragments on the depth buffer will pass the test.
</td>
</tr>
<tr>
<td>
<code>gl.GEQUAL</code>
</td>
<td>
Fragments with greater or equal depth value will pass the test.
</td>
</tr>
<tr>
<td>
<code>gl.GREATER</code>
</td>
<td>
Only fragments with a greater depth value will pass the test.
</td>
</tr>
<tr>
<td>
<code>gl.ALWAYS</code>
</td>
<td>
The depth test always passes.
</td>
</tr>
</tbody>
</table>

<p>
The depth test is disabled by default in WebGL. When enabled, if no depth function is set, the <code>gl.LESS</code> function is selected by default.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending"><h2 id="Alpha Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending">Alpha Blending</a></h2></div>

<p>
Alpha blending is enabled using the following line of code:
</p>

<pre javascript="">gl.enable(gl.BLEND);
</pre>

<p>
For each available fragment, the alpha blending operation reads the color from the framebuffer by the appropriate fragment coordinates and creates a new color based on a linear interpolation between the previously calculated color in the fragment shader and the color from the framebuffer.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blending Function"><h3 id="The Blending Function" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blending Function">The Blending Function</a></h3></div>

<p>
With blending enabled, the next step is to define a blending function. This function will determine how fragment colors from the object (source) are combined with the fragment colors present in the framebuffer (destination).
</p>

<p>
We combine source and destination colors as follows:
</p>

<pre javascript="">color = S * sW + D * dW;
</pre>

<p>
Where:
</p>

<ul>
<li>
<code>S</code>: source color (<code>vec4</code>)

</li><li>
<code>D</code>: destination color (<code>vec4</code>)

</li><li>
<code>sW</code>: source scaling factor

</li><li>
<code>dW</code>: destination scaling factor

</li><li>
<code>S.rgb</code>: RGB components of the source color

</li><li>
<code>S.a</code>: Alpha component of the source color

</li><li>
<code>D.rgb</code>: RGB components of the destination color

</li><li>
<code>D.a</code>: Alpha component of the destination color

</li></ul>
<p>
It's important to note that the rendering order will determine the source and the destination fragments.
</p>

<p>
If a sphere is rendered first, it will then become the destination of the blending operation because the sphere fragments are stored in the framebuffer at the time that the cone is rendered. In other words, alpha blending is a non-commutative operation with respect to rendering order:
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/alpha_blending_rendering_order.png" alt="Rendering Order in Alpha Blending">
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Separate Blending Functions"><h3 id="Separate Blending Functions" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Separate Blending Functions">Separate Blending Functions</a></h3></div>

<p>
It is also possible to determine how the RGB channels are going to be combined independently from the alpha channels. For that, we use the <code>gl.blendFuncSeparate</code> function.
</p>

<p>
We define two independent functions this way:
</p>

<pre>color = S.rgb * sW.rgb + D.rgb * dW.rgb;
alpha = S.a * sW.a + D.a * dW.a;
</pre>

<p>
More precisely:
</p>

<ul>
<li>
<code>S.rgb</code>: RGB components of the source color

</li><li>
<code>sW.rgb</code>: is the source scaling factor (only RGB)

</li><li>
<code>S.a</code>: Alpha component of the source color

</li><li>
<code>sW.a</code>: is the source scaling factor for the source alpha value

</li><li>
<code>D.rgb</code>: RGB components of the destination color

</li><li>
<code>sW.rgb</code>: is the destination scaling factor (only RGB)

</li><li>
<code>D.a</code>: Alpha component of the destination color

</li><li>
<code>sD.a</code>: is the source scaling factor for the destination alpha value

</li></ul>
<p>
Then, we could have something such as the following:
</p>

<pre>color = S.rgb * S.a + D.rgb * (1.0 - S.a);
alpha = S.a * 1.0 + D.a * 0.0;
</pre>

<p>
This would be translated into code as follows:
</p>

<pre>gl.blendFuncSeparate(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA, gl.ONE, gl.ZERO);
</pre>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blend Equation"><h3 id="The Blend Equation" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blend Equation">The Blend Equation</a></h3></div>

<p>
We could have a case where we do not want to interpolate the source and destination fragment colors with scale or add operations. For example, we may want to subtract one from the other. In this case, WebGL provides the <code>gl.blendEquation</code> function.
</p>

<p>
This function receives one parameter that determines the operation on the scaled source and destination fragment colors. For example, <code>gl.blendEquation(gl.FUNC_ADD)</code> is calculated as such:
</p>

<pre>color = S * sW + D * dW;
</pre>

<p>
And, <code>gl.blendEquation(gl.FUNC_SUBTRACT)</code> corresponds to the following:
</p>

<pre>color = S * sW - D * dW;
</pre>

<p>
There is a third option, <code>gl.blendEquation(gl.FUNC_REVERSE_SUBTRACT)</code>, that corresponds to the following:
</p>

<pre>color = D* dw - S * sW;
</pre>

<p>
As expected, you can define the blending equation separately for the RGB channels and for the alpha channel. For that, we use the <code>gl.blendEquationSeparate</code> function.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-WebGL Alpha-Blending API"><h3 id="WebGL Alpha-Blending API" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-WebGL Alpha-Blending API">WebGL Alpha-Blending API</a></h3></div>

<table>
<thead>
<tr>
<th>
WebGL function
</th>
<th>
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<code>gl.enable/disable(gl.BLEND)</code>
</td>
<td>
Enable/disable blending
</td>
</tr>
<tr>
<td>
<code>gl.blendFunc(sW, dW)</code>
</td>
<td>
Specify pixel arithmetic
</td>
</tr>
<tr>
<td>
<code>gl.blendFuncSeparate(sW_rgb, dW_rgb, sW_a, dW_a)</code>
</td>
<td>
Specify pixel arithmetic for RGB and alpha components separately
</td>
</tr>
<tr>
<td>
<code>gl.blendEquation(mode)</code>
</td>
<td>
Specify the equation used for both the RGB blend equation and the alpha blend equation
</td>
</tr>
<tr>
<td>
<code>gl.blendEquationSeparate(modeRGB, modeAlpha)</code>
</td>
<td>
Set the RGB blend equation and the alpha blend equation separately.
</td>
</tr>
<tr>
<td>
<code>gl.blendColor(red, green, blue, alpha)</code>
</td>
<td>
Set the blend color.
</td>
</tr>
<tr>
<td>
<code>gl.getParameter(name)</code>
</td>
<td>
Just like with other WebGL variables, it is possible to query blending parameters using <code>gl.getParameter</code>.
</td>
</tr>
</tbody>
</table>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blend Color"><h3 id="The Blend Color" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-The Blend Color">The Blend Color</a></h3></div>

<p>
WebGL provides the <code>gl.CONSTANT_COLOR</code> and <code>gl.ONE_MINUS_CONSTANT_COLOR</code> scaling factors. These scaling factors can be used with <code>gl.blendFunc</code> and <code>gl.blendFuncSeparate</code>. However, we need to first establish the blend color. We do so by invoking <code>gl.blendColor</code>.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes"><h3 id="Alpha Blending Modes" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes">Alpha Blending Modes</a></h3></div>

<p>
Depending on the parameter selection for sW and dW, we can create different blending modes.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Additive Blending"><h4 id="Additive Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Additive Blending">Additive Blending</a></h4></div>

<p>
Additive blending simply adds the colors of the source and destination fragments, creating a lighter image. We obtain additive blending by writing the following:
</p>

<pre>gl.blendFunc(gl.ONE, gl.ONE);
</pre>

<p>
This assigns the weights for source and destination fragments <code>sW</code> and <code>dW</code> to \(1\). The color output will be as follows:
</p>

<pre>color = S * 1.0 + D * 1.0;
color = S + D;
</pre>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Substractive Blending"><h4 id="Substractive Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Substractive Blending">Substractive Blending</a></h4></div>

<p>
Similarly, we can obtain subtractive blending by writing the following:
</p>

<pre>gl.blendEquation(gl.FUNC_SUBTRACT);
gl.blendFunc(gl.ONE, gl.ONE);
</pre>

<p>
This will change the blending equation to the following:
</p>

<pre>color = S * 1.0 - D * 1.0;
color = S - D;
</pre>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Multiplicative Blending"><h4 id="Multiplicative Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Multiplicative Blending">Multiplicative Blending</a></h4></div>

<p>
We obtain multiplicative blending by writing the following:
</p>

<pre>gl.blendFunc(gl.DST_COLOR, gl.ZERO);
</pre>

<p>
This will be reflected in the blending equation as the following:
</p>

<pre>color = S * D + D * 0.0;
color = S * D;
</pre>

<p>
The result will always be a darker blending
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Interpolative Blending"><h4 id="Interpolative Blending" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Alpha Blending-Alpha Blending Modes-Interpolative Blending">Interpolative Blending</a></h4></div>

<p>
If we set <code>sW</code> to <code>S.a</code> and <code>dW</code> to <code>1 - S.a</code>, then we get the following:
</p>

<pre>color = S * S.a + D *(1 - S.a);
</pre>

<p>
This will create a linear interpolation between the source and destination color using the
source alpha color, <code>S.a</code>, as the scaling factor. In code, this is translated as the following:
</p>

<pre>gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA);
</pre>

<p>
Interpolative blending allows us to create a transparency effect as long as the destination fragments have passed the depth test. As expected, this requires that the objects be rendered from back to front.
</p>

<div id="Colors, Depth Testing, and Alpha Blending-Creating Transparent Objects"><h2 id="Creating Transparent Objects" class="header"><a href="#Colors, Depth Testing, and Alpha Blending-Creating Transparent Objects">Creating Transparent Objects</a></h2></div>

<p>
We've learned that in order to create transparency, we need to:
</p>

<ul>
<li>
Enable alpha blending and select the interpolative blending function

</li><li>
Render the faces of objects back to front

</li></ul>
<p>
How do we create transparent objects when there is nothing to blend them against? In other words, if thereâ€™s only one object, how can we make it transparent? One solution is to use <span id="Colors, Depth Testing, and Alpha Blending-Creating Transparent Objects-face-culling"></span><strong id="face-culling">face-culling</strong>. Face-culling allows us to only render the back or front face of an object.
</p>

<p>
Similar to other options in the pipeline, culling is disabled by default. We enable it by calling the following:
</p>

<pre>gl.enable(gl.FACE_CULLING);
</pre>

<p>
To render only the back faces of an object, we call <code>gl.cullFace(gl.FRONT)</code> before we call <code>drawArrays</code> or <code>drawElements</code>. Similarly, to render only the front face, we use <code>gl.cullFace(gl.BACK)</code> before the draw call.
</p>

<p>
The following diagram summarizes the steps needed to create a transparent object with alpha blending and face-culling:
</p>

<p>
<img src="https://albamr09.github.io/public/images/ComputerScience/CG/RTGW/face_culling.png" alt="Face Culling">
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>