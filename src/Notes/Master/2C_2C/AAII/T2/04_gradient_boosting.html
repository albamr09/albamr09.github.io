<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css">
    <title>Gradient Boosting</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Gradient Boosting"><h1 id="Gradient Boosting" class="header"><a href="#Gradient Boosting">Gradient Boosting</a></h1></div>

<hr>

<div id="Gradient Boosting-Introduction"><h2 id="Introduction" class="header"><a href="#Gradient Boosting-Introduction">Introduction</a></h2></div>

<p>
Imagine you have a machine learning model that makes predictions, but it's not perfect. Gradient boosting is like a smart way to teach this model to make better predictions over time.
</p>

<p>
Instead of trying to fix all the prediction errors at once, gradient boosting focuses on correcting one error at a time. It does this by looking at the direction where the error is the steepest and making adjustments to improve the prediction in that direction.
</p>

<p>
By repeating this process step by step, the model gradually gets better at making predictions, leading to more accurate results.
</p>

<p>
So if you have the following function you want to optimize:
</p>

\begin{align}
L(f) =  \sum_{i=1}^N L(y_i, f(x_i))
\end{align}

<p>
where \(f\) is the prediction function. Therefore the optimization problem with respect to \(f\) can be summarized as follows:
</p>

\begin{align}
\hat{f} = \arg \min_f L(f)
\end{align}

<p>
where \(f \in \mathbb{R}^N\) are the parameters, in this case the predicted values for each instance on the dataset:
</p>

\begin{align}
f = \{f(x_1), \cdots, f(x_i), \cdots, f(x_n)\}
\end{align}

<p>
Solving this entire problem at once may be challenging. To make it easier, numerical optimization procedures break down this big problem into smaller pieces, represented by component vectors. Each component vector addresses a specific aspect of the problem. So:
</p>

\begin{align}
f_{M} = \sum_{m = 0}^M h_m, h_m \in \mathbb{R}^N
\end{align}

<p>
where \(f_M\) represents the final model or prediction function obtained after M iterations or steps of the boosting algorithm.
</p>

<p>
Here \(f_m\) represents the model at iteration \(m\), whereas \(h_m\) represents the increment to the model at iteration \(m\) It is the component vector added to the current model to move towards the optimized solution. Each \(h_m\) is induced based on the current parameter vector \(f_{m-1}\) and contributes to the overall model improvement.
</p>

<p>
Here is a simple layout of how the algorithm optmizes:
</p>

<ul>
<li>
At the beginning of the gradient boosting process, the initial model \(f_0\) is set to an initial guess.

</li><li>
As the algorithm progresses through iterations (\(m = 1, 2, \cdots, M\)), each step involves updating the model based on the gradient information to reduce errors in predictions. Numerical optimization methods differ in their prescriptions for computing each increment vector \(h_m\).

</li></ul>
<div id="Gradient Boosting-Steepest Descent"><h2 id="Steepest Descent" class="header"><a href="#Gradient Boosting-Steepest Descent">Steepest Descent</a></h2></div>

<p>
Steepest descent is a method used in optimization to find the minimum value of a function. This method chooses \(h_m = \rho_m g_m\) where \(\rho\) is a scalar and \(g_m\) is the gradient of \(L(f_{m-1})\), that is, the cost function evaluated at values predicted by the "previous model".
</p>

<p>
The components of the gradient \(g_m\) are defined as follows:
</p>

\begin{align}
g_{im} = \left[\frac{\delta L(y_i, f(x_i))}{\delta f(x_i)}\right]_{f_m(x_i) = f_{m_1}(x_i)}
\end{align}

<p>
The step length (kinda like the learning rate):
</p>

\begin{align}
\rho_m = \arg \min_{\rho} L(f_{m-1} - \rho g_m)
\end{align}

<p>
Thus, at each step, the predictor is updated as follows:
</p>

\begin{align}
f_m = f_{m - 1} - \rho_m g_m \in \mathbb{R}^N
\end{align}

<p>
This updates \(f_m\) towards the direction of maximum descent at \(L(f_{m-1})\), which is why this is often interpreted as a greedy algorithm. 
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>