<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/custom.css">
    <title>Boosting Trees</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Boosting Trees"><h1 id="Boosting Trees" class="header"><a href="#Boosting Trees">Boosting Trees</a></h1></div>

<hr>

<div id="Boosting Trees-Variable Space Partitioning"><h2 id="Variable Space Partitioning" class="header"><a href="#Boosting Trees-Variable Space Partitioning">Variable Space Partitioning</a></h2></div>

<p>
Partitioning the predictor variable space into regions in boosting trees involves recursively splitting the data based on predictor variables to create distinct regions with associated constant values for making predictions.
</p>

<ol>
<li>
<span id="Boosting Trees-Variable Space Partitioning-Starting Point"></span><strong id="Starting Point">Starting Point</strong>: entire predictor variable space is considered as one large region, \(R_1\).

</li><li>
Decision Making: At each step, a decision tree algorithm finds the best split based on a predictor variable \(x_j\) and a split point \(s\) that minimizes a certain criterion.  This split divides region \(R_j\) into two subregions \(R_{left}\) and \(R_{right}\).

</li><li>
<span id="Boosting Trees-Variable Space Partitioning-Splitting Criteria"></span><strong id="Splitting Criteria">Splitting Criteria</strong>: it can be represented as \((j, s) = arg \min_{j, s} [\sum_{x_i \in R_{left}} L(y_i, f(x_i)) + \sum_{x_i \in R_{right}} L(y_i, f(x_i)]\)

</li><li>
<span id="Boosting Trees-Variable Space Partitioning-Recursive Process"></span><strong id="Recursive Process">Recursive Process</strong>: This process is repeated recursively for each resulting subregion until a stopping criterion is met.

</li><li>
<span id="Boosting Trees-Variable Space Partitioning-Terminal Nodes"></span><strong id="Terminal Nodes">Terminal Nodes</strong>: The final regions, or terminal nodes, are denoted as \(R_J\) and are assigned constant values \(\gamma_j\) representing the prediction for data points falling into that region.

</li><li>
<span id="Boosting Trees-Variable Space Partitioning-Constant Assigment"></span><strong id="Constant Assigment">Constant Assigment</strong>: Each terminal node is associated with a constant value, resulting in a piecewise constant function, such that each tree can be denoted as: \(T(x; \Theta) = \sum_{j=1}^J \gamma_j I(x \in R_j)\) where \(I(\cdot)\) is the indication function and \(\Theta = \{R_j, \gamma_j\}_1^J\) are the parameters.

</li><li>
<span id="Boosting Trees-Variable Space Partitioning-Prediction"></span><strong id="Prediction">Prediction</strong>:

<ol>
<li>
When making predictions for new data points, the algorithm determines the region \(R_j\) that the data point belongs to based on the predictor variables.

</li><li>
The prediction for that data point is then based on the constant value \(\gamma_j\) assigned to the corresponding region.

</li></ol>
</li></ol>
<div id="Boosting Trees-Optimization Problem"><h2 id="Optimization Problem" class="header"><a href="#Boosting Trees-Optimization Problem">Optimization Problem</a></h2></div>

<p>
So as we have seen the optimization problem is defined, on a simplified manner, as follows:
</p>

\begin{align}
\hat{\Theta} = \arg \min_{\Theta} \sum_{j=1}^J \sum_{x_i \in R_j} L(y_i, \gamma_j)
\end{align}

<p>
This is a combinatorial problem that we usually aproximate using suboptimal solutions.
</p>

<ul>
<li>
Finding \(\gamma_j\) given \(R_j\): we usually define \(\hat{\gamma}_j = \overline{y}_j\) for regression problems.

</li><li>
Finding \(R_j\): this is the difficult part. We usually resort to a greedy, top-down recursive partitioning algorithm to find \(R_j\).

</li></ul>
<div id="Boosting Trees-Boosting Trees"><h2 id="Boosting Trees" class="header"><a href="#Boosting Trees-Boosting Trees">Boosting Trees</a></h2></div>

<p>
In boosting trees, terminal-node trees refer to the individual decision trees that make up the ensemble model. Each terminal-node tree is denoted as \(T(x; \Theta_m)\) where \(\Theta_m = \{R_{jm}, \gamma_{jm}\}_1^{J_m}\).
</p>

<p>
The boosted tree model is an additive model, where each tree is added sequentially to improve the overall prediction. The model can be expressed as 
</p>

\begin{align}
f_M(x) = \sum_{m=1}^M T(x; \Theta_m)
\end{align}

<p>
where \(M\) represents the total number of trees in the ensemble.
</p>

<div id="Boosting Trees-Boosting Trees-Optimization Problem"><h3 id="Optimization Problem" class="header"><a href="#Boosting Trees-Boosting Trees-Optimization Problem">Optimization Problem</a></h3></div>

<p>
The optimization problem for boosting trees involves finding the optimal regions and constants for each tree in the ensemble model.
</p>

<div id="Boosting Trees-Boosting Trees-Optimization Problem-Optimization Objective"><h4 id="Optimization Objective" class="header"><a href="#Boosting Trees-Boosting Trees-Optimization Problem-Optimization Objective">Optimization Objective</a></h4></div>

<p>
The goal is to minimize the empirical risk:
</p>

\begin{align}
\hat{\Theta} = arg \min_{\Theta} \sum_{x_i \in R_j} L(y_i, \gamma_j)
\end{align}

<p>
Where \(L(y_i, \gamma_i)\) represents the loss incurred for pedicting the target value \(y_i\) with constant \(\gamma_j\) in region \(R_j\).
</p>

<div id="Boosting Trees-Boosting Trees-Optimization Problem-Finding Optimal Consants"><h4 id="Finding Optimal Consants" class="header"><a href="#Boosting Trees-Boosting Trees-Optimization Problem-Finding Optimal Consants">Finding Optimal Consants</a></h4></div>

<p>
Given the regions \(R_{jm}\)  finding the optimal constant in \(\gamma_j\) in each regions involves minimizing the loss function for the data points within that region:
</p>

\begin{align}
\hat{\gamma}_{jm} = arg \min_{\gamma_{jm}} \sum_{x_i \in R_{jm}} L(y_i, f_{m - 1}(x_i) + \gamma_{jm})
\end{align}

\begin{align}
\hat{\gamma}_{jm} = arg \min_{\gamma_{jm}} \sum_{x \in R_{jm}} L(y_i, f_{m - 1}(x_i) + T(x_i; \Theta_m))
\end{align}

<p>
Finding the regions is difficult, and even more difficult than for a single tree. 
</p>

<div id="Boosting Trees-Boosting Trees-Optimization Problem-Solution for Regression Trees"><h4 id="Solution for Regression Trees" class="header"><a href="#Boosting Trees-Boosting Trees-Optimization Problem-Solution for Regression Trees">Solution for Regression Trees</a></h4></div>

<p>
For regressions trees the solution for boosted trees consists on choosing the regression tree that best predicts the current residuals \(y_i - f_{m-1}(x_i)\) and \(\hat{\gamma}_{jm}\)
</p>

<div id="Boosting Trees-Boosting Trees-Optimization Problem-Solution for Classification Trees"><h4 id="Solution for Classification Trees" class="header"><a href="#Boosting Trees-Boosting Trees-Optimization Problem-Solution for Classification Trees">Solution for Classification Trees</a></h4></div>

<p>
For two-class classification and exponential loss, it gives rise to the AdaBoost method. It can be showin that given \(R_{jm}\) the solution is:
</p>

\begin{align}
\hat{\gamma}_{jm} = \log \frac{\sum_{x_i \in R_{jm}} w_i^{(m)} I(y_i = 1)}{\sum_{x_i \in R_{jm}} w_i^{(m)} I(y_i = -1)}
\end{align}

<div id="Boosting Trees-Appendix"><h2 id="Appendix" class="header"><a href="#Boosting Trees-Appendix">Appendix</a></h2></div>

<div id="Boosting Trees-Appendix-Greedy Top-Down Recurisve Partitioning Algorithm"><h3 id="Greedy Top-Down Recurisve Partitioning Algorithm" class="header"><a href="#Boosting Trees-Appendix-Greedy Top-Down Recurisve Partitioning Algorithm">Greedy Top-Down Recurisve Partitioning Algorithm</a></h3></div>

<p>
A greedy, top-down recursive partitioning algorithm is a method used in decision tree construction to recursively split the predictor variable space into regions in a step-by-step manner.
</p>

<ol>
<li>
<span id="Boosting Trees-Appendix-Greedy Top-Down Recurisve Partitioning Algorithm-Greedy Approach"></span><strong id="Greedy Approach">Greedy Approach</strong>: at each step, it makes the best split based on the available data without considering the impact of future splits.

</li><li>
<span id="Boosting Trees-Appendix-Greedy Top-Down Recurisve Partitioning Algorithm-Top Down Process"></span><strong id="Top Down Process">Top Down Process</strong>: starts at the top with the entire predictor variable space considered as one region. It then recursively divides this space into smaller regions

</li><li>
<span id="Boosting Trees-Appendix-Greedy Top-Down Recurisve Partitioning Algorithm-Recursive Partitioning"></span><strong id="Recursive Partitioning">Recursive Partitioning</strong>: At each step the predictor variable space is divided into two or more subregions based on a splitting criterion. This process continues recursively for each resulting subregion until a stopping criterion is met.

</li></ol>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>