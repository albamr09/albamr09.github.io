<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/custom.css">
    <title>Estrategias para la combinación de clasificadores</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Estrategias para la combinación de clasificadores"><h1 id="Estrategias para la combinación de clasificadores" class="header"><a href="#Estrategias para la combinación de clasificadores">Estrategias para la combinación de clasificadores</a></h1></div>

<hr>

<p>
This section provides a detailed review of classifiers combination techniques First, we discuss the different levels at which combination is performed: sensors, features and decisions. We then expand on the concept of soft vs. hard combination techniques. Finally, we discuss how the combination techniques can be grouped as either adaptive or non-adaptive.
</p>

<div id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination"><h2 id="Levels of Classifiers Combination" class="header"><a href="#Estrategias para la combinación de clasificadores-Levels of Classifiers Combination">Levels of Classifiers Combination</a></h2></div>

<p>
Classifiers combination can be carried out at three different levels:
</p>

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Early combination at sensor data level"></span><strong id="Early combination at sensor data level">Early combination at sensor data level</strong>: combination of data collected from two or more sensors before feature selection technique is applied.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Combination at feature level"></span><strong id="Combination at feature level">Combination at feature level</strong>: it may simply involve basic concatenation of feature vectors with equal or different weights (might result in high dimensonal vectors, whose dimension has to be reduced).

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Late combination at the decision level"></span><strong id="Late combination at the decision level">Late combination at the decision level</strong>: they are based on one of three approaches: abstract, rank, and score:

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Abstract-based"></span><strong id="Abstract-based">Abstract-based</strong>: a single output label from each individual classifier is used as input to the combination scheme.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Rank-based"></span><strong id="Rank-based">Rank-based</strong>: each classifier yields several labels ranked from the most likely to the least likely. This information is then used by the combination scheme to reach the final decision. 

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Score-based"></span><strong id="Score-based">Score-based</strong>: each classifier outputs the \(n\) best labels together with their confidence scores. The combination can be <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-density-based"></span><strong id="density-based">density-based</strong>, <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-transformation-based"></span><strong id="transformation-based">transformation-based</strong> or <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-classifier-based"></span><strong id="classifier-based">classifier-based</strong> score fusion.

</li></ul>
</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/classifier_combination_levels.png" alt="Classifier Combination Levels" style="width:700px;height:350px">
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination"><h2 id="Hard and Soft Level Classifier Combination" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination">Hard and Soft Level Classifier Combination</a></h2></div>

<p>
Another way to categorize combination algorithms is whether hard thresholding or soft scoring is used with each of the classifiers.
</p>

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Hard-level combination"></span><strong id="Hard-level combination">Hard-level combination</strong>: uses the output of the classifier after it is hard thresholded.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Soft-level combination"></span><strong id="Soft-level combination">Soft-level combination</strong>: uses estimates of the aposteriori probability of the class.

</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/classifier_hard_soft_combination.png" alt="Classifier Hard/Soft Combination" style="width:800px;height:150px">
The sum, product, max, min rules, etc., fall under the soft level combiners as they use the output aposteriori probability of the classifier or a score.
</p>

<p>
Majority voting is a typical example of hard-level combiners and has found widespread use in the literature. There are three different versions of voting:
</p>

<ul>
<li>
Unanimous voting.

</li><li>
More than half voting.

</li><li>
Highest number of votes.

</li></ul>
<p>
Considering the output label vector of the \(i\)th classifier as:
</p>

\begin{align}
[d_{i, 1}, \cdots, d_{i, N}]^T \in [0, 1]^N
\end{align}

<p>
where \(i = 1, 2, \cdots, M\) and \(d_{i, j} = 1\) if the classifier \(D_i\) labels the \(i\)th instance as class \(\omega_j\) and \(0\) otherwise. The majority vote results in a decision for class \(\omega_k\) if:
</p>

\begin{align}
\sum_{i = 1}^M d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M d_{i, j}
\end{align}

<p>
Where \(M\) is the total number of classifiers and \(N\) is total number of classes. Such that class \(\omega_k\) is the most "selected" on all the classifier.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/classifier_combination_majority_voting.png" alt="Majority Voting" style="width:500px;height:350px">
</p>

<p>
The accuracy of the combination scheme is given as:
</p>

\begin{align}
P_{maj} = \sum_{m = \frac{M}{2} + 1}^M \binom{M}{m} p^m (1-p)^{M - m}
\end{align}

<p>
where \(p\) is the probability of correct classification.
</p>

<p>
Majority voting provides an accurate class label when at least \(\frac{M}{2} + 1\) classifiers give correct classifications, it also requires participating classifiers to have comparable accuracies.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Weighted Majority Voting"><h3 id="Weighted Majority Voting" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Weighted Majority Voting">Weighted Majority Voting</a></h3></div>

<p>
Weighted majority voting is used when the classifiers' accuracies are not similar, so it is reasonable to assign more weight to the most accurate classifier. Now the decision rule becomes:
</p>

\begin{align}
\sum_{i = 1}^M b_i d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M b_i d_{i, j}
\end{align}

<p>
where \(b_i\) is the weight associated with classifier \(D_i\). For the sake of convenience, it is a good practice to normalize the weights such that the sum is one.
</p>

<p>
The weight selection is very important in determining the overall accuracy of the classifier combinations. Therefore, to minimize the classification error of the combination, the weights are assigned as follows:
</p>

\begin{align}
b_i \propto \log\left(\frac{p_i}{1 - p_i}\right)
\end{align}

<p>
where \(p_i, \cdots, p_M\) are the individual accuracies for each independent classifier.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Weighted Consult-and-vote"><h3 id="Dynamic Weighted Consult-and-vote" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Weighted Consult-and-vote">Dynamic Weighted Consult-and-vote</a></h3></div>

<p>
In [30], Muhlbaier et al. proposed a method for combining ensembles of classifiers using dynamic weighted consult-andvote. Voting weights are determined by relative performance of each classifier on the training data. The approach learns a new class by allowing individual classifiers to consult with each other to determine their voting weights for each of the test instances. The following figure, shows the dynamic weighted consult-and-vote approach:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/dynamic_weighted_consult_and_vote.png" alt="Dynamic Weighted Consult and Vote" style="width:650px;height:450px">
</p>

<p>
Classifiers examine each other's decision, cross referencing their decisions with the list of class labels on which they were initially trained, then update the weights.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Divide and Conquer"><h3 id="Divide and Conquer" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Divide and Conquer">Divide and Conquer</a></h3></div>

<p>
Another modification of majority voting is to complement it with a divide and conquer technique [31].It divides the classification task into a set of smaller and simpler problems, then solving each of problem separately, followed by a majority voting scheme.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/classifier_divide_and_conquer.png" alt="Divide and Conquer" style="width:550px;height:350px">
</p>
<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Quality Based Combination Techniques"><h3 id="Quality Based Combination Techniques" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Quality Based Combination Techniques">Quality Based Combination Techniques</a></h3></div>

<p>
Other recent approaches include quality-based combination [32] which gives higher weights to the more reliable classifiers under some given conditions (i.e. better quality).
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Critic Classifier"><h3 id="Critic Classifier" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Critic Classifier">Critic Classifier</a></h3></div>

<p>
The authors introduced a critic associated with each classifier whose purpose is to predict the error of the classifier [34]. The approach is based on classical standard voting techniques for classifiers combination. However, it performed best on two-class problem.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Adaptative Voting Technique"><h3 id="Adaptative Voting Technique" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Adaptative Voting Technique">Adaptative Voting Technique</a></h3></div>

<p>
The adaptative Voting Technique [35] involves the weighting of classifiers based on their estimated recognition performance. The authors applied "Simultaneous Truth and Performance Level Evaluation" (STAPLE).
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Entropy Based Combination Technique"><h3 id="Dynamic Entropy Based Combination Technique" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Entropy Based Combination Technique">Dynamic Entropy Based Combination Technique</a></h3></div>

<p>
The idea of this combination scheme [37] lies in assigning large weights to classifiers that are very confident in their decisions, and small weights to classifiers that are less confident. 
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Assuming Linear Dependency Between Predictors"><h3 id="Assuming Linear Dependency Between Predictors" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Assuming Linear Dependency Between Predictors">Assuming Linear Dependency Between Predictors</a></h3></div>

<p>
In [40], the authors considered linear dependency of both classifiers and features. Two models, Linear Classifier Dependency Modelling (LCDM) and Linear Feature Dependency Modelling (LFDM) were developed. They showed it outperformed existing classifier and feature level combination methods under non-Gaussian distribution. However the LFDM technique takes longer to train.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Fixed Rule Based Combination Techniques"><h3 id="Fixed Rule Based Combination Techniques" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Fixed Rule Based Combination Techniques">Fixed Rule Based Combination Techniques</a></h3></div>

<p>
They use class labels, distances, or confidences from the individual classifiers without the need for a training stage. According to [44], fixed rule techniques are only efficient under strict conditions such as availability of large training sets, generation of reliable confidences from base classifiers, and training of the base classifiers on different feature spaces.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Runtime Weighted Opinion Pool"><h3 id="Runtime Weighted Opinion Pool" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Runtime Weighted Opinion Pool">Runtime Weighted Opinion Pool</a></h3></div>

<p>
In [45], the approach, Runtime Weighted Opinion Pool (RWOP), dynamically assigns weights to the classifiers during runtime and the final combination is weighted according to the local performance of the classifiers. Unlike other weighted sum based approaches, this technique determines the weights with an intuitive runtime strategy. It achieves enhanced performance and reduces the relative error rate significantly.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Other Experiments"><h3 id="Other Experiments" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Other Experiments">Other Experiments</a></h3></div>

<p>
In [33], an effective approach for decision level combination based on spectral reflectance and its higher order derivatives to classify hyperspectral land images was proposed. Their study was carried out under two scenarios: LDA (Linear Discriminant Analysis) based dimensionality reduction followed by a single maximum likelihood classifier, and multiple classifiers decision fusion (MCDF).
</p>

<p>
In [36], three different classifiers (Naive Bayes, J48 Decision tree, and Decision table) were combined using simple, weighted, and probability-based voting. Weighted and probability voting techniques outperformed simple voting and single classifiers.
</p>

<p>
In [39], an approach which combines ANN and KNN based classifiers using majority voting was discussed. This approach was applied to improve accuracy when sensor data is subjected to drift. Two sets of classifiers were implemented (BP-ANN and KNN). For classifiers combination, a simple median voting was applied for ANN based combination and majority voting for the case of KNN. The authors showed a substantial improvement in performance.
</p>

<p>
In [43], an approach based on majority voting was proposed. Unequal weights are assigned to the base classifiers according to their performance. Voting was found to be the best combination rule for the dataset used.
</p>

<p>
In [46], the authors proposed a new approach for classifiers combination which strictly work with HMM based-classifiers.
</p>

<p>
In [12], a new weighted majority voting approach was proposed. It assigns weights to the different classes rather than the different base classifiers. Such weights were computed by estimating the joint probability distribution of each class with the scores provided by all classifiers in the combining pool. The major drawback of this approach lies on the statistical independence assumption of variables.
</p>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners"><h2 id="Adaptative and Non-Adaptative Combiners" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners">Adaptative and Non-Adaptative Combiners</a></h2></div>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Adaptative Combiners"><h3 id="Adaptative Combiners" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Adaptative Combiners">Adaptative Combiners</a></h3></div>

<p>
Adaptive techniques for classifiers combination are mainly based on evolution or artificial intelligence algorithms. They include neural networks combination strategies and genetic algorithms as well as fuzzy set theory. Techniques under these categories are summarized on the following figure:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/adaptative_non_adaptative_combiners.png" alt="Adaptative and Non-Adaptative Combiners" style="width:700px;height:200px">
</p>

<p>
Artificial Neural Networks are usually used as a base classifier [29], however, it has also found wide use in combination of classifiers.
</p>

<p>
In [48], the author proposed two new approaches for robust and fault-tolerant classifiers combination: Attractor dynamics (AD) algorithm and classifier masking (CM) algorithm. The authors show that proposed combination algorithms result in improved robustness and fault tolerance compared with individual classifiers. The developed approaches outperformed the performance of classifiers combination based on averaging and majority voting because the AD and CM algorithms discard corrupted classifier outputs.
</p>

<p>
In [49], the authors compared the performance of combining the outputs of an ensemble of ANNs with that of SVM classifiers for remotely sensed data.
</p>

<p>
In [50], the authors used the ANN, itself, as a model for classifiers combination. A three-layer ANN (see Fig 9) was used where the different classifiers represent the units of the hidden layer. Results obtained show improved performance over individual classifiers, however, the approach was not compared against other classical classifiers combination techniques.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_based_classifier.png" alt="NN Based Classifier" style="width:500px;height:400px">
</p>

<p>
In [51], the authors investigated the role of apriori knowledge using existing classifiers combination techniques, namely the Behavior Knowledge space and the D-S theory. The approach performs well when strongly correlated classifiers are combined. 
</p>

<p>
In [16], the proposed method combines results from multiple classifiers using a hierarchical architecture, called Fuzzy Stacked Generalization (FSG). The authors used satellite images segmented and preprocessed to extract different sets of features. Each set is then used with one of \(K\) different classifiers.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/fsg_classifier.png" alt="FSG Classifier" style="width:600px;height:400px">
</p>

<p>
Decisions from base-layer classifiers were aggregated to form a decision vector which is then fed to a meta-layer classifier to make the final decision. 
</p>

<p>
In [22], the authors considered an approach which involves simultaneous extraction and selection of features/classifiers. While GA was used to simultaneously select features/classifiers, the D-S theory was used to combine the outputs of selected classifiers. The following picture describes the system:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/ga_classifier.png" alt="GA Classifier" style="width:400px;height:400px">
</p>

<p>
With D-S fusion, classification accuracy reached \(93\%\) when five different classifiers were combined. An improved performance accuracy of \(7.5\%\) over the individual classifiers.
</p>

<p>
In [57] an adaptive approach to combining classifiers was introduced. The proposed approach dynamically selects between two different combination strategies based on the belief values obtained from each strategy. Specifically, the study compared the performance of a Bayesian classifiers combination approach and product and max rule combination strategy. In the Bayesian method, the combination of classifiers is based on probabilistic principles, where the posterior probabilities of class labels are calculated using Bayes' theorem. The product rule combines the outputs of individual classifiers by multiplying their probabilities or scores for each class. The max rule operates by selecting the class label that receives the highest score or confidence level among all the individual classifiers in the ensemble.
</p>

<p>
In [58] the authors introduced various modifications to the traditional majority voting rule by incorporating a Bayesian framework and a Genetic Algorithm (GA) to determine the weights assigned to different classifiers in the ensemble. The Bayesian framework allowed for the probabilistic modeling of the weights, while the GA provided an optimization technique to search for the best combination of weights that maximized the ensemble performance. The results of the study indicated that the modified majority voting rule, when combined with the Bayesian framework and GA for weight optimization, achieved significant improvements in accuracy. Specifically, the optimal accuracies obtained were \(94.3\%\) for the majority vote, \(95.4\%\) for the genetic algorithm, and 95.95% for the Bayesian approach.
</p>

<p>
In [59] a novel approach based on Genetic Algorithm (GA) with self-configuration capabilities was developed for classifier combination. The researchers employed a pool of twelve expert classifiers that were already trained on the task of character recognition, including both printed and handwritten characters. These expert classifiers likely had different strengths and weaknesses, making them suitable candidates for ensemble learning. The GA was integrated into the system to optimize the combination of outputs from the expert classifiers. By using the evolutionary principles of genetic algorithms, the system could iteratively adjust the weights assigned to each classifier in the ensemble to maximize the overall accuracy of the system.
</p>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Adaptative Combiners-Fuzzy Based"><h4 id="Fuzzy Based" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Adaptative Combiners-Fuzzy Based">Fuzzy Based</a></h4></div>

<p>
In [53], the authors proposed a classifiers combination technique using fuzzy templates (FT). An object is labeled with the class whose fuzzy template is closest to the objects' decision profile. the authors obtained an improved performance over majority, min, max and product rules, and unweighted average combination techniques
</p>

<p>
In [54], an adaptive fuzzy integral was used to combine multiple classifiers. The parameter \(\lambda\)-fuzzy, which measures performance, is adaptively adjusted depending upon the interaction among the classifiers. The essence of the parameter is to search for the maximum degree of agreement between the conflicting and complementary sources of evidence. 
</p>

<p>
In [55], a fuzzy decision rule was employed to combine the outputs of multiple classifiers without the need for a training stage. Each classifier was independently applied to the input data, but no final decision was made based on their outputs at this stage. These classifiers are pre-existing models that have been trained on labeled data to make predictions or classifications. The results from the classifiers were aggregated using a fuzzy decision rule. This rule considered the membership degrees of the classes assigned by each classifier and selected the class with the highest membership degree (the confidence or certainty with which each classifier assigns a data point to a specific class) as the correct class. Two measures of accuracy, namely information reliability and global accuracy, were utilized in the combination rule to assess the performance of the combined classifiers.
</p>

<p>
In [56] the authors introduced a first-order Takagi-Sugeno-Kang (TSK) fuzzy model for combining multiple classifiers. Unlike conventional linear combination methods that assign different weights to pairs of classifiers and classes, the proposed TSK fuzzy model assigns weights to each individual classifier, class, and region of the classifier output space (decision boundary). This finer granularity in weight assignment allows for a more nuanced and adaptive combination of classifier outputs. The TSK fuzzy model is utilized to integrate the outputs of multiple classifiers. This model leverages fuzzy logic to combine the predictions of individual classifiers in a way that considers the uncertainty and variability in the classifier outputs. The study demonstrated improved accuracy compared to using individual classifiers alone. While the TSK fuzzy model showed promising results in enhancing classification accuracy, the authors did not explicitly address the potential bias and variance reduction that could arise from using a linear model for combining classifiers. 
</p>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Non-Adaptative Combiners"><h3 id="Non-Adaptative Combiners" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Non-Adaptative Combiners">Non-Adaptative Combiners</a></h3></div>

<p>
The highest confidence approach is an example of nonadaptive combination techniques. It involves ranking the individual classifiers based on their confidence then selecting the decision of the top ranked one.
</p>

<p>
The Borda count technique is also an example of nonadaptive methods. It is based on the principle of single winner classifier in which the individual classifiers provide a ranked list of the classes. It is a more sophisticated alternative to majority voting [60] based on ranking level [9]. It does not require training, just like averaging, sum, and voting rules [52].
</p>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Conclusion"><h3 id="Conclusion" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners-Conclusion">Conclusion</a></h3></div>

<p>
In summary, adaptive combiners tend to do better than the non-adaptive types. This is due to the fact that adaptive combiners update the weights given to the individual classifier dynamically before making the final decision. Given the fact that the performance of the individual classifiers can vary over input patterns, such a dynamic combination provides an edge over its non-adaptive counterpart especially when the data space is wide and diverse.
</p>

<div id="Estrategias para la combinación de clasificadores-Classification Based on the Number of Classifiers"><h2 id="Classification Based on the Number of Classifiers" class="header"><a href="#Estrategias para la combinación de clasificadores-Classification Based on the Number of Classifiers">Classification Based on the Number of Classifiers</a></h2></div>

<p>
The most commonly used techniques for ensemble based combinations are displayed in the following figure:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/multiclass_ensemble_techniques.png" alt="Multiclass Ensemble Techniques" style="width:600px;height:150px">
</p>

<p>
Bagging is one of the most intuitive and simple techniques used for ensemble based combination. However, unlike bagging, in boosting, the individual classifiers are trained hierarchically to discriminate more complex regions in the feature space. AdaBoost is a variation of the boosting technique. It is an adaptive boosting meta-algorithm that combines outputs of weak classifiers into a weighted sum that represents the final decision. However, the technique is sensitive to noisy data and outliers. 
</p>

<p>
In [66], the authors used AdaBoost to enhance the performance of a hybrid Hidden Markov Model (HMM) and Neural Network (NN) speech recognition system. HMMs are commonly used for modeling sequential data like speech signals, while NNs are effective in capturing complex patterns in data. The researchers evaluated the performance of the hybrid HMM/NN system with and without the AdaBoost algorithm under noisy environments. Noise in speech signals can introduce distortions and affect the accuracy of the recognition system. By applying AdaBoost, the system was expected to adapt better to noisy conditions and enhance its robustness. The results of the study demonstrated that incorporating AdaBoost into the hybrid HMM/NN speech recognition system led to improved performance, even in the presence of noise. AdaBoost's ability to focus on difficult instances and adjust the weights of the classifiers based on their performance contributed to the system's ability to handle noisy environments and enhance overall recognition accuracy.
</p>

<p>
In [67] the researchers explored a combination approach at the feature level using Support Vector Machine (SVM) classifiers and a Global AdaBoost classifier. The study focused on combining features extracted from different datasets at the feature level. By utilizing SVM classifiers and a Global AdaBoost classifier, the researchers aimed to leverage the strengths of both classifiers in integrating information from multiple datasets to improve classification performance. One significant drawback identified in the study regarding feature-level combination is the issue of high dimensionality. Combining features from multiple datasets can result in a large number of features, which can lead to challenges such as increased computational complexity, overfitting, and reduced interpretability of the model.
</p>

<div id="Estrategias para la combinación de clasificadores-Other Combination Techniques"><h2 id="Other Combination Techniques" class="header"><a href="#Estrategias para la combinación de clasificadores-Other Combination Techniques">Other Combination Techniques</a></h2></div>

<p>
In [68] the researchers introduced a novel classifiers combination technique based on an SVM active learning algorithm. The study proposed a method that leverages Support Vector Machine (SVM) classifiers in conjunction with an active learning algorithm. Active learning refers to a machine learning approach where the algorithm can select the most informative data points for labeling, thereby improving the learning process iteratively. The researchers developed a strategy where an initial classifier, likely an SVM model, is used to generate class aposteriori probabilities. These probabilities serve as inputs to the classifiers-combiner, which is based on the SVM active learning algorithm. The approach outperforms traditional classifiers combination rules when considering class labeling cost and classification accuracy.
</p>

<p>
In [70] and [71] the researchers introduced a novel approach using eigenclassifiers for combining correlated classifiers. The proposed method involves utilizing Principal Component Analysis (PCA) projection to create eigenclassifiers from a set of initially correlated classifiers. By applying PCA, the goal is to transform the correlated classifiers into uncorrelated eigen-classifiers. This transformation process aims to enhance the diversity and independence of the classifiers, enabling them to complement each other effectively during the combination stage. The results of the study indicated that the PCA-based eigenclassifiers technique provided better or comparable accuracy with a reduced number of classifiers compared to Bagging and AdaBoost. This suggests that the uncorrelation process facilitated by PCA enhanced the performance of the combined classifiers, leading to improved classification results with fewer individual classifiers.
</p>

<p>
Similarly, in [72] the researchers explored methods to address linear and non-linear correlations among the outputs of individual classifiers by leveraging Principal Component Analysis (PCA) and a generalized kernel-based PCA approach. Initially, the authors identified linear correlations among the outputs of individual classifiers. To mitigate these linear correlations, the researchers applied a simple PCA approach. Building on the success of addressing linear correlations, the authors extended their approach to consider non-linear dependencies among the outputs of individual classifiers.  o handle these non-linear dependencies, the researchers proposed a generalized kernel-based PCA approach. The results of the experiments demonstrated that the generalized kernel-based PCA approach outperformed alternative methods in improving classification accuracy.
</p>

<p>
In [73] the researchers introduced a novel classifier combination technique that focused on extracting class boundaries and utilizing a set of local linear combination rules. The proposed technique involved extracting class boundaries, which are the decision boundaries that separate different classes in the dataset. The researchers employed a set of local linear combination rules to combine the outputs of individual classifiers. These rules likely involved linear combinations of classifier outputs within specific regions of the feature space, allowing for adaptive and context-aware decision-making. The experimental results demonstrated that the classifier combination technique based on class boundaries and local linear combination rules achieved better accuracy compared to other methods such as linear combination, voting, and decision templates.
</p>

<p>
In [13], the researchers proposed a weighted averaging approach that incorporated graph-theoretical clustering and a Support Vector Machine (SVM) classifier for classifier combination. The researchers utilized graph-theoretical clustering techniques as part of the weighted averaging approach. Graph theory provides a framework for analyzing relationships between data points, and clustering algorithms can group similar data points together based on certain criteria. By incorporating graph-theoretical clustering, the approach likely aimed to identify clusters of data points with similar characteristics for more effective combination of classifier outputs. In addition to clustering, the approach involved the use of an SVM classifier. SVMs are powerful machine learning models commonly used for classification tasks. By integrating an SVM classifier into the weighted averaging process, the researchers likely leveraged its ability to create optimal decision boundaries between classes in the feature space. The results obtained from the experiments indicated that the proposed approach, despite its simplicity and intuitive nature, performed comparably to more sophisticated methods.
</p>

<p>
In [74], the researchers employed three different techniques - Highest Rank (HR), Borda Count (BC), and Logistic Regression (LR) - for combining decisions in a multi-classifier system. The decisions produced by each individual classifier were ranked based on their confidence or accuracy. The HR, BC, and LR techniques were then applied to either reduce the set of possible classes or re-rank them during the combination process. The results obtained from the experiments demonstrated a substantial improvement in the performance of the multi-classifier system.
</p>

<p>
Similarly, in [75] a new combination technique called Mixed Group Rank (MGR) was introduced as a novel approach to balancing between preference and confidence in a multi-classifier system. This technique aimed to generalize the principles of Highest Rank (HR), Borda Count (BC), and Logistic Regression (LR) by incorporating elements of both preference-based ranking and confidence-based decision-making.
</p>

<p>
In [76], the authors introduced an innovative approach that involves dynamically switching between classifier combination and classifier selection based on the characteristics of different regions in the feature space. The authors further introduced a hybrid combination scheme that integrates clustering-and-selection (CS) techniques with decision template (DT) methods. This hybrid approach likely combines the benefits of clustering for identifying regions of dominance and selection of the most appropriate classifier, along with decision templates for combining classifier outputs in a structured manner. The authors discussed the tradeoff between selecting the best classifier and combining classifiers. This tradeoff likely involves considerations of the strengths and weaknesses of individual classifiers versus the potential benefits of combining multiple classifiers.
</p>

<p>
In [77], the authors introduced a method based on classifier selection that focused on identifying the most suitable candidate through confidence evaluation of distance-based classifiers. The method aimed to select the most precise candidate from a set of distance-based classifiers by evaluating their confidence levels. This process likely involved assessing the certainty or reliability of each classifier's decision-making based on the distances between data points in the feature space. The authors likely defined specific rules or criteria for selecting the precise candidate based on the confidence evaluations of the distance-based classifiers. These rules may have considered factors such as the proximity of data points to decision boundaries, the consistency of classifier outputs, or the overall confidence levels of individual classifiers. The experiments conducted in the study likely utilized distance metrics such as Euclidean distance and city block distance for recognizing handwritten characters.
</p>

<p>
In [78], the author used information from the confusion matrix to merge multiple classifiers using a class ranking Borda type reconciliation method. The class ranking Borda type reconciliation method is a technique that combines the outputs of multiple classifiers by ranking the classes based on their performance and then using a Borda count approach to reconcile the rankings. The results obtained from this method were compared with three other classifier combination techniques: majority voting, sum rule, and median rule. The comparison was done using three types of confusion matrices: deterministic, uniform, and stochastic. The APBorda (aposteriori Borda count) and sum rule gave the overall best improvement, except in the case of a stochastic confusion matrix and disparate combination (where classifiers had a \(10\%\) accuracy difference from each other). This means that in most cases, the APBorda and sum rule performed better in combining the classifiers, but there were specific scenarios where they did not perform as well.
</p>

<p>
In [79] a combination technique based on the F-measure was proposed for recognizing human emotions using an SVM (Support Vector Machine) classifier. In this technique, the F-measure was used to form a decision matrix to determine the final emotion. 
</p>

<p>
In [80], the authors proposed an approach for detecting vacant parking spaces by combining two different systems. The first system was based on analyzing image data, while the second system relied on sensor data. The experiments conducted by the authors demonstrated that combining the outputs of these two different systems resulted in a reduced error in detecting vacant parking spaces. 
</p>

<p>
In summary, several classifiers combination techniques have been proposed in the literature with each technique having its own strengths and weaknesses. Recent techniques mostly involve hybridization or modification of previous techniques to achieve better accuracy or to remove an associated constraint on which a particular technique was built on. Some of these constraints include the issue of correlated classifiers, Gaussian distribution, and IID. There is still a need to develop classifiers combination strategies which are not constrained to specific distributions.
</p>

<div id="References"><h1 id="References" class="header"><a href="#References">References</a></h1></div>

<p>
[12] C. De Stefano, F. Fontanella, and A. S. di Freca, "A Novel Naive Bayes Voting Strategy for Combining Classifiers.," in ICFHR, 2012, pp. 467–472.
</p>

<p>
[16] C. Senaras, M. Ozay, and F. T. Yarman Vural, “Building detection with decision fusion,” 2013.
</p>

<p>
[22] Y. Zhan, H. Leung, K.-C. Kwak, and H. Yoon, “Automated speaker recognition for home service robots using genetic algorithm and Dempster--Shafer fusion technique,” Instrum. Meas. IEEE Trans., vol. 58, no. 9, pp. 3058–3068, 2009.
</p>

<p>
[29] L. I. Kuncheva, Combining pattern classifiers: methods and algorithms. John Wiley &amp; Sons, 2004.
</p>

<p>
[32] N. Poh and J. Kittler, "A unified framework for biometric expert fusion incorporating quality measures," Pattern Anal. Mach. Intell. IEEE Trans., vol. 34, no. 1, pp. 3–18, 2012.
</p>

<p>
[33] H. R. Kalluri, S. Prasad, and L. M. Bruce, "Decision-level fusion of spectral reflectance and derivative information for robust hyperspectral land cover classification," Geosci. Remote Sensing, IEEE Trans., vol. 48, no. 11, pp. 4047–4058, 2010.
</p>

<p>
[34] D. J. Miller and L. Yan, "Ensemble classification by critic-driven combining," in Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, 1999, vol. 2, pp. 1029–1032
</p>

<p>
[35] F. Mattern, T. Rohlfing, and J. Denzler, "Adaptive performancebased classifier combination for generic object recognition," in Proc. of International Fall Workshop Vision, Modeling and Visualization (VMV), 2005, pp. 139–146
</p>

<p>
[36] G. Jain, A. Ginwala, and Y. A. Aslandogan, "An approach to text classification using dimensionality reduction and combination of classifiers," in Information Reuse and Integration, 2004. IRI 2004. Proceedings of the 2004 IEEE International Conference on, 2004, pp. 564–569.
</p>

<p>
[37] M. Magimai-Doss, D. Hakkani-Tur, O. Cetin, E. Shriberg, J. Fung, and N. Mirghafori, "Entropy based classifier combination for sentence segmentation," in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, 2007, vol. 4, p. IV--189
</p>

<p>
[39] S. Adhikari and S. Saha, "Multiple classifier combination technique for sensor drift compensation using ANN &amp; KNN," in Advance Computing Conference (IACC), 2014 IEEE International, 2014, pp. 1184–1189.
</p>

<p>
[40] A. J. Ma, P. C. Yuen, and J.-H. Lai, "Linear dependency modeling for classifier fusion and feature combination," Pattern Anal. Mach. Intell. IEEE Trans., vol. 35, no. 5, pp. 1135–1148, 2013.
</p>

<p>
[43] Z. Wu, C.-H. Li, and V. Cheng, "Large margin maximum entropy machines for classifier combination," in Wavelet Analysis and Pattern Recognition, 2008. ICWAPR’08. International Conference on, 2008, vol. 1, pp. 378–383
</p>

<p>
[44] R. P. W. Duin, "The combining classifier: to train or not to train?," in Pattern Recognition, 2002. Proceedings. 16th International Conference on, 2002, vol. 2, pp. 765–770.
</p>

<p>
[45] W. Wang, A. Brakensiek, and G. Rigoll, "Combination of multiple classifiers for handwritten word recognition," in Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International Workshop on, 2002, pp. 117–122.
</p>

<p>
[48] A. V Bogdanov, "Neuroinspired architecture for robust classifier fusion of multisensor imagery," Geosci. Remote Sensing, IEEE Trans., vol. 46, no. 5, pp. 1467–1487, 2008.
</p>

<p>
[49] G. Pasquariello, N. Ancona, P. Blonda, C. Tarantino, G. Satalino, and A. D’Addabbo, "Neural network ensemble and support vector machine classifiers for the analysis of remotely sensed data: a comparison," in Geoscience and Remote Sensing Symposium, 2002. IGARSS’02. 2002 IEEE International, 2002, vol. 1, pp. 509–511.
</p>

<p>
[50] Y.-D. Lan and L. Gao, "A New Model of Combining Multiple Classifiers Based on Neural Network," in Emerging Intelligent Data and Web Technologies (EIDWT), 2013 Fourth International Conference on, 2013, pp. 154–159.
</p>

<p>
[51] V. Di Lecce, G. Dimauro, A. Guerriero, S. Impedovo, G. Pirlo, and A. Salzo, "Knowledge-based methods for classifier combination: an experimental investigation," in Image Analysis and Processing, 1999. Proceedings. International Conference on, 1999, pp. 562–565.
</p>

<p>
[52] A. K. Jain, R. P. W. Duin, and J. Mao, “Statistical pattern recognition: A review,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 22, no. 1, pp. 4–37, 2000.
</p>

<p>
[53] L. I. Kuncheva, J. C. Bezdek, and M. A. Sutton, “On combining multiple classifiers by fuzzy templates,” in Fuzzy Information Processing Society-NAFIPS, 1998 Conference of the North American, 1998, pp. 193–197
</p>

<p>
[54] T. D. Pham, “Combination of multiple classifiers using adaptive fuzzy integral,” in Artificial Intelligence Systems, 2002.(ICAIS 2002). 2002 IEEE International Conference on, 2002, pp. 50–55.
</p>

<p>
[55] M. Fauvel, J. Chanussot, and J. A. Benediktsson, “Decision fusion for the classification of urban remote sensing images,” Geosci. Remote Sensing, IEEE Trans., vol. 44, no. 10, pp. 2828–2838, 2006.
</p>

<p>
[56] M. Cococcioni, B. Lazzerini, and F. Marcelloni, “A TSK fuzzy model for combining outputs of multiple classifiers,” in Fuzzy Information, 2004. Processing NAFIPS’04. IEEE Annual Meeting of the, 2004, vol. 2, pp. 871–876.
</p>

<p>
[57] Y. Yaslan and Z. Cataltepe, “Co-training with adaptive bayesian classifier combination,” in Computer and Information Sciences, 2008. ISCIS’08. 23rd International Symposium on, 2008, pp. 1–4.
</p>

<p>
[58] L. Lam and C. Y. Suen, “Optimal combinations of pattern classifiers,” Pattern Recognit. Lett., vol. 16, no. 9, pp. 945–954, 1995.
</p>

<p>
[59] K. Sirlantzis and M. C. Fairhurst, “Optimisation of multiple classifier systems using genetic algorithms,” in Image Processing, 2001. Proceedings. 2001 International Conference on, 2001, vol. 1, pp. 1094–1097
</p>

<p>
[66] H. Schwenk, “Using boosting to improve a hybrid HMM/neural network speech recognizer,” in Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, 1999, vol. 2, pp. 1009–1012.
</p>

<p>
[67] J. Hu and Y. Chen, “Offline Signature Verification Using Real Adaboost Classifier Combination of Pseudo-dynamic Features,” in Document Analysis and Recognition (ICDAR), 2013 12th International Conference on, 2013, pp. 1345–1349
</p>

<p>
[68] X. Yi, Z. Kou, and C. Zhang, “Classifier combination based on active learning,” in Pattern Recognition, 2004. ICPR 2004. Proceedings of the 17th International Conference on, 2004, vol. 1, pp. 184–187.
</p>

<p>
[69] J. Kremer, K. Steenstrup Pedersen, and C. Igel, “Active learning with support vector machines,” Wiley Interdiscip. Rev. Data Min. Knowl. Discov., vol. 4, no. 4, pp. 313–326, Jul. 2014.
</p>

<p>
[70] A. Ulaş, O. T. Yıldız, and E. Alpaydın, “Eigenclassifiers for combining correlated classifiers,” Inf. Sci. (Ny)., vol. 187, pp. 109– 120, 2012.
</p>

<p>
[71] E. Ulaş, A., Semerci, M., Yıldız, O. T., &amp; Alpaydın, “Incremental construction of classifier and discriminant ensembles,” Inf. Sci. (Ny)., vol. 179, no. 9, pp. 1298–1318, 2009
</p>

<p>
[72] U. Ekmekci and Z. Cataltepe, “Classifier combination with kernelized eigenclassifiers,” in Information Fusion (FUSION), 2013 16th International Conference on, 2013, pp. 743–749.
</p>

<p>
[73] M. Liu, K. Li, and R. Zhao, “A boundary based classifier combination method,” in Control and Decision Conference, 2009. CCDC’09. Chinese, 2009, pp. 3777–3782
</p>

<p>
[74] T. K. Ho, J. J. Hull, and S. N. Srihari, “Decision combination in multiple classifier systems,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 16, no. 1, pp. 66–75, 1994.
</p>

<p>
[75] O. Melnik, Y. Vardi, and C.-H. Zhang, “Mixed group ranks: Preference and confidence in classifier combination,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 26, no. 8, pp. 973–981, 2004.
</p>

<p>
[76] L. I. Kuncheva, “Switching between selection and fusion in combining classifiers: An experiment,” Syst. Man, Cybern. Part B Cybern. IEEE Trans., vol. 32, no. 2, pp. 146–156, 2002.
</p>

<p>
[77] C.-L. Liu and M. Nakagawa, “Precise candidate selection for large character set recognition by confidence evaluation,” Pattern Anal. Mach. Intell. IEEE Trans., vol. 22, no. 6, pp. 636–641, 2000.
</p>

<p>
[78] J. R. Parker, “Combining multiple non-homogeneous classifiers: an empirical approach,” in Cognitive Informatics, IEEE International Conference on, 2002, p. 288.
</p>

<p>
[79] A. Agrawal and N. K. Mishra, “Fusion Based Emotion Recognition System,” in 2016 International Conference on Computational Science and Computational Intelligence (CSCI), 2016, pp. 727–732
</p>

<p>
[80] Junzhao, L., Mohandes, M., Deriche, M., “A Multi-Classifier Image Based Vacant Parking Detection System”, IEEE International Conference on Electronics, Circuits, and Systems ICESC, pp. 933-936, Abu Dhabi, UAE, DEC 8-11, 2013
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>