<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../src/style/custom.css">
    <title>Estrategias para la combinación de clasificadores</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Estrategias para la combinación de clasificadores"><h1 id="Estrategias para la combinación de clasificadores" class="header"><a href="#Estrategias para la combinación de clasificadores">Estrategias para la combinación de clasificadores</a></h1></div>

<hr>

<p>
This section provides a detailed review of classifiers combination techniques First, we discuss the different levels at which combination is performed: sensors, features and decisions. We then expand on the concept of soft vs. hard combination techniques. Finally, we discuss how the combination techniques can be grouped as either adaptive or non-adaptive.
</p>

<div id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination"><h2 id="Levels of Classifiers Combination" class="header"><a href="#Estrategias para la combinación de clasificadores-Levels of Classifiers Combination">Levels of Classifiers Combination</a></h2></div>

<p>
Classifiers combination can be carried out at three different levels:
</p>

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Early combination at sensor data level"></span><strong id="Early combination at sensor data level">Early combination at sensor data level</strong>: combination of data collected from two or more sensors before feature selection technique is applied.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Combination at feature level"></span><strong id="Combination at feature level">Combination at feature level</strong>: it may simply involve basic concatenation of feature vectors with equal or different weights (might result in high dimensonal vectors, whose dimension has to be reduced).

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Late combination at the decision level"></span><strong id="Late combination at the decision level">Late combination at the decision level</strong>: they are based on one of three approaches: abstract, rank, and score:

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Abstract-based"></span><strong id="Abstract-based">Abstract-based</strong>: a single output label from each individual classifier is used as input to the combination scheme.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Rank-based"></span><strong id="Rank-based">Rank-based</strong>: each classifier yields several labels ranked from the most likely to the least likely. This information is then used by the combination scheme to reach the final decision. 

</li><li>
<span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-Score-based"></span><strong id="Score-based">Score-based</strong>: each classifier outputs the \(n\) best labels together with their confidence scores. The combination can be <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-density-based"></span><strong id="density-based">density-based</strong>, <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-transformation-based"></span><strong id="transformation-based">transformation-based</strong> or <span id="Estrategias para la combinación de clasificadores-Levels of Classifiers Combination-classifier-based"></span><strong id="classifier-based">classifier-based</strong> score fusion.

</li></ul>
</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/classifier_combination_levels.png" alt="Classifier Combination Levels" style="width:700px;height:350px">
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination"><h2 id="Hard and Soft Level Classifier Combination" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination">Hard and Soft Level Classifier Combination</a></h2></div>

<p>
Another way to categorize combination algorithms is whether hard thresholding or soft scoring is used with each of the classifiers.
</p>

<ul>
<li>
<span id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Hard-level combination"></span><strong id="Hard-level combination">Hard-level combination</strong>: uses the output of the classifier after it is hard thresholded.

</li><li>
<span id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Soft-level combination"></span><strong id="Soft-level combination">Soft-level combination</strong>: uses estimates of the aposteriori probability of the class.

</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/classifier_hard_soft_combination.png" alt="Classifier Hard/Soft Combination" style="width:800px;height:150px">
The sum, product, max, min rules, etc., fall under the soft level combiners as they use the output aposteriori probability of the classifier or a score.
</p>

<p>
Majority voting is a typical example of hard-level combiners and has found widespread use in the literature. There are three different versions of voting:
</p>

<ul>
<li>
Unanimous voting.

</li><li>
More than half voting.

</li><li>
Highest number of votes.

</li></ul>
<p>
Considering the output label vector of the \(i\)th classifier as:
</p>

\begin{align}
[d_{i, 1}, \cdots, d_{i, N}]^T \in [0, 1]^N
\end{align}

<p>
where \(i = 1, 2, \cdots, M\) and \(d_{i, j} = 1\) if the classifier \(D_i\) labels the \(i\)th instance as class \(\omega_j\) and \(0\) otherwise. The majority vote results in a decision for class \(\omega_k\) if:
</p>

\begin{align}
\sum_{i = 1}^M d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M d_{i, j}
\end{align}

<p>
Where \(M\) is the total number of classifiers and \(N\) is total number of classes. Such that class \(\omega_k\) is the most "selected" on all the classifier.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/classifier_combination_majority_voting.png" alt="Majority Voting" style="width:500px;height:350px">
</p>

<p>
The accuracy of the combination scheme is given as:
</p>

\begin{align}
P_{maj} = \sum_{m = \frac{M}{2} + 1}^M \binom{M}{m} p^m (1-p)^{M - m}
\end{align}

<p>
where \(p\) is the probability of correct classification.
</p>

<p>
Majority voting provides an accurate class label when at least \(\frac{M}{2} + 1\) classifiers give correct classifications, it also requires participating classifiers to have comparable accuracies.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Weighted Majority Voting"><h3 id="Weighted Majority Voting" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Weighted Majority Voting">Weighted Majority Voting</a></h3></div>

<p>
Weighted majority voting is used when the classifiers' accuracies are not similar, so it is reasonable to assign more weight to the most accurate classifier. Now the decision rule becomes:
</p>

\begin{align}
\sum_{i = 1}^M b_i d_{i, k} = \max_{j = 1}^N \sum_{i = 1}^M b_i d_{i, j}
\end{align}

<p>
where \(b_i\) is the weight associated with classifier \(D_i\). For the sake of convenience, it is a good practice to normalize the weights such that the sum is one.
</p>

<p>
The weight selection is very important in determining the overall accuracy of the classifier combinations. Therefore, to minimize the classification error of the combination, the weights are assigned as follows:
</p>

\begin{align}
b_i \propto \log\left(\frac{p_i}{1 - p_i}\right)
\end{align}

<p>
where \(p_i, \cdots, p_M\) are the individual accuracies for each independent classifier.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Weighted Consult-and-vote"><h3 id="Dynamic Weighted Consult-and-vote" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Weighted Consult-and-vote">Dynamic Weighted Consult-and-vote</a></h3></div>

<p>
In [30], Muhlbaier et al. proposed a method for combining ensembles of classifiers using dynamic weighted consult-andvote. Voting weights are determined by relative performance of each classifier on the training data. The approach learns a new class by allowing individual classifiers to consult with each other to determine their voting weights for each of the test instances. The following figure, shows the dynamic weighted consult-and-vote approach:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/dynamic_weighted_consult_and_vote.png" alt="Dynamic Weighted Consult and Vote" style="width:650px;height:450px">
</p>

<p>
Classifiers examine each other's decision, cross referencing their decisions with the list of class labels on which they were initially trained, then update the weights.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Divide and Conquer"><h3 id="Divide and Conquer" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Divide and Conquer">Divide and Conquer</a></h3></div>

<p>
Another modification of majority voting is to complement it with a divide and conquer technique [31].It divides the classification task into a set of smaller and simpler problems, then solving each of problem separately, followed by a majority voting scheme.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/classifier_divide_and_conquer.png" alt="Divide and Conquer" style="width:550px;height:350px">
</p>
<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Quality Based Combination Techniques"><h3 id="Quality Based Combination Techniques" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Quality Based Combination Techniques">Quality Based Combination Techniques</a></h3></div>

<p>
Other recent approaches include quality-based combination [32] which gives higher weights to the more reliable classifiers under some given conditions (i.e. better quality).
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Critic Classifier"><h3 id="Critic Classifier" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Critic Classifier">Critic Classifier</a></h3></div>

<p>
The authors introduced a critic associated with each classifier whose purpose is to predict the error of the classifier [34]. The approach is based on classical standard voting techniques for classifiers combination. However, it performed best on two-class problem.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Adaptative Voting Technique"><h3 id="Adaptative Voting Technique" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Adaptative Voting Technique">Adaptative Voting Technique</a></h3></div>

<p>
The adaptative Voting Technique [35] involves the weighting of classifiers based on their estimated recognition performance. The authors applied "Simultaneous Truth and Performance Level Evaluation" (STAPLE).
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Entropy Based Combination Technique"><h3 id="Dynamic Entropy Based Combination Technique" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Dynamic Entropy Based Combination Technique">Dynamic Entropy Based Combination Technique</a></h3></div>

<p>
The idea of this combination scheme [37] lies in assigning large weights to classifiers that are very confident in their decisions, and small weights to classifiers that are less confident. 
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Assuming Linear Dependency Between Predictors"><h3 id="Assuming Linear Dependency Between Predictors" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Assuming Linear Dependency Between Predictors">Assuming Linear Dependency Between Predictors</a></h3></div>

<p>
In [40], the authors considered linear dependency of both classifiers and features. Two models, Linear Classifier Dependency Modelling (LCDM) and Linear Feature Dependency Modelling (LFDM) were developed. They showed it outperformed existing classifier and feature level combination methods under non-Gaussian distribution. However the LFDM technique takes longer to train.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Fixed Rule Based Combination Techniques"><h3 id="Fixed Rule Based Combination Techniques" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Fixed Rule Based Combination Techniques">Fixed Rule Based Combination Techniques</a></h3></div>

<p>
They use class labels, distances, or confidences from the individual classifiers without the need for a training stage. According to [44], fixed rule techniques are only efficient under strict conditions such as availability of large training sets, generation of reliable confidences from base classifiers, and training of the base classifiers on different feature spaces.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Runtime Weighted Opinion Pool"><h3 id="Runtime Weighted Opinion Pool" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Runtime Weighted Opinion Pool">Runtime Weighted Opinion Pool</a></h3></div>

<p>
In [45], the approach, Runtime Weighted Opinion Pool (RWOP), dynamically assigns weights to the classifiers during runtime and the final combination is weighted according to the local performance of the classifiers. Unlike other weighted sum based approaches, this technique determines the weights with an intuitive runtime strategy. It achieves enhanced performance and reduces the relative error rate significantly.
</p>

<div id="Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Other Experiments"><h3 id="Other Experiments" class="header"><a href="#Estrategias para la combinación de clasificadores-Hard and Soft Level Classifier Combination-Other Experiments">Other Experiments</a></h3></div>

<p>
In [33], an effective approach for decision level combination based on spectral reflectance and its higher order derivatives to classify hyperspectral land images was proposed. Their study was carried out under two scenarios: LDA (Linear Discriminant Analysis) based dimensionality reduction followed by a single maximum likelihood classifier, and multiple classifiers decision fusion (MCDF).
</p>

<p>
In [36], three different classifiers (Naive Bayes, J48 Decision tree, and Decision table) were combined using simple, weighted, and probability-based voting. Weighted and probability voting techniques outperformed simple voting and single classifiers.
</p>

<p>
In [39], an approach which combines ANN and KNN based classifiers using majority voting was discussed. This approach was applied to improve accuracy when sensor data is subjected to drift. Two sets of classifiers were implemented (BP-ANN and KNN). For classifiers combination, a simple median voting was applied for ANN based combination and majority voting for the case of KNN. The authors showed a substantial improvement in performance.
</p>

<p>
In [43], an approach based on majority voting was proposed. Unequal weights are assigned to the base classifiers according to their performance. Voting was found to be the best combination rule for the dataset used.
</p>

<p>
In [46], the authors proposed a new approach for classifiers combination which strictly work with HMM based-classifiers.
</p>

<p>
In [12], a new weighted majority voting approach was proposed. It assigns weights to the different classes rather than the different base classifiers. Such weights were computed by estimating the joint probability distribution of each class with the scores provided by all classifiers in the combining pool. The major drawback of this approach lies on the statistical independence assumption of variables.
</p>

<div id="Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners"><h2 id="Adaptative and Non-Adaptative Combiners" class="header"><a href="#Estrategias para la combinación de clasificadores-Adaptative and Non-Adaptative Combiners">Adaptative and Non-Adaptative Combiners</a></h2></div>

<p>
Adaptive techniques for classifiers combination are mainly based on evolution or artificial intelligence algorithms. They include neural networks combination strategies and genetic algorithms as well as fuzzy set theory. Techniques under these categories are summarized on the following figure:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/adaptative_non_adaptative_combiners.png" alt="Adaptative and Non-Adaptative Combiners" style="width:700px;height:200px">
</p>

<p>
Artificial Neural Networks are usually used as a base classifier [29], however, it has also found wide use in combination of classifiers.
</p>

<p>
In [48], the author proposed two new approaches for robust and fault-tolerant classifiers combination: Attractor dynamics (AD) algorithm and classifier masking (CM) algorithm. The authors show that proposed combination algorithms result in improved robustness and fault tolerance compared with individual classifiers. The developed approaches outperformed the performance of classifiers combination based on averaging and majority voting because the AD and CM algorithms discard corrupted classifier outputs.
</p>

<p>
In [49], the authors compared the performance of combining the outputs of an ensemble of ANNs with that of SVM classifiers for remotely sensed data.
</p>

<p>
In [50], the authors used the ANN, itself, as a model for classifiers combination. A three-layer ANN (see Fig 9) was used where the different classifiers represent the units of the hidden layer. Results obtained show improved performance over individual classifiers, however, the approach was not compared against other classical classifiers combination techniques.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_based_classifier.png" alt="NN Based Classifier" style="width:500px;height:400px">
</p>

<p>
In [51], the authors investigated the role of apriori knowledge using existing classifiers combination techniques, namely the Behavior Knowledge space and the D-S theory. The approach performs well when strongly correlated classifiers are combined. 
</p>

<div id="References"><h1 id="References" class="header"><a href="#References">References</a></h1></div>

<p>
[12] C. De Stefano, F. Fontanella, and A. S. di Freca, "A Novel Naive Bayes Voting Strategy for Combining Classifiers.," in ICFHR, 2012, pp. 467–472.
</p>

<p>
[29] L. I. Kuncheva, Combining pattern classifiers: methods and algorithms. John Wiley &amp; Sons, 2004.
</p>

<p>
[32] N. Poh and J. Kittler, "A unified framework for biometric expert fusion incorporating quality measures," Pattern Anal. Mach. Intell. IEEE Trans., vol. 34, no. 1, pp. 3–18, 2012.
</p>

<p>
[33] H. R. Kalluri, S. Prasad, and L. M. Bruce, "Decision-level fusion of spectral reflectance and derivative information for robust hyperspectral land cover classification," Geosci. Remote Sensing, IEEE Trans., vol. 48, no. 11, pp. 4047–4058, 2010.
</p>

<p>
[34] D. J. Miller and L. Yan, "Ensemble classification by critic-driven combining," in Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, 1999, vol. 2, pp. 1029–1032
</p>

<p>
[35] F. Mattern, T. Rohlfing, and J. Denzler, "Adaptive performancebased classifier combination for generic object recognition," in Proc. of International Fall Workshop Vision, Modeling and Visualization (VMV), 2005, pp. 139–146
</p>

<p>
[36] G. Jain, A. Ginwala, and Y. A. Aslandogan, "An approach to text classification using dimensionality reduction and combination of classifiers," in Information Reuse and Integration, 2004. IRI 2004. Proceedings of the 2004 IEEE International Conference on, 2004, pp. 564–569.
</p>

<p>
[37] M. Magimai-Doss, D. Hakkani-Tur, O. Cetin, E. Shriberg, J. Fung, and N. Mirghafori, "Entropy based classifier combination for sentence segmentation," in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, 2007, vol. 4, p. IV--189
</p>

<p>
[39] S. Adhikari and S. Saha, "Multiple classifier combination technique for sensor drift compensation using ANN &amp; KNN," in Advance Computing Conference (IACC), 2014 IEEE International, 2014, pp. 1184–1189.
</p>

<p>
[40] A. J. Ma, P. C. Yuen, and J.-H. Lai, "Linear dependency modeling for classifier fusion and feature combination," Pattern Anal. Mach. Intell. IEEE Trans., vol. 35, no. 5, pp. 1135–1148, 2013.
</p>

<p>
[43] Z. Wu, C.-H. Li, and V. Cheng, "Large margin maximum entropy machines for classifier combination," in Wavelet Analysis and Pattern Recognition, 2008. ICWAPR’08. International Conference on, 2008, vol. 1, pp. 378–383
</p>

<p>
[44] R. P. W. Duin, "The combining classifier: to train or not to train?," in Pattern Recognition, 2002. Proceedings. 16th International Conference on, 2002, vol. 2, pp. 765–770.
</p>

<p>
[45] W. Wang, A. Brakensiek, and G. Rigoll, "Combination of multiple classifiers for handwritten word recognition," in Frontiers in Handwriting Recognition, 2002. Proceedings. Eighth International Workshop on, 2002, pp. 117–122.
</p>

<p>
[48] A. V Bogdanov, "Neuroinspired architecture for robust classifier fusion of multisensor imagery," Geosci. Remote Sensing, IEEE Trans., vol. 46, no. 5, pp. 1467–1487, 2008.
</p>

<p>
[49] G. Pasquariello, N. Ancona, P. Blonda, C. Tarantino, G. Satalino, and A. D’Addabbo, "Neural network ensemble and support vector machine classifiers for the analysis of remotely sensed data: a comparison," in Geoscience and Remote Sensing Symposium, 2002. IGARSS’02. 2002 IEEE International, 2002, vol. 1, pp. 509–511.
</p>

<p>
[50] Y.-D. Lan and L. Gao, "A New Model of Combining Multiple Classifiers Based on Neural Network," in Emerging Intelligent Data and Web Technologies (EIDWT), 2013 Fourth International Conference on, 2013, pp. 154–159.
</p>

<p>
[51] V. Di Lecce, G. Dimauro, A. Guerriero, S. Impedovo, G. Pirlo, and A. Salzo, "Knowledge-based methods for classifier combination: an experimental investigation," in Image Analysis and Processing, 1999. Proceedings. International Conference on, 1999, pp. 562–565.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>