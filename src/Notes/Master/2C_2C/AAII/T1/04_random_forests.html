<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css">
    <title>Random Forests</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Random Forests"><h1 id="Random Forests" class="header"><a href="#Random Forests">Random Forests</a></h1></div>

<hr>

<div id="Random Forests-Definition"><h2 id="Definition" class="header"><a href="#Random Forests-Definition">Definition</a></h2></div>

<p>
The idea in random forests (Algorithm 15.1) is to <span id="Random Forests-Definition-improve the variance reduction of bagging by reducing the correlation between the trees"></span><strong id="improve the variance reduction of bagging by reducing the correlation between the trees">improve the variance reduction of bagging by reducing the correlation between the trees</strong>, without increasing the variance too much. This is achieved in the tree-growing process through <span id="Random Forests-Definition-random selection of the input variables"></span><strong id="random selection of the input variables">random selection of the input variables</strong>.
</p>

<p>
An average of \(B\) i.i.d. random variables, each with variance \(\sigma^2\), has variance \(\frac{1}{B}\sigma^2\). 
</p>

<p>
If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation ρ, the variance of the average is:
</p>

\begin{align}
\rho\sigma^2 + \frac{1 - \rho}{B}\sigma^2
\end{align}

<p>
As \(B\) increases, the second term disappears, but the first remains, and hence the size of <span id="Random Forests-Definition-the correlation of pairs of bagged trees limits the benefits of averaging"></span><strong id="the correlation of pairs of bagged trees limits the benefits of averaging">the correlation of pairs of bagged trees limits the benefits of averaging</strong>.
</p>

<p>
<img src="/home/alba/Documentos/GitRepos/NoteWorld/public/assets/random_forests_algorithm.png" alt="Random Forests Algorithm" style="width:500px;height:350px">
</p>

<p>
When growing a tree on a bootstrapped dataset before each split, <span id="Random Forests-Definition-select"></span><strong id="select">select</strong> \(m \leq p\) <span id="Random Forests-Definition-of the input variables at random"></span><strong id="of the input variables at random">of the input variables at random</strong> as candidates for splitting.
</p>

<p>
After \(B\) such trees \(\{T(x; \Theta_b)\}_1^B\) are grown, the random forest (regression) predictor is:
</p>

\begin{align}
\hat{f}_{rf}^B(x) = \frac{1}{B}\sum_{b=1}^B T(x; \Theta_b)
\end{align}

<p>
Where \(\Theta_b\) characterizes the bth random forest tree in terms of split variables, cutpoints at each node, and terminal-node values.
</p>

<p>
Intuitively, <span id="Random Forests-Definition-reducing"></span><strong id="reducing">reducing</strong> \(m\) will <span id="Random Forests-Definition-reduce the correlation between any pair of trees in the ensemble"></span><strong id="reduce the correlation between any pair of trees in the ensemble">reduce the correlation between any pair of trees in the ensemble</strong>.
</p>

<div id="Random Forests-Details of Random Forests"><h2 id="Details of Random Forests" class="header"><a href="#Random Forests-Details of Random Forests">Details of Random Forests</a></h2></div>

<p>
For <span id="Random Forests-Details of Random Forests-classification"></span><strong id="classification">classification</strong>, the default value for \(m\) is \(\lfloor \sqrt{p} \rfloor\) and the minimum node size is one.
</p>

<p>
For <span id="Random Forests-Details of Random Forests-regression"></span><strong id="regression">regression</strong>, the default value for \(m\) is \(\lfloor \frac{p}{3} \rfloor\) and the minimum node size is five.
</p>

<p>
In practice the best values for these parameters will depend on the problem, and they should be treated as <span id="Random Forests-Details of Random Forests-tuning parameters"></span><strong id="tuning parameters">tuning parameters</strong> (hyperparamters).
</p>

<div id="Random Forests-Details of Random Forests-Variable Importance"><h3 id="Variable Importance" class="header"><a href="#Random Forests-Details of Random Forests-Variable Importance">Variable Importance</a></h3></div>

<p>
At each split in each tree, the <span id="Random Forests-Details of Random Forests-Variable Importance-improvement in the split-criterion"></span><strong id="improvement in the split-criterion">improvement in the split-criterion</strong> is the <span id="Random Forests-Details of Random Forests-Variable Importance-importance measure attributed to the splitting variable"></span><strong id="importance measure attributed to the splitting variable">importance measure attributed to the splitting variable</strong>, and is <span id="Random Forests-Details of Random Forests-Variable Importance-accumulated over all the trees"></span><strong id="accumulated over all the trees">accumulated over all the trees</strong> in the forest separately for each variable. 
</p>

<p>
Random forests also use the OOB samples to construct a different variable importance measure. 
</p>

<ol>
<li>
When the bth tree is grown, the <span id="Random Forests-Details of Random Forests-Variable Importance-OOB samples"></span><strong id="OOB samples">OOB samples</strong> are passed down the tree, and the <span id="Random Forests-Details of Random Forests-Variable Importance-prediction accuracy is recorded"></span><strong id="prediction accuracy is recorded">prediction accuracy is recorded</strong>. 

</li><li>
Then the values for the jth variable are <span id="Random Forests-Details of Random Forests-Variable Importance-randomly permuted in the OOB samples"></span><strong id="randomly permuted in the OOB samples">randomly permuted in the OOB samples</strong>, and the accuracy is again computed. 

</li><li>
The decrease in accuracy as a result of this permuting is <span id="Random Forests-Details of Random Forests-Variable Importance-averaged over all trees"></span><strong id="averaged over all trees">averaged over all trees</strong>, and is used as a <span id="Random Forests-Details of Random Forests-Variable Importance-measure of the importance of variable"></span><strong id="measure of the importance of variable">measure of the importance of variable</strong> \(j\) in the random forest. 

</li></ol>
<div id="Random Forests-Details of Random Forests-Proximity Plots"><h3 id="Proximity Plots" class="header"><a href="#Random Forests-Details of Random Forests-Proximity Plots">Proximity Plots</a></h3></div>

<p>
In growing a random forest, an \(N \times N\) proximity matrix is accumulated for the training data. Such that the entry \(ij\) contains the number of trees for which the OOB sample \(x_i\) and the OOB sample \(x_j\) are on the same terminal node.
</p>

<p>
This proximity matrix is then represented in two dimensions using multidimensional scaling like the following example:
</p>

<p>
<img src="/home/alba/Documentos/GitRepos/NoteWorld/public/assets/proximity_plot.png" alt="Proximity Plot" style="width:500px;height:350px">
</p>

<p>
The proximity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier.
</p>

<div id="Random Forests-Details of Random Forests-Random Forests and Overfitting"><h3 id="Random Forests and Overfitting" class="header"><a href="#Random Forests-Details of Random Forests-Random Forests and Overfitting">Random Forests and Overfitting</a></h3></div>

<p>
When the number of variables \(p\) is large, but the fraction of relevant variables small, random forests are likely to perform poorly with small \(m\). At each split the chance can be small that the relevant variables will be selected.
</p>

<p>
Another claim is that random forests “cannot overfit” the data.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>