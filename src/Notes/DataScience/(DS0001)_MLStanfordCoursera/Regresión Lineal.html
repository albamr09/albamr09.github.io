<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css">
    <title>Regresión Lineal</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Regresión Lineal"><h1 id="Regresión Lineal" class="header"><a href="#Regresión Lineal">Regresión Lineal</a></h1></div>

<hr>

<ol>
<li>
<a href="Regresión Lineal.html#Regresión Lineal-Descripción de los datos">Descripción de los datos</a>

</li><li>
<a href="Regresión Lineal.html#Regresión Lineal-Hipótesis">Hipótesis</a>

</li><li>
<a href="Regresión Lineal.html#Regresión Lineal-Funcion de coste">Funcion de coste</a>

</li><li>
<a href="Regresión Lineal.html#Regresión Lineal-Descenso gradiente">Descenso gradiente</a>

</li></ol>
<hr>

<div id="Regresión Lineal-Descripción de los datos"><h2 id="Descripción de los datos" class="header"><a href="#Regresión Lineal-Descripción de los datos">Descripción de los datos</a></h2></div>

<ul>
<li>
\(X = (x_{ij})\) una matriz \(n \times m\) donde cada \(x_{ij}\) es la característica \(i\) del ejemplo \(j\), tal que 
\begin{align}
X = 
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1m} \\
\cdots &amp; \ddots &amp; \cdots \\
x_{n1} &amp; \cdots &amp; x_{nm} \\
\end{bmatrix} 
\end{align}

</li></ul>
<p>
<em>Cada columna es un ejemplo</em>
</p>

<p>
<em>En cada fila están los valores de una característica</em>
</p>

<ul>
<li>
\(\Theta = (\theta_i)\) es un vector fila \(1\times n\) donde cada \(\theta_i\) es el peso de la característica \(i\), tal que:
\begin{align}
\Theta = \begin{bmatrix} \theta_1 &amp; \cdots &amp; \theta_n\end{bmatrix} 
\end{align}

</li><li>
\(Y = (y_j)\) es un vector fila \(1\times m\) donde cada \(y_j\) es la salida real para el ejemplo \(j\), tal que:
\begin{align}
Y = \begin{bmatrix} y_1 &amp; \cdots &amp; y_m\end{bmatrix} 
\end{align}

</li></ul>
<div id="Regresión Lineal-Hipótesis"><h2 id="Hipótesis" class="header"><a href="#Regresión Lineal-Hipótesis">Hipótesis</a></h2></div>

<ul>
<li>
Dado un ejemplo, es decir un vector columna \(x\), de dimensiones \(n \times 1\), la hipótesis se define como:
\begin{align}
h_\Theta(x) = \sum_{i=1}^n \theta_i \cdot x_i
\end{align}

</li></ul>
<pre><code>
</code></pre>
<ul>
<li>
Dado un conjunto de \(m\) datos, es decir matriz \(X\), de dimensiones \(n \times m\), la hipótesis se define como:
\begin{align}
h_\Theta(x) = \Theta\cdot X = \begin{bmatrix}\sum_{i=1}^n \theta_i \cdot x_{i1} &amp; \cdots &amp; \sum_{i=1}^n \theta_i \cdot x_{im}\end{bmatrix}
\end{align}

</li></ul>
<p>
El resultado es un vector fila \(1 \times m\), es decir como el vector de salidas \(Y\)
</p>
<pre><code>
</code></pre>
<div id="Regresión Lineal-Funcion de coste"><h2 id="Funcion de coste" class="header"><a href="#Regresión Lineal-Funcion de coste">Funcion de coste</a></h2></div>

<p>
Se define la función de coste, \(J(\Theta)\) como:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m}\sum_{j=1}^m (h_\Theta(x_j) - y_j)^2
\end{align}

<p>
Esta función de coste se denomina <span id="Regresión Lineal-Funcion de coste-Mínimos cuadrados"></span><strong id="Mínimos cuadrados">Mínimos cuadrados</strong>.
</p>
<pre><code>
</code></pre>
<div id="Regresión Lineal-Funcion de coste-Regularización"><h4 id="Regularización" class="header"><a href="#Regresión Lineal-Funcion de coste-Regularización">Regularización</a></h4></div>

<p>
Con regularización, se define la función de coste, \(J(\Theta)\) como:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m}\sum_{j=1}^m (h_\Theta(x_j) - y_j)^2 + \frac{1}{2m} \lambda \sum_{i=1}^n \theta_i^2
\end{align}

<div id="Regresión Lineal-Descenso gradiente"><h2 id="Descenso gradiente" class="header"><a href="#Regresión Lineal-Descenso gradiente">Descenso gradiente</a></h2></div>

<p>
Para actualizar el vector de pesos \(\Theta\) aplicamos el descenso gradiente. Para cada peso \(\theta_i\):
</p>

\begin{align}
\theta_i = \theta_i - \alpha \left( \frac{\delta J(\Theta)}{\delta \theta_i}\right)
\end{align}

<hr>

<p>
La derivada del coste en función del peso \(\theta_i\) se calcula como sigue:
</p>

<p>
<em>Sustituimos la función de coste</em>
</p>

\begin{align}
\frac{\delta J(\Theta)}{\delta \theta_i} = \frac{\delta}{\delta \theta_i} \left(\frac{1}{2m}\sum_{j=1}^m (h_\Theta(x_j) - y_j)^2\right) =
\end{align}

<p>
<em>Sacamos el factor constante de la derivada</em>
</p>

\begin{align}
 = \frac{1}{2m} \frac{\delta}{\delta \theta_i} \left(\sum_{j=1}^m (h_\Theta(x_j) - y_j)^2\right)
\end{align}

<p>
<em>Aplicamos la propiedad que dice que la derivada de una suma equivale a la suma de las derivadas</em>
</p>

\begin{align}
 = \frac{1}{2m} \left(\sum_{j=1}^m \frac{\delta}{\delta \theta_i}(h_\Theta(x_j) - y_j)^2\right)
\end{align}

<p>
<em>Aplicamos la regla de la cadena</em>
</p>

\begin{align}
 = \frac{1}{2m} \left(\sum_{j=1}^m \frac{\delta(h_\Theta(x_j) - y_j)^2}{\delta (h_\Theta(x_j) - y_j)} \frac{\delta (h_\Theta(x_j) - y_j)}{\delta \theta_i}\right)
\end{align}

<p>
<em>Aplicamos aritmética</em>
</p>

\begin{align}
 = \frac{1}{2m} \left(\sum_{j=1}^m 2(h_\Theta(x_j) - y_j) \left[\frac{\delta (h_\Theta(x_j))}{\delta \theta_i} - \frac{\delta (y_j)}{\delta \theta_i}\right]\right)
\end{align}

<hr>

<p>
Como la derivada de \(y_i\) es función de \(\theta_i\) es cero, procedemos a calcular la derivada de \(h_\Theta(x_j)\):
</p>

\begin{align}
\frac{\delta h_\Theta(x_j)}{\delta \theta_i} = \frac{\delta}{\delta \theta_i} \sum_{k=1}^n \theta_k x_{kj} = \sum_{k=1}^n \frac{\delta}{\delta \theta_i} \theta_k x_{kj}
\end{align}

<p>
<em>Donde</em>
</p>

\begin{align}
\frac{\delta}{\delta \theta_i} \theta_k x_{kj} = 
\begin{cases} 
x_{kj}, &amp; k = i \\
0, &amp; k \neq i \\
\end{cases}
\end{align}

<p>
Como para todo \(k\), con \(1 \leq k \leq n\) sólo hay un \(k\), tal que \(k = i\), entonces:
</p>

\begin{align}
\frac{\delta h_\Theta(x_j)}{\delta \theta_i} = x_{kj}
\end{align}

<hr>

<p>
Volemos a la derivada del peso, con \(\frac{\delta h_\Theta(x_j)}{\delta \theta_i} = x_{kj}\) y \(\frac{\delta y_j}{\delta \theta_i} = 0\):
</p>


\begin{align}
 = \frac{1}{2m} \left(\sum_{j=1}^m 2(h_\Theta(x_j) - y_j) \left[x_{ij} - 0\right]\right)
\end{align}

\begin{align}
 = \frac{1}{2m} \left(\sum_{j=1}^m 2(h_\Theta(x_j) - y_j) \cdot x_{ij}\right)
\end{align}

<p>
<em>Sacamos el factor constante 2 como factor común que se elimina con 1/2</em>
</p>

\begin{align}
 = \frac{1}{m} \left(\sum_{j=1}^m (h_\Theta(x_j) - y_j) \cdot x_{ij}\right)
\end{align}

<hr>

<p>
Finalmente, sustituimos todo en la función del gradiente:
</p>

\begin{align}
\theta_i = \theta_i - \alpha \left( \frac{\delta J(\Theta)}{\delta \theta_i}\right) = \theta_i - \alpha \left[\frac{1}{m} \left(\sum_{j=1}^m (h_\Theta(x_j) - y_j) \cdot x_{ij}\right)\right]
\end{align}

<div id="Regresión Lineal-Descenso gradiente-Regularización"><h4 id="Regularización" class="header"><a href="#Regresión Lineal-Descenso gradiente-Regularización">Regularización</a></h4></div>

<p>
Con regularización debemos derivar la función que coste que incluye el parámetro de regularización:
</p>

\begin{align}
\theta_i = \theta_i - \alpha \left( \frac{\delta J(\Theta)}{\delta \theta_i}\right) + \frac{\lambda}{2m} \sum_{k=1}^n \theta_k^2
\end{align}

<p>
El primer término ya lo hemos derivado, por lo tanto procedemos a derivar el segundo término:
</p>

\begin{align}
\frac{\delta}{\delta \theta_i} \left(\frac{\lambda}{2m} \sum_{k=1}^n \theta_k^2\right) = \frac{\lambda}{2m} \left(\sum_{k=1}^n \frac{\delta}{\delta \theta_i} \theta_k^2\right)
\end{align}

<p>
Donde 
</p>

\begin{align}
\frac{\delta}{\delta \theta_i} \theta_k^2 =
\begin{cases}
2 \theta_k, &amp; k = i \\
0, &amp; k \neq i
\end{cases}
\end{align}

<p>
Como para todo \(k\), con \(1 \leq k \leq n\) sólo hay un \(k\), tal que \(k = i\), entonces:
</p>

\begin{align}
\frac{\delta}{\delta \theta_i} \left(\frac{\lambda}{2m} \sum_{k=1}^n \theta_k^2\right) = \frac{\lambda}{2m} 2\theta_i = \frac{\lambda}{m} \theta_i
\end{align}

<hr>

<p>
Por lo tanto la función del descenso gradiente equivale a:
</p>

\begin{align}
\theta_i = \theta_i - \alpha \left( \frac{\delta J(\Theta)}{\delta \theta_i}\right) = \theta_i - \alpha \left[\frac{1}{m} \left(\sum_{j=1}^m (h_\Theta(x_j) - y_j) \cdot x_{ij}\right) + \frac{\lambda}{m}\theta_i\right]
\end{align}
</div>
  
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>

<script src="https://albamr09.github.io/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script src="https://albamr09.github.io/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script src="https://albamr09.github.io/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script src="https://albamr09.github.io/lib/search.js" id="search" data-description="Library to perform search"></script></body></html>