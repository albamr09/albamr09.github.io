<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css">
    <title>Neural Networks</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Neural Networks"><h1 id="Neural Networks" class="header"><a href="#Neural Networks">Neural Networks</a></h1></div>

<hr>

<ol>
<li>
<a href="Neural Networks.html#Neural Networks-Architecture">Architecture</a>

<ol>
<li>
<a href="Neural Networks.html#Neural Networks-Architecture-Algorithm">Algorithm</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Architecture-Output Layer">Output Layer</a>

</li></ol>
</li><li>
<a href="Neural Networks.html#Neural Networks-Forward Propagation">Forward Propagation</a>

<ol>
<li>
<a href="Neural Networks.html#Neural Networks-Forward Propagation-Parameters">Parameters</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Forward Propagation-Outputs">Outputs</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Forward Propagation-Graphical Representation">Graphical Representation</a>

</li></ol>
</li><li>
<a href="Neural Networks.html#Neural Networks-Optimization Problem">Optimization Problem</a>

<ol>
<li>
<a href="Neural Networks.html#Neural Networks-Optimization Problem-Loss Function">Loss Function</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Optimization Problem-Optimization">Optimization</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Optimization Problem-Back-propagation">Back-Propagation</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Optimization Problem-Vectorization">Vectorization</a>

</li></ol>
</li><li>
<a href="Neural Networks.html#Neural Networks-Improving a Neural Network">Improving a Neural Network</a>

<ol>
<li>
<a href="Neural Networks.html#Neural Networks-Improving a Neural Network-Activation Functions">Activation Functions</a>

</li><li>
<a href="Neural Networks.html#Neural Networks-Improving a Neural Network-Initialization Techniques">Initialization Techniques</a>

</li></ol>
</li><li>
<a href="Neural Networks.html#Neural Networks-Anexo">Anexo</a>

</li></ol>
<hr>

<div id="Neural Networks-Architecture"><h2 id="Architecture" class="header"><a href="#Neural Networks-Architecture">Architecture</a></h2></div>

<ul>
<li>
<span id="Neural Networks-Architecture-Input"></span><strong id="Input">Input</strong>: Given any input \(X\) the first thing we do is flatten it. For example if \(X\) is a rgb image of \(64 \times 64\), then \(X \in \mathbb{R}^{64 \times 64 \times 3}\) (for each of the \(64 \times 64\) pixels we have three color channels: red, green, blue), is flattened into a vector in \(\mathbb{R}^{(64*64*3) \times 1}\)

</li><li>
<span id="Neural Networks-Architecture-Neuron"></span><strong id="Neuron">Neuron</strong>: is an operation that has two parts:

<ul>
<li>
Linear part: we denote the linear part like \(z^{[i]}\), where \(i\) is the current layer.

</li></ul>
</li></ul>
  
<p>
  <img src="https://albamr09.github.io/public/assets/neuron.svg" alt="Linear Part Neuron Example" style="transform: translate(22vw, 0)">
</p>
<ul>
<li>
Activation part

</li></ul>
  
<p>
  <img src="https://albamr09.github.io/public/assets/neuron_activation.svg" alt="Activation Part Neuron Example" style="transform: translate(22vw, 0)">
</p>
<ul>
<li>
<span id="Neural Networks-Architecture-Layer"></span><strong id="Layer">Layer</strong>: a layer is a compound of neurons that are not connected with each other.

</li></ul>
  
<p>
  <img src="https://albamr09.github.io/public/assets/layer.svg" alt="Layer Example" style="transform: translate(22vw, 0)">
</p>

<div id="Neural Networks-Architecture-Algorithm"><h3 id="Algorithm" class="header"><a href="#Neural Networks-Architecture-Algorithm">Algorithm</a></h3></div>

<p>
The principal steps of the algorithm are:
</p>

<ol>
<li>
Initialize the weights \(w\) and biases \(b\) randomly

</li><li>
Find the optimal \(w, b\)

</li><li>
Use the optimized \(w, b\) to predict the output by using the formula \(\hat{y} = \sigma(wx +b)\)

</li></ol>
<div id="Neural Networks-Architecture-Output Layer"><h3 id="Output Layer" class="header"><a href="#Neural Networks-Architecture-Output Layer">Output Layer</a></h3></div>

<div id="Neural Networks-Architecture-Output Layer-Sigmoid"><h4 id="Sigmoid" class="header"><a href="#Neural Networks-Architecture-Output Layer-Sigmoid">Sigmoid</a></h4></div>

<p>
The output layer will be different depending on the problem we are tackling. For example if we want to discriminate between 3 classes then the output layer could be as follows:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_multiclass.svg" alt="NN with Multiclass" style="transform: translate(7vw, 0)">
</p>

<p>
So now the output is a vector \(\hat{y} \in \mathbb{R}^{c \times 1}\) where \(c\) is the number of classes.
</p>

<div id="Neural Networks-Architecture-Output Layer-Softmax"><h4 id="Softmax" class="header"><a href="#Neural Networks-Architecture-Output Layer-Softmax">Softmax</a></h4></div>

<p>
The previous classifier allows for outputting multiples classes in the result, that is we can obtain a predicted output of the form \(\hat{y} = \begin{bmatrix} 1 \\1 \\ 0 \end{bmatrix}\). What if we want to add a constraint such that only one class can be predicted. Then we use the softmax function as the activation function on the output layer:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_multiclass_softmax.svg" alt="NN with Multiclass using Softmax" style="transform: translate(7vw, 0)">
</p>

<p>
Thus, instead of a probability for each class what we obtain is a probability distribution for all the classes.
</p>

<div id="Neural Networks-Architecture-Output Layer-ReLU"><h4 id="ReLU" class="header"><a href="#Neural Networks-Architecture-Output Layer-ReLU">ReLU</a></h4></div>

<p>
On linear regression we do not want the activation function to be linear, because then the whole network becomes a very large linear regression. Instead we use as an activation function the ReLU function (Rectified Linear Unit), whose output is zero if the input value is negative and linear otherwise.
</p>

<p>
<img src="https://albamr09.github.io/public/assets/relu.svg" alt="ReLu Function" style="transform: translate(14vw, 0)">
</p>

<div id="Neural Networks-Architecture-Output Layer-Loss Function"><h4 id="Loss Function" class="header"><a href="#Neural Networks-Architecture-Output Layer-Loss Function">Loss Function</a></h4></div>

<p>
The loss function when using the sigmoid function on the output layer is as follows:
</p>

\begin{align}
\mathcal{L} = - \frac{1}{Q} \sum_{k=1}^Q [y^{(k)} \log(\hat{y}^{(k)}) + (1- y^{(k)})\log(1-\hat{y}^{(k)})]
\end{align}

<p>
Where \(\hat{y}^{(k)}\) are the predicted values and \(Q\) is the total number of neurons on the output layer.
</p>

<hr>

<p>
However, if we use the softmax function as the activation function on the last layer we have to use a different derivative because this function does depend on the outputs of the other neurons. Thus, we make use of a function called cross entropy loss:
</p>

\begin{align}
\mathcal{L}_{CE} = - \sum_{k=1}^Q y^{(k)} \log(\hat{y}^{(k)})
\end{align}

<hr>

<p>
For linear regression we use as the loss function the L1-Norm or the L2-Norm. The latter is defined as follows:
</p>

\begin{align}
\mathcal{L} = || \hat{y} - y ||_2^2
\end{align}

<div id="Neural Networks-Forward Propagation"><h2 id="Forward Propagation" class="header"><a href="#Neural Networks-Forward Propagation">Forward Propagation</a></h2></div>

<p>
The forward propagation equations are the following:
</p>

\begin{align}
z^{[i]} = w^{[i]} a^{[i-1]} + b^{[i]} \tag{1}
\end{align}

<p>
Where \(i\) is the layer with \(i \geq 1\), and the first layer equals the input matrix, that is \(a^{[0]} = X\). By applying the activation function over \((1)\):
</p>

\begin{align}
a^{[i]} = g(z^{[i]})
\end{align}

<p>
Where \(g\) is the activation function (e.g \(\sigma(z^{[i]})\)).
</p>

<p>
Now, what are the shapes of these matrices?
</p>

<ul>
<li>
\(z^{[i]} \in \mathbb{R}^{S_i \times m}\)

</li><li>
\(a^{[i]} \in \mathbb{R}^{S_i \times m}\)

</li></ul>
<p>
Where \(S_i\) is the number of neurons on the ith layer and \(m\) is the number of examples. Note that the shape of the final layer changes depending on the task. So if \(K\) is the number of layers:
</p>

<ul>
<li>
In linear regression: \(\hat{y} = a^{[K]} \in \mathbb{R}^{1 \times m}\)

</li><li>
In multi-class classification: \(\hat{y} = a^{[K]} \in \mathbb{R}^{c \times m}\), where \(c\) is the number of classes.

</li></ul>
<p>
Also the shape of the weights are \(w[i] \in \mathbb{R}^{S_i \times S_{i-1}}\), that is, this matrix is compatible with the outputs of the previous layer (\(a^{[i-1]} \in \mathbb{R}^{S_{i-1} \times m}\)) and the linear part of the next layer (\(z^{[i]} \in \mathbb{R}^{S^i \times m}\)).
</p>

<p>
However, the bias are \(b^{[i]} \in \mathbb{R}^{S^i \times 1}\), therefore we cannot perform an element wise summation because the shape of \((w^{[i]} a^{[i-1]}) \in \mathbb{R}^{S_i \times m}\) and \(b^{[i]}\) are not compatible. To avoid this problem we apply a technique called broadcasting to \(b\), such that we replicate \(b^{[i]}\) \(m\) times:
</p>

\begin{align}
\hat{b}^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
b^{[i]} &amp; b^{[i]} &amp; \cdots &amp; b^{[i]} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

<hr>

<p>
To sum up, the shapes of the data and the parameters on each layer \(i\) are:
</p>

<div id="Neural Networks-Forward Propagation-Parameters"><h4 id="Parameters" class="header"><a href="#Neural Networks-Forward Propagation-Parameters">Parameters</a></h4></div>

\begin{align}
\hat{b}^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
b^{[i]} &amp; b^{[i]} &amp; \cdots &amp; b^{[i]} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

\begin{align}
w^{[i]} = \begin{bmatrix}
| &amp; | &amp; \cdots &amp; | \\
w^{[i](1)} &amp; w^{[i](2)} &amp; \cdots &amp; w^{[i](S_{i-1})} \\
| &amp; | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times S_{i-1}}
\end{align}

<div id="Neural Networks-Forward Propagation-Outputs"><h4 id="Outputs" class="header"><a href="#Neural Networks-Forward Propagation-Outputs">Outputs</a></h4></div>

<p>
Note that for each example \(j\) on layer \(i\) \(z^{[i](j)} = (w^{[i]} a^{[i-1](j)} + \hat{b}^{[i]})\), then:
</p>

\begin{align}
z^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
z^{[i](1)} &amp; z^{[i](2)} &amp; \cdots &amp; z^{[i](m)} \\
| &amp;  | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

\begin{align}
a^{[i]} = \begin{bmatrix}
| &amp;  | &amp; \cdots &amp; | \\
g(z^{[i](1)}) &amp; g(z^{[i](2)}) &amp; \cdots &amp; g(z^{[i](m)}) \\
| &amp;  | &amp; \cdots &amp; | \\
\end{bmatrix} \in \mathbb{R}^{S_i \times m}
\end{align}

<div id="Neural Networks-Forward Propagation-Graphical Representation"><h4 id="Graphical Representation" class="header"><a href="#Neural Networks-Forward Propagation-Graphical Representation">Graphical Representation</a></h4></div>

<p>
Now we present a small example of how forward propagation works on neural networks:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_forward_propagation.svg" alt="Forward Propagation Neural Network" style="width: 1000px">
</p>

<div id="Neural Networks-Optimization Problem"><h2 id="Optimization Problem" class="header"><a href="#Neural Networks-Optimization Problem">Optimization Problem</a></h2></div>

<p>
What we want to do is find the parameters \(w^{[i]}, b^{[i]}\) for each layer \(i\) that minimize the cost.
</p>

<div id="Neural Networks-Optimization Problem-Loss Function"><h3 id="Loss Function" class="header"><a href="#Neural Networks-Optimization Problem-Loss Function">Loss Function</a></h3></div>

<p>
So first of all we define a cost function for the objective \(\mathcal{L}(\hat{y}, y)\), where \(\hat{y}\) is the predicted output and \(y\) is the real output. The cost function will depend on the type of problem (classification, regression).
</p>

<div id="Neural Networks-Optimization Problem-Optimization"><h3 id="Optimization" class="header"><a href="#Neural Networks-Optimization Problem-Optimization">Optimization</a></h3></div>

<p>
The we optimize the loss function we defined by using backward propagation. For each layer \(l=1, \cdots, K\), where \(K\) is the number of layers, we apply Batch Gradient Descent (not mandatory, but here it is convenient as we can vectorize the derivatives) as follows:
</p>

\begin{align}
w^{[l]} = w^{[l]} - \alpha \frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[l]}}
\end{align}

\begin{align}
b^{[l]} = b^{[l]} - \alpha \frac{\delta \mathcal{L}(\hat{y}, y)}{\delta b^{[l]}}
\end{align}

<div id="Neural Networks-Optimization Problem-Back-propagation"><h3 id="Back-propagation" class="header"><a href="#Neural Networks-Optimization Problem-Back-propagation">Back-propagation</a></h3></div>

<p>
To compute the derivatives of the cost function with respect to \(w^{[l]}\) and \(b^{[l]}\) we use the chain rule. 
</p>

<div id="Neural Networks-Optimization Problem-Back-propagation-Output Layer"><h4 id="Output Layer" class="header"><a href="#Neural Networks-Optimization Problem-Back-propagation-Output Layer">Output Layer</a></h4></div>

<p>
Suppose we have \(K\) layers, then we start by calculating \(\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[K]}}\)  and \(\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta b^{[K]}}\), that is, the derivatives on the last layer. Thus, to update \(w^{[K]}\) (we apply the same logic for \(b^{[K]}\)):
</p>

\begin{align}
\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[K]}} = \sum_{i=1}^m \frac{\delta \mathcal{L}(\hat{y^{(i)}}, y^{(i)})}{\delta w^{[K]}} = 
\end{align}

<p>
Because \(\hat{y^{(i)}} = (a^{[K]})^{(i)}\):
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta w^{[K]}}
\end{align}

<p>
We apply the chain rule on the derivative, therefore:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta (a^{[K]})^{(i)}}{\delta w^{[K]}}
\end{align}

<p>
Because \((a^{[K]})^{(i)} = g((z^{[K]})^{(i)})\), where \(g\) is the activation function:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta g((z^{[K]})^{(i)})}{\delta w^{[K]}}
\end{align}

<p>
We apply the chain rule on the last derivative, therefore:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta g((z^{[K]})^{(i)})}{\delta (z^{[K]})^{(i)}} \frac{\delta (z^{[K]})^{(i)}}{\delta w^{[K]}}
\end{align}

<div id="Neural Networks-Optimization Problem-Back-propagation-Hidden Layers"><h4 id="Hidden Layers" class="header"><a href="#Neural Networks-Optimization Problem-Back-propagation-Hidden Layers">Hidden Layers</a></h4></div>

<p>
What about the previous layer \(K-1\)?
</p>

\begin{align}
\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[K-1]}} = \sum_{i=1}^m \frac{\delta \mathcal{L}(\hat{y^{(i)}}, y^{(i)})}{\delta w^{[K-1]}} = 
\end{align}

<p>
Because \(\hat{y^{(i)}} = (a^{[K]})^{(i)}\):
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta w^{[K-1]}}
\end{align}

<p>
We apply the chain rule on the derivative, therefore:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta (a^{[K]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<p>
Because \((a^{[K]})^{(i)} = g((z^{[K]})^{(i)})\), where \(g\) is the activation function:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta g((z^{[K]})^{(i)})}{\delta w^{[K-1]}}
\end{align}

<p>
We apply the chain rule on the last derivative, therefore:
</p>

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta g((z^{[K]})^{(i)})}{\delta (z^{[K]})^{(i)}} \frac{\delta (z^{[K]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

\begin{align}
 = \sum_{i=1}^m \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta (a^{[K]})^{(i)}}{\delta (z^{[K]})^{(i)}} \frac{\delta (z^{[K]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<p>
As you can see the first two derivatives are the same as the derivatives on the layer \(K\), let's denote \((\Delta^{[K]})^{(i)} = \frac{\delta \mathcal{L}((a^{[K]})^{(i)}, y^{(i)})}{\delta (a^{[K]})^{(i)}} \frac{\delta (a^{[K]})^{(i)}}{\delta (z^{[K]})^{(i)}}\) the accumulated gradient on layer \(K\) for example \(i\), then:
</p>

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (z^{[K]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<p>
Because \((z^{[K]})^{(i)} = w^{[K]} (a^{[K-1]})^{(i)} + b^{[k]}\):
</p>

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (w^{[K]} (a^{[K-1]})^{(i)} + b^{[k]})}{\delta w^{[K-1]}}
\end{align}

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (z^{[K-1]})^{(i)}}{\delta (a^{[K-1]})^{(i)}} \frac{\delta (a^{[K-1]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<p>
Because \((a^{[K-1]})^{(i)} = g((z^{[K-1]})^{(i)})\)
</p>

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (z^{[K-1]})^{(i)}}{\delta (a^{[K-1]})^{(i)}} \frac{\delta g((z^{[K-1]})^{(i)})}{\delta (z^{[K-1]})^{(i)}} \frac{\delta (z^{[K-1]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<p>
We apply the chain rule on the last derivative, hence:
</p>

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (z^{[K-1]})^{(i)}}{\delta (a^{[K-1]})^{(i)}} \frac{\delta g((z^{[K-1]})^{(i)})}{\delta (z^{[K-1]})^{(i)}} \frac{\delta (z^{[K-1]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

\begin{align}
 = \sum_{i=1}^m (\Delta^{[K]})^{(i)} \frac{\delta (z^{[K-1]})^{(i)}}{\delta (a^{[K-1]})^{(i)}} \frac{\delta (a^{[K-1]})^{(i)}}{\delta (z^{[K-1]})^{(i)}} \frac{\delta (z^{[K-1]})^{(i)}}{\delta w^{[K-1]}}
\end{align}

<hr>

<div id="Neural Networks-Optimization Problem-Vectorization"><h3 id="Vectorization" class="header"><a href="#Neural Networks-Optimization Problem-Vectorization">Vectorization</a></h3></div>

<div id="Neural Networks-Optimization Problem-Vectorization-Output Layer"><h4 id="Output Layer" class="header"><a href="#Neural Networks-Optimization Problem-Vectorization-Output Layer">Output Layer</a></h4></div>

<ol>
<li>
Accumulated gradient for layer \(K\): \(\Delta_w^{[K]} = \frac{\delta \mathcal{L}(\hat{y}, y)}{\delta a^{[K]}} \frac{\delta a^{[K]}}{\delta z^{[K]}}\)

</li><li>
Gradient for layer \(K\): \(\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[K]}} = \Delta_w^{[K]} \frac{\delta z^{[K]}}{\delta w^{[K]}}\)

</li></ol>
<div id="Neural Networks-Optimization Problem-Vectorization-Hidden Layer"><h4 id="Hidden Layer" class="header"><a href="#Neural Networks-Optimization Problem-Vectorization-Hidden Layer">Hidden Layer</a></h4></div>

<ol>
<li>
Accumulated gradient for layer \(K-1\): \(\Delta_w^{[K-1]} = \Delta_w^{[K]} \frac{\delta z^{[K]}}{\delta a^{[K-1]}} \frac{\delta a^{[K-1]}}{\delta z^{[K-1]}}\)

</li><li>
Gradient for layer \(K-1\): \(\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w^{[K-1]}} = \Delta_w^{[K-1]} \frac{\delta z^{[K-1]}}{\delta w^{[K-1]}}\)

</li></ol>
<div id="Neural Networks-Optimization Problem-Graphical Representation"><h3 id="Graphical Representation" class="header"><a href="#Neural Networks-Optimization Problem-Graphical Representation">Graphical Representation</a></h3></div>

<p>
On the following image we show how to obtain the gradient of the first element of the first layer's first neuron's weights \(w^{[1]}_{11}\) on the first layer:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/backwards_propagation.svg" alt="Graphical Representation">
</p>

<div id="Neural Networks-Improving a Neural Network"><h2 id="Improving a Neural Network" class="header"><a href="#Neural Networks-Improving a Neural Network">Improving a Neural Network</a></h2></div>

<div id="Neural Networks-Improving a Neural Network-Activation Functions"><h3 id="Activation Functions" class="header"><a href="#Neural Networks-Improving a Neural Network-Activation Functions">Activation Functions</a></h3></div>

<p>
Why do we need activation functions? Well, suppose you have the following network where the activation function is the identity function. That is \(a^{[i]} = g(z^{[i]}) = z^{[i]}\):
</p>

<p>
<img src="https://albamr09.github.io/public/assets/nn_no_activation_function.svg" alt="Neural Network Without Activation Function" style="transform: translate(22vw)">
</p>

<p>
Then:
</p>

\begin{align}
\hat{y} = a^{[3]} = z^{[3]} = w^{[3]} a^{[2]} + b^{[3]} = w^{[3]} z^{[2]} + b^{[3]} = w^{[3]} (w^{[2]} a^{[1]} + b^{[2]}) + b^{[3]} 
\end{align}

\begin{align}
= w^{[3]} (w^{[2]} z^{[1]} + b^{[2]}) + b^{[3]} = w^{[3]} (w^{[2]} (w^{[1]} x + b^{[1]}) + b^{[2]}) + b^{[3]}
\end{align}

\begin{align}
= w^{[3]} (w^{[2]} w^{[1]} x + w^{[2]} b^{[1]} + b^{[2]}) + b^{[3]}
\end{align}

\begin{align}
= w^{[3]} w^{[2]} w^{[1]} x + w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]}
\end{align}

<p>
If 
</p>

\begin{align}
W = w^{[3]} w^{[2]} w^{[1]}
\end{align}

\begin{align}
B = w^{[3]} w^{[2]} b^{[1]} + w^{[3]} b^{[2]} + b^{[3]}
\end{align}

<p>
Then: 
</p>

\begin{align}
\hat{y} = WX + B
\end{align}

<p>
As you can see if we do not use activation functions, it does not mater how deep your network is, it is going to be equivalent to a linear regression.
</p>

<hr>

<p>
Depending on the task at hand we use different activation functions:
</p>

<ul>
<li>
Sigmoid: \(\sigma(z) = \frac{1}{1 + e^{-z}}\), it maps \(z \in (-\infty, \infty)\) to \((0, 1)\)

<ul>
<li>
It is good for classification

</li><li>
Works well when the values are in the linear region of the function

</li><li>
However when the values are on the extremes the gradient (slope) is very small, therefore it ends up vanishing in the network.

</li></ul>
</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/activation_function_sigmoid.svg" alt="Sigmoid Activation Function">
</p>

<ul>
<li>
ReLU: \(ReLU(z) = \begin{cases}0 &amp; z \leq 0 \\ 1 &amp; z &gt; 0\end{cases}\)

</li><li>
tanh: \(tanh(z) = \frac{e^z - e^{-z}}{(e^z + e^{-z})}\)

</li></ul>
<div id="Neural Networks-Improving a Neural Network-Initialization Techniques"><h3 id="Initialization Techniques" class="header"><a href="#Neural Networks-Improving a Neural Network-Initialization Techniques">Initialization Techniques</a></h3></div>

<p>
Usually we normalize the input to avoid having saturated activation functions. To normalize:
</p>

\begin{align}
x^{(i)}_j = \frac{x^{(i)}_j - \mu_j}{\sigma_j}
\end{align}

<p>
For every example \(i\) and feature \(j\). Where:
</p>

<ul>
<li>
\(\mu_j\) is the mean of the \(j\) feature, thus: \(\mu_j = \frac{1}{m} \sum_{i=1}^m x^{(i)}_j\)

</li><li>
\(\sigma_j^2\) is the variance of the \(j\) feature, thus: \(\sigma_j^2 = \frac{1}{m} \sum_{i=1}^m (x^{(i)}_j - \mu_j)^2\)

</li></ul>
<div id="Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients"><h3 id="Vanishing/Exploding Gradients" class="header"><a href="#Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients">Vanishing/Exploding Gradients</a></h3></div>

<p>
Suppose you have the following network, where the activation function is the identity function and \(b=0\).
</p>

<p>
<img src="https://albamr09.github.io/public/assets/vanishing_exploding_gradients.svg" alt="Vanishing and Exploding Gradients" style="transform: translate(22vw, 0); margin: 5px">
</p>

<p>
Then \(\hat{y} = w^{[L]} a^{[L-1]} = w^{[L]} w^{[L-1]} a^{[L-2]} = \cdots = w^{[L]} w^{[L-1]} \cdots w^{[1]} x\)
</p>

<p>
Therefore, if:
</p>

\begin{align}
w^{[L]} = \begin{bmatrix}
1.5 &amp; 0 \\
0 &amp; 1.5 \\
\end{bmatrix}
\end{align}

<p>
then: 
</p>

\begin{align}
\hat{y} = \begin{bmatrix}
1.5^L &amp; 0 \\
0 &amp; 1.5^L \\
\end{bmatrix}
\end{align}

<p>
Which means we end up with an exploding gradient. The inverse happens when:
</p>

\begin{align}
w^{[L]} = \begin{bmatrix}
0.5 &amp; 0 \\
0 &amp; 0.5 \\
\end{bmatrix}
\end{align}

<p>
then:
</p>

\begin{align}
\hat{y} = \begin{bmatrix}
0.5^L &amp; 0 \\
0 &amp; 0.5^L \\
\end{bmatrix}
\end{align}

<p>
Which results in a vanishing gradient.
</p>

<p>
To avoid this somewhat, we need to initialize the weights properly. What we want is for the weights to be very close to one to avoid the exploding/diminishing problem.
</p>

<div id="Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients-Intuition"><h4 id="Intuition" class="header"><a href="#Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients-Intuition">Intuition</a></h4></div>

<p>
Given a single neuron:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/initialize_weights.svg" alt="Weight Initialization" style="transform: translate(22vw, 0)">
</p>

<p>
Then \(a = g(z)\) and \(z = w_1 x_1 + \cdots + w_n x_n\). We can see that \(z\) will increase if \(n\) increases, therefore we would want \(w_i\) to be as small as \(n\) is large, that is:
</p>

\begin{align}
w_i = \frac{1}{n}
\end{align}

<div id="Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients-Initialization Techniques"><h4 id="Initialization Techniques" class="header"><a href="#Neural Networks-Improving a Neural Network-Vanishing/Exploding Gradients-Initialization Techniques">Initialization Techniques</a></h4></div>

<ul>
<li>
If we want the value of \(w^{[L]}\) to be proportional to the number of inputs coming from the layer \(L\) (\(n^{[L-1]}\)). It works very well for sigmoid activation:
<pre python="">w[k] = np.random.randn(shape)*np.sqrt(1/n[L-1])
</pre>

</li><li>
For ReLU:

</li></ul>
 
<pre python="">w[k] = np.random.randn(shape)*np.sqrt(2/n[L-1])
</pre>

<ul>
<li>
Xavier initialization (used with tanh): \(w^{[L]} \sim \sqrt{\frac{1}{n^{[L-1]}}}\)

</li><li>
Her initialization: \(w^{[L]} \sim \sqrt{\frac{2}{n^{[L]} + n^{[L-1]}}}\)

</li></ul>
<p>
Also you need to initialize the weights randomly, else you will run into the symmetry problem, where all neurons learn the same thing (that is they update very similarly).
</p>

<div id="Neural Networks-Improving a Neural Network-Optimization"><h3 id="Optimization" class="header"><a href="#Neural Networks-Improving a Neural Network-Optimization">Optimization</a></h3></div>

<div id="Neural Networks-Improving a Neural Network-Optimization-Mini Batch Gradient Descent"><h4 id="Mini Batch Gradient Descent" class="header"><a href="#Neural Networks-Improving a Neural Network-Optimization-Mini Batch Gradient Descent">Mini Batch Gradient Descent</a></h4></div>

<p>
Mini Batch Gradient Descent is a trade off between batch gradient descent and stochastic gradient descent. Also, because Mini Batch Gradient Descent is an approximation it introduces some noise on the loss function:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/mini_batch_vs_batch.png" alt="Mini Batch Gradient Descent VS Batch Gradient Descent" style="transform: translate(17vw, 0)">
</p>

<p>
However Mini Batch Gradient Descent is more used because Batch Gradient Descent can be very computationally expensive.
</p>

<div id="Neural Networks-Improving a Neural Network-Optimization-Momentum Algorithm"><h4 id="Momentum Algorithm" class="header"><a href="#Neural Networks-Improving a Neural Network-Optimization-Momentum Algorithm">Momentum Algorithm</a></h4></div>

<p>
This algorithm combines Gradient Descent and momentum. Suppose you have the following contour plot, where the horizontal axis is much more extended that the vertical axis. By default on Gradient Descent the gradient of the loss will be orthogonal to the contour at the given point, as we can see:
</p>

<p>
<img src="https://albamr09.github.io/public/assets/gradient_descent.svg" alt="Gradient Descent" style="transform: translate(22vw, 0)">
</p>

<p>
However, what we would like, so it would converge faster, is to make it move more horizontally than vertically. In order to do that we use a technique called momentum. It takes intro account past updates to find the right way to go. If you take an average of past updates, then:
</p>

<ul>
<li>
Vertical axis: it practically cancels itself because it oscillates a lot

</li><li>
Horizontal axis: its value it's maintained because the past and present gradients go in the same direction

</li></ul>
<p>
<img src="https://albamr09.github.io/public/assets/momentum_gradient_descent.svg" alt="Gradient Descent wiht Momentum" style="transform: translate(22vw, 0)">
</p>

<p>
To update the weights we apply the following equation:
</p>

\begin{align}
\upsilon = \beta \upsilon + (1 - \beta) \frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w}
\end{align}

<p>
Where:
</p>

<ul>
<li>
\(\upsilon\): stores past updates

</li><li>
\(\frac{\delta \mathcal{L}(\hat{y}, y)}{\delta w}\): stores the current update

</li><li>
We average with \(\beta\) and \((1 - \beta)\)

</li></ul>
<p>
Finally we update the weights:
</p>

\begin{align}
w = w - \alpha \upsilon
\end{align}

<div id="Neural Networks-Anexo"><h2 id="Anexo" class="header"><a href="#Neural Networks-Anexo">Anexo</a></h2></div>

<p>
For more info about cost function and how to derive them:
</p>

<p>
<a href="Anexo/index.html">Anexo</a>
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>