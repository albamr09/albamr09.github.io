<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Information criteria and cross-validation</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Information criteria and cross-validation"><h1 id="Information criteria and cross-validation" class="header"><a href="#Information criteria and cross-validation">Information criteria and cross-validation</a></h1></div>

<hr>

<p>
For historical reasons, measures of predictive accuracy are referred to as <span id="Information criteria and cross-validation-information criteria"></span><strong id="information criteria">information criteria</strong>. These criteria are usually based on something called deviance that it's calculated by taking the negative two times the logarithm of the likelihood of the data given a point estimate of the model, \(-2 \log p(y|\hat{\theta}))\).
</p>

<p>
When we make predictions about new data that the model hasn't seen before, these predictions are usually less accurate than what we'd expect based on how well the model fits the data it was trained on. even if the family of models being fit happens to include the true data-generating process, and even if the parameters in the model happen to be sampled exactly from the specified prior distribution.
</p>

<p>
We are interested in prediction accuracy for two reasons: 
</p>

<ol>
<li>
To measure the performance of a model.

</li><li>
To compare models.

</li></ol>
<p>
When comparing models with the same number of parameters, we can directly compare their best-fit log predictive densities. But when comparing models of different sizes or complexities we need to adjust for the fact that larger models can sometimes fit data better just by chance.
</p>

<div id="Information criteria and cross-validation-Estimating out-of-sample predictive accuracy using available data"><h2 id="Estimating out-of-sample predictive accuracy using available data" class="header"><a href="#Information criteria and cross-validation-Estimating out-of-sample predictive accuracy using available data">Estimating out-of-sample predictive accuracy using available data</a></h2></div>

<p>
Several methods are available to estimate the expected predictive accuracy without waiting for out-of-sample data.
</p>

<ul>
<li>
<span id="Information criteria and cross-validation-Estimating out-of-sample predictive accuracy using available data-Within-sample predictive accuracy"></span><strong id="Within-sample predictive accuracy">Within-sample predictive accuracy</strong>: A naive estimate of the expected log predictive density for new data is the log predictive density for existing data using the computed lppd. This summary is in general an overestimate of elppd because it is evaluated on the data from which the model was fit.

</li><li>
<span id="Information criteria and cross-validation-Estimating out-of-sample predictive accuracy using available data-Adjusted within-sample predictive accuracy"></span><strong id="Adjusted within-sample predictive accuracy">Adjusted within-sample predictive accuracy</strong>: Given that lppd is a biased estimate of elppd, the next logical step is to correct that bias. Formulas such as AIC, DIC, and WAIC (all discussed below) give approximately unbiased estimates of elppd.

</li><li>
<span id="Information criteria and cross-validation-Estimating out-of-sample predictive accuracy using available data-Cross-validation"></span><strong id="Cross-validation">Cross-validation</strong>: One can attempt to capture out-of-sample prediction error by fitting the model to training data and then evaluating this predictive accuracy on a holdout set. Cross-validation can be computationally expensive.

</li></ul>
<div id="Information criteria and cross-validation-Akaike information criterion (AIC)"><h2 id="Akaike information criterion (AIC)" class="header"><a href="#Information criteria and cross-validation-Akaike information criterion (AIC)">Akaike information criterion (AIC)</a></h2></div>

<p>
In much of the statistical literature on predictive accuracy, inference for \(\theta\) is summarized not by a posterior distribution \(p_{\text{post}}\) but by a point estimate \(\hat{\theta}\), typically the maximum
likelihood estimate. Out-of-sample predictive accuracy is then defined not by the expected log posterior predictive density (elppd) but by \(elpd_{\hat{\theta}} = \mathbb{E}_f[\log p(\tilde{y}|\tilde{\theta}(y))]\).
</p>

<p>
Let \(k\) be the number of parameters estimated in the model. AIC is defined as follows:
</p>

\begin{align}
\hat{\text{elpd}}_{\text{AIC}} = -2 \log p(y|\hat{\theta}_{\text{mle}}) + 2k
\end{align}

<p>
Subtracting \(k\) from the log predictive density given the maximum likelihood estimate \(\theta_{\text{mle}}\) is a correction to account for how much the fitting of \(k\) parameters will increase predictive accuracy, purely by chance.
</p>

<p>
When we move beyond linear models with simple priors, just adding the number of fitted parameters \(k\) to adjust the deviance isn't accurate. Informative priors and hierarchical structures typically decrease overfitting compared to simple estimation methods like least squares or maximum likelihood. In models with informative priors or hierarchical setups, the actual number of parameters depends heavily on the variance of the group-level parameter.
</p>

<div id="Information criteria and cross-validation-Deviance Information Criterion (DIC) and Effective Number of Parameters"><h2 id="Deviance Information Criterion (DIC) and Effective Number of Parameters" class="header"><a href="#Information criteria and cross-validation-Deviance Information Criterion (DIC) and Effective Number of Parameters">Deviance Information Criterion (DIC) and Effective Number of Parameters</a></h2></div>

<p>
DIC is a somewhat Bayesian version of AIC making two changes, replacing the maximum likelihood estimate with the posterior mean \(\hat{\theta}_{\text{Bayes}} = \mathbb{E}[\theta|y]\) and replacing \(k\) with a data-based bias correction. The new measure of predictive accuracy is:
</p>

\begin{align}
\hat{\text{elpd}}_{DIC} = \log p(y|\hat{\theta}_{\text{Bayes}}) - p_{\text{DIC}}
\end{align}

<p>
where \(p_{\text{DIC}}\) is the effective number of parameters, defined as:
</p>

\begin{align}
p_{\text{DIC}} = 2 \left(\log p(y|\hat{\theta}_{\text{Bayes}}) - \mathbb{E}_{post}[\log p(y|\theta)]\right)
\end{align}

<p>
where \(\mathbb{E}_{post}[\log p(y|\theta)]\) is an average of Î¸ over its posterior distribution. This is computed using simulation \(\theta^s, s= 1, \cdots, S\) as:
</p>

\begin{align}
\text{computed} p_{\text{DIC}} = 2 \left(\log p(y|\hat{\theta}_{\text{Bayes}}) - \frac{1}{S} \sum_{s=1}^S \log p(y|\theta^s)\right)
\end{align}

<p>
When the average value of \(\theta\) in the posterior distribution matches the highest point (mode), it leads to the maximum log predictive density.  However, if the average value is significantly different from the mode, it can result in a negative value for \(p_{\text{DIC}}\).
</p>

<p>
An alternative version of DIC uses a slightly different effective number of parameters:
</p>

\begin{align}
p_{\text{DIC}}_{\text{alt}} = 2 \mathbb{V}_{\text{post}}[\log p(y|\theta)]
\end{align}

<p>
Of these two measures, \(p_{\text{DIC}}\) is more numerically stable but \(p_{\text{DIC}_{\text{alt}}}\) has the advantage of always being positive. The actual quantity called DIC is defined in terms of the deviance rather than the log predictive density; thus:
</p>

\begin{align}
\text{DIC} = -2 \log p(y|\hat{\theta}_{\text{Bayes}}) + 2p_{DIC}
\end{align}

<div id="Information criteria and cross-validation-Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)"><h2 id="Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)" class="header"><a href="#Information criteria and cross-validation-Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)">Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)</a></h2></div>

<p>
<span id="Information criteria and cross-validation-Watanabe-Akaike or Widely Applicable Information Criterion (WAIC)-WAIC"></span><strong id="WAIC">WAIC</strong> is a more fully Bayesian approach for estimating the out-of-sample expectation. Starting with the computed lppd and then adding a correction for effective number of parameters to adjust for overfitting. Two adjustments have been proposed:
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>