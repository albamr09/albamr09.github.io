<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Model Comparison Using Bayes Factors</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Model Comparison Using Bayes Factors"><h1 id="Model Comparison Using Bayes Factors" class="header"><a href="#Model Comparison Using Bayes Factors">Model Comparison Using Bayes Factors</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="04_model_comparison_using_bayes_factors.html#Example: A discrete example in which Bayes factors are helpful">Example: A discrete example in which Bayes factors are helpful</a>

</li><li>
<a href="04_model_comparison_using_bayes_factors.html#Example. A continuous example where Bayes factors are a distraction">Example. A continuous example where Bayes factors are a distraction</a>

</li></ul>
<hr>

<p>
In this chapter, we've been talking about how to evaluate and compare models by looking at how well they predict outcomes. Another way to compare models is using Bayesian analysis, where we use something called the Bayes factor. So, if we have two models, let's call them \(H_1\) and \(H_2\), the Bayes factor is just the ratio of how probable they are after we've looked at the data:
</p>

\begin{align}
\frac{p(H_2|y)}{p(H_1|y)} = \frac{p(H_2)}{p(H_1)} \times \text{ Bayes factor }(H_2;H_1)
\end{align}

<p>
where
</p>

\begin{align}
\text{ Bayes Factor }(H_1; H_2) = \frac{p(y|H_2)}{p(y|H_2)} = \frac{\int p(\theta_2|H_2) p(y|\theta_2, H_2) d\theta_2}{\int p(\theta_1|H_1) p(y|\theta_1, H_1) d\theta_1}
\end{align}

<p>
The Bayes factor is only defined when the marginal density of \(y\) under each model is proper.
</p>

<p>
This Bayesian approach might sound good, but we usually don't recommend it. That's because the likelihood of the data given the model can be strongly influenced by parts of the model that are chosen somewhat arbitrarily and can't be tested with data.
</p>

<p>
Bayes factors can work well when the underlying model is truly discrete and for which it makes sense to consider one or the other model as being a good description of the data. We illustrate with an example from genetics.
</p>

<div id="Model Comparison Using Bayes Factors-Example: A discrete example in which Bayes factors are helpful"><h2 id="Example: A discrete example in which Bayes factors are helpful" class="header"><a href="#Model Comparison Using Bayes Factors-Example: A discrete example in which Bayes factors are helpful">Example: A discrete example in which Bayes factors are helpful</a></h2></div>

<p>
In the genetics example we talked about earlier, we can use Bayes factors to help us make sense of things. Imagine we have two possible scenarios: \(H_1\), where the woman is affected, and \(H_2\), where she's not affected. We can represent these scenarios using some numbers. For example, let's say \(\theta = 1\) means she's affected, and \(\theta = 0\) means she's not.
</p>

<p>
Now, let's say before we look at any data, we're equally likely to believe either scenario. So, the odds of H2 compared to H1 are 1 to 1, that is \(\frac{p(H_2)}{p(H_1)} = 1\).
</p>

<p>
Then, when we look at the data and find out the woman has two unaffected sons, the data is \(4\) times more likely under \(H_2\) than under \(H_1\). That is \(\frac{p(y|H_2)}{p(y|H_1)} = \frac{1.0}{0.25}\). The posterior odds are thus \(\frac{p(H_2|y)}{p(H_1|y)} = 4\)
</p>

<p>
This example is helpful for Bayes factors because the scenarios we're comparing make sense scientifically, and there are no other possible scenarios in between. Also, the way the data fits with each scenario makes sense and gives us clear results.
</p>

<p>
Bayes factors don't work as well for models that are continuous. For instance, if we're looking at something like the effectiveness of a treatment, which can vary along a scale, it doesn't make sense to assign a probability to it being exactly zero.
</p>

<p>
Similarly, if we're comparing different models in regression, like deciding which variables to include, it's better to have all the possible variables in our consideration. We can then use a prior distribution to decide how much to trust each variable, even if we think some might not have much impact. To show why Bayes factors struggle with continuous models, let's consider the example of the 8 schools problem, comparing the no-pooling and complete-pooling models.
</p>

<div id="Model Comparison Using Bayes Factors-Example. A continuous example where Bayes factors are a distraction"><h2 id="Example. A continuous example where Bayes factors are a distraction" class="header"><a href="#Model Comparison Using Bayes Factors-Example. A continuous example where Bayes factors are a distraction">Example. A continuous example where Bayes factors are a distraction</a></h2></div>

<p>
Suppose we had analyzed the data from the 8 schools using Bayes factors for the discrete collection of previously proposed standard models, no pooling (\(H_1\)) and complete pooling (\(H_2\)):
</p>

\begin{align}
H_1: p(y|\theta_1, \cdots, \theta_J) = \prod_{j=1}^J text{N}(y_j|\theta_j, \sigma_j^2), p(\theta_1, \cdots, \theta_J) \propto 1
\end{align}

\begin{align}
H_2: p(y|\theta_1, \cdots, \theta_J) = \prod_{j=1}^J text{N}(y_j|\theta_j, \sigma_j^2), \theta_1 = \cdots = \theta_J = \theta \propto 1 
\end{align}

<p>
If we try to use Bayes factors to pick or combine these models, we run into a problem. The Bayes factor, which is the ratio of how likely the data is under one model compared to another, isn't defined here. That's because the prior distributions we're using are improper, which means they don't behave properly in the calculations. Specifically, when we try to divide one function by another, we end up with \(\frac{0}{0}\), which doesn't give us a clear answer.
</p>

<p>
So, if we want to stick with the idea of assigning probabilities to these two specific models, we have two options: either use proper prior distributions or carefully construct improper ones in a way that makes sense. However, no matter which route we take, the results won't be very satisfying.
</p>

<p>
More explicitly, suppose we replace the flat prior distributions in \(H_1\) and \(H_2\) by independent normal prior distributions, \(\text{N}(0, A^2)\), for some large \(A\). The resulting posterior distribution for the effect in school \(j\) is:
</p>

\begin{align}
p(\theta_j|y) = (1 - \lambda)p(\theta_j|y, H_1) + \lambda p(\theta_j|y, H_2)
\end{align}

<p>
The Bayes factor, which compares how likely the data is under different models, is very sensitive to the prior variance, which is represented by \(A^2\). As we increase \(A\) (while keeping the data and prior odds fixed), the results tend to favor one model over the other more strongly. This means that Bayes factors can't be reliably used with non-informative prior densities, even if we carefully define them in certain ways.
</p>

<p>
Another problem with Bayes factors in this example is that they behave differently as we change the number of schools in the model. The results can vary significantly depending on how many schools are included, which doesn't make much sense from a scientific perspective.
</p>

<p>
So, if we were to use Bayes factors here, we'd likely run into issues during the model-checking stage, where we compare the model's predictions to what we know from real-world knowledge. Instead, it might be better to use a smoother, continuous family of models that bridges the gap between the extreme models. This continuous model doesn't assign discrete probabilities to extreme values that don't make scientific sense.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>