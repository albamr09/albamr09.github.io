<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Hierarchical Normal Modeling</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Hierarchical Normal Modeling"><h1 id="Hierarchical Normal Modeling" class="header"><a href="#Hierarchical Normal Modeling">Hierarchical Normal Modeling</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="12_hierarchical_normal_modeling.html#Example%3A%20ratings%20of%20animation%20movies">Example  ratings of animation movies</a>

</li><li>
[[#A hierarchical Normal model with random \(\sigma\)|A hierarchical Normal model with random \(\sigma\)]]

<ul>
<li>
<a href="12_hierarchical_normal_modeling.html#Graphical%20representation%20of%20the%20hierarchical%20model">Graphical representation of the hierarchical model</a>

</li><li>
<a href="12_hierarchical_normal_modeling.html#Second-stage%20prior">Second-stage prior</a>

</li></ul>
</li><li>
<a href="12_hierarchical_normal_modeling.html#Inference%20through%20MCMC">Inference through MCMC</a>

<ul>
<li>
<a href="12_hierarchical_normal_modeling.html#Describe%20the%20model%20by%20a%20script">Describe the model by a script</a>

</li><li>
<a href="12_hierarchical_normal_modeling.html#Define%20the%20data%20and%20prior%20parameters">Define the data and prior parameters</a>

</li><li>
<a href="12_hierarchical_normal_modeling.html#MCMC%20diagnostics%20and%20summarization">MCMC diagnostics and summarization</a>

</li><li>
<a href="12_hierarchical_normal_modeling.html#Shrinkage">Shrinkage</a>

</li><li>
<a href="12_hierarchical_normal_modeling.html#Sources%20of%20variability">Sources of variability</a>

</li></ul>
</li></ul>
<hr>

<div id="Hierarchical Normal Modeling-Example: ratings of animation movies"><h2 id="Example: ratings of animation movies" class="header"><a href="#Hierarchical Normal Modeling-Example: ratings of animation movies">Example: ratings of animation movies</a></h2></div>

<p>
MovieLens is a website which provides personalized movie recommendations from users who create accounts and rate movies that they have seen. Based on such information, MovieLens works to build a custom preference profile for each user and provide movie recommendations.
</p>

<div id="Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$"><h2 id="A hierarchical Normal model with random $\sigma$" class="header"><a href="#Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$">A hierarchical Normal model with random \(\sigma\)</a></h2></div>

<p>
In this situation it is reasonable to develop a model for the movie ratings where the grouping variable is the movie title. We index a rating by two subscripts, where \(Y_{ij}\) denotes the \(i\)th rating for the \(j\)th movie title, with \(j = 1, \cdots, 8\).
</p>

<p>
Since the ratings are continuous, it is reasonable to use the Normal data model. For simplicity and ease of illustration, a common and shared unknown standard deviation \(\sigma\) is assumed for all Normal models (however it could also be modeled). Therefore we define the sampling distribution as:
</p>

\begin{align}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{align}

<p>
Since these movies are all animations, it is reasonable to believe that the mean ratings are similar across movies. So one assigns each mean rating the same Normal prior distribution at the first stage:
</p>

\begin{align}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{align}

<p>
The hyperparameters \(\mu\) and  \(\tau\) are treated as random since we are unsure about the degree of pooling of the eight sets of ratings. After observing data, inference is performed about \(\mu\) and \(\tau\) based on their posterior distributions.
</p>

<p>
Treating \(\mu\) and \(\tau\) as random, one arrives at the following hierarchical model:
</p>

<ul>
<li>
<span id="Hierarchical Normal Modeling-A hierarchical Normal model with random \(\sigma\)-Sampling"></span><strong id="Sampling">Sampling</strong> for \(j = 1, \cdots, 8\) and \(i = 1, \cdots, n_j\):

</li></ul>
 
\begin{align}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{align}
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-A hierarchical Normal model with random \(\sigma\)-Prior"></span><strong id="Prior">Prior</strong> for \(\mu_j\), Stage 1, \(j = 1, \cdots, 8\):
\begin{align}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{align}

</li></ul>
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-A hierarchical Normal model with random \(\sigma\)-Prior"></span><strong id="Prior">Prior</strong> for \(\mu_j\), Stage 2, the hyperprior:
\begin{align}
\mu, \tau \sim \pi(\mu, \tau)
\end{align}

</li></ul>
<p>
To complete the model, one needs to specify a prior distribution for the standard deviation parameter, \(\sigma\):
</p>

\begin{align}
\frac{1}{\sigma^2} | a_{\sigma}, b_{\sigma} \sim \text{Gamma}(a_{\sigma}, b_{\sigma})
\end{align}

<p>
One assigns a known Gamma prior distribution for \(\frac{1}{\sigma^2}\), with fixed hyperparameter values \(a_{\sigma}\) and \(b_{\sigma}\). In some situations, one may consider the situation where \(a_{\sigma}\) and \(b_{\sigma}\) are random and assign hyperprior distributions for these unknown hyperparameters.
</p>

<p>
It is helpful to contrast the two-stage prior distribution for \(\{\mu_j\}\) and the one-stage prior distribution for \(\sigma\).
</p>

<p>
For the means \(\{\mu_j\}\), we have discussed that specifying a common prior distribution for different \(j\) pools information across the movies. One is simultaneously estimating both a mean for each movie (the \(\mu_j\)'s) and the variation among the movies (\(\mu\) and \(\tau\)). For the standard deviation, the hierarchical model also pools information across movies. However, all of the observations are combined in the estimation of \(\sigma\). Since separate values of \(\sigma_j\), one cannot learn about the differences and similarities among the \(\sigma_j\)'s.
</p>
 
<div id="Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$-Graphical representation of the hierarchical model"><h3 id="Graphical representation of the hierarchical model" class="header"><a href="#Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$-Graphical representation of the hierarchical model">Graphical representation of the hierarchical model</a></h3></div>

<p>
An alternative way of expressing this hierarchical model uses the following graphical representation.
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/MBJ/T2/normal_hierarchical_model.png" alt="Normal Hierarchical Model" style="width:300px;height:150px">
</p>

<p>
In the middle section of the graph, \(Y_{ij}\) represents the collection of random variables for all ratings of movie \(j\). The upper section of the graph focuses on the \(\mu_j\)'s. All means follow the same prior, a Normal distribution with mean \(\mu\) and standard deviation \(\sigma\).
</p>

<p>
Since \(\mu\) and \(\tau\) are random, these second-stage parameters are associated with the prior label \(\pi(\mu, \tau)\).
</p>

<div id="Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$-Second-stage prior"><h3 id="Second-stage prior" class="header"><a href="#Hierarchical Normal Modeling-A hierarchical Normal model with random $\sigma$-Second-stage prior">Second-stage prior</a></h3></div>

<p>
The hierarchical Normal model presented in Equations (10.6) through (10.9) has not specified the hyperprior distribution \(\pi(\mu, \tau)\). How does one construct a prior on these second-stage hyperparameters?
</p>

<p>
A typical approach for Normal models is to assign two independent prior distributions — a Normal distribution for the mean \(\mu\) and a Gamma distribution for the precision \(\frac{1}{\tau^2}\). Such a specification facilitates the use of the Gibbs sampling. Using this approach, the density \(\pi(\mu, \tau)\) is replaced by the two hyperprior distributions below:
</p>

\begin{align}
\mu | \mu_0, \gamma_0 \sim \text{Normal}(\mu_0, \gamma_0)
\end{align}

\begin{align}
\frac{1}{\tau^2} | a, b \sim \text{Gamma}(a_{\tau}, b_{\tau})
\end{align}

<p>
The task of choosing a prior for \((\mu, \tau)\) reduces to the problem of choosing values for the four hyperparameters \(\mu_0, \gamma_0, a_{\tau}\) and \(b_{\tau}\). If one believes that \(mu\) is located around the value of \(3\) and she is not very confident of this choice, the set of values \(\mu_0 = 3\) and \(\gamma_0 = 1\) could be chosen. As for \(\tau\), one chooses a weakly informative prior with \(a_{\tau} = b_{\tau} = 1\) as \(\text{Gamma}(1, 1)\). Moreover, to choose a prior for \(\sigma\), let \(a_{\sigma} = b_{\sigma} = 1\) to have the weakly informative \(\text{Gamma}(1, 1)\) prior.
</p>

<div id="Hierarchical Normal Modeling-Inference through MCMC"><h2 id="Inference through MCMC" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC">Inference through MCMC</a></h2></div>

<p>
With the specification of the prior, the complete hierarchical model is described as follows:
</p>
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Sampling"></span><strong id="Sampling">Sampling</strong> for \(j = 1, \cdots, 8\) and \(i = 1, \cdots, n_j\):

</li></ul>
 
\begin{align}
Y_{ij} | \mu_j, \sigma \sim \text{Normal}(\mu_j, \sigma)
\end{align}
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Prior"></span><strong id="Prior">Prior</strong> for \(\mu_j\), Stage 1, \(j = 1, \cdots, 8\):
\begin{align}
\mu_j | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{align}

</li></ul>
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Prior"></span><strong id="Prior">Prior</strong> for \(\mu_j\), Stage 2: the hyperpriors:
\begin{align}
\mu \sim \text{Normal}(3, 1)
\end{align}
\begin{align}
\frac{1}{\tau^2} \sim \text{Gamma}(1, 1)
\end{align}

</li><li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Prior"></span><strong id="Prior">Prior</strong> for \(\sigma\)
\begin{align}
\frac{1}{\sigma^2} \sim \text{Gamma}(1, 1)
\end{align}

</li></ul>
<div id="Hierarchical Normal Modeling-Inference through MCMC-Describe the model by a script"><h3 id="Describe the model by a script" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC-Describe the model by a script">Describe the model by a script</a></h3></div>

<p>
The first step in using the <a href="https://mcmc-jags.sourceforge.io/">JAGS</a> software is to write the following script defining the hierarchical model. The model is saved in the character string modelString.
</p>

<pre r="">modelString &lt;-"
model {
## sampling
for (i in 1:N){
   y[i] ~ dnorm(mu_j[MovieIndex[i]], invsigma2)
}
## priors
for (j in 1:J){
   mu_j[j] ~ dnorm(mu, invtau2)
}
invsigma2 ~ dgamma(a_s, b_s)
sigma &lt;- sqrt(pow(invsigma2, -1))
## hyperpriors
mu ~ dnorm(mu0, g0)
invtau2 ~ dgamma(a_t, b_t)
tau &lt;- sqrt(pow(invtau2, -1))
}
"
</pre>

<p>
In the sampling part of the script, note that the loop goes from <code>1</code> to <code>N</code>, where <code>N</code> is the number of observations with index <code>i</code>. However, because now <code>N</code> observations are grouped according to movies, indicated by <code>j</code>, one needs to create one vector, <code>mu_j</code> of length eight, and use <code>MovieIndex[i]</code> to grab the corresponding <code>mu_j</code> based on the movie index.
</p>

<p>
In the priors part of the script, the loop goes from <code>1</code> to <code>J</code>, and <code>J = 8</code> in the current example. Inside the loop, the first line corresponds to the prior distribution for <code>mu_j</code>. Due to a commonly shared <code>sigma</code>, <code>invsigma2</code> follows <code>dgamma(a_g, b_g)</code> outside of the loop. In addition, <code>sigma &lt;- sqrt(pow(invsigma2, -1))</code> is added to help tracksigma directly.
</p>

<p>
Finally in the hyperpriors section of the script, one specifies the Normal hyperprior for <code>mu</code>, a Gamma hyperprior for <code>invtau2</code>. Keep in mind that the arguments in the <code>dnorm</code> in JAGS are the mean and the precision (std). If one is interested instead in the standard deviation parameter <code>tau</code>, one could return it in the script by using <code>tau &lt;- sqrt(pow(invtau2, -1))</code>, enabling the tracking of its MCMC chain in the posterior inferences.
</p>

<div id="Hierarchical Normal Modeling-Inference through MCMC-Define the data and prior parameters"><h3 id="Define the data and prior parameters" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC-Define the data and prior parameters">Define the data and prior parameters</a></h3></div>

<p>
After one has defined the model script, the next step is to provide the data and values for parameters of the prior. 
</p>

<p>
In the R script below, a list <code>the_data</code> contains the vector of observations, the vector of movie indices, the number of observations, and the number of movies. It also contains the Normal hyperparameters <code>mu0</code> and <code>g0</code>, and two sets of Gamma hyperparameters (<code>a_t</code> and <code>b_t</code>) for <code>invtau2</code>, and (<code>a_s</code> and <code>b_s</code>) for <code>invsigma2</code>.
</p>

<pre r="">y &lt;- MovieRatings$rating      
MovieIndex &lt;- MovieRatings$Group_Number      
N &lt;- length(y)  
J &lt;- length(unique(MovieIndex)) 
the_data &lt;- list("y" = y, "MovieIndex" = MovieIndex, 
                 "N" = N, "J" = J,
                 "mu0" = 3, "g0" = 1,
                 "a_t" = 1, "b_t" = 1,
                 "a_s" = 1, "b_s" = 1)
</pre>

<p>
One uses the <code>run.jags()</code> function in the runjags R package to generate posterior samples by using the MCMC algorithms in JAGS.
</p>

<p>
The script below runs one MCMC chain with \(1000\) iterations in the adapt period (preparing for MCMC), \(5000\) iterations of burn-in and an additional set of \(5000\) iterations to be run and collected for inference. By using <code>monitor = c("mu", "tau", "mu_j", "sigma")</code>, one collects the values of all parameters in the model. In the end, the output variable <code>posterior</code> contains a matrix of simulated draws.
</p>

<pre r="">posterior &lt;- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("mu", "tau", "mu_j", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000)
</pre>

<div id="Hierarchical Normal Modeling-Inference through MCMC-MCMC diagnostics and summarization"><h3 id="MCMC diagnostics and summarization" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC-MCMC diagnostics and summarization">MCMC diagnostics and summarization</a></h3></div>

<p>
To perform some MCMC diagnostics in our example, one uses the <code>plot()</code> function, specifying the variable to be checked by the vars argument. For example, the script below returns four diagnostic plots (trace plot, empirical PDF, histogram, and autocorrelation plot) for the hyperparameter \(\tau\).
</p>

<pre r="">plot(posterior, vars = "tau")
</pre>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/MBJ/T2/tau_posterior_diagnostics.png" alt="Tau Posterior Diagnostics" style="width:500px;height:350px">
</p>

<p>
In practice MCMC diagnostics should be performed for all parameters to justify the overall MCMC convergence. In our example, the above diagnostics should be implemented for each of the eleven parameters in the model: \(\mu, \tau, \mu_1, \cdots, \mu_8\) and \(\sigma\).
</p>

<p>
Once diagnostics are done, one reports posterior summaries of the parameters using <code>print()</code>. Note that these summaries are based on the 5000 iterations from the sample period, excluding the adapt and burn-in iterations.
</p>

<pre r="">print(posterior, digits = 3)                                                                                     
        Lower95 Median Upper95  Mean     SD Mode   MCerr 
mu         3.19   3.78    4.34  3.77  0.286   -- 0.00542     
tau       0.357  0.638    1.08 0.677    0.2   -- 0.00365  
mu_j[1]    2.96   3.47    3.99  3.47  0.262   -- 0.00376  
mu_j[2]    3.38   3.81    4.25  3.82  0.221   -- 0.00313    
mu_j[3]    3.07   3.91    4.75  3.91  0.425   -- 0.00677    
mu_j[4]    3.21   3.74    4.31  3.74  0.285   -- 0.00428 
mu_j[5]    3.09   4.15    5.43  4.18  0.588   --  0.0115   
mu_j[6]     2.7   3.84    4.99  3.85  0.576   -- 0.00915   
mu_j[7]    2.74   3.53    4.27  3.51  0.388   -- 0.00595  
mu_j[8]    3.58   4.12    4.66  4.12  0.276   -- 0.00423  
sigma     0.763   0.92    1.12  0.93 0.0923   -- 0.00142 
</pre>

<p>
For example, the movies "How to Train Your Dragon" (corresponding to \(\mu_1\)) and "Megamind" (corresponding to \(\mu_7\)) have the lowest average ratings with short \(90\%\) credible intervals, \((2.96, 3.99)\) and \((2.74, 4.27)\) respectively, whereas "Legend of the Guardians: The Owls of Ga’Hoole" (corresponding to \(μ_6\)) also has a low average rating but with a wider \(90\%\) credible interval \((2.70, 4.99)\). The differences in the width of the credible intervals stem from the sample sizes: there are eleven ratings for "How to Train Your Dragon", four ratings for "Megamind", and only a single rating for "Legend of the Guardians: The Owls of Ga’Hoole". The smaller the sample size, the larger the variability in the inference, even if one pools information across groups.
</p>

<div id="Hierarchical Normal Modeling-Inference through MCMC-Shrinkage"><h3 id="Shrinkage" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC-Shrinkage">Shrinkage</a></h3></div>

<p>
Recall that the two-stage prior specifies a shared prior Normal \((\mu, \tau)\) for all \(\mu_j\)'s which facilitates simultaneous estimation of the movie mean ratings (the \(\mu_j\)'s), and estimation of the variation among the movie mean ratings through the parameters \(\mu\) and \(\tau\). The posterior mean of the rating for a particular movie \(\mu_j\) shrinks the observed mean rating towards an average rating. The following figure displays a shrinkage plot which illustrates the movement of the observed sample mean ratings towards an average rating.
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/MBJ/T2/mean_shrinkage.png" alt="Shrinkage" style="width:500px;height:350px">
</p>

<p>
The left side plots the sample movie rating means and lines connect the sample means to the corresponding posterior means (i.e. means of the posterior draws of \(\mu_j\)). The shrinkage effect is obvious for the movie "Batman: Under the Red Hood" which corresponds to the dot at the value \(5.0\) on the left. This movie only received one rating of \(5.0\) and its mean rating \(\mu_5\) shrinks to the value \(4.178\) on the right, which is still the highest posterior mean among the nine movie posterior means.
</p>

<p>
A large shrinkage is desirable for a movie with a small number of ratings such as "Batman: Under the Red Hood". For a movie with a small sample size, information about other ratings of similar movies helps to produce a more reasonable estimate at the true average movie rating. The amount of shrinkage is more modest for movies with larger sample sizes.
</p>

<div id="Hierarchical Normal Modeling-Inference through MCMC-Sources of variability"><h3 id="Sources of variability" class="header"><a href="#Hierarchical Normal Modeling-Inference through MCMC-Sources of variability">Sources of variability</a></h3></div>

<p>
We know that the prior distribution \(\text{Normal}(\mu, \tau)\) is shared among the means \(\mu_j\)'s of all groups in a hierarchical Normal model, and the hyperparameters \(\mu\) and \(\tau\) provide information about the population of \(\mu_j\)'s. Specifically, the standard deviation \(\tau\) measures the variability among the \(\mu_j\)'s. When the hierarchical model is estimated through MCMC, summaries from the simulation draws from the posterior of \(\tau\) provide information about this source of variation after analyzing the data.
</p>

<p>
There are actually two sources for the variability among the observed \(Y_{ij}\)'s:
</p>

<ul>
<li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Sources of variability-Sampling level"></span><strong id="Sampling level">Sampling level</strong>: within-group variability:
\begin{align}
Y_{ij} \sim \text{Normal}(\mu_j, \sigma)
\end{align}

</li></ul>
 
<ul>
<li>
<span id="Hierarchical Normal Modeling-Inference through MCMC-Sources of variability-Group level"></span><strong id="Group level">Group level</strong> between-group variability:
\begin{align}
\mu_{j} | \mu, \tau \sim \text{Normal}(\mu, \tau)
\end{align}

</li></ul>
<p>
When the hierarchical model is fit through MCMC, summaries from the marginal posterior distributions of \(\sigma\) and \(\tau\) provide information about the two sources of variability.
</p>

<p>
The Bayesian posterior inference in the hierarchical model is able to compare these two sources of variability, taking into account the prior belief and the information from the data. One initially provides prior beliefs about the values of the standard deviations \(\sigma\) and \(\tau\) through Gamma distributions. 
</p>

<p>
What can be said about these two sources of variability after the estimation of the hierarchical model? As seen in the output of <code>print(posterior, digits = 3)</code>, the \(90\%\) credible interval for \(\sigma\) is \((0.763, 1.12)\) and the \(90\%\) credible interval for \(\tau\) is \((0.357, 1.08)\). After observing the data, the within-group variability in the measurements is estimated to be larger than the between-group variability.
</p>

<p>
To compare both variability sources we compute:
</p>

\begin{align}
R = \frac{\tau^2}{\tau^2 + \sigma^2}
\end{align}

<p>
It represents the fraction of the total variability in the movie ratings due to the differences between groups. If the value of \(R\) is close to \(1\), most of the total variability is attributed to the between-group variability. On the other side, if \(R\) is close to \(0\), most of the variation is within groups and there is little significant differences between groups.
</p>

<p>
A \(95\%\) credible interval for \(R\) is \((0.149, 0.630)\). Since much of the posterior probability of \(R\) is located below the value \(0.5\), this confirms that the variation between the mean movie rating titles is smaller than the variation of the ratings within the movie titles in this example.
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/MBJ/T2/IC_variability_comparison.png" alt="Variability Comparison" style="width:400px;height:250px">
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>