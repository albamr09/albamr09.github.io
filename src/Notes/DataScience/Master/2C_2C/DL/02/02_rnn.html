<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Recurrent Neural Nets</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Recurrent Neural Nets"><h1 id="Recurrent Neural Nets" class="header"><a href="#Recurrent Neural Nets">Recurrent Neural Nets</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="02_rnn.html#SimpleRNN%20cells">SimpleRNN cells</a>

</li><li>
<a href="02_rnn.html#RNN%20topologies">RNN topologies</a>

</li><li>
<a href="02_rnn.html#Vanishing%20and%20exploding%20gradients">Vanishing and exploding gradients</a>

</li><li>
<a href="02_rnn.html#Long%20short%20term%20memory%20%2014%20LSTM">Long short term memory   LSTM</a>

</li><li>
<a href="02_rnn.html#Gated%20recurrent%20unit%20%2014%20GRU">Gated recurrent unit   GRU</a>

</li></ul>
<hr>

<p>
A <span id="Recurrent Neural Nets-recurrent neural network"></span><strong id="recurrent neural network">recurrent neural network</strong> (RNN) is a class of neural networks that exploit the sequential nature of their input.
</p>

<p>
Such inputs could be text, speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it. 
</p>

<div id="Recurrent Neural Nets-SimpleRNN cells"><h2 id="SimpleRNN cells" class="header"><a href="#Recurrent Neural Nets-SimpleRNN cells">SimpleRNN cells</a></h2></div>

<p>
RNN cells incorporate this dependence by having a <span id="Recurrent Neural Nets-SimpleRNN cells-hidden state"></span><strong id="hidden state">hidden state</strong>, or memory. The value of the hidden state is a function of the value of the hidden state at the previous time step and the value of the input at the current time step.
</p>

\begin{align}
h_t = \phi(h_{t-1}, x_t)
\end{align}

<p>
where \(h_t\) and \(h_{t-1}\) are the values of the hidden states at the time steps \(t\) and \(t-1\) and \(x_t\) is the values of the input at time \(t\). <span id="Recurrent Neural Nets-SimpleRNN cells-Note that the equation is recursive"></span><strong id="Note that the equation is recursive">Note that the equation is recursive</strong>
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/DL/02/rnn_unrolled.png" alt="Unrolled RNN" style="width:500px;height:300px">
</p>

<p>
At time \(t\) the cell has an input \(x_t\) and an output \(y_t\). Part of the output \(y_t\) (the hidden state \(h_t\)) is fed back into the cell for use at a later time step \(t+1\). On the previous image we show the behaviour of a single cell unrolled.
</p>

<p>
Notice that the weight matrices \(U\), \(V\), and \(W\) are shared across the steps. We can also describe the computations within an RNN in terms of equations:
</p>

\begin{align}
h_t = tanh(Wh_{t-1} + Ux_t)
\end{align}

\begin{align}
y_t = sofmax(Vh_t)
\end{align}

<div id="Recurrent Neural Nets-RNN topologies"><h2 id="RNN topologies" class="header"><a href="#Recurrent Neural Nets-RNN topologies">RNN topologies</a></h2></div>

<p>
RNNs can be arranged in many ways to solve specific problems.
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/DL/02/rnn_topologies.png" alt="RNN Topologies" style="width:800px;height:300px">
</p>

<p>
In the basic topology, all input sequences are of the same length and an output is produced at each time step. 
</p>

<p>
Another example of a many to many RNN could be a machine translation network shown on the <span id="Recurrent Neural Nets-RNN topologies-many-to-many"></span><strong id="many-to-many">many-to-many</strong> topology. These take in a sequence and produces another sequence. For example, the input could be a sequence in English and the output could be the translation in Spanish.
</p>

<p>
Other variants are the <span id="Recurrent Neural Nets-RNN topologies-one-to-many network"></span><strong id="one-to-many network">one-to-many network</strong>, an example of which could be an image captioning network, where the input is an image and the output a sequence of words.
</p>

<p>
Similarly, an example of a <span id="Recurrent Neural Nets-RNN topologies-many-to-one network"></span><strong id="many-to-one network">many-to-one network</strong> could be a network that does sentiment analysis of sentences, where the input is a sequence of words and the output is a positive or negative sentiment.
</p>

<div id="Recurrent Neural Nets-Vanishing and exploding gradients"><h2 id="Vanishing and exploding gradients" class="header"><a href="#Recurrent Neural Nets-Vanishing and exploding gradients">Vanishing and exploding gradients</a></h2></div>

<p>
Training the RNN involves backpropagation, where the gradient at each output depends not only on the current time step, but also on the previous ones, this process is called <span id="Recurrent Neural Nets-Vanishing and exploding gradients-backpropagation through time"></span><strong id="backpropagation through time">backpropagation through time</strong> (BPTT).
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/DL/02/backpropagation_through_time.png" alt="Backpropagation through time" style="width:600px;height:300px">
</p>

<p>
During backpropagation (shown by dotted lines), the gradients of the loss with respect to the parameters \(U\), \(V\), and \(W\) are computed at each time step and the parameters are updated with the sum of the gradients.
</p>

<p>
The following equation shows the gradient of the loss with respect to \(W\):
</p>

\begin{align}
\frac{\delta L}{\delta W} = \sum_t \frac{\delta L_t}{\delta W}
\end{align}

<p>
Let us now look at what happens to the gradient of the loss at the last time step (\(t=3\))
</p>

\begin{align}
\frac{\delta L_3}{\delta W} = \frac{\delta L_3}{\delta y_3} \frac{\delta y_3}{\delta h_2} \frac{\delta h_2}{\delta_W}
\end{align}

<p>
The previous equation is simply deriving by applying the chain rule, where:
</p>

<ol>
<li>
The loss function \(L_3\) is defined as a function of \(y_3\), 

</li><li>
Then \(y_3 = softmax(Vh_2)\) 

</li><li>
And finally \(h_2 = tanh(Wh_1 + Ux_1)\)

</li></ol>
<p>
The gradient of the hidden state \(h_2\) with respect to \(W\) can be further decomposed as the sum of the gradient of each hidden state with respect to the previous one.
</p>

\begin{align}
\frac{\delta L_3}{\delta W} = \sum_{t=0}^2 \frac{\delta L_3}{\delta y_3} \frac{\delta y_3}{\delta h_2} \frac{\delta h_2}{\delta h_t}\frac{\delta h_t}{\delta_W}
\end{align}

<p>
Finally, each gradient of the hidden state with respect to the previous one can be further decomposed as the product of gradients of the current hidden state against the previous one.
</p>

\begin{align}
\frac{\delta L_3}{\delta W} = \sum_{t=0}^2 \frac{\delta L_3}{\delta y_3} \frac{\delta y_3}{\delta h_2} \left(\prod_{j=t+1}^2 \frac{\delta h_j}{\delta h_{j-1}}\right)\frac{\delta h_t}{\delta_W}
\end{align}

<p>
For example for \(t = 3\):
</p>

\begin{align}
\frac{\delta L_4}{\delta W} = \frac{\delta L_4}{\delta y_4} \frac{\delta y_4}{\delta h_3} \left(\prod_{j=4}^2 \frac{\delta h_j}{\delta h_{j-1}}\right)\frac{\delta h_4}{\delta_W}
\end{align}

\begin{align}
\frac{\delta L_4}{\delta W} = \frac{\delta L_4}{\delta y_4} \frac{\delta y_4}{\delta h_3} \left(\frac{\delta h_4}{\delta h_3}\frac{\delta h_3}{\delta h_2}\frac{\delta h_2}{\delta h_1}\right)\frac{\delta h_4}{\delta_W}
\end{align}

<p>
On general:
</p>

\begin{align}
\frac{\delta L_i}{\delta W} = \sum_{t=0}^i \frac{\delta L_i}{\delta y_i} \frac{\delta y_i}{\delta h_{i-1}} \left(\prod_{j=t+1}^i \frac{\delta h_j}{\delta h_{j-1}}\right)\frac{\delta h_i}{\delta_W}
\end{align}

<p>
Consider the case where the individual gradients of a hidden state with respect to the previous one is less than one. As we backpropagate across multiple time steps, the product of gradients get smaller and smaller, leading to the problem of <span id="Recurrent Neural Nets-Vanishing and exploding gradients-vanishing gradients"></span><strong id="vanishing gradients">vanishing gradients</strong>.
</p>

<p>
Similarly, if the gradients are larger than one, the products get larger and larger, leading to
the problem of <span id="Recurrent Neural Nets-Vanishing and exploding gradients-exploding gradients"></span><strong id="exploding gradients">exploding gradients</strong>.
</p>

<p>
The effect of vanishing gradients is that the gradients from steps that are far away do not contribute anything to the learning process, so the RNN ends up not learning long range dependencies.
</p>

<p>
While there are a few approaches to minimize the problem of vanishing gradients, such as:
</p>

<ol>
<li>
Proper initialization of the \(W\) matrix

</li><li>
Using a ReLU instead of tanh layers

</li><li>
Pre-training the layers using unsupervised methods

</li></ol>
<p>
<span id="Recurrent Neural Nets-Vanishing and exploding gradients-The most popular solution is to use the LSTM or GRU architectures."></span><strong id="The most popular solution is to use the LSTM or GRU architectures.">The most popular solution is to use the LSTM or GRU architectures.</strong>
</p>

<div id="Recurrent Neural Nets-Long short term memory — LSTM"><h2 id="Long short term memory — LSTM" class="header"><a href="#Recurrent Neural Nets-Long short term memory — LSTM">Long short term memory — LSTM</a></h2></div>

<p>
The LSTM is a variant of RNN that is capable of learning long term dependencies.
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/DL/02/ltsm.png" alt="Long Short Term Memory" style="width:500px;height:300px">
</p>

<p>
The line across the top of the diagram is the cell state c, and represents the internal memory of the unit. 
</p>

<p>
The line across the bottom is the hidden state.
</p>

<p>
Also, \(i\), \(f\), and \(o\) are the input, forget, and output gates.
</p>

<p>
The forget gate defines how much of the previous state \(h_{t-1}\) you want to allow to pass through.
</p>

<p>
The input gate defines how much of the newly computed state for the current input \(x_t\) you want to let through.
</p>

<p>
The output gate defines how much of the internal state you want to expose to the next layer.
</p>

<p>
The internal hidden state \(g\) is computed based on the current input \(x_t\) and the previous hidden state \(h_{t-1}\).
</p>

<p>
Such that:
</p>

\begin{align}
i = \sigma(W_ih_{t-1} + U_ix_t)
\end{align}
\begin{align}
f = \sigma(W_fh_{t-1} + U_fx_t)
\end{align}
\begin{align}
o = \sigma(W_oh_{t-1} + U_ox_t)
\end{align}
\begin{align}
g = \tanh(W_gh_{t-1} + U_gx_t)
\end{align}
\begin{align}
c_t = (c_{t-1} \otimes f) \oplus (g \otimes i)
\end{align}
\begin{align}
h_t = tanh(c_t) \otimes o
\end{align}

<p>
One thing to realize is that an LSTM is a drop-in replacement for a SimpleRNN on the recurrent neural network.
</p>

<div id="Recurrent Neural Nets-Gated recurrent unit — GRU"><h2 id="Gated recurrent unit — GRU" class="header"><a href="#Recurrent Neural Nets-Gated recurrent unit — GRU">Gated recurrent unit — GRU</a></h2></div>

<p>
This type of cell has two gates, an <span id="Recurrent Neural Nets-Gated recurrent unit — GRU-update gate"></span><strong id="update gate">update gate</strong> \(z\), and a <span id="Recurrent Neural Nets-Gated recurrent unit — GRU-reset gate"></span><strong id="reset gate">reset gate</strong> \(r\). 
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/DL/02/gru_rnn.png" alt="GRU RNN" style="width:500px;height:300px">
</p>

<p>
The update gate defines how much previous memory to keep around.
</p>

<p>
The reset gate defines how to combine the new input with the previous memory.
</p>

<p>
The following equations define the gating mechanism in a GRU:
</p>

\begin{align}
z = \sigma(W_zh_{t-1} + U_z x_t)
\end{align}

\begin{align}
r = \sigma(W_rh_{t-1} + U_r x_t)
\end{align}

\begin{align}
c_t = tanh(W_c(h_{t-1} \otimes r) + U_cx_t)
\end{align}

\begin{align}
h_t = (z \otimes c) \oplus ((1 - z) \otimes h_{t-1})
\end{align}

<p>
GRU and LSTM have comparable performance, while GRUs are faster to train and need less data to generalize  in situations where there is enough data, an LSTM's greater expressive power may lead to better results.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>