<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Gradient Boosting</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Gradient Boosting"><h1 id="Gradient Boosting" class="header"><a href="#Gradient Boosting">Gradient Boosting</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="04_gradient_boosting.html#Numerical%20Optimization%20via%20Gradient">Numerical Optimization via Gradient</a>

</li><li>
<a href="04_gradient_boosting.html#Steepest%20Descent">Steepest Descent</a>

</li><li>
<a href="04_gradient_boosting.html#Gradient%20Boosting">Gradient Boosting</a>

</li><li>
<a href="04_gradient_boosting.html#Implementations%20of%20Gradient%20Boosting">Implementations of Gradient Boosting</a>

<ul>
<li>
<a href="04_gradient_boosting.html#Step%20Length">Step Length</a>

</li><li>
<a href="04_gradient_boosting.html#Characteristics">Characteristics</a>

</li></ul>
</li></ul>
<hr>

<div id="Gradient Boosting-Numerical Optimization via Gradient"><h2 id="Numerical Optimization via Gradient" class="header"><a href="#Gradient Boosting-Numerical Optimization via Gradient">Numerical Optimization via Gradient</a></h2></div>

<p>
Imagine you have a machine learning model that makes predictions, but it's not perfect. Gradient boosting is like a smart way to teach this model to make better predictions over time.
</p>

<p>
Instead of trying to fix all the prediction errors at once, gradient boosting focuses on correcting one error at a time. It does this by looking at the direction where the error is the steepest and making adjustments to improve the prediction in that direction.
</p>

<p>
By repeating this process step by step, the model gradually gets better at making predictions, leading to more accurate results.
</p>

<p>
So if you have the following function you want to optimize:
</p>

\begin{align}
L(f) =  \sum_{i=1}^N L(y_i, f(x_i))
\end{align}

<p>
where \(f \in \mathbb{R}^N\) is the prediction function and its evaluation at each instance \(x_i\) are the parameteres we want to optimize:
</p>

\begin{align}
f = \{f(x_1), \cdots, f(x_i), \cdots, f(x_n)\}
\end{align}

<p>
Therefore the optimization problem with respect to \(f\) can be summarized as follows:
</p>

\begin{align}
\hat{f} = \arg \min_f L(f)
\end{align}

<p>
Solving this entire problem at once may be challenging. To make it easier, numerical optimization procedures break down this big problem into smaller pieces, represented by component vectors. Each component vector addresses a specific aspect of the problem. So:
</p>

\begin{align}
f_{M} = \sum_{m = 0}^M h_m, h_m \in \mathbb{R}^N
\end{align}

<p>
where \(f_M\) represents the final model or prediction function obtained after M iterations or steps of the boosting algorithm.
</p>

<p>
Here \(f_m\) represents the model at iteration \(m\), whereas \(h_m\) represents the increment to the model at iteration \(m\) It is the component vector added to the current model to move towards the optimized solution. Each \(h_m\) is induced based on the current parameter vector \(f_{m-1}\) and contributes to the overall model improvement.
</p>

<p>
Here is a simple layout of how the algorithm optimizes:
</p>

<ul>
<li>
At the beginning of the gradient boosting process, the initial model \(f_0\) is set to an initial guess.

</li><li>
As the algorithm progresses through iterations (\(m = 1, 2, \cdots, M\)), each step involves updating the model based on the gradient information to reduce errors in predictions. Numerical optimization methods differ in their prescriptions for computing each increment vector \(h_m\).

</li></ul>
<div id="Gradient Boosting-Steepest Descent"><h2 id="Steepest Descent" class="header"><a href="#Gradient Boosting-Steepest Descent">Steepest Descent</a></h2></div>

<p>
Steepest descent is a method used in optimization to find the minimum value of a function. This method chooses \(h_m = \rho_m g_m\) where \(\rho_m\) is a scalar and \(g_m\) is the gradient of \(L(f_{m-1})\), that is, the cost function evaluated at values predicted by the "previous model".
</p>

<p>
The components of the gradient \(g_m\) are defined as follows:
</p>

\begin{align}
g_{im} = \left[\frac{\delta L(y_i, f(x_i))}{\delta f(x_i)}\right]_{f_m(x_i) = f_{m-1}(x_i)}
\end{align}

<p>
The step length (kinda like the learning rate):
</p>

\begin{align}
\rho_m = \arg \min_{\rho} L(f_{m-1} - \rho g_m)
\end{align}

<p>
Thus, at each step, the predictor is updated as follows:
</p>

\begin{align}
f_m = f_{m - 1} - \rho_m g_m \in \mathbb{R}^N
\end{align}

<p>
This updates \(f_m\) towards the direction of maximum descent at \(L(f_{m-1})\), which is why this is often interpreted as a greedy algorithm. 
</p>

<div id="Gradient Boosting-Gradient Boosting"><h2 id="Gradient Boosting" class="header"><a href="#Gradient Boosting-Gradient Boosting">Gradient Boosting</a></h2></div>

<p>
Gradient Boosting aims to create a strong predictive model by combining multiple weak models. It starts with a simple model and gradually enhances it to minimize errors.
</p>

<p>
At each iteration, a new tree model is fit to the negative gradient of the loss function. The predictions from these trees guide the model towards better predictions. Using squared error to measure closeness, this leads us to:
</p>

\begin{align}
\tilde{\Theta}_m = \arg \min_{\Theta} \sum_{i=1}^N (-g_{im} - T(x_i; \Theta))^2
\end{align}

<p>
This measures how close each prediction \(T(x_i; \Theta)\) is to the gradient \(-g_{im}\).
</p>

<p>
The negative gradient of the loss function represents the direction in which the model's predictions need to be adjusted to reduce errors. By fitting a new tree to this negative gradient, the model learns how to correct its predictions to move closer to the actual target values. That is at each iteration, the new tree model focuses on capturing the errors or residuals of the current ensemble model. 
</p>

<p>
While the exact regions where the new tree makes corrections may not match perfectly with the original model's regions, they are close enough to serve the same purpose of improving the model's accuracy. Here the original model is the ensemble model. 
</p>

<p>
The following figure summarizes the gradients for commonly used loss functions:
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/AAII/T2/gradients_gradient_boosting.png" alt="Common Gradients" style="width:650px;height:300px">
</p>

<div id="Gradient Boosting-Implementations of Gradient Boosting"><h2 id="Implementations of Gradient Boosting" class="header"><a href="#Gradient Boosting-Implementations of Gradient Boosting">Implementations of Gradient Boosting</a></h2></div>

<p>
Algorithm \(10.3\) presents the generic gradient tree-boosting algorithm for regression. Specific algorithms are obtained by inserting different loss criteria \(L(y,f(x))\).
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/Master/2C_2C/AAII/T2/gradient_boosting_algorithms.png" alt="Gradient Boosting Algorithm" style="width:650px;height:500px">
</p>

<ul>
<li>
Start with an initial model \(f_0(x)\) that minimizes the loss function \(L(y, f(x))\).

</li><li>
For each boosting round \(m = 1, \cdots, M\):

<ul>
<li>
Calculate the negative gradient for each data point 
\begin{align}
r_{im} = -\left[\frac{\delta L(y_i, f(x_i))}{\delta f(x_i)}\right]_{f(x_i) = f_{m-1}(x_i)}
\end{align}

</li><li>
Fit a regression tree to the gradients \(r_{im}\), which gives us the regions \(R_{jm}, j = 1, 2, \cdots, J_m\)

</li><li>
The step length \(\gamma\) is determined by minimizing the loss using the previous model (\(f_{m-1}\)):
\begin{align}
\gamma_{jm} = \arg \min_{\gamma} \sum_{x_i \in R_{jm}} L(y_i, f_{m-1}(x_i) + \gamma)
\end{align}

</li><li>
Update the model by adding a new tree to the ensemble \(f_m(x) = f_{m-1}(x) + \gamma T(x; \Theta_m)\), where \(T(x; \Theta)\) is the new tree model with parameters \(\Theta_m\) that corrects the errors in the previous model.

</li></ul>
</li><li>
The output of the ensemble model is defined as \(\hat{f}(x) = f_M(x)\), that is as the sum of the weaker models.

</li></ul>
<div id="Gradient Boosting-Implementations of Gradient Boosting-Step Length"><h3 id="Step Length" class="header"><a href="#Gradient Boosting-Implementations of Gradient Boosting-Step Length">Step Length</a></h3></div>

<p>
The step length \(\gamma\) is crucial in determining how much each new tree should contribute to the ensemble model. It controls the impact of the new tree on the overall model's predictions.
</p>

<p>
The line search aims to find the value of γ that minimizes the loss function:
</p>

\begin{align}
L(f_{m-1} - \gamma g_m)
\end{align}

<p>
This means finding the optimal step length that results in the smallest possible loss when updating the model with the new tree. By minimizing the loss function with respect to \(\gamma\), the algorithm is essentially performing a form of gradient descent.
</p>

<div id="Gradient Boosting-Implementations of Gradient Boosting-Characteristics"><h3 id="Characteristics" class="header"><a href="#Gradient Boosting-Implementations of Gradient Boosting-Characteristics">Characteristics</a></h3></div>

<p>
Two basic tuning parameters are the number of iterations \(M\) and the sizes of each of the constituent trees
\(J_m, m = 1, 2, \cdots, M\).
</p>

<p>
The original implementation of this algorithm was called MART for "multiple additive regression trees".
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>