<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Cluster Analysis</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Cluster Analysis"><h1 id="Cluster Analysis" class="header"><a href="#Cluster Analysis">Cluster Analysis</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="02_cluster_analysis.html#Proximity%20Matrices">Proximity Matrices</a>

</li><li>
<a href="02_cluster_analysis.html#Dissimilarities%20Based%20on%20Attributes">Dissimilarities Based on Attributes</a>

<ul>
<li>
<a href="02_cluster_analysis.html#Quantitative%20variables">Quantitative variables</a>

</li><li>
<a href="02_cluster_analysis.html#Ordinal%20Variables">Ordinal Variables</a>

</li><li>
<a href="02_cluster_analysis.html#Categorical%20variables">Categorical variables</a>

</li></ul>
</li><li>
<a href="02_cluster_analysis.html#Object%20Dissimilarity">Object Dissimilarity</a>

</li><li>
<a href="02_cluster_analysis.html#Clustering%20Algorithms">Clustering Algorithms</a>

<ul>
<li>
<a href="02_cluster_analysis.html#Combinatorial%20Algorithms">Combinatorial Algorithms</a>

</li><li>
<a href="02_cluster_analysis.html#K-Means">K-Means</a>

</li><li>
<a href="02_cluster_analysis.html#Gaussian%20Mixtures%20as%20Soft%20K-means%20Clustering">Gaussian Mixtures as Soft K-means Clustering</a>

</li><li>
<a href="02_cluster_analysis.html#Vector%20Quantization">Vector Quantization</a>

</li><li>
<a href="02_cluster_analysis.html#K-Medoids">K-Medoids</a>

</li><li>
<a href="02_cluster_analysis.html#Practical%20Issues">Practical Issues</a>

</li><li>
<a href="02_cluster_analysis.html#Hierarchical%20Clustering">Hierarchical Clustering</a>

<ul>
<li>
<a href="02_cluster_analysis.html#Agglomerative%20Clustering">Agglomerative Clustering</a>

</li><li>
<a href="02_cluster_analysis.html#Divisive%20Clustering">Divisive Clustering</a>

</li></ul>
</li></ul>
</li></ul>
<hr>

<p>
Cluster analysis, also called data segmentation, has a variety of goals. All relate to grouping or segmenting a collection of objects into subsets or "clusters", such that those within each cluster are more closely related to one another than objects assigned to different clusters. The method we use to group these objects depends on what we consider to be similar, and that decision usually comes from what we know about the subject we're studying.
</p>

<p>
An object can be described by a set of measurements, or by its relation to other objects.
</p>

<div id="Cluster Analysis-Proximity Matrices"><h2 id="Proximity Matrices" class="header"><a href="#Cluster Analysis-Proximity Matrices">Proximity Matrices</a></h2></div>

<p>
Sometimes the data is represented directly in terms of the proximity (alikeness or affinity) between pairs of objects. This type of data can be represented by an \(N \times N\) matrix \(D\), where \(N\) is the number of objects, and each element \(d_{ij}\) records the proximity between the \(j\)th and \(j\)th objects. This matrix is then provided as input to the clustering algorithm.
</p>

<p>
Most algorithms assume symmetric dissimilarity matrices, so if the original matrix \(D\) is not symmetric it must be replaced by \(\frac{(D + D^T)}{2}\).
</p>

<div id="Cluster Analysis-Dissimilarities Based on Attributes"><h2 id="Dissimilarities Based on Attributes" class="header"><a href="#Cluster Analysis-Dissimilarities Based on Attributes">Dissimilarities Based on Attributes</a></h2></div>

<p>
Since most of the popular clustering algorithms take a dissimilarity matrix as their input, we must first construct pairwise dissimilarities between the observations. By far the most common
choice is squared distance:
</p>

\begin{align}
d_j(x_{ij}, x_{i'j}) = (x_{ij} - x_{i'j})^2
\end{align}

<p>
where \(j\) denotes the attribute and \(i, i'\) denotes the instance.
</p>

<p>
We first discuss alternatives in terms of the attribute type:
</p>

<div id="Cluster Analysis-Dissimilarities Based on Attributes-Quantitative variables"><h3 id="Quantitative variables" class="header"><a href="#Cluster Analysis-Dissimilarities Based on Attributes-Quantitative variables">Quantitative variables</a></h3></div>

<p>
Measurements of this type of variable or attribute are represented by continuous real-valued numbers. One way to do this is by looking at the absolute difference between them:
</p>

\begin{align}
d(x_i, x_{i'}) = l(|x_i - x_{i'}|)
\end{align}

<p>
Alternatively, clustering can be based on the correlation
</p>

\begin{align}
\rho (x_i, x_{i'}) = \frac{\sum_{j} (x_{ij} - \overline{x}_i)(x_{ij} - \overline{x}_i)}{\sqrt{\sum_{j} (x_{ij} - \overline{x}_i)^2 \sum_j(x_{ij} - \overline{x}_i)^2}}
\end{align}

<div id="Cluster Analysis-Dissimilarities Based on Attributes-Ordinal Variables"><h3 id="Ordinal Variables" class="header"><a href="#Cluster Analysis-Dissimilarities Based on Attributes-Ordinal Variables">Ordinal Variables</a></h3></div>

<p>
Error measures for ordinal variables are generally defined by replacing their \(M\) original values with:
</p>

\begin{align}
\frac{i - \frac{1}{2}}{M}, i = 1, \cdots, M
\end{align}

<p>
in the prescribed order of their original values. They are then treated as quantitative variables on this scale.
</p>

<div id="Cluster Analysis-Dissimilarities Based on Attributes-Categorical variables"><h3 id="Categorical variables" class="header"><a href="#Cluster Analysis-Dissimilarities Based on Attributes-Categorical variables">Categorical variables</a></h3></div>

<p>
If the variable assumes \(M\) distinct values, these can be arranged in a symmetric \(M \times M\) matrix with elements:
</p>

\begin{align}
m_{ij} = \begin{cases}
0 &amp; x_{i} = x_{j} \\
1 &amp; x_{i} \neq x_{j} \\
\end{cases}
\end{align}

<div id="Cluster Analysis-Object Dissimilarity"><h2 id="Object Dissimilarity" class="header"><a href="#Cluster Analysis-Object Dissimilarity">Object Dissimilarity</a></h2></div>

<p>
Next we define a procedure for combining the \(p\)-individual attribute dissimilarities \(d_j(x_{ij},x_{i'j}), j = 1,2, \cdots, p\) into a single overall measure of dissimilarity \(D(x_i, x_i')\).
</p>

<p>
This is nearly always done by means of a weighted average:
</p>

\begin{align}
D(x_i, x_{i'}) = \sum_{j=1}^p w_j d_j(x_{ij}, x_{i'j})
\end{align}

<p>
where:
</p>

\begin{align}
\sum_{j=1}^p w_j = 1
\end{align}

<p>
Here \(w_j\) is a weight assigned to the \(j\)th attribute regulating the relative influence of that variable in determining the overall dissimilarity between objects. 
</p>

<p>
If the goal is to discover natural groupings in the data, some attributes may exhibit more of a grouping tendency than others. Variables that are more relevant in separating the groups should be assigned a higher influence in defining object dissimilarity. Giving all attributes equal influence
in this case will tend to obscure the groups to the point where a clustering algorithm cannot uncover them.
</p>

<p>
Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than choice of clustering algorithm.
</p>

<div id="Cluster Analysis-Clustering Algorithms"><h2 id="Clustering Algorithms" class="header"><a href="#Cluster Analysis-Clustering Algorithms">Clustering Algorithms</a></h2></div>

<p>
Clustering algorithms fall into three distinct types: 
</p>

<ul>
<li>
<span id="Cluster Analysis-Clustering Algorithms-Combinatorial algorithms"></span><strong id="Combinatorial algorithms">Combinatorial algorithms</strong>: work directly on the observed data with no direct reference to an underlying probability model.

</li><li>
<span id="Cluster Analysis-Clustering Algorithms-Mixture modeling"></span><strong id="Mixture modeling">Mixture modeling</strong>: supposes that the data is an i.i.d sample from some population described by a probability density function.

</li><li>
<span id="Cluster Analysis-Clustering Algorithms-Mode seeking"></span><strong id="Mode seeking">Mode seeking</strong>: take a nonparametric perspective, attempting to directly estimate distinct modes of the probability density function

</li></ul>
<div id="Cluster Analysis-Clustering Algorithms-Combinatorial Algorithms"><h3 id="Combinatorial Algorithms" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Combinatorial Algorithms">Combinatorial Algorithms</a></h3></div>

<p>
Each observation is uniquely labeled by an integer \(i \in {1, \cdots, N}\). One seeks the particular encoder \(C^*(i)\) that assigns the \(i\)th observation to the \(k\)th cluster that satisfies the required goal based on the dissimilarities \(d(x_i, x_{i'})\). The "parameters" of the procedure are the individual cluster assignments for each of the \(N\) observations. These are adjusted so as to minimize a "loss" function.
</p>

<p>
Since the goal is to assign close points to the same cluster, a natural loss function would be:
</p>

\begin{align}
W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k}\sum_{C(i') = k} d(x_i, d_{i'})
\end{align}

<p>
This measure tells us how close together the things in the same group are. It is sometimes referred to as the "within cluster" point scatter. The total point scatter is given by:
</p>

\begin{align}
T = \frac{1}{2} \sum_{i=1}^N\sum_{i'=1}^N d_{ii'} = \frac{1}{2}\sum_{k=1}^K\sum_{C(i) = k} \left(\sum_{C(i') = k} d_{ii'} +  \sum_{C(i') \neq k} d_{ii'}\right)
\end{align}

<p>
This basically divides, for each \(i\)th instance on cluster \(k\), so \(C(i) = k\), the distances into two categories, distances to instances on the same cluster \(\sum_{C(i') = k} d_{ii'}\), and distances to instances on a different cluster \(\sum_{C(i') \neq k} d_{ii'}\). That is:
</p>

\begin{align}
T = W(C) + B(C)
\end{align}

<p>
where \(B(C)\) is the between-cluster point scatter:
</p>

\begin{align}
B(C) = \frac{1}{2} \sum_{k=1}^K \sum_{C(i) = k} \sum_{C(i')\neq k} d_{ii'}
\end{align}

<p>
So, minimizing \(W(C)\) is equivalent to maximizing \(B(C)\) given \(W(C) = T - B(C)\).
</p>

<p>
Cluster analysis by combinatorial optimization is straightforward in principle. One simply minimizes \(W\) or equivalently maximizes \(B\) over all possible assignments of the \(N\) data points to \(K\) clusters. Unfortunately, such optimization by complete enumeration is feasible only for very small data sets.
</p>

<p>
For this reason, practical clustering algorithms are able to examine only a very small fraction of all possible encoders \(k = C(i)\). The goal is to identify a small subset that is likely to contain the optimal one, or at least a good suboptimal partition. Such feasible strategies are based on iterative greedy descent. An initial partition is specified. At each iterative step, the cluster assignments are changed in such a way that the value of the criterion is improved from its previous value. Clustering algorithms of this type differ in their prescriptions for modifying the cluster assignments at each iteration. When the prescription is unable to provide an improvement, the algorithm terminates with the current assignments as its solution. However, these algorithms converge to local optima which may be highly suboptimal when compared to the global optimum.
</p>

<div id="Cluster Analysis-Clustering Algorithms-K-Means"><h3 id="K-Means" class="header"><a href="#Cluster Analysis-Clustering Algorithms-K-Means">K-Means</a></h3></div>

<p>
The K-means algorithm is one of the most popular iterative descent clustering methods, which uses the squared Euclidean distance. So the within point-scatter can be written as:
</p>

\begin{align}
W(C) = \frac{1}{2} \sum_{k=1}^K\sum_{C(i) = k}\sum_{C(i')=k} ||x_i - x_{i'}||^2
\end{align}

\begin{align}
= \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - \overline{x}_k||^2
\end{align}

<p>
where \(\overline{x}_k\) is the mean vector associated with the \(k\)th cluster and \(N_k = \sum_{i=1}^N I(C(i) = k)\) is the number of instances on the \(k\)th cluster. Therefore the goal is to group the \(N\) observations into \(K\) clusters in a way that minimizes the average difference between each observation and the mean of its cluster.
</p>

\begin{align}
C^* \min_{C} \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - \overline{x}_k||^2
\end{align}

<p>
can be obtained by noting that for any set of observations \(S\):
</p>

\begin{align}
\overline{x}_S = \arg \min_{m} \sum_{i \in S} ||x_i - m||^2
\end{align}

<p>
That is we defined the centroid for \(S\) as the point \(m\) that minimizes the sum of distances for each instance on \(S\). Hence we can obtain \(C^*\) by solving the enlarged optimization problem:
</p>

\begin{align}
\min_{\{C, m_k\}^K_1} \sum_{k=1}^K N_k \sum_{C(i) = k} ||x_i - m_k||^2
\end{align}

<p>
Where for each cluster \(k\), we search for the encoder \(C\) and the optimal centroid \(m_k\). Thus the optmimization process can be performed in two steps as seen in Algorithm 14.1. On (1) we obtain the optimal centroids \(m_i, i = 1, \cdots, K\), and on (2) we try to find the best encoder \(C\) given the set of centroids \(\{m_1, \cdots, m_K\}\).
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/k_means_algorithm.png" alt="KMeans Algorithm" style="witdth:500px; height:200px">
</p>

<div id="Cluster Analysis-Clustering Algorithms-Gaussian Mixtures as Soft K-means Clustering"><h3 id="Gaussian Mixtures as Soft K-means Clustering" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Gaussian Mixtures as Soft K-means Clustering">Gaussian Mixtures as Soft K-means Clustering</a></h3></div>

<p>
The K-means clustering procedure is closely related to the EM algorithm for estimating a certain Gaussian mixture model.
</p>

<p>
The E-step of the EM algorithm assigns "responsibilities" for each data point based in its relative density under each mixture component (cluster). While the M-step recomputes the component density parameters based on the current responsibilities. Where the relative density is a monotone function of the euclidean distance between the data point and the mixture center. Hence in this setup EM is a "soft" version of K-means clustering, making probabilistic (rather than deterministic) assignments of points to cluster centers.
</p>

<p>
As \(\sigma^2\) tends to \(1\), these probabilities become \(0\) and \(1\), and the two methods coincide:
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/k_means_vs_gaussian_mixtures.png" alt="KMeans and Gaussian Mixtures" style="height:500px">
</p>

<div id="Cluster Analysis-Clustering Algorithms-Vector Quantization"><h3 id="Vector Quantization" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Vector Quantization">Vector Quantization</a></h3></div>

<p>
Vector quantization (VQ) is a technique used in data compression, particularly in the compression of digital signals or images. It involves representing a large set of data points (vectors) by a smaller set of representative values, called codewords or centroids. These centroids are selected from the original data set and are used to approximate the original data. By replacing groups of similar data points with these representative values, vector quantization can significantly reduce the amount of data needed to represent the information while minimizing the loss of quality.
</p>

<p>
In this example of vector quantization, given an image of \(1024\times 1024\) pixels we start by dividing the image into small blocks of \(2\times 2\). Each block, which contains four pixels, is treated like a tiny picture of its own. Each of the \(512 \times 512\) blocks of four numbers is regarded as vector in \(\mathbb{R}^4\). Then, using  K-means clustering, we group these blocks together based on their similarity.
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/vector_quantization.png" alt="Vector Quantization Example" style="height:300px">
</p>

<p>
The clustering process is called the <span id="Cluster Analysis-Clustering Algorithms-Vector Quantization-encoding step"></span><strong id="encoding step">encoding step</strong>, and the collection of centroids is called the <span id="Cluster Analysis-Clustering Algorithms-Vector Quantization-codebook"></span><strong id="codebook">codebook</strong>. Why do we expect VQ to work at all? The reason is that for typical everyday images like photographs, many of the blocks look the same. What we have described is known as lossy compression, since our images are degraded versions of the original.
</p>

<div id="Cluster Analysis-Clustering Algorithms-K-Medoids"><h3 id="K-Medoids" class="header"><a href="#Cluster Analysis-Clustering Algorithms-K-Medoids">K-Medoids</a></h3></div>

<p>
The K-Means algorithm can be generalized for use with arbitrarily defined dissimilarities \(D(x_i, x_{i'})\).  The process of finding the centers of the clusters stays similar, but the way we measure similarity can change. This makes the algorithm versatile and applicable to various types of data. This gives way to the K-medoids algorithm:
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/k_medoids_algorithm.png" alt="K Medoids Algorithm" style="height:300px">
</p>

<div id="Cluster Analysis-Clustering Algorithms-Practical Issues"><h3 id="Practical Issues" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Practical Issues">Practical Issues</a></h3></div>

<p>
In order to apply K-means or K-medoids one must select the number of clusters \(K^*\) and an initialization.
</p>

<p>
The latter can be defined by specifying an initial set of centers \(\{m_1,\cdots,m_K\}\) or \(\{i_1, \cdots,i_K\}\) or an initial encoder \(C(i)\). A choice for the number of clusters \(K\) depends on the goal. For data segmentation \(K\) is usually defined as part of the problem.
</p>

<p>
Data-based methods for estimating \(K^*\) typically examine the withincluster dissimilarity \(W_K\) as a function of the number of clusters \(K\). The corresponding values generally decrease with increasing \(K\). Thus cross-validation techniques, so useful for model selection in supervised learning, cannot be utilized in this context.
</p>

<p>
As we increase the number of clusters, the solution quality will improve because more natural groups will be captured separately. So, if we keep adding more clusters beyond the true number (\(K &gt; K^*\)), some estimated clusters will start to split the real groups. However, splitting a group that's already close together won't improve the solution as much as properly separating two distinct groups.
</p>

<p>
To the extent this scenario is realized, there will be a sharp decrease in successive differences in criterion value, \(W_K − W_{K+1}\), at \(K = K^*\). That is, \(\{W_K − W_{K+1} |K &lt; K^*\} \geq \{W_K − W_{K+1} |K \geq K^*\}\). An estimate \(\hat{K}*\) for \(K^*\) is then obtained by identifying a “kink” in the plot of \(W_K\) as a function of \(K\).
</p>

<p>
The recently proposed Gap statistic compares the shape of a curve based on our data to a curve we'd get if the data were spread out evenly. We're looking for a point where there's a big gap between these two curves. This point tells us the best number of clusters for our data. If \(G(K)\) is the Gap curve at \(K\) clusters, the formal rule for estimating \(K^*\) is:
</p>

\begin{align}
K^* = \text{argmin}_{K} \{K | G(K) \geq G(K + 1 - s'_{K + 1}) \}
\end{align}

<p>
where \(s_K\) is the standard deviation. The following figure shows and example on how to choose the optimal number of clusters:
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/gap_statistic.png" alt="Gap Statistic" style="height:400px">
</p>

<div id="Cluster Analysis-Clustering Algorithms-Hierarchical Clustering"><h3 id="Hierarchical Clustering" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Hierarchical Clustering">Hierarchical Clustering</a></h3></div>

<p>
As the name suggests, they produce hierarchical representations in which the clusters at each level of the hierarchy are created by merging clusters at the next lower level. At the lowest level, each cluster contains a single observation. At the highest level there is only one cluster containing all of the data.
</p>

<p>
Strategies for hierarchical clustering divide into two basic paradigms: agglomerative (bottom-up) and divisive (top-down). Each level of the hierarchy represents a particular grouping of the data into disjoint clusters of observations. The entire hierarchy represents an ordered sequence of such groupings. It is up to the user to decide which level (if any) actually represents a "natural" clustering 
</p>

<p>
A dendrogram provides a highly interpretable complete description of the hierarchical clustering in a graphical format. Cutting the dendrogram horizontally at a particular height partitions the data into disjoint clusters represented by the vertical lines that intersect it. The height at which we cut represents the level of similarity required to form a cluster. Generally, groups that merge at higher levels in the dendrogram are considered more significant clusters. However, it's essential to be cautious when interpreting dendrograms because different clustering methods or slight changes in the data can lead to different dendrogram structures. 
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/dendogram_example.png" alt="Dendogram Example" style="height:500px">
</p>

<p>
Also, dendrogram interpretations are only valid if the data truly exhibits the hierarchical structure imposed by the clustering algorithm. This can be assesed using the <em>cophenetic correlation coefficient</em>, it measures the correlation between the <em>cophenetic dissimilarities</em> \(C_{ii'}\) and the distances between observations in the original data \(d_{ii'}\). The <em>cophenetic dissimilarity</em> \(C_{ii'}\) between two observations \((i, i')\) is the intergroup dissimilarity at which observations \(i\) and \(i'\) are first joined together in the same cluster.
</p>

<div id="Cluster Analysis-Clustering Algorithms-Hierarchical Clustering-Agglomerative Clustering"><h4 id="Agglomerative Clustering" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Hierarchical Clustering-Agglomerative Clustering">Agglomerative Clustering</a></h4></div>

<p>
Agglomerative clustering starts with each observation as its own cluster. Then, at each step, it merges the two closest clusters together until there's only one big cluster left. To measure dissimilarity between two clusters, let's call them \(G\) and \(H\). We look at all the pairs of observations, one from \(G\) and one from \(H\), and find the dissimilarity between them.
</p>

<p>
<em>Single linkage</em> (SL) or nearest neighbour agglomerative clustering chooses the closest pair of observations between the two clusters as the measure of dissimilarity between the clusters:
</p>

\begin{align}
d_{SL}(G, H) = \min_{i \in G, i' \in H} d_{ii'}
\end{align}

<p>
<em>Complete linkage</em> (CL) or furthest-neighbor technique agglomerative clustering measures the dissimilarity between two clusters as the distance the pair of observations that are the farthest apart.
</p>

\begin{align}
d_{CL}(G, H) = \max_{i \in G, i' \in H} d_{ii'}
\end{align}

<p>
<em>Group average</em> (GA) clustering uses the average dissimilarity between the groups:
</p>

\begin{align}
d_{GA}(G, H) = \frac{1}{N_G N_H} \sum_{i \in G} \sum_{i \in H} d_{ii'}
\end{align}

<p>
where \(N_G\) and \(N_H\) are the respective number of observations in each group.
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/2C_2C/AAII/T4/agglomerative_clustering_dissimilarity_comparison.png" alt="Agglomerative Clustering Dissimilarity Comparison" style="height:500px">
</p>

<p>
If the data shows clear clusters that are close together and distinct from each other, all three methods—single linkage, complete linkage, and average linkage—will give similar results. However, if the data doesn't exhibit this pattern, the results of the three methods will differ. Single linkage tends to join clusters even if just one pair of observations is close together, which can lead to long chains of connections between clusters. This phenomenon is known as <em>chaining</em>.
</p>

<p>
Complete linkage will tend to produce compact clusters with small diameters because it considers two groups close only if all the observations in their combined set are similar. Sometimes it may violate the rule that observations within a cluster should be closer to each other than to observations in other clusters.
</p>

<p>
Group average clustering strikes a balance between single and complete linkage. It tries to make clusters compact while keeping them relatively far apart. However, its outcome can be affected by how the dissimilarities between observations are measured. Changing the measurement scale can change the clustering result. In contrast, single and complete linkage methods are not affected by such changes in scale.
</p>

<div id="Cluster Analysis-Clustering Algorithms-Hierarchical Clustering-Divisive Clustering"><h4 id="Divisive Clustering" class="header"><a href="#Cluster Analysis-Clustering Algorithms-Hierarchical Clustering-Divisive Clustering">Divisive Clustering</a></h4></div>

<p>
Divisive clustering starts with the entire dataset as one cluster and then splits it into smaller clusters step by step. This method isn't as widely studied as agglomerative clustering, but it has been explored, especially in engineering contexts like compression.
</p>

<p>
One potential advantage of divisive clustering is when you want to divide the data into only a few clusters. Divisive methods can be used recursively with techniques like K-means or K-medoids to split clusters into smaller ones. However the way you begin the splitting process at each step can influence the final outcome.
</p>

<p>
A method has been developed to overcome this limitations. The divisive algorithm, proposed by Macnaughton Smith et al. (1965), is defined as: 
</p>

<ol>
<li>
Puts all observations in one big cluster called \(G\).

</li><li>
It picks the observation that's farthest on average from all the others and makes it the first member of a new cluster called \(H\).

</li><li>
At each successive step that observation in \(G\) whose average distance from those in \(H\), minus that for the remaining observations in \(G\) is largest, is transferred to \(H\).

</li><li>
This continues until there are no more observations in \(G\) that are closer to those in \(H\).

</li></ol>
<p>
This splitting procedure is repeated for each new cluster formed at the previous level, creating a hierarchical structure. Kaufman and Rousseeuw (1990) suggest choosing the cluster with the largest diameter for splitting at each level, but another option is to pick the one with the largest average dissimilarity among its members.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>