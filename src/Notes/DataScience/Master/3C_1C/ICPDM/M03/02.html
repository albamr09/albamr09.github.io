<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka"><h1 id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka">Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="02.html#Introduction">Introduction</a>

</li><li>
<a href="02.html#Apache%20Kafka">Apache Kafka</a>

<ul>
<li>
<a href="02.html#Architecture%20of%20Kafka">Architecture of Kafka</a>

</li><li>
<a href="02.html#Reading%20and%20Writing%20in%20Kafka">Reading and Writing in Kafka</a>

</li><li>
<a href="02.html#Kafka%20Command%20Line%20Interface%20%28CLI%29">Kafka Command Line Interface  CLI</a>

</li></ul>
</li><li>
<a href="02.html#Conclusion">Conclusion</a>

</li></ul>
<hr>

<p>
This summary explains Apache Kafka, a platform for handling large streams of data, based on the provided source text.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Introduction"><h2 id="Introduction" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Introduction">Introduction</a></h2></div>

<p>
In large-scale data processing, data needs to be transmitted efficiently from its source to the processing system. Traditional methods rely on direct connections between devices, which is not scalable. Message queues and pub/sub systems offer improved scalability.
</p>

<p>
Message queues involve a single consumer receiving and processing each message. This approach is common in microservices and suitable for bulk task processing. If one consumer fails, another can take over the message.
</p>

<p>
Pub/sub systems use a central node called a broker to manage message queues, called topics. All consumers subscribed to a topic receive copies of the messages. This is useful for distributing data to multiple systems and is fault-tolerant, allowing consumers to recover missed messages after a failure.
</p>

<p>
However, traditional pub/sub systems can face performance issues and limited storage capacity in massive data environments. This is where Apache Kafka comes in.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka"><h2 id="Apache Kafka" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka">Apache Kafka</a></h2></div>

<p>
<span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Apache Kafka is a streaming platform that uses the pub/sub model for sending messages and monitoring events"></span><strong id="Apache Kafka is a streaming platform that uses the pub/sub model for sending messages and monitoring events">Apache Kafka is a streaming platform that uses the pub/sub model for sending messages and monitoring events</strong>. It is designed to handle large data streams with high performance and low latency. Kafka offers persistent data storage for a user-defined duration and even includes a processing engine (Kafka Streams) for data transformation before it reaches the consumers.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka"><h3 id="Architecture of Kafka" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka">Architecture of Kafka</a></h3></div>

<p>
Kafka's architecture is <span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka-distributed and fault-tolerant"></span><strong id="distributed and fault-tolerant">distributed and fault-tolerant</strong>, thanks to its high data replication. A Kafka cluster consists of multiple brokers, each typically located on a different server. These brokers store data and can manage multiple topics. Each topic can be distributed across multiple brokers, further enhancing fault tolerance.
</p>

<p>
<span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka-Topics are divided into partitions to improve fault tolerance and throughput"></span><strong id="Topics are divided into partitions to improve fault tolerance and throughput">Topics are divided into partitions to improve fault tolerance and throughput</strong>. A partition is essentially a data stream, acting as the fundamental data structure within Kafka. It can be viewed as a log file where data is appended. Sequential writing and reading of data in partitions improve performance. Each data entry in a partition has a unique identifier called an offset, which is helpful for resuming reading from a specific point.
</p>

<p>
Partitions offer scalability, allowing the size of a topic to exceed the capacity of a single machine. They increase throughput by enabling parallel data serving to multiple consumers. Additionally, partitions provide redundancy because multiple copies of the same partition (called replicas) are stored on different brokers. If one broker fails, the partition can be recovered from another broker.
</p>

<p>
<span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka-It's crucial to note that while the order of data arrival is guaranteed within a partition, it is not guaranteed between different partitions."></span><strong id="It's crucial to note that while the order of data arrival is guaranteed within a partition, it is not guaranteed between different partitions.">It's crucial to note that while the order of data arrival is guaranteed within a partition, it is not guaranteed between different partitions.</strong>
</p>

<p>
<span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka-Replication is a core feature of Kafka's architecture."></span><strong id="Replication is a core feature of Kafka's architecture.">Replication is a core feature of Kafka's architecture.</strong> A replica is a copy of a partition and plays a vital role in fault tolerance. The replication factor determines the number of copies made for each partition. A designated replica acts as the leader, responsible for receiving and sending data to consumers. The remaining replicas, called followers, synchronise with the leader asynchronously using Zookeeper.
</p>

<p>
<span id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Architecture of Kafka-Zookeeper is another key component in the architecture, managing service discovery and leader election for Kafka brokers"></span><strong id="Zookeeper is another key component in the architecture, managing service discovery and leader election for Kafka brokers">Zookeeper is another key component in the architecture, managing service discovery and leader election for Kafka brokers</strong>. It informs Kafka about changes in the cluster's topology, ensuring each node knows about new brokers, broker failures, topic additions or removals, and other events. This provides a synchronised view of the Kafka cluster's configuration.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Reading and Writing in Kafka"><h3 id="Reading and Writing in Kafka" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Reading and Writing in Kafka">Reading and Writing in Kafka</a></h3></div>

<p>
Producers send events or data to Kafka, which are distributed among the different partitions. Each piece of data goes to a single partition, ensuring that the order of arrival is maintained only within those partitions. Write operations are append-only, meaning data is sequentially added to the end of the partition on disk.
</p>

<p>
Consumers can choose the offset from which they want to read data. Kafka doesn't keep track of which messages have been read, which simplifies the system but makes complex delivery logic more challenging. Consumers are typically organised into groups, ensuring each consumer reads from a different partition and enhancing scalability.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Kafka Command Line Interface (CLI)"><h3 id="Kafka Command Line Interface (CLI)" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Apache Kafka-Kafka Command Line Interface (CLI)">Kafka Command Line Interface (CLI)</a></h3></div>

<p>
The Kafka CLI provides a way to interact with Kafka from the command line. It is used for tasks such as initialising Zookeeper and brokers, creating and managing topics, publishing data to topics, and consuming data from them.
</p>

<div id="Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Conclusion"><h2 id="Conclusion" class="header"><a href="#Componentes tecnológicos de adquisición y transmisión/distribución de eventos: Kafka-Conclusion">Conclusion</a></h2></div>

<p>
Kafka is a powerful platform designed to handle high-volume data streams in a distributed and fault-tolerant manner. Understanding its architecture, features, and command-line interface is crucial for effectively utilising Kafka in data processing pipelines.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>