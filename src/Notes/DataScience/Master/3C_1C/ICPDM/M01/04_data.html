<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Inyección/Extracción y Serialización/Deserialización de los Datos</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos"><h1 id="Inyección/Extracción y Serialización/Deserialización de los Datos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos">Inyección/Extracción y Serialización/Deserialización de los Datos</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="04_data.html#Apache%20Flume">Apache Flume</a>

<ul>
<li>
<a href="04_data.html#Las%20Fuentes%20de%20datos">Las Fuentes de datos</a>

</li><li>
<a href="04_data.html#Batch%20vs%20Streaming">Batch vs Streaming</a>

</li><li>
<a href="04_data.html#Herramientas%20para%20la%20Inyecci%F3n%20y%20Extracci%F3n%20de%20Datos">Herramientas para la Inyecci n y Extracci n de Datos</a>

<ul>
<li>
<a href="04_data.html#Apache%20Scoop">Apache Scoop</a>

</li><li>
<a href="04_data.html#Apache%20Flume">Apache Flume</a>

<ul>
<li>
<a href="04_data.html#Ventajas">Ventajas</a>

</li><li>
<a href="04_data.html#Desventajas">Desventajas</a>

</li></ul>
</li></ul>
</li><li>
<a href="04_data.html#Arquitectura%20de%20Apache%20Flume">Arquitectura de Apache Flume</a>

<ul>
<li>
<a href="04_data.html#Agentes">Agentes</a>

</li><li>
<a href="04_data.html#Eventos">Eventos</a>

</li><li>
<a href="04_data.html#Flujo%20de%20Datos">Flujo de Datos</a>

</li></ul>
</li></ul>
</li><li>
<a href="04_data.html#Apache%20Avro">Apache Avro</a>

<ul>
<li>
<a href="04_data.html#Serializaci%F3n%2FDeserializaci%F3n">Serializaci n Deserializaci n</a>

</li><li>
<a href="04_data.html#La%20Evoluci%F3n%20de%20los%20Formatos">La Evoluci n de los Formatos</a>

</li><li>
<a href="04_data.html#Estructura">Estructura</a>

</li><li>
<a href="04_data.html#La%20Evoluci%F3n%20de%20los%20Esquemas">La Evoluci n de los Esquemas</a>

</li><li>
<a href="04_data.html#Ventajas">Ventajas</a>

</li><li>
<a href="04_data.html#Ejemplo%20de%20Esquema">Ejemplo de Esquema</a>

</li></ul>
</li><li>
<a href="04_data.html#Apache%20Parquet">Apache Parquet</a>

<ul>
<li>
<a href="04_data.html#Formato%20Columnar">Formato Columnar</a>

<ul>
<li>
<a href="04_data.html#Ventajas">Ventajas</a>

</li></ul>
</li><li>
<a href="04_data.html#Apache%20Parquet">Apache Parquet</a>

<ul>
<li>
<a href="04_data.html#Estructura%20de%20un%20Fichero%20Parquet">Estructura de un Fichero Parquet</a>

</li></ul>
</li></ul>
</li></ul>
<hr>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume"><h2 id="Apache Flume" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume">Apache Flume</a></h2></div>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos"><h3 id="Las Fuentes de datos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos">Las Fuentes de datos</a></h3></div>

<p>
Las empresas buscan tener todos sus datos centralizados, sean de la naturaleza que sean. Tal que existirá una única fuente de verdad o Single Source of Truth que permite la consistencia de los datos. Bajo esta premisa nacen los conceptos de Data Lake y Data Warehouse.
</p>

<p>
En un <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos-Data Warehouse"></span><strong id="Data Warehouse">Data Warehouse</strong> los datos deberán adaptarse a una estructura definida antes de poder ser guardados. Se utilizan los procesos <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos-ETL"></span><strong id="ETL">ETL</strong> de Extracción, Transformación y Carga para adaptar los datos y su formato a la estructura definida antes de volcarlos.
</p>

<p>
En un <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos-Data Lake"></span><strong id="Data Lake">Data Lake</strong> los datos se guardan tal cual se reciben, pero pueden ser transformados antes de su extracción. Este utiliza procesos <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Las Fuentes de datos-ELT"></span><strong id="ELT">ELT</strong> en los que primero se realiza la carga y cuando queramos extraer información útil de dichos datos, realizaremos una transformación en caso de ser necesario.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Batch vs Streaming"><h3 id="Batch vs Streaming" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Batch vs Streaming">Batch vs Streaming</a></h3></div>

<p>
El <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Batch vs Streaming-Procesamiento Batch"></span><strong id="Procesamiento Batch">Procesamiento Batch</strong> se da cuando los datos a procesar se encuentran en un almacén de datos estático y estes son finitos, tal que se escogen y procesan por lotes.
</p>

<p>
El <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Batch vs Streaming-Procesamiento Streaming"></span><strong id="Procesamiento Streaming">Procesamiento Streaming</strong> es el procesamiento sobre datos que fluyen a través de un sistema, conforme se van añadiendo al mismo. Estes datos no son finitos y la toma de decisiones sobre ellos se hace en tiempo real.
</p>

<p>
A continuación mostramos las diferencias entre los dos tipos de procesamiento:
</p>

<table>
<thead>
<tr>
<th>
&nbsp;
</th>
<th>
Procesamiento Batch
</th>
<th>
Procesamiento Streaming
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Hardware
</td>
<td>
Los recursos deben ser capaces de procesar y almacenar grandes conjuntos de datos
</td>
<td>
Los datos tienen menor tamaño, por lo que los requisitos computacionales y el almacenamiento puede ser menor
</td>
</tr>
<tr>
<td>
Latencia
</td>
<td>
La latencia puede ser de minutos, horas o días.
</td>
<td>
La latencia debe ser en segundos o milisegundos.
</td>
</tr>
<tr>
<td>
Tamaño del conjunto de datos
</td>
<td>
Grandes lotes de datos.
</td>
<td>
Un paquete de datos o varios de ellos, siempre de tamaño menor.
</td>
</tr>
<tr>
<td>
Análisis
</td>
<td>
Cálculo complejo y análisis en un marco temporal más amplio.
</td>
<td>
Informes o cálculos simples sobre los datos.
</td>
</tr>
</tbody>
</table>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos"><h3 id="Herramientas para la Inyección y Extracción de Datos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos">Herramientas para la Inyección y Extracción de Datos</a></h3></div>

<p>
A continuación comentaremos las herramientas que nos permiten la ingesta y extracción de los datos de forma masiva.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Scoop"><h4 id="Apache Scoop" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Scoop">Apache Scoop</a></h4></div>

<p>
Está pensado para la transferencia de datos desde un almacén estructurado a otro y utiliza procesamiento por lotes. Sin embargo, desde junio de 2021 el proyecto Sqoop dejó de tener continuidad. 
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume"><h4 id="Apache Flume" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume">Apache Flume</a></h4></div>

<p>
Flume es un software para la ingesta de datos masivos en streaming. Fue presentado por Cloudera en el año 2010 y posteriormente se incorporó bajo licencia Apache como Open Source a la Fundación Apache.
</p>

<p>
Flume está basado en el flujo de datos en streaming de eventos sencillos y permite la lectura y escritura de múltiples fuentes de datos. Además de ello, Flume tiene mecanismos que aseguran la fiabilidad y confiabilidad de los datos.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas"><h5 id="Ventajas" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas">Ventajas</a></h5></div>

<ul>
<li>
Puede manejar <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas-grandes volúmenes de datos"></span><strong id="grandes volúmenes de datos">grandes volúmenes de datos</strong> eficientemente distribuyendo la carga entre múltiples agentes.

</li><li>
Presenta una <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas-gran flexibilidad"></span><strong id="gran flexibilidad">gran flexibilidad</strong>, ya que nos permite recoger datos de diversas fuentes sin atender a su formato.

</li><li>
Se <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas-integra perfectamente"></span><strong id="integra perfectamente">integra perfectamente</strong> con el Ecosistema Hadoop.

</li><li>
Presenta <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Ventajas-tolerancia a fallos"></span><strong id="tolerancia a fallos">tolerancia a fallos</strong>.

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Desventajas"><h5 id="Desventajas" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Herramientas para la Inyección y Extracción de Datos-Apache Flume-Desventajas">Desventajas</a></h5></div>

<ul>
<li>
Puede resultar difícil configurar los parámetros adecuados para los agentes. Esto puede derivar en un fenómeno conocido como Backpressure que ocurre cuando el volumen de datos entrantes supera a la cantidad de datos que pueden ser consumidos por Flume dando lugar a pérdida de eventos y por lo tanto de información, para evitar esto hay que configurar adecuadamente a los agentes.

</li><li>
Está estrechamente ligado a la ingesta de datos en Hadoop, para la ingesta en otro tipo de sistemas podemos utilizar herramientas como Kafka. 

</li><li>
No ofrece herramientas para el monitoreo y diagnóstico de errores de forma clara para el usuario.

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume"><h3 id="Arquitectura de Apache Flume" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume">Arquitectura de Apache Flume</a></h3></div>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Agentes"><h4 id="Agentes" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Agentes">Agentes</a></h4></div>

<p>
Se trata de un conjunto de componentes independientes que dirigen los eventos desde la entrada a la salida. Además los agentes pueder recibir y enviarse datos entre sí. Un agente está compuesto de tres componentes fundamentales:
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Agentes-Source"></span><strong id="Source">Source</strong>: es el punto de entrada de datos de un agente. Cada source es configurada para leer datos desde un lugar o ubicación específica y enviarlos al channel del agente.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Agentes-Channels"></span><strong id="Channels">Channels</strong>: es el lugar temporal donde los datos llegan desde el source y se procesan o no, dependiendo del caso de uso, para transmitirlos al destino final (sink).

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Agentes-Sinks"></span><strong id="Sinks">Sinks</strong>: Son los encargados de leer los datos de los canales y enviarlos al siguiente componente del sistema, que será u otro agente, o el destino final. Si los datos son consumidos por los sinks se eliminan de los canales.

</li></ul>
<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/3C_1C/ICPDM/apache_flume_arch.png" alt="Apache Flume Architecture">
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Eventos"><h4 id="Eventos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Eventos">Eventos</a></h4></div>

<p>
Se trata de una unidad de "dato". El dato es extraído por la fuente, enviado y procesado por el canal y consumido por el sink o sumidero.
</p>

<p>
Un evento se compone de dos partes:
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Eventos-Cabecera"></span><strong id="Cabecera">Cabecera</strong>: registra información de metadata mediante pares clave-valor

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Eventos-Datos"></span><strong id="Datos">Datos</strong>: son almacenados en forma de array en el cuerpo de un evento.

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Flujo de Datos"><h4 id="Flujo de Datos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Flume-Arquitectura de Apache Flume-Flujo de Datos">Flujo de Datos</a></h4></div>

<p>
El flujo de datos describe el recorrido de los eventos desde el comienzo hasta el destino final.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro"><h2 id="Apache Avro" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro">Apache Avro</a></h2></div>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización"><h3 id="Serialización/Deserialización" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización">Serialización/Deserialización</a></h3></div>

<p>
La <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización-serialización"></span><strong id="serialización">serialización</strong> de un objeto consiste en la conversión de dicho objeto a un formato que puede ser transmitido y almacenado de forma eficiente (p.ej. binario o JSON). La conversión inversa se denomina <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización-deserialización"></span><strong id="deserialización">deserialización</strong>.
</p>

<p>
Este proceso se lleva a cabo por los siguientes motivos:
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización-Almacenamiento"></span><strong id="Almacenamiento">Almacenamiento</strong>: generalmente queremos almacenar los datos de una forma eficiente (minimizando su tamaño).

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización-Transmisión"></span><strong id="Transmisión">Transmisión</strong>: en sistemas distribuidos es necesario el paso de datos entre sistemas, por lo tanto necesitamos que los datos esten en un formato que optmice su envío y asegure la reconstrucción de los datos.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Serialización/Deserialización-Intercambio de datos"></span><strong id="Intercambio de datos">Intercambio de datos</strong>: La serialización y deserialización permite intercambiar información manteniendo la integridad de los datos, independientemente del lenguaje de programación.

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Formatos"><h3 id="La Evolución de los Formatos" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Formatos">La Evolución de los Formatos</a></h3></div>

<p>
Apache Avro pertenece a la Fundación Apache y se integra perfectamente con el ecosistema Hadoop. Avro es un formato que contiene tanto los datos como el <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Formatos-esquema"></span><strong id="esquema">esquema</strong> que deben de seguir los datos.
</p>

<p>
En comparación con JSON, este no fuerza al uso de un esquema de los datos y su tamaño aumenta cuando existen claves repetidas.
</p>

<p>
Avro specifica dos formas de llevar a cabo la serialización: 
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Formatos-Binaria"></span><strong id="Binaria">Binaria</strong>: resulta en ficheros más pequeños.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Formatos-JSON"></span><strong id="JSON">JSON</strong>: mejor para depurar y para comunicación en entornos web.

</li></ul>
<p>
Avro specifica un orden de datos estándar.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Estructura"><h3 id="Estructura" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Estructura">Estructura</a></h3></div>

<p>
Un fichero está conformado por dos componentes:
</p>

<ul>
<li>
Cabecera: contiene metadatos acerca del esquema de los datos entre otras cosas.

</li><li>
Uno o más bloques que pueden contener metadatos o datos. En todo fichero hay al menos un bloque de metadatos (ver Block 1 en la siguiente imagen).

</li></ul>
<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/3C_1C/ICPDM/avro_file_structure.png" alt="File Structure">
</p>

<p>
Avro sigue un formato basado en filas, tal que se agupan los datos en grupos de filas.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas"><h3 id="La Evolución de los Esquemas" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas">La Evolución de los Esquemas</a></h3></div>

<p>
Avro se puede adaptar a la evolución de los esquemas. En concreto tenemos dos tipos de esquema:
</p>

<ul>
<li>
Esquema de <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas-escritura"></span><strong id="escritura">escritura</strong>: se utiliza en la serialización.

</li><li>
Esquema de <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas-lectura"></span><strong id="lectura">lectura</strong>: se utiliza en la deserialización.

</li></ul>
<p>
Existen los siguientes tipos de compatibilidad:
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas-Compatibilidad hacia delante"></span><strong id="Compatibilidad hacia delante">Compatibilidad hacia delante</strong>: un nuevo esquema puede leer datos escritos con un esquema anterior.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas-Compatibilidad hacia atrás"></span><strong id="Compatibilidad hacia atrás">Compatibilidad hacia atrás</strong>: un esquema antiguo puede leer datos escritos con un nuevo esquema.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-La Evolución de los Esquemas-Compatibilidad completa"></span><strong id="Compatibilidad completa">Compatibilidad completa</strong>: combina la compatibilidad hacia delante y hacia atrás.

</li></ul>
<p>
Para asegurar la compatibilidad de los esquema tenemos diversas reglas:
</p>

<ul>
<li>
Al añadir campos estes siempre deben de tener un valor por defecto.

</li><li>
Sólo se pueden eliminar campo si estes no son necesarios para las aplicaciones que leen/escriben el esquema.

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Ventajas"><h3 id="Ventajas" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Ventajas">Ventajas</a></h3></div>

<ul>
<li>
La compresión de los datos es automática

</li><li>
Está totalmente tipado

</li><li>
Los ficheros contienen tanto los datos como la definición del esquema

</li><li>
Los datos pueden ser procesado por casi cualquier lenguaje

</li><li>
Permite la evolución sencilla de la definición del esquema de los datos

</li><li>
Permite el paso de datos entre sistemas escritos en distintos lenguajes

</li></ul>
<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Ejemplo de Esquema"><h3 id="Ejemplo de Esquema" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Avro-Ejemplo de Esquema">Ejemplo de Esquema</a></h3></div>

<p>
Vamos a ver cómo definir un esquema para los siguientes datos, uno está almacenado como un archivo CSV, mientras que el otro está definido como un fichero JSON.
</p>

<pre csv="">name, email, age
Ana,ana@avro.com,34
Juan,juan@avro.com,28
Alvaro,alvaro@avro.com,35
Maria,maria@avro.com,25
Luis,luis@avro.com,30
</pre>

<pre json="">[
     {"name": "David", "email": "david@avro.com", "age": 28},
     {"name": "Pablo", "email": "pablo@avro.com", "age": 31},
     {"name": "Ines", "email": "ines@avro.com", "age": 34}
]
</pre>

<p>
Definimos un esquema que define un objeto, en concreto un usuario. Por lo tanto, el esquema será de tipo record. Los esquemas se escriben en formato JSON, y hay que indicar por cada campo el nombre de éste y el tipo de datos asociado.
</p>

<pre json="">{
     "type": "record",
     "name": "User",
     "fields": [
         {"name": "name", "type": "string"},
         {"name": "email", "type": "string"},
         {"name": "age", "type": "int"},
     ]
}
</pre>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet"><h2 id="Apache Parquet" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet">Apache Parquet</a></h2></div>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Formato Columnar"><h3 id="Formato Columnar" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Formato Columnar">Formato Columnar</a></h3></div>

<p>
Los formatos que habíamos estado tratando guardan los datos en filas (CSV, Avro, JSON), los formatos de archivos columnares guardan los datos en columnas. Lo formatos más populares son Apace Parquet y Apache OCR.
</p>

<p>
Por ejemplo, dada la siguiente tabla:
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/3C_1C/ICPDM/apache_parquet_table.png" alt="Example Table">
</p>

<p>
La siguiente imagen nos ilustra las diferencia entre guardarla en formato fila y en formato columna:
</p>

<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/3C_1C/ICPDM/apache_parquet_row_vs_col.png" alt="Row vs Column Format">
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Formato Columnar-Ventajas"><h4 id="Ventajas" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Formato Columnar-Ventajas">Ventajas</a></h4></div>

<p>
El formato columnar presenta ventajas a la hora de llevar a cabo agrupaciones en grandes conjuntos de datos. Si utilizamos el formato basado en filas deberemos de recorrer toda la tabla, sin embargo con el formato columnar sólo es necesario leer el campo por el cual se lleva a cabo la agrupación.
</p>

<p>
Por el mismo motivo este tipo de formato puede suponer menos costes (p.ej. si están alojados en el cloud). Generalmente no sólo se cobra por el almacenamiento si no también por la consulta y el uso de los datos. Tal que si sólo queremos consultar una columna, con un formato basado en filas de nuevo deberemos de consultar la fila completa, mientras que el formato columnar nos permite sólo obtener el campo consultado.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet"><h3 id="Apache Parquet" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet">Apache Parquet</a></h3></div>

<p>
Parquet es un tipo de fichero en formato columnar diseñado para realizar consultas eficientes sobre las columnas. Además permite estructuras complejas de datos anidados y ofrece esquemas de compresión muy eficientes. Por ello es popular en campos como el Big Data, ya que a parte de su eficiencia reduce costes.
</p>

<p>
Parquet está sostenido y mantenido en la Fundación Apache y, por tanto, está disponible para cualquier proyecto bajo licencia Apache.
</p>

<div id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet"><h4 id="Estructura de un Fichero Parquet" class="header"><a href="#Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet">Estructura de un Fichero Parquet</a></h4></div>

<p>
Los ficheros Parquet tienen una <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-estructura jerarquizada"></span><strong id="estructura jerarquizada">estructura jerarquizada</strong>: encabezado, metadatos y grupos de filas.
</p>

<ul>
<li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-Encabezado"></span><strong id="Encabezado">Encabezado</strong>: contiene información necesaria para la lectura del fichero. <span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-Magic number"></span><strong id="Magic number">Magic number</strong> se trata de un número al final y al principio del fichero que asegura que está completo y no corrupto.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-Metadatos"></span><strong id="Metadatos">Metadatos</strong>: contienen información sobre los datos; el esquema de las columnas, tipos de datos, algunas estadísticas y otros datos relevantes.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-Grupos de filas"></span><strong id="Grupos de filas">Grupos de filas</strong>: contienen los datos organizados por columnas. Cada grupo de filas tiene un subconjunto de la tabla, donde las columnas de cada grupo de filas se dividen en páginas, que son las unidades más pequeñas de almacenamiento. La subdivisión en páginas tiene como fin mejorar la eficiencia ya que admiten la compresión individual. Podemos encontrar páginas de datos, índices o de diccionario.

</li><li>
<span id="Inyección/Extracción y Serialización/Deserialización de los Datos-Apache Parquet-Apache Parquet-Estructura de un Fichero Parquet-Esquema"></span><strong id="Esquema">Esquema</strong>: define la estructura de los datos y los tipos de datos de cada columna. En caso de que los datos estén anidados también define la jerarquía de estos.

</li></ul>
<p>
<img src="https://albamr09.github.io/public/images/DataScience/Master/3C_1C/ICPDM/apache_parquet_file_structure.png" alt="Apache Parquet File Structure">
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>