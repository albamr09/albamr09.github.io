<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Librerías/Componentes de Spark</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Librerías/Componentes de Spark"><h1 id="Librerías/Componentes de Spark" class="header"><a href="#Librerías/Componentes de Spark">Librerías/Componentes de Spark</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="03_lib_spark.html#Spark%20SQL">Spark SQL</a>

<ul>
<li>
<a href="03_lib_spark.html#Spark%20SQL%20Architecture">Spark SQL Architecture</a>

</li><li>
<a href="03_lib_spark.html#Spark%20SQL%20Evolution">Spark SQL Evolution</a>

</li><li>
<a href="03_lib_spark.html#Spark%20SQL%20Programming">Spark SQL Programming</a>

</li><li>
<a href="03_lib_spark.html#Example%20Workflow">Example Workflow</a>

</li><li>
<a href="03_lib_spark.html#Important%20Points">Important Points</a>

</li><li>
<a href="03_lib_spark.html#Code%20Examples%20in%20Spark%20SQL">Code Examples in Spark SQL</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#Machine%20Learning%20with%20Spark%20ML%20Pipelines">Machine Learning with Spark ML Pipelines</a>

<ul>
<li>
<a href="03_lib_spark.html#Spark%20for%20Machine%20Learning">Spark for Machine Learning</a>

</li><li>
<a href="03_lib_spark.html#ML%20Pipelines%3A%20Addressing%20the%20Data%20Pipeline%20in%20ML">ML Pipelines  Addressing the Data Pipeline in ML</a>

</li><li>
<a href="03_lib_spark.html#Structure%20of%20Spark%20ML%20APIs">Structure of Spark ML APIs</a>

</li><li>
<a href="03_lib_spark.html#Code%20Examples%20from%20Spark%20Machine%20Learning">Code Examples from Spark Machine Learning</a>

<ul>
<li>
<a href="03_lib_spark.html#Basic%20Statistics%20with%20Spark%20Datasets">Basic Statistics with Spark Datasets</a>

</li><li>
<a href="03_lib_spark.html#Linear%20Regression%20with%20Data%20Transformation%20and%20Model%20Evaluation">Linear Regression with Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Classification%20with%20Decision%20Tree%2C%20Data%20Transformation%20and%20Model%20Evaluation">Classification with Decision Tree  Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Clustering%20with%20K-means%2C%20Data%20Transformation%20and%20Model%20Evaluation">Clustering with K-means  Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Recommendation%20with%20ALS%2C%20Data%20Transformation%20and%20Model%20Evaluation">Recommendation with ALS  Data Transformation and Model Evaluation</a>

</li></ul>
</li></ul>
</li></ul>
<hr>

<div id="Librerías/Componentes de Spark-Spark SQL"><h2 id="Spark SQL" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL">Spark SQL</a></h2></div>

<p>
Spark SQL is an important feature in the Spark ecosystem that allows integration with different data sources and other subsystems, such as visualization. Spark SQL is not meant to replace SQL databases, but rather to complement Spark's data wrangling and input capabilities by providing a versatile query interface for Spark data. This ability to scale complex data operations is only valuable if the results can be used flexibly, which is what Spark SQL achieves.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Architecture"><h3 id="Spark SQL Architecture" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Architecture">Spark SQL Architecture</a></h3></div>

<p>
Spark SQL's architecture is layered, with each layer performing specific functions.
</p>

<ul>
<li>
The bottom layer is the data access layer, which works with multiple formats and typically utilizes a distributed filesystem such as HDFS.

</li><li>
The computation layer leverages the distributed processing power of the Spark engine, including its streaming capabilities, and typically operates on RDDs (Resilient Distributed Datasets).

</li><li>
The Dataset/DataFrame layer provides the API for interacting with the data.

</li><li>
Spark SQL sits on top of this layer, providing data access for various applications, dashboards, and BI tools.

</li></ul>
<p>
This architecture allows Spark to leverage the vast knowledge base of SQL among data professionals and use it to query Spark data.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Evolution"><h3 id="Spark SQL Evolution" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Evolution">Spark SQL Evolution</a></h3></div>

<p>
Prior to Spark 2.0, SchemaRDD was at the heart of Spark SQL. It essentially attached a schema to an RDD, enabling SQL queries to be run on RDDs. However, with Spark 2.0, Datasets became the primary way to work with data. Datasets offer the advantages of both RDDs and strong typing, providing a more robust and efficient way to handle data. In languages like Python and R, which lack compile-time type checking, Datasets and DataFrames are merged and referred to as DataFrames.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Programming"><h3 id="Spark SQL Programming" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Programming">Spark SQL Programming</a></h3></div>

<p>
Spark 2.0 introduced <code>sparkSession</code>, which replaced <code>sqlcontext</code>, <code>hivecontext</code>, and other components. The <code>sparkSession</code> instance has a versatile <code>read</code> method capable of handling various data formats like CSV, Parquet, JSON, and JDBC. This method allows you to specify format-related options such as headers and delimiters.
</p>

<p>
To use Spark SQL, you first need to create a Dataset by reading data from a source and informing Spark about its structure and types. You can then apply SQL statements to query the data. To create a view that can be queried using SQL, you can use the <code>createOrReplaceTempView</code> method. You can then use SQL statements to filter, join, and aggregate data within these views.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Example Workflow"><h3 id="Example Workflow" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Example Workflow">Example Workflow</a></h3></div>

<p>
A typical Spark SQL workflow involves:
</p>

<ul>
<li>
Defining a case class to represent the data structure.

</li><li>
Reading the data file using <code>sparkSession.read</code>, specifying options like <code>header</code> and <code>inferSchema</code>.

</li><li>
Creating a Dataset with the case class as its element type.

</li><li>
Creating a temporary view using <code>createOrReplaceTempView</code> for SQL access.

</li><li>
Running SQL queries on the view using <code>spark.sql</code>.

</li><li>
Displaying and analyzing the results using methods like <code>show</code>, <code>head</code>, and <code>orderBy</code>

</li></ul>
<div id="Librerías/Componentes de Spark-Spark SQL-Important Points"><h3 id="Important Points" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Important Points">Important Points</a></h3></div>

<ul>
<li>
Spark 2.0 simplified Spark SQL by introducing Datasets and <code>sparkSession</code>.

</li><li>
You can start the Spark shell with the <code>-deprecation</code> flag to receive messages about deprecated methods.

</li><li>
The read method can infer schema automatically using the <code>inferSchema</code> option.

</li><li>
Use <code>createOrReplaceTempView</code> to avoid the <code>TempTableAlreadyExists</code> exception.

</li><li>
Spark SQL enables complex queries involving multiple tables and various operations like filtering, joining, and aggregation.

</li></ul>
<div id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL"><h3 id="Code Examples in Spark SQL" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL">Code Examples in Spark SQL</a></h3></div>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 1: Loading a CSV File into a Dataset"></span><strong id="Example 1: Loading a CSV File into a Dataset">Example 1: Loading a CSV File into a Dataset</strong>
</p>

<pre java="">case class Employee(EmployeeID : String,   LastName : String, FirstName : String, Title : String,   BirthDate : String, HireDate : String,   City : String, State : String, Zip : String, Country : String,   ReportsTo : String)
// ... ...
val filePath = "/Users/ksankar/fdps-v3/"
println(s"Running Spark Version ${sc.version}")
// val employees = spark.read.option("header","true"). csv(filePath + "data/NW-Employees.csv").as[Employee] 
println("Employees has "+employees.count()+" rows") 
employees.show(5) 
employees.head()
</pre>

<p>
This code snippet first defines a case class called <code>Employee</code> representing the structure of the employee data. Then, it sets a <code>filePath</code> variable pointing to the directory containing the data files. The code then uses the <code>spark.read.csv</code> method to read the CSV file into a Dataset called <code>employees</code>. The <code>option("header", "true")</code> tells Spark that the first row of the CSV file contains column headers. The <code>.as[Employee]</code> part specifies that the Dataset should be composed of <code>Employee</code> objects. Finally, the code prints the number of rows in the Dataset, displays the first \(5\) rows using <code>show(5)</code>, and retrieves the first row using <code>head()</code>.
</p>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 2: Creating a View and Running SQL Queries"></span><strong id="Example 2: Creating a View and Running SQL Queries">Example 2: Creating a View and Running SQL Queries</strong>
</p>

<pre java="">employees.createOrReplaceTempView("EmployeesTable")
var result = spark.sql("SELECT * from EmployeesTable")
result.show(5)
result.head(3)
// employees.explain(true)

result = spark.sql("SELECT * from EmployeesTable WHERE State = 'WA'")
result.show(5)
result.head(3)
// result.explain(true)
</pre>

<p>
This code creates a temporary view called "<code>EmployeesTable</code>" from the employees Dataset using <code>createOrReplaceTempView</code>. This view enables you to query the Dataset using SQL statements. The first <code>spark.sql</code> statement selects all columns from the "<code>EmployeesTable</code>" view. The second query filters the results to include only employees from the state of Washington (<code>WHERE State = 'WA'</code>). Both queries use <code>show(5)</code> to display the first \(5\) rows of the result and <code>head(3)</code> to retrieve the first \(3\) rows.
</p>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 3: Handling Multiple Tables and Joins"></span><strong id="Example 3: Handling Multiple Tables and Joins">Example 3: Handling Multiple Tables and Joins</strong>
</p>

<pre java="">// ... ...
val orders = spark.read.option("header","true"). 
option("inferSchema","true"). 
csv(filePath + "data/NW-Orders.csv").as[Order] 
println("Orders has "+orders.count()+" rows") 
orders.show(5)
orders.head() 
orders.dtypes
// ... ...
// // Now the interesting part // 
result = spark.sql("SELECT OrderDetailsTable.OrderID, ShipCountry, UnitPrice, Qty, Discount FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID")
result.show(10) 
result.head(3) 
// // Sales By Country // 
result = spark.sql("SELECT ShipCountry, SUM(OrderDetailsTable.UnitPrice * Qty * Discount) AS ProductSales FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID GROUP BY ShipCountry")
result.count() 
result.show(10) 
result.head(3) 
result.orderBy($"ProductSales".desc).show(10) // Top 10 by Sales
</pre>

<p>
This example demonstrates loading the "<code>Orders</code>" table, creating a view, and then performing joins and aggregations. It first reads the "<code>NW-Orders.csv</code>" file into an orders Dataset. Notably, it uses the <code>option("inferSchema", "true")</code> option, which tells Spark to automatically infer the schema for the data. This eliminates the need to define a case class beforehand.
</p>

<p>
The code then executes two SQL queries. The first query performs an inner join between the "<code>OrdersTable</code>" and "<code>OrderDetailsTable</code>" based on the common "<code>OrderID</code>" column and selects specific columns from the joined result. The second query calculates total sales (<code>SUM(OrderDetailsTable.UnitPrice * Qty * Discount)</code>) per country, groups the results by "<code>ShipCountry</code>", and orders the final output by "<code>ProductSales</code>" in descending order to show the top \(10\) countries by sales.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines"><h2 id="Machine Learning with Spark ML Pipelines" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines">Machine Learning with Spark ML Pipelines</a></h2></div>

<p>
This section provides a summary of Spark's capabilities for Machine Learning (ML), focusing on ML Pipelines and the transition from MLlib to ML APIs. It covers various ML algorithms, data transformation techniques, and the concept of pipelines for streamlined ML workflows.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Spark for Machine Learning"><h3 id="Spark for Machine Learning" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Spark for Machine Learning">Spark for Machine Learning</a></h3></div>

<p>
Spark is attractive for ML due to its ability to handle massive computations. Spark 2.0.0 onwards, Spark is considered a leading platform for building ML algorithms and applications.
</p>

<p>
Spark's ML capabilities are primarily accessed through the <code>org.apache.spark.ml</code> package for Scala and Java, and <code>pyspark.ml</code> for Python. Spark supports a wide array of ML algorithms, including basic statistics, linear regression, classification, clustering, recommendation systems, dimensionality reduction, feature extraction, and more.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-ML Pipelines: Addressing the Data Pipeline in ML"><h3 id="ML Pipelines: Addressing the Data Pipeline in ML" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-ML Pipelines: Addressing the Data Pipeline in ML">ML Pipelines: Addressing the Data Pipeline in ML</a></h3></div>

<p>
Before Spark 1.6.0, the MLlib APIs operated on RDDs, but they lacked support for the data pipelines inherent in ML. With the introduction of DataFrames and Datasets, MLlib evolved into the ML pipeline framework, offering more capabilities and addressing the entire ML workflow.
</p>

<p>
MLlib APIs are now in maintenance mode and will eventually be deprecated. While you should use ML APIs going forward, some functionalities might require using MLlib and converting the output RDD to a DataFrame for further processing with ML APIs.
</p>

<p>
A typical ML process involves several steps:
</p>
<ol>
<li>
Data Acquisition: Obtain data from internal or external sources, ensuring anonymity and removal of personally identifiable information (PII).

</li><li>
Data Transformation: Convert raw data into a usable format, for example, transforming a CSV file into a DataFrame.

</li><li>
Feature Extraction: Extract relevant features from the data, such as separating text into words or normalizing them.

</li><li>
Data Splitting: Divide the data into training and testing sets, using appropriate strategies based on data characteristics like time series or class imbalance.

</li><li>
Model Training:

<ul>
<li>
Fit the training data to different ML models.

</li><li>
Tune hyperparameters for optimal performance.

</li><li>
Select the best-performing model for the specific problem.

</li></ul>
</li><li>
Model Evaluation: Assess the model's performance using the test data.

</li><li>
Model Deployment: Implement the trained model in a production environment for real-time predictions.

</li></ol>
<p>
ML pipelines in Spark address all stages of this workflow.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Structure of Spark ML APIs"><h3 id="Structure of Spark ML APIs" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Structure of Spark ML APIs">Structure of Spark ML APIs</a></h3></div>

<p>
Spark ML APIs have a specific structure that can be challenging to navigate initially. Familiarity with this structure is key to effectively utilizing Spark for ML tasks. The source material provides a diagram to illustrate this, recommending a deeper understanding of the pipeline concept to enhance proficiency in using Spark ML classes.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning"><h3 id="Code Examples from Spark Machine Learning" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning">Code Examples from Spark Machine Learning</a></h3></div>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Basic Statistics with Spark Datasets"><h4 id="Basic Statistics with Spark Datasets" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Basic Statistics with Spark Datasets">Basic Statistics with Spark Datasets</a></h4></div>

<p>
This code snippet shows how to load car mileage data from a CSV file, compute basic statistics using Spark Datasets, and calculate the correlation and covariance between specific variables:
</p>

<pre java="">val spark = SparkSession.builder       
  .master("local")       
  .appName("Chapter 11")       
  .config("spark.logConf","true")       
  .config("spark.logLevel","ERROR")       
  .getOrCreate()   

println(s"Running Spark Version ${spark.version}")   

val filePath = "/Users/ksankar/fdps-v3/"   

val cars = spark.read.option("header","true").option("inferSchema","true")
  .csv(filePath + "data/car-data/car-milage.csv") 

println("Cars has "+cars.count()+" rows") 

cars.show(5) 

cars.printSchema()

// Computing statistics

cars.describe("mpg","hp","weight","automatic").show()  

var cor = cars.stat.corr("hp","weight")   
println("hp to weight : Correlation = %2.4f".format(cor))   

var cov = cars.stat.cov("hp","weight")   
println("hp to weight : Covariance = %2.4f".format(cov))   

cor = cars.stat.corr("RARatio","width")   
println("Rear Axle Ratio to width : Correlation = %2.4f".format(cor))   

cov = cars.stat.cov("RARatio","width")   
println("Rear Axle Ratio to width : Covariance = %2.4f".format(cov))
</pre>

<ol>
<li>
SparkSession Creation: The code starts by creating a SparkSession, which is the entry point for Spark applications.

</li><li>
Data Loading: The <code>spark.read.csv()</code> method loads data from the specified CSV file into a DataFrame named cars. The <code>option("header","true")</code> indicates that the first row contains column headers, and <code>option("inferSchema","true")</code> instructs Spark to automatically infer the data types for each column.

</li><li>
Basic Statistics: The <code>describe()</code> method computes summary statistics like <code>count</code>, <code>mean</code>, <code>standard deviation</code>, <code>min</code>, and <code>max</code> for the specified columns ("mpg", "hp", "weight", "automatic").

</li><li>
Correlation and Covariance: The <code>stat.corr()</code> and <code>stat.cov()</code> methods are used to calculate the correlation and covariance between pairs of variables ("hp" and "weight", "RARatio" and "width").

</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Linear Regression with Data Transformation and Model Evaluation"><h4 id="Linear Regression with Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Linear Regression with Data Transformation and Model Evaluation">Linear Regression with Data Transformation and Model Evaluation</a></h4></div>

<p>
This code example demonstrates a linear regression model using Spark ML Pipelines. It includes data transformation, feature extraction, splitting data into training and testing sets, fitting the model, making predictions, and evaluating the model:
</p>

<pre java="">// Data Transformation and Feature Extraction
val cars1 = cars.na.drop() 

val assembler = new VectorAssembler() 
assembler.setInputCols(Array("displacement", "hp", "torque", "CRatio", "RARatio", "CarbBarrells", "NoOfSpeed", "length", "width", "weight", "automatic")) 
assembler.setOutputCol("features") 

val cars2 = assembler.transform(cars1)
cars2.show(40)

// Data Split
val train = cars2.filter(cars1("weight") &lt;= 4000) 
val test = cars2.filter(cars1("weight") &gt; 4000) 
test.show() 
println("Train = "+train.count()+" Test = "+test.count())

// Linear Regression Model
val algLR = new LinearRegression()     
algLR.setMaxIter(100)     
algLR.setRegParam(0.3)     
algLR.setElasticNetParam(0.8)     
algLR.setLabelCol("mpg")     

val mdlLR = algLR.fit(train)     
println(s"Coefficients: ${mdlLR.coefficients} Intercept: ${mdlLR.intercept}")     

val trSummary = mdlLR.summary 
println(s"numIterations: ${trSummary.totalIterations}") 
println(s"Iteration Summary History: ${trSummary.objectiveHistory.toList}") 
trSummary.residuals.show() 
println(s"RMSE: ${trSummary.rootMeanSquaredError}") 
println(s"r2: ${trSummary.r2}")

// Predictions
val predictions = mdlLR.transform(test) 
predictions.show()

// Model Evaluation
val evaluator = new RegressionEvaluator() 
evaluator.setLabelCol("mpg") 
val rmse = evaluator.evaluate(predictions) 
println("Root Mean Squared Error = "+"%6.3f".format(rmse)) 

val mse = evaluator.evaluate(predictions) 
println("Mean Squared Error = "+"%6.3f".format(mse))
</pre>

<ol>
<li>
Data Preprocessing:

<ul>
<li>
<code>cars.na.drop()</code> removes rows with missing values (NA).

</li><li>
<code>VectorAssembler</code> combines multiple input columns into a single vector column named "features", which is required for many ML algorithms.

</li></ul>
</li><li>
Data Splitting: The data is split into training and test sets based on the "weight" column.

</li><li>
Linear Regression:

<ul>
<li>
A <code>LinearRegression</code> object is created and configured with parameters like maximum iterations (<code>setMaxIter</code>), regularization parameter (<code>setRegParam</code>), and elastic net parameter (<code>setElasticNetParam</code>).

</li><li>
The model is trained using the <code>fit()</code> method on the training data.

</li><li>
Model coefficients, intercept, and training summary are printed.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data using the <code>transform()</code> method, adding a "predictions" column to the resulting DataFrame.

</li><li>
Model Evaluation:

<ul>
<li>
A <code>RegressionEvaluator</code> is used to calculate RMSE and MSE.

</li><li>
The <code>evaluate()</code> method calculates the metrics based on the "mpg" label column and the predicted values.

</li></ul>
</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Classification with Decision Tree, Data Transformation and Model Evaluation"><h4 id="Classification with Decision Tree, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Classification with Decision Tree, Data Transformation and Model Evaluation">Classification with Decision Tree, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example demonstrates a classification task using a decision tree algorithm. It involves data loading, transformation, feature extraction, model training, prediction, and evaluation:
</p>

<pre java="">// Loading Data
val filePath = "/Users/ksankar/fdps-v3/"   
val passengers = spark.read.option("header","true").option("inferSchema","true")
  .csv(filePath + "data/titanic3_02.csv")   
println("Passengers has "+passengers.count()+" rows")   
passengers.show(5)   
passengers.printSchema()

// Data Transformation and Feature Extraction
val passengers1 = passengers.select(
  passengers("Pclass"),
  passengers("Survived").cast(DoubleType).as("Survived"),
  passengers("Gender"),
  passengers("Age"),
  passengers("SibSp"),
  passengers("Parch"),
  passengers("Fare")
) 

passengers1.show(5)

val indexer = new StringIndexer() 
indexer.setInputCol("Gender") 
indexer.setOutputCol("GenderCat") 

val passengers2 = indexer.fit(passengers1).transform(passengers1) 
passengers2.show(5)    

val passengers3 = passengers2.na.drop() 
println("Orig = "+passengers2.count()+" Final = "+ passengers3.count() + " Dropped = "+ (passengers2.count() - passengers3.count()))

val assembler = new VectorAssembler() 
assembler.setInputCols(Array("Pclass", "GenderCat", "Age", "SibSp", "Parch", "Fare")) 
assembler.setOutputCol("features") 
val passengers4 = assembler.transform(passengers3) 
passengers4.show(5)

// Data Split
val Array(train, test) = passengers4.randomSplit(Array(0.9, 0.1)) 
println("Train = "+train.count()+" Test = "+test.count())

// Decision Tree Model
val algTree = new DecisionTreeClassifier() 
algTree.setLabelCol("Survived") 
algTree.setImpurity("gini") 
algTree.setMaxBins(32)
algTree.setMaxDepth(5)     

val mdlTree = algTree.fit(train) 
println("The tree has %d nodes.".format(mdlTree.numNodes)) 
println(mdlTree.toDebugString) 
println(mdlTree.toString) 
println(mdlTree.featureImportances)

// Predictions
val predictions = mdlTree.transform(test) 
predictions.show(5)

// Model Evaluation
val evaluator = new MulticlassClassificationEvaluator() 
evaluator.setLabelCol("Survived") 
evaluator.setMetricName("accuracy") 

val accuracy = evaluator.evaluate(predictions) 
println("Test Accuracy = %.2f%%".format(accuracy*100))
</pre>

<ol>
<li>
Data Loading: The Titanic passenger data is loaded from a CSV file.

</li><li>
Data Transformation and Feature Extraction:

<ul>
<li>
Relevant columns are selected.

</li><li>
<code>StringIndexer</code> converts the categorical "Gender" column into a numerical "GenderCat" column.

</li><li>
Rows with missing values are dropped.

</li><li>
<code>VectorAssembler</code> combines selected features into a "features" vector column.

</li></ul>
</li><li>
Data Splitting: The data is split into training and test sets using <code>randomSplit()</code>.

</li><li>
Decision Tree Model:

<ul>
<li>
A <code>DecisionTreeClassifier</code> is created and configured with parameters like label column, impurity measure ("gini"), maximum bins, and maximum depth.

</li><li>
The model is trained using the <code>fit()</code> method.

</li><li>
Model details like the number of nodes, tree structure, and feature importances are printed.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data.

</li><li>
Model Evaluation:

<ul>
<li>
A <code>MulticlassClassificationEvaluator</code> is used to calculate the accuracy of the model.

</li></ul>
</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Clustering with K-means, Data Transformation and Model Evaluation"><h4 id="Clustering with K-means, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Clustering with K-means, Data Transformation and Model Evaluation">Clustering with K-means, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example illustrates K-means clustering:
</p>

<pre java="">// Loading data
// ... (Code for loading data, similar to previous examples)

// Data Transformation and Feature Extraction
val assembler = new VectorAssembler() 
assembler.setInputCols(Array("X", "Y")) 
assembler.setOutputCol("features") 
val data1 = assembler.transform(data) 
data1.show(5)

// Clustering Model (K=2)
var algKMeans = new KMeans().setK(2) 
var mdlKMeans = algKMeans.fit(data1)

// Predictions
var predictions = mdlKMeans.transform(data1) 
predictions.show(3)     
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-2K.csv")

// Model Evaluation and Interpretation (K=2)
var WSSSE = mdlKMeans.computeCost(data1) 
println(s"Within Set Sum of Squared Errors (K=2) = %.3f".format(WSSSE))

println("Cluster Centers (K=2) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;")) 
println("Cluster Sizes (K=2) : " + mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))

// Clustering Model (K=4)
algKMeans = new KMeans().setK(4) 
mdlKMeans = algKMeans.fit(data1)

// Model Evaluation and Interpretation (K=4)
WSSSE = mdlKMeans.computeCost(data1) 
println(s"Within Set Sum of Squared Errors (K=4) = %.3f".format(WSSSE))

println("Cluster Centers (K=4) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;")) 
println("Cluster Sizes (K=4) : " + mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))

predictions = mdlKMeans.transform(data1) 
predictions.show(30)     
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-4K.csv")
</pre>

<ol>
<li>
Data Loading: Data with two dimensions (X and Y) is loaded.

</li><li>
Data Transformation: <code>VectorAssembler</code> creates a "features" vector column.

</li><li>
Clustering Model:

<ul>
<li>
A <code>KMeans</code> object is created and the number of clusters (\(K\)) is set.

</li><li>
The model is trained using the <code>fit()</code> method.

</li></ul>
</li><li>
Predictions: Cluster assignments for each data point are predicted.

</li><li>
Model Evaluation:

<ul>
<li>
The <code>computeCost()</code> method calculates the Within Set Sum of Squared Errors (WSSE), which is a measure of cluster cohesion.

</li><li>
Cluster centers and cluster sizes are printed.

</li></ul>
</li><li>
Running with Different K: The code runs the clustering with K=2 and K=4, comparing the WSSE and cluster characteristics to illustrate the effect of choosing different values for K.

</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Recommendation with ALS, Data Transformation and Model Evaluation"><h4 id="Recommendation with ALS, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Recommendation with ALS, Data Transformation and Model Evaluation">Recommendation with ALS, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example demonstrates a recommendation system using the Alternating Least Squares (ALS) algorithm
</p>

<pre java="">// Loading data
// ... (Code for loading data from text files, using RDDs and DataFrames)

// Data Transformation and Feature Extraction
// ... (Code for transforming data using RDDs and DataFrames)

// Data Splitting
val Array(train, test) = ratings3.randomSplit(Array(0.8, 0.2)) 
println("Train = "+train.count()+" Test = "+test.count())

// Recommendation Model
val algALS = new ALS() 
algALS.setItemCol("product") 
algALS.setRank(12) 
algALS.setRegParam(0.1) 
algALS.setMaxIter(20) 
val mdlReco = algALS.fit(train)

// Predicting Using the Model
val predictions = mdlReco.transform(test) 
predictions.show(5) 
predictions.printSchema()

// Model Evaluation and Interpretation
val pred = predictions.na.drop() 
println("Orig = "+predictions.count()+" Final = "+ pred.count() + " Dropped = "+ (predictions.count() - pred.count())) 

val evaluator = new RegressionEvaluator() 
evaluator.setLabelCol("rating") 
var rmse = evaluator.evaluate(pred) 
println("Root Mean Squared Error = "+"%.3f".format(rmse)) 

var mse = evaluator.evaluate(pred) 
println("Mean Squared Error = "+"%.3f".format(mse)) 
mse = pred.rdd.map(r =&gt; rowSqDiff(r)).reduce(_+_) / predictions.count().toDouble 
println("Mean Squared Error (Calculated) = "+"%.3f".format(mse))
</pre>

<ol>
<li>
Data Loading: MovieLens data is loaded from text files using RDDs and then converted to DataFrames.

</li><li>
Data Transformation: The data is transformed to a suitable format for the recommendation algorithm.

</li><li>
Data Splitting: The data is split into training and test sets.

</li><li>
Recommendation Model:

<ul>
<li>
An <code>ALS</code> object is created and configured with parameters like rank, regularization parameter, and maximum iterations.

</li><li>
The model is trained on the training data.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data.

</li><li>
Model Evaluation:

<ul>
<li>
Rows with NaN predictions are dropped to address the cold start problem.

</li><li>
RMSE and MSE are calculated using a <code>RegressionEvaluator</code>.

</li><li>
MSE is also calculated manually for demonstration purposes.

</li></ul>
</li></ol>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>