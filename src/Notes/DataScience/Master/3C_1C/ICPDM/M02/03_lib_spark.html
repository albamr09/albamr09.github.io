<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Librerías/Componentes de Spark</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Librerías/Componentes de Spark"><h1 id="Librerías/Componentes de Spark" class="header"><a href="#Librerías/Componentes de Spark">Librerías/Componentes de Spark</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="03_lib_spark.html#Spark%20SQL">Spark SQL</a>

<ul>
<li>
<a href="03_lib_spark.html#Spark%20SQL%20Architecture">Spark SQL Architecture</a>

</li><li>
<a href="03_lib_spark.html#Spark%20SQL%20Evolution">Spark SQL Evolution</a>

</li><li>
<a href="03_lib_spark.html#Spark%20SQL%20Programming">Spark SQL Programming</a>

</li><li>
<a href="03_lib_spark.html#Example%20Workflow">Example Workflow</a>

</li><li>
<a href="03_lib_spark.html#Important%20Points">Important Points</a>

</li><li>
<a href="03_lib_spark.html#Code%20Examples%20in%20Spark%20SQL">Code Examples in Spark SQL</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#Machine%20Learning%20with%20Spark%20ML%20Pipelines">Machine Learning with Spark ML Pipelines</a>

<ul>
<li>
<a href="03_lib_spark.html#Spark%20for%20Machine%20Learning">Spark for Machine Learning</a>

</li><li>
<a href="03_lib_spark.html#ML%20Pipelines%3A%20Addressing%20the%20Data%20Pipeline%20in%20ML">ML Pipelines  Addressing the Data Pipeline in ML</a>

</li><li>
<a href="03_lib_spark.html#Structure%20of%20Spark%20ML%20APIs">Structure of Spark ML APIs</a>

</li><li>
<a href="03_lib_spark.html#Code%20Examples%20from%20Spark%20Machine%20Learning">Code Examples from Spark Machine Learning</a>

<ul>
<li>
<a href="03_lib_spark.html#Basic%20Statistics%20with%20Spark%20Datasets">Basic Statistics with Spark Datasets</a>

</li><li>
<a href="03_lib_spark.html#Linear%20Regression%20with%20Data%20Transformation%20and%20Model%20Evaluation">Linear Regression with Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Classification%20with%20Decision%20Tree%2C%20Data%20Transformation%20and%20Model%20Evaluation">Classification with Decision Tree  Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Clustering%20with%20K-means%2C%20Data%20Transformation%20and%20Model%20Evaluation">Clustering with K-means  Data Transformation and Model Evaluation</a>

</li><li>
<a href="03_lib_spark.html#Recommendation%20with%20ALS%2C%20Data%20Transformation%20and%20Model%20Evaluation">Recommendation with ALS  Data Transformation and Model Evaluation</a>

</li></ul>
</li></ul>
</li><li>
<a href="03_lib_spark.html#GraphX">GraphX</a>

<ul>
<li>
<a href="03_lib_spark.html#Introduction%20to%20Graph%20Processing">Introduction to Graph Processing</a>

</li><li>
<a href="03_lib_spark.html#Graph%20Processing%20Systems">Graph Processing Systems</a>

</li><li>
<a href="03_lib_spark.html#Challenges%20of%20Graph%20Processing">Challenges of Graph Processing</a>

</li><li>
<a href="03_lib_spark.html#Spark%20GraphX">Spark GraphX</a>

<ul>
<li>
<a href="03_lib_spark.html#GraphX%20Architecture">GraphX Architecture</a>

</li><li>
<a href="03_lib_spark.html#GraphX%20Computational%20Model">GraphX Computational Model</a>

</li><li>
<a href="03_lib_spark.html#Building%20Graphs%20with%20GraphX">Building Graphs with GraphX</a>

</li><li>
<a href="03_lib_spark.html#GraphX%20API%20Landscape">GraphX API Landscape</a>

</li><li>
<a href="03_lib_spark.html#Structural%20APIs">Structural APIs</a>

</li><li>
<a href="03_lib_spark.html#Community%20Detection%20and%20Analysis">Community Detection and Analysis</a>

</li><li>
<a href="03_lib_spark.html#GraphX%20Algorithms">GraphX Algorithms</a>

</li><li>
<a href="03_lib_spark.html#Graph%20Parallel%20Computation%20APIs">Graph Parallel Computation APIs</a>

</li><li>
<a href="03_lib_spark.html#Partition%20Strategies">Partition Strategies</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#Case%20Study%3A%20AlphaGo%20Tweet%20Analytics">Case Study  AlphaGo Tweet Analytics</a>

</li><li>
<a href="03_lib_spark.html#Code%20Examples%20in%20the%20GraphX%20Source">Code Examples in the GraphX Source</a>

<ul>
<li>
<a href="03_lib_spark.html#Building%20a%20Simple%20Graph">Building a Simple Graph</a>

<ul>
<li>
<a href="03_lib_spark.html#Defining%20Vertices%20and%20Edges">Defining Vertices and Edges</a>

</li><li>
<a href="03_lib_spark.html#Creating%20RDDs%20and%20the%20Graph">Creating RDDs and the Graph</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#Structural%20APIs">Structural APIs</a>

<ul>
<li>
<a href="03_lib_spark.html#Extracting%20Subgraphs">Extracting Subgraphs</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#Community%20Detection">Community Detection</a>

</li><li>
<a href="03_lib_spark.html#PageRank%20Calculation">PageRank Calculation</a>

</li><li>
<a href="03_lib_spark.html#Aggregate%20Messages%20API">Aggregate Messages API</a>

<ul>
<li>
<a href="03_lib_spark.html#Finding%20the%20Oldest%20Follower">Finding the Oldest Follower</a>

</li><li>
<a href="03_lib_spark.html#Finding%20the%20Oldest%20Followee">Finding the Oldest Followee</a>

</li><li>
<a href="03_lib_spark.html#Calculating%20In-Degree%20and%20Out-Degree">Calculating In-Degree and Out-Degree</a>

</li></ul>
</li><li>
<a href="03_lib_spark.html#AlphaGo%20Tweet%20Analytics">AlphaGo Tweet Analytics</a>

<ul>
<li>
<a href="03_lib_spark.html#Loading%20Data%20and%20Creating%20a%20DataFrame">Loading Data and Creating a DataFrame</a>

</li><li>
<a href="03_lib_spark.html#Mapping%20Data%20to%20Vertices%20and%20Edges">Mapping Data to Vertices and Edges</a>

</li><li>
<a href="03_lib_spark.html#Creating%20the%20Graph%20and%20Running%20Algorithms">Creating the Graph and Running Algorithms</a>

</li></ul>
</li></ul>
</li></ul>
</li></ul>
<hr>

<div id="Librerías/Componentes de Spark-Spark SQL"><h2 id="Spark SQL" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL">Spark SQL</a></h2></div>

<p>
Spark SQL is an important feature in the Spark ecosystem that allows integration with different data sources and other subsystems, such as visualization. Spark SQL is not meant to replace SQL databases, but rather to complement Spark's data wrangling and input capabilities by providing a versatile query interface for Spark data. This ability to scale complex data operations is only valuable if the results can be used flexibly, which is what Spark SQL achieves.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Architecture"><h3 id="Spark SQL Architecture" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Architecture">Spark SQL Architecture</a></h3></div>

<p>
Spark SQL's architecture is layered, with each layer performing specific functions.
</p>

<ul>
<li>
The bottom layer is the data access layer, which works with multiple formats and typically utilizes a distributed filesystem such as HDFS.

</li><li>
The computation layer leverages the distributed processing power of the Spark engine, including its streaming capabilities, and typically operates on RDDs (Resilient Distributed Datasets).

</li><li>
The Dataset/DataFrame layer provides the API for interacting with the data.

</li><li>
Spark SQL sits on top of this layer, providing data access for various applications, dashboards, and BI tools.

</li></ul>
<p>
This architecture allows Spark to leverage the vast knowledge base of SQL among data professionals and use it to query Spark data.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Evolution"><h3 id="Spark SQL Evolution" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Evolution">Spark SQL Evolution</a></h3></div>

<p>
Prior to Spark 2.0, SchemaRDD was at the heart of Spark SQL. It essentially attached a schema to an RDD, enabling SQL queries to be run on RDDs. However, with Spark 2.0, Datasets became the primary way to work with data. Datasets offer the advantages of both RDDs and strong typing, providing a more robust and efficient way to handle data. In languages like Python and R, which lack compile-time type checking, Datasets and DataFrames are merged and referred to as DataFrames.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Spark SQL Programming"><h3 id="Spark SQL Programming" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Spark SQL Programming">Spark SQL Programming</a></h3></div>

<p>
Spark 2.0 introduced <code>sparkSession</code>, which replaced <code>sqlcontext</code>, <code>hivecontext</code>, and other components. The <code>sparkSession</code> instance has a versatile <code>read</code> method capable of handling various data formats like CSV, Parquet, JSON, and JDBC. This method allows you to specify format-related options such as headers and delimiters.
</p>

<p>
To use Spark SQL, you first need to create a Dataset by reading data from a source and informing Spark about its structure and types. You can then apply SQL statements to query the data. To create a view that can be queried using SQL, you can use the <code>createOrReplaceTempView</code> method. You can then use SQL statements to filter, join, and aggregate data within these views.
</p>

<div id="Librerías/Componentes de Spark-Spark SQL-Example Workflow"><h3 id="Example Workflow" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Example Workflow">Example Workflow</a></h3></div>

<p>
A typical Spark SQL workflow involves:
</p>

<ul>
<li>
Defining a case class to represent the data structure.

</li><li>
Reading the data file using <code>sparkSession.read</code>, specifying options like <code>header</code> and <code>inferSchema</code>.

</li><li>
Creating a Dataset with the case class as its element type.

</li><li>
Creating a temporary view using <code>createOrReplaceTempView</code> for SQL access.

</li><li>
Running SQL queries on the view using <code>spark.sql</code>.

</li><li>
Displaying and analyzing the results using methods like <code>show</code>, <code>head</code>, and <code>orderBy</code>

</li></ul>
<div id="Librerías/Componentes de Spark-Spark SQL-Important Points"><h3 id="Important Points" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Important Points">Important Points</a></h3></div>

<ul>
<li>
Spark 2.0 simplified Spark SQL by introducing Datasets and <code>sparkSession</code>.

</li><li>
You can start the Spark shell with the <code>-deprecation</code> flag to receive messages about deprecated methods.

</li><li>
The read method can infer schema automatically using the <code>inferSchema</code> option.

</li><li>
Use <code>createOrReplaceTempView</code> to avoid the <code>TempTableAlreadyExists</code> exception.

</li><li>
Spark SQL enables complex queries involving multiple tables and various operations like filtering, joining, and aggregation.

</li></ul>
<div id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL"><h3 id="Code Examples in Spark SQL" class="header"><a href="#Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL">Code Examples in Spark SQL</a></h3></div>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 1: Loading a CSV File into a Dataset"></span><strong id="Example 1: Loading a CSV File into a Dataset">Example 1: Loading a CSV File into a Dataset</strong>
</p>

<pre java="">case class Employee(EmployeeID : String,   LastName : String, FirstName : String, Title : String,   BirthDate : String, HireDate : String,   City : String, State : String, Zip : String, Country : String,   ReportsTo : String)
// ... ...
val filePath = "/Users/ksankar/fdps-v3/"
println(s"Running Spark Version ${sc.version}")
// val employees = spark.read.option("header","true"). csv(filePath + "data/NW-Employees.csv").as[Employee] 
println("Employees has "+employees.count()+" rows") 
employees.show(5) 
employees.head()
</pre>

<p>
This code snippet first defines a case class called <code>Employee</code> representing the structure of the employee data. Then, it sets a <code>filePath</code> variable pointing to the directory containing the data files. The code then uses the <code>spark.read.csv</code> method to read the CSV file into a Dataset called <code>employees</code>. The <code>option("header", "true")</code> tells Spark that the first row of the CSV file contains column headers. The <code>.as[Employee]</code> part specifies that the Dataset should be composed of <code>Employee</code> objects. Finally, the code prints the number of rows in the Dataset, displays the first \(5\) rows using <code>show(5)</code>, and retrieves the first row using <code>head()</code>.
</p>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 2: Creating a View and Running SQL Queries"></span><strong id="Example 2: Creating a View and Running SQL Queries">Example 2: Creating a View and Running SQL Queries</strong>
</p>

<pre java="">employees.createOrReplaceTempView("EmployeesTable")
var result = spark.sql("SELECT * from EmployeesTable")
result.show(5)
result.head(3)
// employees.explain(true)

result = spark.sql("SELECT * from EmployeesTable WHERE State = 'WA'")
result.show(5)
result.head(3)
// result.explain(true)
</pre>

<p>
This code creates a temporary view called "<code>EmployeesTable</code>" from the employees Dataset using <code>createOrReplaceTempView</code>. This view enables you to query the Dataset using SQL statements. The first <code>spark.sql</code> statement selects all columns from the "<code>EmployeesTable</code>" view. The second query filters the results to include only employees from the state of Washington (<code>WHERE State = 'WA'</code>). Both queries use <code>show(5)</code> to display the first \(5\) rows of the result and <code>head(3)</code> to retrieve the first \(3\) rows.
</p>

<p>
<span id="Librerías/Componentes de Spark-Spark SQL-Code Examples in Spark SQL-Example 3: Handling Multiple Tables and Joins"></span><strong id="Example 3: Handling Multiple Tables and Joins">Example 3: Handling Multiple Tables and Joins</strong>
</p>

<pre java="">// ... ...
val orders = spark.read.option("header","true"). 
option("inferSchema","true"). 
csv(filePath + "data/NW-Orders.csv").as[Order] 
println("Orders has "+orders.count()+" rows") 
orders.show(5)
orders.head() 
orders.dtypes
// ... ...
// // Now the interesting part // 
result = spark.sql("SELECT OrderDetailsTable.OrderID, ShipCountry, UnitPrice, Qty, Discount FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID")
result.show(10) 
result.head(3) 
// // Sales By Country // 
result = spark.sql("SELECT ShipCountry, SUM(OrderDetailsTable.UnitPrice * Qty * Discount) AS ProductSales FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID GROUP BY ShipCountry")
result.count() 
result.show(10) 
result.head(3) 
result.orderBy($"ProductSales".desc).show(10) // Top 10 by Sales
</pre>

<p>
This example demonstrates loading the "<code>Orders</code>" table, creating a view, and then performing joins and aggregations. It first reads the "<code>NW-Orders.csv</code>" file into an orders Dataset. Notably, it uses the <code>option("inferSchema", "true")</code> option, which tells Spark to automatically infer the schema for the data. This eliminates the need to define a case class beforehand.
</p>

<p>
The code then executes two SQL queries. The first query performs an inner join between the "<code>OrdersTable</code>" and "<code>OrderDetailsTable</code>" based on the common "<code>OrderID</code>" column and selects specific columns from the joined result. The second query calculates total sales (<code>SUM(OrderDetailsTable.UnitPrice * Qty * Discount)</code>) per country, groups the results by "<code>ShipCountry</code>", and orders the final output by "<code>ProductSales</code>" in descending order to show the top \(10\) countries by sales.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines"><h2 id="Machine Learning with Spark ML Pipelines" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines">Machine Learning with Spark ML Pipelines</a></h2></div>

<p>
This section provides a summary of Spark's capabilities for Machine Learning (ML), focusing on ML Pipelines and the transition from MLlib to ML APIs. It covers various ML algorithms, data transformation techniques, and the concept of pipelines for streamlined ML workflows.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Spark for Machine Learning"><h3 id="Spark for Machine Learning" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Spark for Machine Learning">Spark for Machine Learning</a></h3></div>

<p>
Spark is attractive for ML due to its ability to handle massive computations. Spark 2.0.0 onwards, Spark is considered a leading platform for building ML algorithms and applications.
</p>

<p>
Spark's ML capabilities are primarily accessed through the <code>org.apache.spark.ml</code> package for Scala and Java, and <code>pyspark.ml</code> for Python. Spark supports a wide array of ML algorithms, including basic statistics, linear regression, classification, clustering, recommendation systems, dimensionality reduction, feature extraction, and more.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-ML Pipelines: Addressing the Data Pipeline in ML"><h3 id="ML Pipelines: Addressing the Data Pipeline in ML" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-ML Pipelines: Addressing the Data Pipeline in ML">ML Pipelines: Addressing the Data Pipeline in ML</a></h3></div>

<p>
Before Spark 1.6.0, the MLlib APIs operated on RDDs, but they lacked support for the data pipelines inherent in ML. With the introduction of DataFrames and Datasets, MLlib evolved into the ML pipeline framework, offering more capabilities and addressing the entire ML workflow.
</p>

<p>
MLlib APIs are now in maintenance mode and will eventually be deprecated. While you should use ML APIs going forward, some functionalities might require using MLlib and converting the output RDD to a DataFrame for further processing with ML APIs.
</p>

<p>
A typical ML process involves several steps:
</p>
<ol>
<li>
Data Acquisition: Obtain data from internal or external sources, ensuring anonymity and removal of personally identifiable information (PII).

</li><li>
Data Transformation: Convert raw data into a usable format, for example, transforming a CSV file into a DataFrame.

</li><li>
Feature Extraction: Extract relevant features from the data, such as separating text into words or normalizing them.

</li><li>
Data Splitting: Divide the data into training and testing sets, using appropriate strategies based on data characteristics like time series or class imbalance.

</li><li>
Model Training:

<ul>
<li>
Fit the training data to different ML models.

</li><li>
Tune hyperparameters for optimal performance.

</li><li>
Select the best-performing model for the specific problem.

</li></ul>
</li><li>
Model Evaluation: Assess the model's performance using the test data.

</li><li>
Model Deployment: Implement the trained model in a production environment for real-time predictions.

</li></ol>
<p>
ML pipelines in Spark address all stages of this workflow.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Structure of Spark ML APIs"><h3 id="Structure of Spark ML APIs" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Structure of Spark ML APIs">Structure of Spark ML APIs</a></h3></div>

<p>
Spark ML APIs have a specific structure that can be challenging to navigate initially. Familiarity with this structure is key to effectively utilizing Spark for ML tasks. The source material provides a diagram to illustrate this, recommending a deeper understanding of the pipeline concept to enhance proficiency in using Spark ML classes.
</p>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning"><h3 id="Code Examples from Spark Machine Learning" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning">Code Examples from Spark Machine Learning</a></h3></div>

<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Basic Statistics with Spark Datasets"><h4 id="Basic Statistics with Spark Datasets" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Basic Statistics with Spark Datasets">Basic Statistics with Spark Datasets</a></h4></div>

<p>
This code snippet shows how to load car mileage data from a CSV file, compute basic statistics using Spark Datasets, and calculate the correlation and covariance between specific variables:
</p>

<pre java="">val spark = SparkSession.builder       
  .master("local")       
  .appName("Chapter 11")       
  .config("spark.logConf","true")       
  .config("spark.logLevel","ERROR")       
  .getOrCreate()   

println(s"Running Spark Version ${spark.version}")   

val filePath = "/Users/ksankar/fdps-v3/"   

val cars = spark.read.option("header","true").option("inferSchema","true")
  .csv(filePath + "data/car-data/car-milage.csv") 

println("Cars has "+cars.count()+" rows") 

cars.show(5) 

cars.printSchema()

// Computing statistics

cars.describe("mpg","hp","weight","automatic").show()  

var cor = cars.stat.corr("hp","weight")   
println("hp to weight : Correlation = %2.4f".format(cor))   

var cov = cars.stat.cov("hp","weight")   
println("hp to weight : Covariance = %2.4f".format(cov))   

cor = cars.stat.corr("RARatio","width")   
println("Rear Axle Ratio to width : Correlation = %2.4f".format(cor))   

cov = cars.stat.cov("RARatio","width")   
println("Rear Axle Ratio to width : Covariance = %2.4f".format(cov))
</pre>

<ol>
<li>
SparkSession Creation: The code starts by creating a SparkSession, which is the entry point for Spark applications.

</li><li>
Data Loading: The <code>spark.read.csv()</code> method loads data from the specified CSV file into a DataFrame named cars. The <code>option("header","true")</code> indicates that the first row contains column headers, and <code>option("inferSchema","true")</code> instructs Spark to automatically infer the data types for each column.

</li><li>
Basic Statistics: The <code>describe()</code> method computes summary statistics like <code>count</code>, <code>mean</code>, <code>standard deviation</code>, <code>min</code>, and <code>max</code> for the specified columns ("mpg", "hp", "weight", "automatic").

</li><li>
Correlation and Covariance: The <code>stat.corr()</code> and <code>stat.cov()</code> methods are used to calculate the correlation and covariance between pairs of variables ("hp" and "weight", "RARatio" and "width").

</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Linear Regression with Data Transformation and Model Evaluation"><h4 id="Linear Regression with Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Linear Regression with Data Transformation and Model Evaluation">Linear Regression with Data Transformation and Model Evaluation</a></h4></div>

<p>
This code example demonstrates a linear regression model using Spark ML Pipelines. It includes data transformation, feature extraction, splitting data into training and testing sets, fitting the model, making predictions, and evaluating the model:
</p>

<pre java="">// Data Transformation and Feature Extraction
val cars1 = cars.na.drop() 

val assembler = new VectorAssembler() 
assembler.setInputCols(Array("displacement", "hp", "torque", "CRatio", "RARatio", "CarbBarrells", "NoOfSpeed", "length", "width", "weight", "automatic")) 
assembler.setOutputCol("features") 

val cars2 = assembler.transform(cars1)
cars2.show(40)

// Data Split
val train = cars2.filter(cars1("weight") &lt;= 4000) 
val test = cars2.filter(cars1("weight") &gt; 4000) 
test.show() 
println("Train = "+train.count()+" Test = "+test.count())

// Linear Regression Model
val algLR = new LinearRegression()     
algLR.setMaxIter(100)     
algLR.setRegParam(0.3)     
algLR.setElasticNetParam(0.8)     
algLR.setLabelCol("mpg")     

val mdlLR = algLR.fit(train)     
println(s"Coefficients: ${mdlLR.coefficients} Intercept: ${mdlLR.intercept}")     

val trSummary = mdlLR.summary 
println(s"numIterations: ${trSummary.totalIterations}") 
println(s"Iteration Summary History: ${trSummary.objectiveHistory.toList}") 
trSummary.residuals.show() 
println(s"RMSE: ${trSummary.rootMeanSquaredError}") 
println(s"r2: ${trSummary.r2}")

// Predictions
val predictions = mdlLR.transform(test) 
predictions.show()

// Model Evaluation
val evaluator = new RegressionEvaluator() 
evaluator.setLabelCol("mpg") 
val rmse = evaluator.evaluate(predictions) 
println("Root Mean Squared Error = "+"%6.3f".format(rmse)) 

val mse = evaluator.evaluate(predictions) 
println("Mean Squared Error = "+"%6.3f".format(mse))
</pre>

<ol>
<li>
Data Preprocessing:

<ul>
<li>
<code>cars.na.drop()</code> removes rows with missing values (NA).

</li><li>
<code>VectorAssembler</code> combines multiple input columns into a single vector column named "features", which is required for many ML algorithms.

</li></ul>
</li><li>
Data Splitting: The data is split into training and test sets based on the "weight" column.

</li><li>
Linear Regression:

<ul>
<li>
A <code>LinearRegression</code> object is created and configured with parameters like maximum iterations (<code>setMaxIter</code>), regularization parameter (<code>setRegParam</code>), and elastic net parameter (<code>setElasticNetParam</code>).

</li><li>
The model is trained using the <code>fit()</code> method on the training data.

</li><li>
Model coefficients, intercept, and training summary are printed.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data using the <code>transform()</code> method, adding a "predictions" column to the resulting DataFrame.

</li><li>
Model Evaluation:

<ul>
<li>
A <code>RegressionEvaluator</code> is used to calculate RMSE and MSE.

</li><li>
The <code>evaluate()</code> method calculates the metrics based on the "mpg" label column and the predicted values.

</li></ul>
</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Classification with Decision Tree, Data Transformation and Model Evaluation"><h4 id="Classification with Decision Tree, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Classification with Decision Tree, Data Transformation and Model Evaluation">Classification with Decision Tree, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example demonstrates a classification task using a decision tree algorithm. It involves data loading, transformation, feature extraction, model training, prediction, and evaluation:
</p>

<pre java="">// Loading Data
val filePath = "/Users/ksankar/fdps-v3/"   
val passengers = spark.read.option("header","true").option("inferSchema","true")
  .csv(filePath + "data/titanic3_02.csv")   
println("Passengers has "+passengers.count()+" rows")   
passengers.show(5)   
passengers.printSchema()

// Data Transformation and Feature Extraction
val passengers1 = passengers.select(
  passengers("Pclass"),
  passengers("Survived").cast(DoubleType).as("Survived"),
  passengers("Gender"),
  passengers("Age"),
  passengers("SibSp"),
  passengers("Parch"),
  passengers("Fare")
) 

passengers1.show(5)

val indexer = new StringIndexer() 
indexer.setInputCol("Gender") 
indexer.setOutputCol("GenderCat") 

val passengers2 = indexer.fit(passengers1).transform(passengers1) 
passengers2.show(5)    

val passengers3 = passengers2.na.drop() 
println("Orig = "+passengers2.count()+" Final = "+ passengers3.count() + " Dropped = "+ (passengers2.count() - passengers3.count()))

val assembler = new VectorAssembler() 
assembler.setInputCols(Array("Pclass", "GenderCat", "Age", "SibSp", "Parch", "Fare")) 
assembler.setOutputCol("features") 
val passengers4 = assembler.transform(passengers3) 
passengers4.show(5)

// Data Split
val Array(train, test) = passengers4.randomSplit(Array(0.9, 0.1)) 
println("Train = "+train.count()+" Test = "+test.count())

// Decision Tree Model
val algTree = new DecisionTreeClassifier() 
algTree.setLabelCol("Survived") 
algTree.setImpurity("gini") 
algTree.setMaxBins(32)
algTree.setMaxDepth(5)     

val mdlTree = algTree.fit(train) 
println("The tree has %d nodes.".format(mdlTree.numNodes)) 
println(mdlTree.toDebugString) 
println(mdlTree.toString) 
println(mdlTree.featureImportances)

// Predictions
val predictions = mdlTree.transform(test) 
predictions.show(5)

// Model Evaluation
val evaluator = new MulticlassClassificationEvaluator() 
evaluator.setLabelCol("Survived") 
evaluator.setMetricName("accuracy") 

val accuracy = evaluator.evaluate(predictions) 
println("Test Accuracy = %.2f%%".format(accuracy*100))
</pre>

<ol>
<li>
Data Loading: The Titanic passenger data is loaded from a CSV file.

</li><li>
Data Transformation and Feature Extraction:

<ul>
<li>
Relevant columns are selected.

</li><li>
<code>StringIndexer</code> converts the categorical "Gender" column into a numerical "GenderCat" column.

</li><li>
Rows with missing values are dropped.

</li><li>
<code>VectorAssembler</code> combines selected features into a "features" vector column.

</li></ul>
</li><li>
Data Splitting: The data is split into training and test sets using <code>randomSplit()</code>.

</li><li>
Decision Tree Model:

<ul>
<li>
A <code>DecisionTreeClassifier</code> is created and configured with parameters like label column, impurity measure ("gini"), maximum bins, and maximum depth.

</li><li>
The model is trained using the <code>fit()</code> method.

</li><li>
Model details like the number of nodes, tree structure, and feature importances are printed.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data.

</li><li>
Model Evaluation:

<ul>
<li>
A <code>MulticlassClassificationEvaluator</code> is used to calculate the accuracy of the model.

</li></ul>
</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Clustering with K-means, Data Transformation and Model Evaluation"><h4 id="Clustering with K-means, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Clustering with K-means, Data Transformation and Model Evaluation">Clustering with K-means, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example illustrates K-means clustering:
</p>

<pre java="">// Loading data
// ... (Code for loading data, similar to previous examples)

// Data Transformation and Feature Extraction
val assembler = new VectorAssembler() 
assembler.setInputCols(Array("X", "Y")) 
assembler.setOutputCol("features") 
val data1 = assembler.transform(data) 
data1.show(5)

// Clustering Model (K=2)
var algKMeans = new KMeans().setK(2) 
var mdlKMeans = algKMeans.fit(data1)

// Predictions
var predictions = mdlKMeans.transform(data1) 
predictions.show(3)     
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-2K.csv")

// Model Evaluation and Interpretation (K=2)
var WSSSE = mdlKMeans.computeCost(data1) 
println(s"Within Set Sum of Squared Errors (K=2) = %.3f".format(WSSSE))

println("Cluster Centers (K=2) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;")) 
println("Cluster Sizes (K=2) : " + mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))

// Clustering Model (K=4)
algKMeans = new KMeans().setK(4) 
mdlKMeans = algKMeans.fit(data1)

// Model Evaluation and Interpretation (K=4)
WSSSE = mdlKMeans.computeCost(data1) 
println(s"Within Set Sum of Squared Errors (K=4) = %.3f".format(WSSSE))

println("Cluster Centers (K=4) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;")) 
println("Cluster Sizes (K=4) : " + mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))

predictions = mdlKMeans.transform(data1) 
predictions.show(30)     
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-4K.csv")
</pre>

<ol>
<li>
Data Loading: Data with two dimensions (X and Y) is loaded.

</li><li>
Data Transformation: <code>VectorAssembler</code> creates a "features" vector column.

</li><li>
Clustering Model:

<ul>
<li>
A <code>KMeans</code> object is created and the number of clusters (\(K\)) is set.

</li><li>
The model is trained using the <code>fit()</code> method.

</li></ul>
</li><li>
Predictions: Cluster assignments for each data point are predicted.

</li><li>
Model Evaluation:

<ul>
<li>
The <code>computeCost()</code> method calculates the Within Set Sum of Squared Errors (WSSE), which is a measure of cluster cohesion.

</li><li>
Cluster centers and cluster sizes are printed.

</li></ul>
</li><li>
Running with Different K: The code runs the clustering with K=2 and K=4, comparing the WSSE and cluster characteristics to illustrate the effect of choosing different values for K.

</li></ol>
<div id="Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Recommendation with ALS, Data Transformation and Model Evaluation"><h4 id="Recommendation with ALS, Data Transformation and Model Evaluation" class="header"><a href="#Librerías/Componentes de Spark-Machine Learning with Spark ML Pipelines-Code Examples from Spark Machine Learning-Recommendation with ALS, Data Transformation and Model Evaluation">Recommendation with ALS, Data Transformation and Model Evaluation</a></h4></div>

<p>
This example demonstrates a recommendation system using the Alternating Least Squares (ALS) algorithm
</p>

<pre java="">// Loading data
// ... (Code for loading data from text files, using RDDs and DataFrames)

// Data Transformation and Feature Extraction
// ... (Code for transforming data using RDDs and DataFrames)

// Data Splitting
val Array(train, test) = ratings3.randomSplit(Array(0.8, 0.2)) 
println("Train = "+train.count()+" Test = "+test.count())

// Recommendation Model
val algALS = new ALS() 
algALS.setItemCol("product") 
algALS.setRank(12) 
algALS.setRegParam(0.1) 
algALS.setMaxIter(20) 
val mdlReco = algALS.fit(train)

// Predicting Using the Model
val predictions = mdlReco.transform(test) 
predictions.show(5) 
predictions.printSchema()

// Model Evaluation and Interpretation
val pred = predictions.na.drop() 
println("Orig = "+predictions.count()+" Final = "+ pred.count() + " Dropped = "+ (predictions.count() - pred.count())) 

val evaluator = new RegressionEvaluator() 
evaluator.setLabelCol("rating") 
var rmse = evaluator.evaluate(pred) 
println("Root Mean Squared Error = "+"%.3f".format(rmse)) 

var mse = evaluator.evaluate(pred) 
println("Mean Squared Error = "+"%.3f".format(mse)) 
mse = pred.rdd.map(r =&gt; rowSqDiff(r)).reduce(_+_) / predictions.count().toDouble 
println("Mean Squared Error (Calculated) = "+"%.3f".format(mse))
</pre>

<ol>
<li>
Data Loading: MovieLens data is loaded from text files using RDDs and then converted to DataFrames.

</li><li>
Data Transformation: The data is transformed to a suitable format for the recommendation algorithm.

</li><li>
Data Splitting: The data is split into training and test sets.

</li><li>
Recommendation Model:

<ul>
<li>
An <code>ALS</code> object is created and configured with parameters like rank, regularization parameter, and maximum iterations.

</li><li>
The model is trained on the training data.

</li></ul>
</li><li>
Predictions: Predictions are made on the test data.

</li><li>
Model Evaluation:

<ul>
<li>
Rows with NaN predictions are dropped to address the cold start problem.

</li><li>
RMSE and MSE are calculated using a <code>RegressionEvaluator</code>.

</li><li>
MSE is also calculated manually for demonstration purposes.

</li></ul>
</li></ol>
<div id="Librerías/Componentes de Spark-GraphX"><h2 id="GraphX" class="header"><a href="#Librerías/Componentes de Spark-GraphX">GraphX</a></h2></div>

<p>
This summary provides an introduction to graph processing and the Spark GraphX framework.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Introduction to Graph Processing"><h3 id="Introduction to Graph Processing" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Introduction to Graph Processing">Introduction to Graph Processing</a></h3></div>

<p>
Graph processing involves analysing and manipulating graph structures, which consist of vertices (nodes) connected by edges. This field has long been crucial in industries like logistics, transportation, and social networking, with applications ranging from route optimisation to social network analysis.
</p>

<p>
The importance of graph processing has surged with the rise of the internet, social media, and large datasets. Applications now include analysing research collaborations, understanding social behaviour in animal populations, and investigating financial networks like the Panama Papers.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Graph Processing Systems"><h3 id="Graph Processing Systems" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Graph Processing Systems">Graph Processing Systems</a></h3></div>

<p>
There are two main categories of graph-based systems:
</p>

<ul>
<li>
<span id="Librerías/Componentes de Spark-GraphX-Graph Processing Systems-Graph processing systems"></span><strong id="Graph processing systems">Graph processing systems</strong> excel at executing complex algorithms on large graph datasets. Examples include Spark GraphX, Pregel BSP, and GraphLab.

</li><li>
<span id="Librerías/Componentes de Spark-GraphX-Graph Processing Systems-Graph databases"></span><strong id="Graph databases">Graph databases</strong>, like AllegroGraph, Titan, Neo4j, and RDF stores, are designed for efficient graph-based queries.

</li></ul>
 
<p>
Organisations with extensive graph-based applications often employ both a graph database and a graph processing system as part of a larger data processing workflow
</p>

<div id="Librerías/Componentes de Spark-GraphX-Challenges of Graph Processing"><h3 id="Challenges of Graph Processing" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Challenges of Graph Processing">Challenges of Graph Processing</a></h3></div>

<p>
Traditional relational database systems struggle with complex graph algorithms due to their iterative and recursive nature, which often span the entire graph. Partitioning data across multiple systems, common in database systems, is suboptimal for graph algorithms, particularly for "long-tail" graphs with many sparsely connected nodes.
</p>

<p>
Frameworks like MapReduce, based on data parallelism and disk-based partitioning, also face challenges in efficiently representing and processing graphs, especially when dealing with the numerous edge cuts inherent in long-tail graphs.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX"><h3 id="Spark GraphX" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX">Spark GraphX</a></h3></div>

<p>
Spark GraphX addresses these challenges by offering graph parallelism over data parallelism and utilising Spark's data-distributed RDD mechanism. This approach combines the strengths of both data and graph parallelism, enabling efficient processing of complex graph algorithms on large datasets.
</p>

<p>
GraphX provides various partitioning and storage schemes to optimise performance and allows for tuning based on specific application requirements and data characteristics. While GraphX excels at computation, the new GraphFrames API integrates DataFrames with graphs to facilitate powerful graph queries.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Architecture"><h4 id="GraphX Architecture" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Architecture">GraphX Architecture</a></h4></div>

<p>
GraphX is built on top of Spark and leverages its distributed processing capabilities, algorithms, and versioned computation graph. Some machine learning algorithms within Spark also utilise GraphX APIs.
GraphX offers a rich computational model, built-in algorithms, and APIs for developing custom algorithms. It provides functionalities for:
</p>

<ul>
<li>
Graph creation

</li><li>
Structure queries

</li><li>
Attribute transformers

</li><li>
Structure transformers

</li><li>
Connection mining primitives

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Computational Model"><h4 id="GraphX Computational Model" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Computational Model">GraphX Computational Model</a></h4></div>

<p>
GraphX uses a property graph model, where:
</p>

<ol>
<li>
Vertices are connected by edges.

</li><li>
Both vertices and edges can have arbitrary objects as properties, accessible to the APIs.

</li><li>
It is a directed multigraph, meaning edges have direction, and multiple edges can exist between vertices.

</li></ol>
<p>
This model supports various graph types, including bipartite and tripartite graphs. Each vertex consists of a unique ID (64-bit integer) and a property object, while edges comprise source and destination vertex IDs and a property object.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-Building Graphs with GraphX"><h4 id="Building Graphs with GraphX" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-Building Graphs with GraphX">Building Graphs with GraphX</a></h4></div>

<p>
There are four ways to create a graph in GraphX:
</p>

<ul>
<li>
Loading an edge list file using <code>GraphLoader.edgeListFile(...)</code>.

</li><li>
Loading edge tuples from RDDs using <code>fromEdgeTuples()</code>.

</li><li>
Creating a graph from a list of edges using <code>fromEdges()</code>.

</li><li>
Creating a graph using edge and vertex RDDs.

</li></ul>
<p>
The last method offers flexibility, especially when manipulating user-defined objects for vertices and edges
</p>

<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX API Landscape"><h4 id="GraphX API Landscape" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX API Landscape">GraphX API Landscape</a></h4></div>

<p>
The GraphX APIs are organized into different categories:
</p>

<ul>
<li>
Objects: Edge, EdgeRDD, and others reside under org.apache.spark.graphx.

</li><li>
Graph Object: Contains APIs like triplets, persist, subgraph, etc..

</li><li>
Graph Algorithms: Separated under the GraphOps object to distinguish algorithms from graph implementation.

</li><li>
Analytic Functions: Functions like SVD++, ShortestPath, and others are located under lib

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-Structural APIs"><h4 id="Structural APIs" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-Structural APIs">Structural APIs</a></h4></div>

<p>
GraphX provides structural APIs for analysing graph structure, such as:
</p>

<ul>
<li>
numEdges and numVertices for getting the number of edges and vertices.

</li><li>
triplets for accessing edge and connected vertex information together.

</li><li>
inDegrees and outDegrees for retrieving incoming and outgoing edge counts for vertices.

</li><li>
subgraph for extracting subgraphs based on edge and vertex property predicates.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-Community Detection and Analysis"><h4 id="Community Detection and Analysis" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-Community Detection and Analysis">Community Detection and Analysis</a></h4></div>

<p>
GraphX offers algorithms for exploring network connections and communities, with applications in areas like fraud detection and security. Some key algorithms include:
</p>

<ul>
<li>
<code>triangleCount</code> for identifying and counting triangles within the graph, useful for spam detection and community ranking.

</li><li>
<code>connectedComponents</code> for finding groups of vertices connected by paths.

</li><li>
<code>stronglyConnectedComponents</code> for identifying communities with bidirectional connections between all members

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Algorithms"><h4 id="GraphX Algorithms" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-GraphX Algorithms">GraphX Algorithms</a></h4></div>

<p>
GraphX includes various built-in algorithms for graph analysis:
</p>

<table>
<thead>
<tr>
<th>
Algorithm Type
</th>
<th>
GraphX Method/Exmaple
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Graph-Parallel Computation
</td>
<td>
<code>aggregateMessages(), Pregel()</code>
</td>
</tr>
<tr>
<td>
PageRank
</td>
<td>
<code>PageRank(), staticPageRank(), personalizedPageRank()</code>
</td>
</tr>
<tr>
<td>
Shortest Paths and SVD++
</td>
<td>
<code>ShortestPaths(), SVD++</code>
</td>
</tr>
<tr>
<td>
Label Propagation (LPA)
</td>
<td>
<code>LabelPropagation()</code>
</td>
</tr>
</tbody>
</table>

<p>
PageRank is a prominent algorithm for ranking the importance of vertices, with variations for static iterations, dynamic convergence, and personalized ranking based on a specified vertex.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-Graph Parallel Computation APIs"><h4 id="Graph Parallel Computation APIs" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-Graph Parallel Computation APIs">Graph Parallel Computation APIs</a></h4></div>

<p>
GraphX provides two primary APIs for implementing custom graph algorithms:
</p>

<ul>
<li>
aggregateMessages(): A versatile API for aggregating information from neighbouring edges and vertices, operating similarly to a MapReduce paradigm on the graph.

</li><li>
Pregel(): A more general API that encompasses aggregateMessages() and offers greater flexibility in algorithm design.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Spark GraphX-Partition Strategies"><h4 id="Partition Strategies" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Spark GraphX-Partition Strategies">Partition Strategies</a></h4></div>

<p>
Efficient partitioning of large graphs is crucial for performance in distributed processing. GraphX offers different partition strategies to address the challenges of long-tail graphs and minimise communication overhead:
</p>

<ul>
<li>
Edge cut: Partitions vertices across machines, with communication cost proportional to the number of edges cut.

</li><li>
Vertex cut: Partitions edges, potentially duplicating vertices, with cost proportional to the number of machines spanned by each vertex.

</li></ul>
 
<p>
GraphX defaults to a vertex cut strategy to mitigate hotspot issues caused by uneven distribution of connections. It offers four main strategies: RandomVertexCut, CanonicalRandomVertexCut, EdgePartition1D, and EdgePartition2D.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Case Study: AlphaGo Tweet Analytics"><h3 id="Case Study: AlphaGo Tweet Analytics" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Case Study: AlphaGo Tweet Analytics">Case Study: AlphaGo Tweet Analytics</a></h3></div>

<p>
The source provides a case study applying GraphX to analyse a retweet network of tweets related to the AlphaGo project. It outlines a data pipeline for collecting, processing, and analysing tweets to understand user rankings, locations, time zones, and follower-followee relationships.
</p>

<p>
The case study demonstrates the process of modelling the retweet network as a graph, defining vertices as users, edges as retweets, and creating objects to store user and tweet attributes. It then showcases using GraphX to create the graph from the processed tweet data and apply algorithms like PageRank to analyse user influence within the retweet network.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source"><h3 id="Code Examples in the GraphX Source" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source">Code Examples in the GraphX Source</a></h3></div>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph"><h4 id="Building a Simple Graph" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph">Building a Simple Graph</a></h4></div>

<p>
The source demonstrates building a graph representing a "Giraffe Graph" with two strongly connected groups (cliques) linked by a weak connection.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph-Defining Vertices and Edges"><h5 id="Defining Vertices and Edges" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph-Defining Vertices and Edges">Defining Vertices and Edges</a></h5></div>

<p>
The first step involves defining the vertices and edges, along with their associated properties:
</p>

<pre java="">case class Person(name:String,age:Int)
val defaultPerson = Person("NA",0)
val vertexList = List( (1L, Person("Alice", 18)), (2L, Person("Bernie", 17)), (3L, Person("Cruz", 15)), (4L, Person("Donald", 12)), (5L, Person("Ed", 15)), (6L, Person("Fran", 10)), (7L, Person("Genghis",854)) )
val edgeList = List( Edge(1L, 2L, 5), Edge(1L, 3L, 1), Edge(3L, 2L, 5), Edge(2L, 4L, 12), Edge(4L, 5L, 4), Edge(5L, 6L, 2), Edge(6L, 7L, 2), Edge(7L, 4L, 5), Edge(6L, 4L, 4) )
</pre>

<ul>
<li>
<code>case class Person(name:String,age:Int)</code>: Defines a case class to represent a person with attributes for name and age. This will be used as the vertex property.

</li><li>
<code>defaultPerson</code>: Creates an instance of Person with default values, used for vertices not explicitly defined in vertexList but present in edgeList.

</li><li>
<code>vertexList</code>: A list of tuples, where each tuple represents a vertex. The first element of the tuple is the vertex ID (a long integer), and the second is a Person object.

</li><li>
<code>edgeList</code>: A list of Edge objects, where each Edge represents a connection between two vertices. The Edge constructor takes the source vertex ID, destination vertex ID, and an integer representing "betweenness centrality" as arguments.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph-Creating RDDs and the Graph"><h5 id="Creating RDDs and the Graph" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Building a Simple Graph-Creating RDDs and the Graph">Creating RDDs and the Graph</a></h5></div>

<p>
Next, the code creates RDDs from the vertex and edge lists and constructs the graph:
</p>

<pre java="">val vertexRDD = sc.parallelize(vertexList) 
val edgeRDD = sc.parallelize(edgeList) 
val graph = Graph(vertexRDD, edgeRDD,defaultPerson)
</pre>

<ul>
<li>
<code>sc.parallelize(...)</code>: The parallelize method of the SparkContext (sc) creates an RDD from the provided list.

</li><li>
<code>Graph(vertexRDD, edgeRDD, defaultPerson)</code>: This constructs the graph using the vertex RDD, edge RDD, and the <code>defaultPerson</code> object for handling missing vertices.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Structural APIs"><h4 id="Structural APIs" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Structural APIs">Structural APIs</a></h4></div>

<p>
The source presents examples using structural APIs to query the graph:
</p>

<pre java="">graph.numEdges 
graph.numVertices 
val vertices = graph.vertices 
vertices.collect.foreach(println) 
val edges = graph.edges 
edges.collect.foreach(println) 
val triplets = graph.triplets 
triplets.take(3) 
triplets.map(t=&gt;t.toString).collect().foreach(println)
</pre>

<ul>
<li>
<code>numEdges</code> and <code>numVertices</code>: These methods return the number of edges and vertices in the graph, respectively.

</li><li>
<code>vertices</code> and <code>edges</code>: These properties provide access to the RDDs containing the vertices and edges of the graph.

</li><li>
<code>collect</code>: This action retrieves all elements of an RDD to the driver program.

</li><li>
<code>foreach(println)</code>: Iterates through the collected elements and prints each one.

</li><li>
<code>triplets</code>: This property returns an RDD of EdgeTriplet objects, each representing an edge along with its source and destination vertices and their properties.

</li><li>
<code>take(3)</code>: Retrieves the first three elements of the RDD.

</li><li>
<code>map(t =&gt; t.toString)</code>: Transforms each EdgeTriplet into a string representation.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Structural APIs-Extracting Subgraphs"><h5 id="Extracting Subgraphs" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Structural APIs-Extracting Subgraphs">Extracting Subgraphs</a></h5></div>

<p>
The source demonstrates extracting subgraphs based on edge and vertex properties:
</p>

<pre java="">val inDeg = graph.inDegrees 
inDeg.collect() 
val outDeg = graph.outDegrees 
outDeg.collect() 
val allDeg = graph.degrees 
allDeg.collect() 
val g1 = graph.subgraph(epred = (edge) =&gt; edge.attr &gt; 4) 
g1.triplets.collect.foreach(println)
val g2 = graph.subgraph(vpred = (id, person) =&gt; person.age &gt; 21) 
g2.triplets.collect.foreach(println)
</pre>

<ul>
<li>
<code>inDegrees</code>, <code>outDegrees</code>, <code>degrees</code>: These methods calculate the incoming, outgoing, and total degrees for each vertex, respectively.

</li><li>
<code>subgraph(epred = ..., vpred = ...)</code>: This method creates a subgraph by applying predicates to filter edges and vertices.

<ul>
<li>
<code>epred</code> is a function that takes an Edge object and returns a boolean indicating whether to include the edge.

</li><li>
<code>vpred</code> is a function that takes a vertex ID and its corresponding property object and returns a boolean indicating whether to include the vertex.

</li></ul>
</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Community Detection"><h4 id="Community Detection" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Community Detection">Community Detection</a></h4></div>

<p>
The source shows examples of using GraphX algorithms for community detection:
</p>

<pre java="">val cc = graph.connectedComponents() 
cc.triplets.collect 
graph.connectedComponents.vertices.map(_.swap).groupByKey.map(_._2).collect 
cc.vertices.map(_._2).collect.distinct.size 
cc.vertices.groupBy(_._2).map(p=&gt;(p._1,p._2.size)).sortBy(x=&gt;x._2,false).collect() 
val ccS = graph.stronglyConnectedComponents(10) 
ccS.triplets.collect 
ccS.vertices.map(_.swap).groupByKey.map(_._2).collect 
ccS.vertices.map(_._2).collect.distinct.size 
val triCounts = graph.triangleCount() 
val triangleCounts = triCounts.vertices.collect 
</pre>

<ul>
<li>
<code>connectedComponents()</code>: This algorithm finds groups of vertices connected by paths, returning a new graph where each vertex is assigned the ID of its connected component.

</li><li>
<code>stronglyConnectedComponents(10)</code>: This algorithm identifies strongly connected components, where bidirectional paths exist between all members. The argument 10 specifies the maximum number of iterations.

</li><li>
<code>triangleCount()</code>: This algorithm counts the number of triangles each vertex participates in.

</li></ul>
<p>
These code snippets showcase various operations on the resulting graphs, such as collecting triplets, grouping vertices, and counting connected components.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-PageRank Calculation"><h4 id="PageRank Calculation" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-PageRank Calculation">PageRank Calculation</a></h4></div>

<pre java="">val ranks = graph.pageRank(0.1).vertices 
ranks.collect().foreach(println) 
val topVertices = ranks.sortBy(_._2,false).collect.foreach(println)
</pre>

<ul>
<li>
<code>pageRank(0.1)</code>: This algorithm calculates the PageRank for each vertex in the graph. The argument 0.1 specifies the damping factor.

</li><li>
<code>sortBy(_._2, false)</code>: Sorts the PageRank results in descending order based on the PageRank value <code>(_._2)</code>.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API"><h4 id="Aggregate Messages API" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API">Aggregate Messages API</a></h4></div>

<p>
The source provides examples using the aggregateMessages() API to perform computations on the graph:
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Finding the Oldest Follower"><h5 id="Finding the Oldest Follower" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Finding the Oldest Follower">Finding the Oldest Follower</a></h5></div>

<pre java="">val oldestFollower = graph.aggregateMessages[Int]( 
  edgeContext =&gt; edgeContext.sendToDst(edgeContext.srcAttr.age),
  (x,y) =&gt; math.max(x,y) 
) 
oldestFollower.collect()
</pre>

<ul>
<li>
<code>aggregateMessages[Int](...)</code>: This API aggregates values from neighbouring vertices and edges. It takes two functions as arguments:

</li><li>
<code>sendMsg</code>: A function that operates on each edge context (EdgeContext) and sends messages to either the source or destination vertex. In this case, it sends the age of the source vertex (srcAttr.age) to the destination vertex using sendToDst.

</li><li>
<code>mergeMsg</code>: A function that combines messages received at each vertex. Here, it uses math.max to determine the oldest age among the received messages.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Finding the Oldest Followee"><h5 id="Finding the Oldest Followee" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Finding the Oldest Followee">Finding the Oldest Followee</a></h5></div>

<pre java="">val oldestFollowee = graph.aggregateMessages[Int]( 
  edgeContext =&gt; edgeContext.sendToSrc(edgeContext.dstAttr.age),
  (x,y) =&gt; math.max(x,y) 
) 
oldestFollowee.collect()
</pre>

<p>
This example is similar to the previous one but uses <code>sendToSrc</code> to send the age of the destination vertex (dstAttr.age) to the source vertex.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Calculating In-Degree and Out-Degree"><h5 id="Calculating In-Degree and Out-Degree" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-Aggregate Messages API-Calculating In-Degree and Out-Degree">Calculating In-Degree and Out-Degree</a></h5></div>

<pre java="">var iDegree = graph.aggregateMessages[Int]( 
  edgeContext =&gt; edgeContext.sendToDst(1),
  (x,y) =&gt; x+y 
) 
iDegree.collect() 
graph.inDegrees.collect() 

val oDegree = graph.aggregateMessages[Int]( 
  edgeContext =&gt; edgeContext.sendToSrc(1),
  (x,y) =&gt; x+y 
) 
oDegree.collect() 
graph.outDegrees.collect()
</pre>

<p>
These examples demonstrate implementing the <code>inDegrees</code> and <code>outDegrees</code> functionality using the <code>aggregateMessages()</code> API. They send a message of 1 to either the destination (for in-degree) or source (for out-degree) vertex for each edge and then sum the messages received at each vertex using <code>x + y</code>.
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics"><h4 id="AlphaGo Tweet Analytics" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics">AlphaGo Tweet Analytics</a></h4></div>

<p>
We present a case study using GraphX to analyse a retweet network of tweets related to the AlphaGo project. The provided code snippets focus on data loading and transformation to create the graph:
</p>

<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Loading Data and Creating a DataFrame"><h5 id="Loading Data and Creating a DataFrame" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Loading Data and Creating a DataFrame">Loading Data and Creating a DataFrame</a></h5></div>

<pre java="">import org.apache.spark.SparkContext 
import org.apache.spark.SparkConf 
import org.apache.spark.graphx._
println(new java.io.File( "." ).getCanonicalPath) 
println(s"Running Spark Version ${sc.version}") 
val df = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").option("inferSchema", "true").option("delimiter","|").load("file:/Users/ksankar/fdps-v3/data/reTweetNetwork-small.psv") 
df.show(5) 
df.count() 
case class User(name:String, location:String, tz : String, fr:Int,fol:Int) 
case class Tweet(id:String,count:Int)
val graphData = df.rdd 
println("--- The Graph Data ---") 
graphData.take(2).foreach(println)
</pre>

<ul>
<li>
Import statements: Import necessary classes from Spark and GraphX.

</li><li>
Print statements: Print the current directory and Spark version.

</li><li>
<code>sqlContext.read...</code>: Reads the data from a pipe-separated value (PSV) file into a DataFrame using the spark-csv package.

</li><li>
<code>case class User(...)</code> and <code>case class Tweet(...)</code>: Define case classes to represent user and tweet data.

</li><li>
<code>graphData = df.rdd</code>: Extracts the underlying RDD from the DataFrame.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Mapping Data to Vertices and Edges"><h5 id="Mapping Data to Vertices and Edges" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Mapping Data to Vertices and Edges">Mapping Data to Vertices and Edges</a></h5></div>

<pre java="">val vert1 = graphData.map(row =&gt; (row(3).toString.toLong,User(row(4).toString,row(5).toString,row(6).toString,row(7).toString.toInt,row(8).toString.toInt))) 
println("--- Vertices-1 ---") 
vert1.count() 
vert1.take(3).foreach(println)
val vert2 = graphData.map(row =&gt; (row(9).toString.toLong,User(row(10).toString,row(11).toString,row(12).toString,row(13).toString.toInt,row(14).toString.toInt))) 
println("--- Vertices-2 ---") 
vert2.count() 
vert2.take(3).foreach(println)
val vertX = vert1.++(vert2) 
println("--- Vertices-combined ---") 
vertX.count()
val edgX = graphData.map(row =&gt; (Edge(row(3).toString.toLong,row(9).toString.toLong,Tweet(row(0).toString,row(1).toString.toInt)))) 
println("--- Edges ---") 
edgX.take(3).foreach(println)
</pre>

<ul>
<li>
<code>vert1</code> and <code>vert2</code>: These variables use map transformations on the graphData RDD to extract user data from different columns of the data and create RDDs of vertices.

</li><li>
<code>vertX = vert1.++(vert2)</code>: Combines the two vertex RDDs.

</li><li>
<code>edgX</code>: Uses a map transformation to extract tweet data and create an RDD of edges.

</li></ul>
<div id="Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Creating the Graph and Running Algorithms"><h5 id="Creating the Graph and Running Algorithms" class="header"><a href="#Librerías/Componentes de Spark-GraphX-Code Examples in the GraphX Source-AlphaGo Tweet Analytics-Creating the Graph and Running Algorithms">Creating the Graph and Running Algorithms</a></h5></div>

<pre java="">val rtGraph = Graph(vertX,edgX) 
val ranks = rtGraph.pageRank(0.1).vertices 
println("--- Page Rank ---") 
ranks.take(2) 
println("--- Top Users ---") 
val topUsers = ranks.sortBy(_._2,false).take(3).foreach(println) 
val topUsersWNames = ranks.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println) 
println("--- How Big ? ---") 
rtGraph.vertices.count 
rtGraph.edges.count 
println("--- How many retweets ? ---") 
val iDeg = rtGraph.inDegrees 
val oDeg = rtGraph.outDegrees 
iDeg.sortBy(_._2,false).take(3).foreach(println) 
oDeg.sortBy(_._2,false).take(3).foreach(println) 
println("--- Max retweets ---") 
val topRT = iDeg.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println) 
val topRT1 = oDeg.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println)
</pre>

<ul>
<li>
<code>rtGraph = Graph(vertX, edgX)</code>: Constructs the graph from the vertex and edge RDDs.

</li><li>
<code>pageRank(0.1)</code>: Calculates PageRank.

</li><li>
<code>sortBy(...)</code>: Sorts results.

</li><li>
<code>join(...)</code>: Joins RDDs to combine data.

</li><li>
<code>inDegrees</code> and <code>outDegrees</code>: Calculate in-degree and out-degree values.

</li></ul>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>