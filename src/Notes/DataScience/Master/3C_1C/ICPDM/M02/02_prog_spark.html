<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Programming Spark Applications</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Programming Spark Applications"><h1 id="Programming Spark Applications" class="header"><a href="#Programming Spark Applications">Programming Spark Applications</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="02_prog_spark.html#Chapter%203%3A%20Building%20and%20Running%20a%20Spark%20Application">Chapter 3  Building and Running a Spark Application</a>

<ul>
<li>
<a href="02_prog_spark.html#Building%20Spark%20Jobs%20with%20Maven">Building Spark Jobs with Maven</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%204%3A%20Creating%20a%20SparkSession%20Object">Chapter 4  Creating a SparkSession Object</a>

<ul>
<li>
<a href="02_prog_spark.html#Understanding%20the%20SparkSession%20Object">Understanding the SparkSession Object</a>

</li><li>
<a href="02_prog_spark.html#Working%20with%20Datasets%2C%20DataFrames%2C%20and%20RDDs">Working with Datasets  DataFrames  and RDDs</a>

</li><li>
<a href="02_prog_spark.html#Building%20a%20SparkSession%20Object">Building a SparkSession Object</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%205%3A%20Loading%20and%20Saving%20Data%20in%20Spark">Chapter 5  Loading and Saving Data in Spark</a>

</li><li>
<a href="02_prog_spark.html#Chapter%206%3A%20Manipulating%20Your%20RDD">Chapter 6  Manipulating Your RDD</a>

</li><li>
<a href="02_prog_spark.html#Chapter%209%3A%20Foundations%20of%20Datasets%2FDataFrames%20%2013%20The%20Proverbial%20Workhorse%20for%20Data%20Scientists">Chapter 9  Foundations of Datasets DataFrames   The Proverbial Workhorse for Data Scientists</a>

</li></ul>
<hr>

<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application"><h2 id="Chapter 3: Building and Running a Spark Application" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application">Chapter 3: Building and Running a Spark Application</a></h2></div>

<p>
Spark applications can be built and run in different ways:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Interactive Mode with Spark Shell"></span><strong id="Interactive Mode with Spark Shell">Interactive Mode with Spark Shell</strong>: This method is suitable for quick prototyping and interactive data exploration. Spark provides a shell interface for Scala, Python, and R, allowing users to execute commands and get immediate feedback. This interactive mode is excellent for learning Spark APIs, testing code snippets, and performing ad-hoc data analysis.

</li><li>
<span id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-IDE for Application Development"></span><strong id="IDE for Application Development">IDE for Application Development</strong>: For developing more complex applications, Integrated Development Environments (IDEs) like Eclipse and IntelliJ are popular choices. These IDEs provide features like code completion, debugging, and project management, making it easier to develop, test, and deploy Spark applications.

</li></ul>
<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven"><h3 id="Building Spark Jobs with Maven" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven">Building Spark Jobs with Maven</a></h3></div>

<p>
Building Spark jobs is slightly more intricate than building standard applications. This complexity arises because Spark applications often need to be executed on a cluster of machines, requiring Spark dependencies to be available on all nodes. There are two primary methods for building Spark jobs:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven-Building with Maven"></span><strong id="Building with Maven">Building with Maven</strong>: Maven is a widely used build tool in the Java ecosystem, and it is officially recommended for packaging Spark applications. Maven simplifies the build process by managing dependencies, compiling code, and packaging it into a JAR file. Developers can include Spark dependencies through Maven Central, a public repository for Java libraries. Maven can also package Spark and its dependencies into a single executable JAR file, making it easier to deploy and run on a cluster.

</li><li>
<span id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven-Building with Other Build Systems"></span><strong id="Building with Other Build Systems">Building with Other Build Systems</strong>: While Maven is the recommended build tool, Spark supports building a "fat JAR" file that contains all its dependencies. This fat JAR can be used with other build systems like Sbt, Gradle, or even custom build scripts. The process usually involves building the Spark assembly JAR using Sbt and then including it in the build path of the other build system. This approach allows developers to use their preferred build tools while still ensuring that all necessary dependencies are included.

</li></ul>
<p>
Steps to build a Spark Job with <span id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven-Maven"></span><strong id="Maven">Maven</strong>:
</p>
<ul>
<li>
Create a new directory and generate the Maven template. The example shows building a Java Spark job:
<pre bash="">mkdir example-java-build/; cd example-java-build
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.maven.archetypes \
    -DgroupId=spark.examples \
    -DartifactId=JavaWordCount \
    -Dfilter=org.apache.maven.archetypes:maven-archetype-quickstart
cp ../examples/src/main/java/spark/examples/JavaWordCount.java \
JavaWordCount/src/main/java/spark/examples/JavaWordCount.java
</pre>

</li><li>
Update Maven <code>pom.xml</code> to include Spark version and JDK version information. Add the following code between the &lt;project&gt; tags:
<pre xml="">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.spark-project&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.0.0&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.7&lt;/source&gt;
                &lt;target&gt;1.7&lt;/target&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</pre>

</li><li>
Build the JAR file:
<pre bash="">mvn package
</pre>

</li><li>
Run the Spark job:
<pre bash="">SPARK_HOME="../"
SPARK_EXAMPLES_JAR="./target/JavaWordCount-1.0-SNAPSHOT.jar"
java -cp ./target/JavaWordCount-1.0-SNAPSHOT.jar:../../core/target/spark-core-assembly-1.5.2.jar spark.examples.JavaWordCount local[1] ../../README
</pre>

</li></ul>
 
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object"><h2 id="Chapter 4: Creating a SparkSession Object" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object">Chapter 4: Creating a SparkSession Object</a></h2></div>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object"><h3 id="Understanding the SparkSession Object" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object">Understanding the SparkSession Object</a></h3></div>

<p>
The SparkSession object acts as the primary entry point for interacting with Spark functionalities. Introduced in Spark 2.0.0, it represents a connection to a Spark cluster, which can be either local for development and testing or remote for distributed processing on a cluster of machines. The SparkSession provides a unified interface for various Spark components and operations:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object-Unified Entry Point"></span><strong id="Unified Entry Point">Unified Entry Point</strong>: Before SparkSession, developers had to interact with multiple context objects like SparkContext, SQLContext, and HiveContext for different functionalities. SparkSession encapsulates these contexts, providing a single entry point for all Spark operations, simplifying the development process.

</li><li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object-Dataset and DataFrame Creation"></span><strong id="Dataset and DataFrame Creation">Dataset and DataFrame Creation</strong>: SparkSession enables the creation of Datasets and DataFrames, which are high-level abstractions for representing structured data in Spark. Datasets are type-safe, providing compile-time type checking, while DataFrames offer a schema-based view of the data.

</li><li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object-SQL Execution"></span><strong id="SQL Execution">SQL Execution</strong>: SparkSession facilitates the execution of SQL queries against data in Spark. It allows users to register DataFrames as temporary views and then run SQL queries against those views, providing a familiar way to interact with data.

</li><li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Understanding the SparkSession Object-RDD Access"></span><strong id="RDD Access">RDD Access</strong>: While Datasets and DataFrames are the preferred abstractions in Spark 2.0.0 and later, SparkSession still provides access to the underlying RDDs (Resilient Distributed Datasets). Developers can obtain the SparkContext from the SparkSession to work with RDDs when necessary.

</li></ul>
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Working with Datasets, DataFrames, and RDDs"><h3 id="Working with Datasets, DataFrames, and RDDs" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Working with Datasets, DataFrames, and RDDs">Working with Datasets, DataFrames, and RDDs</a></h3></div>

<p>
Spark provides different abstractions for representing and manipulating data:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Working with Datasets, DataFrames, and RDDs-Datasets and DataFrames"></span><strong id="Datasets and DataFrames">Datasets and DataFrames</strong>: These are high-level, schema-based abstractions introduced in Spark 2.0.0. Datasets provide type safety and compile-time checking, while DataFrames are untyped but offer a schema-based view of the data. Both Datasets and DataFrames offer a rich API for data manipulation, including filtering, sorting, grouping, aggregation, and joining operations. They are built on top of RDDs and leverage Catalyst, Spark's query optimizer, to optimize execution plans for better performance.

</li><li>
<span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Working with Datasets, DataFrames, and RDDs-RDDs"></span><strong id="RDDs">RDDs</strong>: RDDs are the fundamental data structure in Spark, representing an immutable, partitioned collection of data distributed across the cluster. RDDs provide low-level control over data and operations, allowing developers to implement custom data processing logic. They are useful for complex computations that cannot be efficiently expressed using Dataset or DataFrame APIs. RDDs follow lazy evaluation, meaning that transformations on RDDs are not executed immediately but are computed only when an action requiring the results is called.

</li></ul>
<p>
The choice of abstraction depends on the specific use case and the level of control required. Datasets and DataFrames are generally preferred for most data manipulation tasks due to their higher-level API, performance optimizations, and ease of use. RDDs are suitable for situations demanding low-level control or when dealing with unstructured data.
</p>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object"><h3 id="Building a SparkSession Object" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object">Building a SparkSession Object</a></h3></div>

<p>
Scala and Python:
</p>

<pre java="">val sparkSession = new SparkSession.builder.master(master_path).appName("application name").config("optional configuration parameters").getOrCreate()
</pre>

<ul>
<li>
It's recommended to read values from the environment with reasonable defaults for flexibility in changing environments.

</li><li>
<code>spark-shell/pyspark</code> automatically creates the SparkSession object and assigns it to the spark variable.

</li><li>
Access the SparkContext object using spark.sparkContext.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark"><h2 id="Chapter 5: Loading and Saving Data in Spark" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark">Chapter 5: Loading and Saving Data in Spark</a></h2></div>

<p>
Spark offers flexible mechanisms for loading and saving data from various sources and formats:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Loading Data into RDDs"></span><strong id="Loading Data into RDDs">Loading Data into RDDs</strong>: Data can be loaded into RDDs from various sources, including local collections, text files, CSV files, sequence files, and external databases like HBase. SparkContext provides functions like <code>parallelize()</code>, <code>textFile()</code>, <code>sequenceFile()</code>, and <code>newAPIHadoopRDD()</code> to load data into RDDs.

</li><li>
<span id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Saving Data from RDDs"></span><strong id="Saving Data from RDDs">Saving Data from RDDs</strong>: RDDs can be saved to different formats like text files, sequence files, and object files. Functions like <code>saveAsTextFile()</code>, <code>saveAsObjectFile()</code>, and <code>saveAsSequenceFile()</code> are used to save RDD data.

</li><li>
<span id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Loading and Saving Data with Datasets and DataFrames"></span><strong id="Loading and Saving Data with Datasets and DataFrames">Loading and Saving Data with Datasets and DataFrames</strong>: Datasets and DataFrames provide more streamlined and efficient methods for data loading and saving. SparkSession's read API supports reading data from various formats like CSV, JSON, Parquet, Avro, and JDBC. Similarly, the write API allows saving data to different formats.

</li></ul>
<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD"><h2 id="Chapter 6: Manipulating Your RDD" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD">Chapter 6: Manipulating Your RDD</a></h2></div>

<p>
Spark offers a rich set of operations for manipulating data in RDDs, Datasets, and DataFrames:
</p>
<ul>
<li>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Transformations"></span><strong id="Transformations">Transformations</strong>: Transformations are operations that create new RDDs, Datasets, or DataFrames from existing ones without changing the original data. Common transformations include <code>map()</code>, <code>filter()</code>, <code>flatMap()</code>, <code>reduceByKey()</code>, <code>groupByKey()</code>, and <code>sortByKey()</code>.

</li><li>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Actions"></span><strong id="Actions">Actions</strong>: Actions are operations that trigger computations on RDDs, Datasets, or DataFrames and return results to the driver program. Examples of actions include <code>count()</code>, <code>collect()</code>, <code>reduce()</code>, <code>take()</code>, and <code>saveAsTextFile()</code>.

</li><li>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Shared States and Accumulators"></span><strong id="Shared States and Accumulators">Shared States and Accumulators</strong>: While distributed computation in Spark generally discourages shared states, accumulators provide a safe mechanism for aggregating values from different partitions across the cluster. Accumulators are write-only variables that can be used to count events or sum values from different parts of the data.

</li><li>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Broadcast Variables"></span><strong id="Broadcast Variables">Broadcast Variables</strong>: Broadcast variables enable efficient sharing of read-only data across the cluster. Instead of sending the data to every task, Spark broadcasts the data once to each executor node, making it available to all tasks running on that node.

</li></ul>
<p>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Example: Parsing CSV Files with Error Handling (Scala)"></span><strong id="Example: Parsing CSV Files with Error Handling (Scala)">Example: Parsing CSV Files with Error Handling (Scala)</strong>
</p>

<pre java="">import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkFiles
import org.apache.spark.api.java.JavaSparkContext
import au.com.bytecode.opencsv.CSVReader
import java.io.StringReader

object LoadCsvWithCountersExample {

  def main(args: Array[String]) {

    val sc = new SparkContext("local", "Chapter 6")
    println(s"Running Spark Version ${sc.version}")

    val invalidLineCounter = sc.accumulator(0)
    val invalidNumericLineCounter = sc.accumulator(0)

    val inFile = sc.textFile("/Volumes/sdxc-01/fdps-vii/data/Line_of_numbers.csv")

    val splitLines = inFile.flatMap(line =&gt; {

      try {
        val reader = new CSVReader(new StringReader(line))
        Some(reader.readNext())
      } catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
    })

    val numericData = splitLines.flatMap(line =&gt; {

      try {
        Some(line.map(_.toDouble))
      } catch {
        case _ =&gt; {
          invalidNumericLineCounter += 1
          None
        }
      }
    })

    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
    println("Errors: " + invalidLineCounter + "," + invalidNumericLineCounter)
  }
}
</pre>

<p>
This example demonstrates the use of accumulators to count invalid lines and lines with invalid numeric data. It also utilises <code>flatMap()</code> to handle parsing errors and filter out invalid lines.
</p>

<p>
<span id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Example: Word Frequency Analysis (Python)"></span><strong id="Example: Word Frequency Analysis (Python)">Example: Word Frequency Analysis (Python)</strong>
</p>

<pre python="">from pyspark.context import SparkContext
from pyspark.conf import SparkConf
from operator import add

print("Running Spark Version %s" % (sc.version))
conf = SparkConf()
print(conf.toDebugString())

# Read and process Barack Obama's speeches
lines = sc.textFile("sotu/2009-2014-BO.txt")
word_count_bo = lines.flatMap(lambda x: x.split(' ')). \
    map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \
    reduceByKey(add)

word_count_bo.count()  # 6658 without lower, 6299 with lower, rstrip, lstrip 4835

# Read and process Abraham Lincoln's speeches
lines = sc.textFile("sotu/1861-1864-AL.txt")
word_count_al = lines.flatMap(lambda x: x.split(' ')). \
    map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \
    reduceByKey(add)

word_count_al.count()

# Sort word counts for Barack Obama's speeches
word_count_bo_1 = word_count_bo.sortBy(lambda x: x[1], ascending=False)

# Print top 10 most frequent words
for x in word_count_bo_1.take(10):
    print(x)

# Filter out common words
common_words = [...]
word_count_bo_clean = word_count_bo_1.filter(lambda x: x not in common_words)
word_count_al_clean = word_count_al.filter(lambda x: x not in common_words)

# Find words spoken by Obama but not Lincoln
for x in word_count_bo_clean.subtractByKey(word_count_al_clean).sortBy(lambda x: x[1], ascending=False).take(15):
    print(x)
</pre>

<p>
This example illustrates reading text files, splitting into words, calculating word frequencies, sorting, filtering, and comparing word usage between two sets of speeches.
</p>

<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists"><h2 id="Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists">Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists</a></h2></div>

<p>
Spark supports multiple programming languages, including Scala, Java, Python, and R. This multilingual capability allows developers to use the language they are most comfortable with for different parts of a Spark application. This approach, known as polyglot programming, offers several advantages:
</p>

<ul>
<li>
<span id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Leveraging Existing Skills"></span><strong id="Leveraging Existing Skills">Leveraging Existing Skills</strong>: Developers can use their existing language skills to work with Spark without having to learn a new language.

</li><li>
<span id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Using Specialized Libraries"></span><strong id="Using Specialized Libraries">Using Specialized Libraries</strong>: Different languages have different strengths and specialized libraries. Polyglot programming allows developers to use the most suitable language and libraries for specific tasks within a Spark application.

</li><li>
<span id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Code Reusability"></span><strong id="Code Reusability">Code Reusability</strong>: Code written in one language can often be reused or adapted for use in other languages, promoting code sharing and reducing development time.

</li><li>


</li></ul>
<p>
Spark encourages polyglot programming by providing consistent APIs across different languages, making it easy to switch between languages and integrate code written in different languages.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>