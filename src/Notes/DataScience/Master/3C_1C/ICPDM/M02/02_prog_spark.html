<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Programming Spark Applications</title>
  <link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../index.html">Back</a>
</p>

<div id="Programming Spark Applications"><h1 id="Programming Spark Applications" class="header"><a href="#Programming Spark Applications">Programming Spark Applications</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="02_prog_spark.html#Chapter%203%3A%20Building%20and%20Running%20a%20Spark%20Application">Chapter 3  Building and Running a Spark Application</a>

<ul>
<li>
<a href="02_prog_spark.html#Building%20Spark%20Applications">Building Spark Applications</a>

<ul>
<li>
<a href="02_prog_spark.html#Popular%20IDE%20Choices">Popular IDE Choices</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Building%20Spark%20Jobs%20with%20Maven">Building Spark Jobs with Maven</a>

</li><li>
<a href="02_prog_spark.html#Building%20Spark%20Jobs%20with%20Other%20Build%20Systems">Building Spark Jobs with Other Build Systems</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%204%3A%20Creating%20a%20SparkSession%20Object">Chapter 4  Creating a SparkSession Object</a>

<ul>
<li>
<a href="02_prog_spark.html#Introduction%20to%20SparkSession">Introduction to SparkSession</a>

</li><li>
<a href="02_prog_spark.html#Importance%20of%20SparkSession">Importance of SparkSession</a>

</li><li>
<a href="02_prog_spark.html#Key%20Points%20about%20SparkSession">Key Points about SparkSession</a>

</li><li>
<a href="02_prog_spark.html#Building%20a%20SparkSession%20Object">Building a SparkSession Object</a>

<ul>
<li>
<a href="02_prog_spark.html#Rules%20for%20using%20SparkSession%20and%20SparkContext">Rules for using SparkSession and SparkContext</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#SparkContext%20Metadata">SparkContext Metadata</a>

</li><li>
<a href="02_prog_spark.html#Shared%20Java%20and%20Scala%20APIs%20for%20SparkSession.SparkContext">Shared Java and Scala APIs for SparkSession SparkContext</a>

</li><li>
<a href="02_prog_spark.html#Python%20SparkSession">Python SparkSession</a>

</li><li>
<a href="02_prog_spark.html#iPython%20Interaction%20with%20SparkContext">iPython Interaction with SparkContext</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%205%3A%20Loading%20and%20Saving%20Data%20in%20Spark">Chapter 5  Loading and Saving Data in Spark</a>

<ul>
<li>
<a href="02_prog_spark.html#Spark%20Abstractions">Spark Abstractions</a>

</li><li>
<a href="02_prog_spark.html#Recommendations%20for%20using%20Datasets%2C%20DataFrames%2C%20and%20RDDs">Recommendations for using Datasets  DataFrames  and RDDs</a>

</li><li>
<a href="02_prog_spark.html#Understanding%20RDDs">Understanding RDDs</a>

</li><li>
<a href="02_prog_spark.html#Data%20Modalities">Data Modalities</a>

</li><li>
<a href="02_prog_spark.html#Data%20Modalities%20and%20Spark%20Abstractions">Data Modalities and Spark Abstractions</a>

</li><li>
<a href="02_prog_spark.html#Loading%20Data%20into%20an%20RDD">Loading Data into an RDD</a>

</li><li>
<a href="02_prog_spark.html#Saving%20Data%20from%20an%20RDD">Saving Data from an RDD</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%206%3A%20Manipulating%20Your%20RDD">Chapter 6  Manipulating Your RDD</a>

<ul>
<li>
<a href="02_prog_spark.html#Manipulating%20RDDs%20in%20Scala%20and%20Java">Manipulating RDDs in Scala and Java</a>

<ul>
<li>
<a href="02_prog_spark.html#Key%20RDD%20Operations">Key RDD Operations</a>

</li><li>
<a href="02_prog_spark.html#Shared%20States%20and%20Accumulators">Shared States and Accumulators</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Manipulating%20RDDs%20in%20Python">Manipulating RDDs in Python</a>

<ul>
<li>
<a href="02_prog_spark.html#Key%20RDD%20Operations%20in%20Python">Key RDD Operations in Python</a>

</li><li>
<a href="02_prog_spark.html#Broadcast%20Values%20and%20Accumulators%20in%20Python">Broadcast Values and Accumulators in Python</a>

</li></ul>
</li></ul>
</li><li>
<a href="02_prog_spark.html#Chapter%209%3A%20Foundations%20of%20Datasets%2FDataFrames%20%2013%20The%20Proverbial%20Workhorse%20for%20Data%20Scientists">Chapter 9  Foundations of Datasets DataFrames   The Proverbial Workhorse for Data Scientists</a>

<ul>
<li>
<a href="02_prog_spark.html#Datasets%3A%20A%20Quick%20Introduction">Datasets  A Quick Introduction</a>

</li><li>
<a href="02_prog_spark.html#Preferred%20Mechanisms%20in%20Spark">Preferred Mechanisms in Spark</a>

</li><li>
<a href="02_prog_spark.html#Key%20Points%20about%20Datasets%20and%20DataFrames">Key Points about Datasets and DataFrames</a>

</li><li>
<a href="02_prog_spark.html#Dataset%20APIs%3A%20An%20Overview">Dataset APIs  An Overview</a>

</li><li>
<a href="02_prog_spark.html#Key%20Classes%20and%20Functions">Key Classes and Functions</a>

</li><li>
<a href="02_prog_spark.html#Additional%20Notes%20on%20Data%20Manipulation">Additional Notes on Data Manipulation</a>

</li><li>
<a href="02_prog_spark.html#Data%20Wrangling%20Example%3A%20Analyzing%20Northwind%20Sales%20Data%20%28Scala%29">Data Wrangling Example  Analyzing Northwind Sales Data  Scala</a>

</li></ul>
</li><li>
<a href="02_prog_spark.html#Emphasised%20Points">Emphasised Points</a>

</li></ul>
<hr>

<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application"><h2 id="Chapter 3: Building and Running a Spark Application" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application">Chapter 3: Building and Running a Spark Application</a></h2></div>

<p>
This chapter focuses on the mechanics of building Spark applications, including various toolchains and IDEs used for building and compiling applications.
</p>

<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Applications"><h3 id="Building Spark Applications" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Applications">Building Spark Applications</a></h3></div>

<ul>
<li>
Interactive mode with Spark shell: Useful for quick prototyping.

</li><li>
IDE for application development: A range of IDEs available for developing algorithms, exploring data (data wrangling), and modelling analytics applications.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Applications-Popular IDE Choices"><h4 id="Popular IDE Choices" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Applications-Popular IDE Choices">Popular IDE Choices</a></h4></div>

<ul>
<li>
Data exploration:

<ul>
<li>
iPython: Python-based

</li><li>
Zeppelin: Supports Scala/Java (but can handle major languages including Scala, Java, Python, and SQL)

</li></ul>
</li><li>
Scala and Java development:

<ul>
<li>
Eclipse

</li><li>
IntelliJ

</li></ul>
</li></ul>
<p>
The book primarily uses the Spark shell and occasionally iPython for data wrangling and exploring Spark APIs. Deploying Spark applications requires compiling for Java and Scala. Building Spark jobs is more complex than regular applications because dependencies must be available on all cluster machines.
</p>

<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven"><h3 id="Building Spark Jobs with Maven" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Maven">Building Spark Jobs with Maven</a></h3></div>

<ul>
<li>
Maven is an open-source tool for building Spark jobs in Java or Scala.

</li><li>
Officially recommended for packaging Spark and is the "build of reference".

</li><li>
Can include Spark dependency through Maven Central, simplifying the build process.

</li><li>
Ability to package Spark and dependencies into a single JAR file using a plugin or build Spark as a monolithic JAR file using sbt/sbt assembly.

</li></ul>
 
<p>
Steps to build a Spark Job with Maven:
</p>
<ul>
<li>
Create a new directory and generate the Maven template. The example shows building a Java Spark job:
<pre bash="">mkdir example-java-build/; cd example-java-build
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.maven.archetypes \
    -DgroupId=spark.examples \
    -DartifactId=JavaWordCount \
    -Dfilter=org.apache.maven.archetypes:maven-archetype-quickstart
cp ../examples/src/main/java/spark/examples/JavaWordCount.java \
JavaWordCount/src/main/java/spark/examples/JavaWordCount.java
</pre>

</li><li>
Update Maven <code>pom.xml</code> to include Spark version and JDK version information. Add the following code between the &lt;project&gt; tags:
<pre xml="">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.spark-project&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.0.0&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.7&lt;/source&gt;
                &lt;target&gt;1.7&lt;/target&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</pre>

</li><li>
Build the JAR file:
<pre bash="">mvn package
</pre>

</li><li>
Run the Spark job:
<pre bash="">SPARK_HOME="../"
SPARK_EXAMPLES_JAR="./target/JavaWordCount-1.0-SNAPSHOT.jar"
java -cp ./target/JavaWordCount-1.0-SNAPSHOT.jar:../../core/target/spark-core-assembly-1.5.2.jar spark.examples.JavaWordCount local[1] ../../README
</pre>

</li></ul>
<div id="Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Other Build Systems"><h3 id="Building Spark Jobs with Other Build Systems" class="header"><a href="#Programming Spark Applications-Chapter 3: Building and Running a Spark Application-Building Spark Jobs with Other Build Systems">Building Spark Jobs with Other Build Systems</a></h3></div>

<ul>
<li>
Spark supports building a fat JAR file containing all its dependencies.

</li><li>
Useful for including in other build systems.

</li><li>
Run sbt/sbt assembly in the Spark directory and copy the resulting assembly JAR file (core/target/spark-core-assembly-1.5.2.jar) to build dependencies.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object"><h2 id="Chapter 4: Creating a SparkSession Object" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object">Chapter 4: Creating a SparkSession Object</a></h2></div>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Introduction to SparkSession"><h3 id="Introduction to SparkSession" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Introduction to SparkSession">Introduction to SparkSession</a></h3></div>

<ul>
<li>
Introduced in Spark 2.0.0.

</li><li>
Represents a connection to a Spark cluster, either local or remote.

</li><li>
Provides entry point to interact with Spark.

</li><li>
Necessary for distributing jobs and interacting with Spark.

</li><li>
Encapsulates the SparkContext object.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Importance of SparkSession"><h3 id="Importance of SparkSession" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Importance of SparkSession">Importance of SparkSession</a></h3></div>

<p>
Datasets/DataFrames, the main distributed data abstraction interface from Spark 2.0.0 onwards. Although Datasets/DataFrames are now the primary interface, RDDs are still important, because the underlying structure of Datasets/DataFrames are in fact RDDs.
</p>

<p>
Spark used to have three main connection objects: SparkContext, SqlContext and HiveContext. SparkContext created RDDs, SQLContext worked with SparkSQL in the background of SparkContext and HiveContext interacted with Hive stores. However, these connection objects were replaced by the SparkSession and Datasets/DataFrames in Spark 2.0.0.
</p>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Key Points about SparkSession"><h3 id="Key Points about SparkSession" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Key Points about SparkSession">Key Points about SparkSession</a></h3></div>

<ul>
<li>
In Scala and Java, <span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Key Points about SparkSession-Datasets"></span><strong id="Datasets">Datasets</strong> are the primary data abstraction as typed data. In Python and R, the abstraction is <span id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Key Points about SparkSession-DataFrame"></span><strong id="DataFrame">DataFrame</strong> due to the lack of compile-time type checking.

</li><li>
Datasets in Scala/Java are functionally equivalent to DataFrames in Python/R.

</li><li>
SparkContext is still required for interacting with RDDs and can be obtained from the SparkSession object.

</li></ul>
<p>
SparkSession encapsulates the SparkContext object and therefore unifies the process of reading data in different formats, creating Datasets/DataFrames, creating views to execute SQL statements and working with RDDs. SparkSession should be used for reading and creating Datasets, but to interact with RDDs, a SparkContext object is still required and can be obtained from the SparkSession.
</p>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object"><h3 id="Building a SparkSession Object" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object">Building a SparkSession Object</a></h3></div>

<p>
Scala and Python:
</p>

<pre java="">val sparkSession = new SparkSession.builder.master(master_path).appName("application name").config("optional configuration parameters").getOrCreate()
</pre>

<ul>
<li>
It's recommended to read values from the environment with reasonable defaults for flexibility in changing environments.

</li><li>
<code>spark-shell/pyspark</code> automatically creates the SparkSession object and assigns it to the spark variable.

</li><li>
Access the SparkContext object using spark.sparkContext.

</li></ul>
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object-Rules for using SparkSession and SparkContext"><h4 id="Rules for using SparkSession and SparkContext" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Building a SparkSession Object-Rules for using SparkSession and SparkContext">Rules for using SparkSession and SparkContext</a></h4></div>

<ul>
<li>
Build a SparkSession object (or use the existing one in <code>spark-shell/pyspark</code>).

</li><li>
Use SparkSession object for reads, creating views for SQL statements, and creating Datasets and DataFrames.

</li><li>
Get the SparkContext object from SparkSession for operations like accumulators, distributing cache files, and working with RDDs.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-SparkContext Metadata"><h3 id="SparkContext Metadata" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-SparkContext Metadata">SparkContext Metadata</a></h3></div>

<p>
The SparkContext object provides useful metadata, including:
</p>

<ul>
<li>
<code>appName</code>: Application name.

</li><li>
<code>getConf</code>: Returns configuration information.

</li><li>
<code>getExecutorMemoryStatus</code>: Retrieves memory details.

</li><li>
<code>master</code>: Master node name.

</li><li>
<code>version</code>: Spark version.

</li></ul>
 
<p>
Example: Accessing SparkContext metadata in the Spark shell (Scala)
</p>

<pre>scala&gt; spark.version
res0: String = 2.0.0

scala&gt; sc.appName
res1: String = Spark shell

scala&gt; sc.version
res2: String = 2.0.0

scala&gt; sc.getExecutorMemoryStatus
res3: scala.collection.Map[String,(Long, Long)] = Map(10.0.1.2:54783 -&gt; (384093388,384093388))

scala&gt; sc.getConf.toDebugString
res5: String = hive.metastore.warehouse.dir=file:/Users/ksankar/fdps-v3/spark-warehouse spark.app.id=local-1471217311152 spark.app.name=Spark shell spark.driver.host=10.0.1.2 spark.driver.port=54782 spark.executor.id=driver spark.home=/Users/ksankar/Downloads/spark-2.0.0 spark.jars= spark.master=local[*] spark.repl.class.outputDir=/private/var/folders/gq/70vnnyfj6913b6lms_td7gb4 0000gn/T/spark-63174a71-e33f-4265-a427-bdc140553210/repl-35c5c348-cd19-482c-af47-3aff08a7fa42 spark.repl.class.uri=spark://10.0.1.2:54782/classes spark.sql.catalogImplementation=in-memory spark.submit.deployMode=client
</pre>

<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Shared Java and Scala APIs for SparkSession.SparkContext"><h3 id="Shared Java and Scala APIs for SparkSession.SparkContext" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Shared Java and Scala APIs for SparkSession.SparkContext">Shared Java and Scala APIs for SparkSession.SparkContext</a></h3></div>

<p>
Methods for non-data-driven operations:
</p>

<ul>
<li>
<code>addJar(path)</code>: Adds a JAR file for future jobs running through the SparkContext object.

</li><li>
<code>addFile(path)</code>: Downloads a file to all cluster nodes.

</li><li>
<code>listFiles/listJars</code>: Lists all added files/JARs.

</li><li>
<code>stop()</code>: Shuts down SparkContext.

</li><li>
<code>clearFiles()</code>: Removes files so new nodes don't download them.

</li><li>
<code>clearJars()</code>: Removes JARs from future job requirements.

</li></ul>
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Python SparkSession"><h3 id="Python SparkSession" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-Python SparkSession">Python SparkSession</a></h3></div>

<p>
Python SparkSession functions similarly to Scala.
</p>

<p>
Example: Accessing SparkContext metadata in pyspark
</p>

<pre python="">&gt;&gt;&gt; spark.version
u'2.0.0'

&gt;&gt;&gt; sc.version
u'2.0.0'

&gt;&gt;&gt; sc.appName
u'PySparkShell'

&gt;&gt;&gt; sc.master
u'local[*]'

&gt;&gt;&gt; sc.getMemoryStatus
Traceback (most recent call last):
    File "&lt;stdin&gt;", line 1, in &lt;module&gt;
AttributeError: 'SparkContext' object has no attribute 'getMemoryStatus'

&gt;&gt;&gt; from pyspark.conf import SparkConf
&gt;&gt;&gt; conf = SparkConf()
&gt;&gt;&gt; conf.toDebugString()
u'spark.app.name=PySparkShell\nspark.master=local[*]\nspark.submit.deployMode=client'
</pre>

<ul>
<li>
The <code>getExecutorMemoryStatus</code> call is not yet available in PySpark.

</li></ul>
<div id="Programming Spark Applications-Chapter 4: Creating a SparkSession Object-iPython Interaction with SparkContext"><h3 id="iPython Interaction with SparkContext" class="header"><a href="#Programming Spark Applications-Chapter 4: Creating a SparkSession Object-iPython Interaction with SparkContext">iPython Interaction with SparkContext</a></h3></div>

<p>
Steps to start iPython and interact with SparkContext:
</p>

<ul>
<li>
Navigate to the directory containing the code and data (e.g., <code>~/fdps-v3</code>).

</li><li>
Run the following command to start iPython:
<pre python="">PYSPARK_DRIVER_PYTHON=ipython
PYSPARK_DRIVER_PYTHON_OPTS="notebook"
~/Downloads/spark-2.0.0/bin/pyspark
</pre>

</li><li>
This launches the iPython notebook in the web browser.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark"><h2 id="Chapter 5: Loading and Saving Data in Spark" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark">Chapter 5: Loading and Saving Data in Spark</a></h2></div>

<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Spark Abstractions"><h3 id="Spark Abstractions" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Spark Abstractions">Spark Abstractions</a></h3></div>

<ul>
<li>
RDDs:

<ul>
<li>
Spark's primary unit for data representation.

</li><li>
Allow for easy parallel operations.

</li><li>
Low-level raw structures that can be optimised for performance and scalability.

</li></ul>
</li><li>
Datasets/DataFrames:

<ul>
<li>
API-level abstractions introduced in Spark 2.0.0.

</li><li>
Provide most RDD operations but are layered over RDDs via optimised query plans.

</li><li>
Similar to a table or spreadsheet with column headings and data types.

</li></ul>
</li></ul>
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Recommendations for using Datasets, DataFrames, and RDDs"><h3 id="Recommendations for using Datasets, DataFrames, and RDDs" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Recommendations for using Datasets, DataFrames, and RDDs">Recommendations for using Datasets, DataFrames, and RDDs</a></h3></div>

<ul>
<li>
Use Datasets and DataFrames whenever possible.

</li><li>
Use RDDs for low-level manipulations to implement complex operations or algorithms.

</li><li>
Datasets/DataFrames and RDDs can be converted back and forth using <code>dataset.rdd()</code> and <code>SparkSession.createDataset(rdd)/SparkSession.createDataFrame(rdd)</code>.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Understanding RDDs"><h3 id="Understanding RDDs" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Understanding RDDs">Understanding RDDs</a></h3></div>

<ul>
<li>
Created by parallelizing a collection or reading data from external sources.

</li><li>
Follow lazy evaluation: expressions are only evaluated when needed (when an action is called).

</li><li>
Immutable: operations generate new RDDs.

</li><li>
Computation occurs when data is referenced by caching or writing out the RDD.

</li><li>
Recomputed each time they are materialized.

</li><li>
Caching RDDs improves performance if used frequently.

</li></ul>
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Data Modalities"><h3 id="Data Modalities" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Data Modalities">Data Modalities</a></h3></div>

<ul>
<li>
Structured: Typically stored in databases with fixed formats and data types (e.g., relational tables).

</li><li>
Semi-structured: Possess some structure but allow variability in size, type, and format (e.g., CSV, JSON, Parquet).

</li><li>
Unstructured: Comprise about 85% of data and lack predefined structure (e.g., images, audio files, social media data).

</li></ul>
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Data Modalities and Spark Abstractions"><h3 id="Data Modalities and Spark Abstractions" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Data Modalities and Spark Abstractions">Data Modalities and Spark Abstractions</a></h3></div>

<ul>
<li>
Unstructured data: Use SparkContext and RDDs.

</li><li>
Semi-structured and structured data: Use SparkSession and Datasets/DataFrames.

</li></ul>
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Loading Data into an RDD"><h3 id="Loading Data into an RDD" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Loading Data into an RDD">Loading Data into an RDD</a></h3></div>

<ul>
<li>
From a Scala collection:

<ul>
<li>
Use the <code>parallelize()</code> function of the SparkContext object to convert a Scala collection into an RDD.

</li></ul>
</li><li>
From a text file:

<ul>
<li>
Use the <code>textFile()</code> function of the SparkContext object.

</li><li>
Ensure the file is available on all cluster nodes.

</li><li>
Use <code>addFile()</code> to copy the file to all machines in distributed mode.

</li></ul>
</li><li>
From a CSV file:

<ul>
<li>
Parse using custom functions: Use <code>split()</code> and <code>map()</code> functions to parse the CSV data.

</li><li>
Use a CSV library: Employ libraries like <code>opencsv</code> (Java and Scala) or the <code>csv</code> library (Python).

</li></ul>
</li><li>
From a sequence file:

<ul>
<li>
Use the <code>sequenceFile()</code> function of the SparkContext object.

</li><li>
Specify the types of keys and values, which must be subclasses of Hadoop's Writable class or implicitly convertible.

</li></ul>
</li><li>
From HBase:

<ul>
<li>
Use the <code>newAPIHadoopRDD()</code> function of the SparkContext object.

</li><li>
Configure the connection to HBase using <code>HBaseConfiguration</code>.

</li></ul>
</li></ul>
<div id="Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Saving Data from an RDD"><h3 id="Saving Data from an RDD" class="header"><a href="#Programming Spark Applications-Chapter 5: Loading and Saving Data in Spark-Saving Data from an RDD">Saving Data from an RDD</a></h3></div>

<ul>
<li>
Saving methods are defined on the RDD classes.

</li><li>
Scala and Java:

<ul>
<li>
<code>saveAsTextFile("out.txt")</code>: Saves RDD as a text file.

</li><li>
<code>saveAsObjectFile("sequenceOut")</code>: Saves RDD as a sequence file.

</li></ul>
</li><li>
Python:

<ul>
<li>
<code>saveAsTextFile("out.txt")</code>: Saves RDD as a text file.

</li></ul>
</li><li>
Compressed text file:

<ul>
<li>
Use the <code>saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec])</code> function.

</li></ul>
</li></ul>
<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD"><h2 id="Chapter 6: Manipulating Your RDD" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD">Chapter 6: Manipulating Your RDD</a></h2></div>

<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java"><h3 id="Manipulating RDDs in Scala and Java" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java">Manipulating RDDs in Scala and Java</a></h3></div>

<ul>
<li>
RDDs are immutable: operations create new RDDs.

</li><li>
Java lacks type inference and anonymous functions, making code more verbose compared to Scala.

</li><li>
Spark requires Scala's Tuple2 class for key-value pairs in Java.

</li></ul>
<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java-Key RDD Operations"><h4 id="Key RDD Operations" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java-Key RDD Operations">Key RDD Operations</a></h4></div>

<ul>
<li>
<code>map()</code>: Applies a function to each element of the RDD, creating a new RDD with transformed elements.

</li><li>
<code>reduce()</code>: Combines all elements of the RDD using a specified function, returning a single value.

</li><li>
<code>reduceByKey()</code>: Works on PairRDDs (key-value pairs), applying the reduce operation to values with the same key.

</li><li>
<code>flatMap()</code>: Applies a function that returns an iterable to each element, flattening the results into a new RDD.

</li><li>
<code>groupByKey()</code>: Groups values with the same key.

</li><li>
<code>combineByKey()</code>: A more general function for combining values by key, offering greater flexibility and efficiency compared to groupByKey().

</li><li>
<code>foldByKey()</code>: Similar to a traditional fold operation, applying a function iteratively to elements with the same key.

</li><li>
<code>addFile()</code>: Distributes files across the cluster, useful for libraries requiring accompanying files.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java-Shared States and Accumulators"><h4 id="Shared States and Accumulators" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Scala and Java-Shared States and Accumulators">Shared States and Accumulators</a></h4></div>

<ul>
<li>
Shared states should be avoided in distributed computations.

</li><li>
Accumulators provide a safe mechanism for sharing counters or aggregates across the cluster [40].

</li></ul>
 
<p>
Example: Parsing CSV Files with Error Handling (Scala)
</p>

<pre java="">import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkFiles
import org.apache.spark.api.java.JavaSparkContext
import au.com.bytecode.opencsv.CSVReader
import java.io.StringReader

object LoadCsvWithCountersExample {

  def main(args: Array[String]) {

    val sc = new SparkContext("local", "Chapter 6")
    println(s"Running Spark Version ${sc.version}")

    val invalidLineCounter = sc.accumulator(0)
    val invalidNumericLineCounter = sc.accumulator(0)

    val inFile = sc.textFile("/Volumes/sdxc-01/fdps-vii/data/Line_of_numbers.csv")

    val splitLines = inFile.flatMap(line =&gt; {

      try {
        val reader = new CSVReader(new StringReader(line))
        Some(reader.readNext())
      } catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
    })

    val numericData = splitLines.flatMap(line =&gt; {

      try {
        Some(line.map(_.toDouble))
      } catch {
        case _ =&gt; {
          invalidNumericLineCounter += 1
          None
        }
      }
    })

    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
    println("Errors: " + invalidLineCounter + "," + invalidNumericLineCounter)
  }
}
</pre>

<p>
This example demonstrates the use of accumulators to count invalid lines and lines with invalid numeric data. It also utilises <code>flatMap()</code> to handle parsing errors and filter out invalid lines.
</p>

<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python"><h3 id="Manipulating RDDs in Python" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python">Manipulating RDDs in Python</a></h3></div>

<p>
Spark's Python API is more limited than Java and Scala but supports most core functionality [42].
</p>

<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python-Key RDD Operations in Python"><h4 id="Key RDD Operations in Python" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python-Key RDD Operations in Python">Key RDD Operations in Python</a></h4></div>

<ul>
<li>
<code>map()</code>: Applies a function to each element, returning a new RDD [42].

</li><li>
<code>reduce()</code>: Combines all elements using a function, returning a single value [42].

</li><li>
<code>reduceByKey()</code>: Applies reduce to values with the same key in PairRDDs [42].

</li><li>
<code>flatMap()</code>: Flattens the results of a function that returns an iterable for each element [42].

</li></ul>
 
<p>
Example: Word Frequency Analysis (Python)
</p>

<pre python="">from pyspark.context import SparkContext
from pyspark.conf import SparkConf
from operator import add

print("Running Spark Version %s" % (sc.version))
conf = SparkConf()
print(conf.toDebugString())

# Read and process Barack Obama's speeches
lines = sc.textFile("sotu/2009-2014-BO.txt")
word_count_bo = lines.flatMap(lambda x: x.split(' ')). \
    map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \
    reduceByKey(add)

word_count_bo.count()  # 6658 without lower, 6299 with lower, rstrip, lstrip 4835

# Read and process Abraham Lincoln's speeches
lines = sc.textFile("sotu/1861-1864-AL.txt")
word_count_al = lines.flatMap(lambda x: x.split(' ')). \
    map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)). \
    reduceByKey(add)

word_count_al.count()

# Sort word counts for Barack Obama's speeches
word_count_bo_1 = word_count_bo.sortBy(lambda x: x[1], ascending=False)

# Print top 10 most frequent words
for x in word_count_bo_1.take(10):
    print(x)

# Filter out common words
common_words = [...]
word_count_bo_clean = word_count_bo_1.filter(lambda x: x not in common_words)
word_count_al_clean = word_count_al.filter(lambda x: x not in common_words)

# Find words spoken by Obama but not Lincoln
for x in word_count_bo_clean.subtractByKey(word_count_al_clean).sortBy(lambda x: x[1], ascending=False).take(15):
    print(x)
</pre>

<p>
This example illustrates reading text files, splitting into words, calculating word frequencies, sorting, filtering, and comparing word usage between two sets of speeches.
</p>

<div id="Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python-Broadcast Values and Accumulators in Python"><h4 id="Broadcast Values and Accumulators in Python" class="header"><a href="#Programming Spark Applications-Chapter 6: Manipulating Your RDD-Manipulating RDDs in Python-Broadcast Values and Accumulators in Python">Broadcast Values and Accumulators in Python</a></h4></div>

<ul>
<li>
Broadcast values: Allow sharing read-only values efficiently across partitions.

</li><li>
Accumulators: Provide a safe way to aggregate values from different partitions.

</li></ul>
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists"><h2 id="Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists">Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists</a></h2></div>

<p>
This chapter focuses on Datasets, a crucial feature for data wrangling in Spark 2.0.0. It covers the stack perspective of Datasets, including layering and optimisations, and the Dataset APIs.
</p>

<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Datasets: A Quick Introduction"><h3 id="Datasets: A Quick Introduction" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Datasets: A Quick Introduction">Datasets: A Quick Introduction</a></h3></div>

<ul>
<li>
A Spark Dataset is a collection of specified heterogeneous columns, similar to a spreadsheet or a relational database table.

</li><li>
Datasets introduce compile-time type checking, providing richer and more robust interfaces compared to RDDs.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Preferred Mechanisms in Spark"><h3 id="Preferred Mechanisms in Spark" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Preferred Mechanisms in Spark">Preferred Mechanisms in Spark</a></h3></div>

<ul>
<li>
Semantic-rich Datasets: The primary choice for data manipulation.

</li><li>
DataFrames: Used as untyped views within a Dataset.

</li><li>
RDDs: For low-level operations and underlying data representation.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Key Points about Datasets and DataFrames"><h3 id="Key Points about Datasets and DataFrames" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Key Points about Datasets and DataFrames">Key Points about Datasets and DataFrames</a></h3></div>

<ul>
<li>
Python and R:

<ul>
<li>
The class is still DataFrame but includes all Dataset APIs.

</li><li>
DataFrames in Python and R are essentially equivalent to Datasets in Scala and Java.

</li></ul>
</li><li>
Scala and Java:

<ul>
<li>
Datasets are the main interface, and there is no separate DataFrame class.

</li></ul>
</li><li>
Interchangeability:

<ul>
<li>
The terms DataFrame and Dataset are often interchangeable, but the context will clarify any distinctions.

</li></ul>
</li></ul>
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Dataset APIs: An Overview"><h3 id="Dataset APIs: An Overview" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Dataset APIs: An Overview">Dataset APIs: An Overview</a></h3></div>

<ul>
<li>
The hierarchy of classes for data manipulation in Spark is mirrored in both org.apache.spark.sql (Scala) and pyspark.sql (Python).

</li><li>
<code>pyspark.sql.DataFrame</code> is essentially the equivalent of org.apache.spark.sql.Dataset.

</li><li>
Polyglot programming is encouraged, leveraging both compiled (Scala/Java) and interpreted (Python/R) languages for different tasks.

</li></ul>
 
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Key Classes and Functions"><h3 id="Key Classes and Functions" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Key Classes and Functions">Key Classes and Functions</a></h3></div>

<ul>
<li>
<code>org.apache.spark.sql.SparkSession/pyspark.sql.SparkSession</code>:

<ul>
<li>
Single entry point for interacting with Spark data .

</li><li>
Functions:

<ul>
<li>
createDataset(): Creates a Dataset from an RDD.

</li><li>
<code>read.*</code>: Reads data from various formats (CSV, text, Parquet, JDBC, JSON, etc.).

</li><li>
<code>sql()</code>: Executes SQL queries.

</li><li>
sparkContext: Accesses the underlying SparkContext object.

</li></ul>
</li></ul>
</li><li>
<code>org.apache.spark.sql.Dataset/pyspark.sql.DataFrame</code>:

<ul>
<li>
Primary class for data wrangling.

</li><li>
Numerous functions (over 100), including <code>na.*</code> and <code>stat.*</code> mapped from other packages for convenience.

</li></ul>
</li><li>
<code>org.apache.spark.sql.{Column, Row}/pyspark.sql.(Column, Row)</code>:

<ul>
<li>
Column represents a column in a DataFrame.

</li><li>
Row represents a row in a DataFrame.

</li><li>
Functionality:

<ul>
<li>
Column selection using dot notation (e.g., <code>df.columnName</code>) or quoted notation (e.g., <code>df["columnName"]</code>).

</li><li>
Column operations (arithmetic, logical, comparisons, etc.) [56].

</li><li>
Meta operations (type conversion, alias, not null, etc.) [57].

</li></ul>
</li></ul>
</li></ul>
<p>
Example: Reading, Writing, and Aggregating Data (Scala)
</p>

<pre java="">val spark = SparkSession.builder
    .master("local")
    .appName("Chapter 9")
    .config("spark.logConf", "true")
    .config("spark.logLevel", "ERROR")
    .getOrCreate()

println("Running Spark Version ${spark.version}")

// Read data
val filePath = "/Users/ksankar/fdps-v3/"
val cars = spark.read.option("header", "true").option("inferSchema", "true").csv(filePath + "data/spark-csv/cars.csv")

println("Cars has " + cars.count() + " rows")

cars.show(5)
cars.printSchema()

// Write data
cars.write.mode("overwrite").option("header", "true").csv(filePath + "data/cars-out-csv.csv")  // CSV format
cars.write.mode("overwrite").partitionBy("year").parquet(filePath + "data/cars-out-pqt")  // Parquet format

// Aggregate functions
import org.apache.spark.sql.functions.{avg, mean}
cars.describe("mpg", "hp", "weight", "automatic").show()
cars.groupBy("automatic").avg("mpg", "torque").show()
cars.groupBy().avg("mpg", "torque").show()
cars.agg(avg(cars("mpg")), mean(cars("torque"))).show()
</pre>

<p>
This example covers reading CSV data, writing to CSV and Parquet formats, and performing basic aggregations using <code>describe()</code>, <code>groupBy()</code>, and <code>agg()</code>.
</p>

<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Additional Notes on Data Manipulation"><h3 id="Additional Notes on Data Manipulation" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Additional Notes on Data Manipulation">Additional Notes on Data Manipulation</a></h3></div>

<ul>
<li>
<code>partitionBy()</code> during writing creates subdirectories for efficient querying based on the partitioned column.

</li><li>
<code>mode()</code> parameter in <code>write()</code> controls the behaviour when the target file exists (overwrite, append, ignore, error).

</li><li>
<code>withColumn()</code> adds new columns to a Dataset.

</li><li>
<code>to_date()</code> converts strings to date format, requiring the yy-mm-dd format.

</li><li>
SQL functions (e.g., <code>avg()</code>, <code>mean()</code>, <code>sum()</code>) are imported from <code>org.apache.spark.sql.functions</code>.

</li><li>
<code>stat.*</code> functions provide statistical operations (e.g., correlation, covariance, crosstab).

</li><li>
Scientific functions (e.g., <code>log()</code>, <code>sqrt()</code>, <code>hypot()</code>) are also available in <code>org.apache.spark.sql.functions</code>.

</li><li>
Column objects support operations like arithmetic, comparisons, and logical operations.

</li></ul>
<div id="Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Data Wrangling Example: Analyzing Northwind Sales Data (Scala)"><h3 id="Data Wrangling Example: Analyzing Northwind Sales Data (Scala)" class="header"><a href="#Programming Spark Applications-Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for Data Scientists-Data Wrangling Example: Analyzing Northwind Sales Data (Scala)">Data Wrangling Example: Analyzing Northwind Sales Data (Scala)</a></h3></div>

<p>
This example demonstrates using Datasets to answer questions about sales data.
</p>

<p>
Steps:
</p>
<ul>
<li>
Read data into Datasets:

<ul>
<li>
Use read.csv() to load orders and order details data.

</li></ul>
</li><li>
Aggregate and sort:

<ul>
<li>
Use <code>groupBy()</code>, <code>count()</code>, and <code>sort()</code> to determine the number of orders per customer and per country.

</li></ul>
</li><li>
Create new columns and perform aggregations:

<ul>
<li>
Calculate order totals by adding a new column.

</li><li>
Add date-related columns (date, month, year).

</li><li>
Perform final aggregations to answer questions about orders by month/year, total sales by customer and year, and average sales by customer and year.

</li></ul>
</li></ul>
<div id="Programming Spark Applications-Emphasised Points"><h2 id="Emphasised Points" class="header"><a href="#Programming Spark Applications-Emphasised Points">Emphasised Points</a></h2></div>

<p>
The source provides code snippets and explanations primarily for Scala, though it mentions that Python APIs are similar and sometimes easier. The source encourages polyglot programming, switching between compiled and interpreted languages as needed. Datasets are presented as a powerful tool for data wrangling and analysis in Spark 2.0.0. The source suggests referring to the Spark documentation for comprehensive coverage of available functions and APIs.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="jQuery library"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/fuse.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>