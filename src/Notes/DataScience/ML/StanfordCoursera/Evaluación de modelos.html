<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../src/style/custom.css">
    <title>Evaluación de modelos</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Evaluación de modelos"><h1 id="Evaluación de modelos" class="header"><a href="#Evaluación de modelos">Evaluación de modelos</a></h1></div>

<hr>

<div id="Contents" class="toc"><h2 id="Contents" class="header"><a href="#Contents">Contents</a></h2></div>
<ul>
<li>
<a href="Evaluación de modelos.html#Separaci%F3n%20de%20datos">Separaci n de datos</a>

</li><li>
<a href="Evaluación de modelos.html#Entrenamiento%20en%20regresi%F3n%20lineal">Entrenamiento en regresi n lineal</a>

</li><li>
<a href="Evaluación de modelos.html#Entrenamiento%20en%20regresi%F3n%20log%EDstica">Entrenamiento en regresi n log stica</a>

</li><li>
<a href="Evaluación de modelos.html#Selecci%F3n%20de%20modelos">Selecci n de modelos</a>

<ul>
<li>
<a href="Evaluación de modelos.html#Cross%20Validation">Cross Validation</a>

<ul>
<li>
<a href="Evaluación de modelos.html#Proceso%20de%20selecci%F3n">Proceso de selecci n</a>

</li></ul>
</li></ul>
</li><li>
<a href="Evaluación de modelos.html#Diagn%F3stico%3A%20Sesgo%20vs%20Varianza">Diagn stico  Sesgo vs Varianza</a>

</li><li>
<a href="Evaluación de modelos.html#Regresi%F3n%20lineal%20con%20regularizaci%F3n">Regresi n lineal con regularizaci n</a>

<ul>
<li>
<a href="Evaluación de modelos.html#Escoger%20el%20par%E1metro%20de%20regularizaci%F3n">Escoger el par metro de regularizaci n</a>

</li></ul>
</li><li>
<a href="Evaluación de modelos.html#Curva%20de%20aprendizaje">Curva de aprendizaje</a>

</li><li>
<a href="Evaluación de modelos.html#Debugging%20un%20algoritmo%20de%20aprendizaje">Debugging un algoritmo de aprendizaje</a>

</li><li>
<a href="Evaluación de modelos.html#Medidas%20de%20evaluaci%F3n">Medidas de evaluaci n</a>

<ul>
<li>
<a href="Evaluación de modelos.html#Balance%20entre%20precisi%F3n%20y%20recall">Balance entre precisi n y recall</a>

</li></ul>
</li></ul>
<hr>

<div id="Evaluación de modelos-Separación de datos"><h2 id="Separación de datos" class="header"><a href="#Evaluación de modelos-Separación de datos">Separación de datos</a></h2></div>

<p>
A la hora de entrenar un modelo, separamos los datos en dos conjuntos:
</p>

<ul>
<li>
Conjunto de entrenamiento: \(70\%\) - \(80\%\)

</li><li>
Conjunto de test: \(30\%\) - \(20\%\)

</li></ul>
<div id="Evaluación de modelos-Entrenamiento en regresión lineal"><h2 id="Entrenamiento en regresión lineal" class="header"><a href="#Evaluación de modelos-Entrenamiento en regresión lineal">Entrenamiento en regresión lineal</a></h2></div>

<p>
El proceso de entrenamiento en la regresión lineal consiste en:
</p>

<ul>
<li>
Entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \(\Theta\) minimizando el coste \(J(\Theta)\)

</li><li>
Calcular el coste sobre el conjunto de test \(J_{test}(\Theta)\)
\[%align
J_{test}(\Theta) = \frac{1}{2m} \sum_{i=1}^{m_{test}} (h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2
\]

</li></ul>
<div id="Evaluación de modelos-Entrenamiento en regresión logística"><h2 id="Entrenamiento en regresión logística" class="header"><a href="#Evaluación de modelos-Entrenamiento en regresión logística">Entrenamiento en regresión logística</a></h2></div>

<p>
El proceso de entrenamiento en la regresión logística consiste en:
</p>

<ul>
<li>
Entrenar el modelo sobre el conjunto de entrenamiento y obtener la matriz de pesos \(\Theta\) minimizando el coste \(J(\Theta)\)

</li><li>
Calcular el coste sobre el conjunto de test \(J_{test}(\Theta)\)
\[%align
J_{test}(\Theta) = - \frac{1}{m} \sum_{i=1}^{m_{test}} \left[y^{(i)}_{test} \log(h_\Theta(x^{(i)}_{test})) + (1-y^{(i)}_{test})\log(1-h_\Theta(x^{(i)}_{test})) \right] 
\]

</li></ul>
<p>
El error de clasificación en la regresión logística se define como sigue:
</p>

\begin{align}
error(h_\Theta(x), y) =
\begin{cases}
1, &amp; \text{ si } h_\Theta(x) \geq 0.5 \rightarrow \log(h_\Theta(x)) = 1 \text{ e } y = 0 \\
1, &amp; \text{ si } h_\Theta(x) &lt; 0.5 \rightarrow \log(h_\Theta(x)) = 0 \text{ e } y = 1 \\
0, \text{ en cualquier otro caso } \\
\end{cases}
\end{align}

<div id="Evaluación de modelos-Selección de modelos"><h2 id="Selección de modelos" class="header"><a href="#Evaluación de modelos-Selección de modelos">Selección de modelos</a></h2></div>

<p>
Supongamos que tenemos \(n\) modelos, tal que cada modelo es equivalente al anterior pero con una característica más en sus datos:
</p>

<ul>
<li>
Modelo 1: \(h_\Theta(x) = \theta_0 + \theta_1 \cdot x_1\)

</li><li>
Modelo 2: \(h_\Theta(x) = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2\)

</li><li>
Modelo n: \(h_\Theta(x) = \theta_0 + \theta_1 \cdot x_1 + \theta_2 \cdot x_2 + \cdots + \theta_n \cdot x_n\)

</li></ul>
<p>
Para evaluar los modelos lo que hacemos es escoger el que menor coste obtenga sobre el conjunto de test, tras ser entrenado sobre el conjunto de entrenamiento. 
</p>

\begin{align}
\begin{bmatrix}
\Theta^{(1)} \\
\Theta^{(2)} \\
\vdots \\
\Theta^{(n)} \\
\end{bmatrix} \rightarrow
\begin{bmatrix}
J_{test}(\Theta^{(1)}) \\
J_{test}(\Theta^{(2)}) \\
\vdots \\
J_{test}(\Theta^{(n)}) \\
\end{bmatrix}
\end{align}

<p>
Sin embargo, se puede dar el <span id="Evaluación de modelos-Selección de modelos-problema"></span><strong id="problema">problema</strong> de que el mejor simplemente produzca overfitting sobre el conjunto de test (lo cual es probable cuando el vector de pesos tiene dimensiones grandes). Para solventar este problema:
</p>

<div id="Evaluación de modelos-Selección de modelos-Cross Validation"><h3 id="Cross Validation" class="header"><a href="#Evaluación de modelos-Selección de modelos-Cross Validation">Cross Validation</a></h3></div>

<p>
Separaremos el conjunto de datos en tres conjuntos:
</p>

<ul>
<li>
Conjunto de entrenamiento: \(60\%\)

</li><li>
Conjunto de validación cruzada (Cross validation): \(20\%\)

</li><li>
Conunto de test: \(20\%\)

</li></ul>
<p>
Por lo tanto ahora la función de coste para cada conjunto tiene la forma:
</p>

<ul>
<li>
Función de coste para el conjunto de entrenamiento: 
\begin{align}
J_{train}(\Theta) = \frac{1}{2m_{train}} \sum_{i=1}^{m_{train}} error(h_\Theta(x^{(i)}), y^{(i)})
\end{align}

</li><li>
Función de coste para el conjunto de test: 
\begin{align}
J_{test}(\Theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}} error(h_\Theta(x^{(i)}), y^{(i)})
\end{align}

</li><li>
Función de coste para el conjunto de validación cruzada: 
\begin{align}
J_{cv}(\Theta) = \frac{1}{2m_{cv}} \sum_{i=1}^{m_{cv}} error(h_\Theta(x^{(i)}), y^{(i)})
\end{align}

</li></ul>
<div id="Evaluación de modelos-Selección de modelos-Cross Validation-Proceso de selección"><h4 id="Proceso de selección" class="header"><a href="#Evaluación de modelos-Selección de modelos-Cross Validation-Proceso de selección">Proceso de selección</a></h4></div>

<p>
Entonces ahora para seleccionar un modelo lo que hacemos que para cada modelo \(q\):
</p>

<ol>
<li>
Minimizamos \(J_{train}(\Theta^{(q)})\) para obtener los pesos \(\Theta^{(q)}\) óptimos.

</li><li>
Calculamos el coste sobre el conjunto de validación cruzada \(J_{cv}(\Theta^{(q)})\)

</li></ol>
<p>
Una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validación cruzada y calculamos \(J_{test}(\Theta^{(q)})\) para evaluar la capacidad de generalización del modelo.
</p>

<div id="Evaluación de modelos-Diagnóstico: Sesgo vs Varianza"><h2 id="Diagnóstico: Sesgo vs Varianza" class="header"><a href="#Evaluación de modelos-Diagnóstico: Sesgo vs Varianza">Diagnóstico: Sesgo vs Varianza</a></h2></div>

<ul>
<li>
Underfitting: cuando se produce underfitting el coste de entrenamiento y el coste de validación tienen valores similares y ambos tiene valores bastante altos

</li><li>
Overfitting: cuando se produce overfitting el coste de entrenamiento es mucho menor que el coste de validación cruzada.

</li></ul>
<p>
<img src="https://albamr09.github.io/public/DataScience/ML/StanfordCoursera/bias_vs_variance.png" alt="Bias VS Variance" style="width:300px;height:200px">
</p>

<div id="Evaluación de modelos-Regresión lineal con regularización"><h2 id="Regresión lineal con regularización" class="header"><a href="#Evaluación de modelos-Regresión lineal con regularización">Regresión lineal con regularización</a></h2></div>

<p>
También es importante observar cómo afecta el parámetro de regularización a nuestros modelos. Por ejemplo, en la regresión linear, la función de coste tiene la forma:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m}\sum_{j=1}^m (h_\Theta(x_j) - y_j)^2 + \frac{1}{2m} \lambda \sum_{i=1}^n \theta_i^2
\end{align}

<p>
Por lo tanto, el aumentar o reducir \(\lambda\) es directamente proporcional al coste.
</p>

<ul>
<li>
Si el parámetro de regularización \(\lambda\) es muy grande entonces los pesos van a tender a ser muy pequeños (ya que el coste aumenta al aumentar el valor de \(\lambda\)) 

</li><li>
Si el parámetro de regularización \(\lambda\) es muy pequeño entonces los pesos van a poder ser grandes (ya que el coste se reduce al reducir el valor de \(\lambda\)) 

</li></ul>
<p>
<img src="https://albamr09.github.io/public/DataScience/ML/StanfordCoursera/tune_regularizacion.png" alt="Escoger parametro de regularizacion">
</p>

<div id="Evaluación de modelos-Regresión lineal con regularización-Escoger el parámetro de regularización"><h3 id="Escoger el parámetro de regularización" class="header"><a href="#Evaluación de modelos-Regresión lineal con regularización-Escoger el parámetro de regularización">Escoger el parámetro de regularización</a></h3></div>

<p>
Para escoger el parámetro de regularización seguimos el mismo proceso que para escoger el mejor modelo, para cada modelo \(q\):
</p>

<ol>
<li>
Minimizamos \(J_{train}(\Theta^{(q)})\) para obtener los pesos \(\Theta^{(q)}\) óptimos.

</li><li>
Calculamos el coste sobre el conjunto de validación cruzada \(J_{cv}(\Theta^{(q)})\)

</li></ol>
<p>
Una vez hecho esto para todos, escogemos el modelo que ofrezca el mejor coste sobre el conjunto de validación cruzada y calculamos \(J_{test}(\Theta^{(q)})\) para evaluar la capacidad de generalización del modelo.
</p>

<div id="Evaluación de modelos-Curva de aprendizaje"><h2 id="Curva de aprendizaje" class="header"><a href="#Evaluación de modelos-Curva de aprendizaje">Curva de aprendizaje</a></h2></div>

<p>
A continuación vamos a estudiar cómo afecta el tamaño del conjunto de datos \(m\), el sesgo y la varianza a nuestro modelo:
</p>

<ul>
<li>
Cuanto mayor es el tamaño, más difícil es encontrar una hipótesis que se adapte (\(J_{train}(\Theta)\) es mayor), pero el modelo generaliza mejor (\(J_{cv}(\Theta)\) es menor)

</li><li>
Cuando el sesgo (bias) es grande, entonces se produce underfitting y las predicciones de nuestro modelo son malas:

<ul>
<li>
El error del modelo es elevado, tanto sobre el conjunto de entrenamiento como sobre el conjunto de validación cruzada

</li><li>
Tener más ejemplos ayuda al modelo

</li></ul>
</li></ul>
   
<ul>
<li>
Cuando la varianza (variance) es grande, entonces se produce overfitting, tal que el error en el conjunto de validación cruzada es muy alto:

<ul>
<li>
El modelo se adapta al conjunto de datos, por lo que el error de entrenamiento es menor

</li><li>
Tener más muestras ayuda al modelo

</li></ul>
</li></ul>
<div id="Evaluación de modelos-Debugging un algoritmo de aprendizaje"><h2 id="Debugging un algoritmo de aprendizaje" class="header"><a href="#Evaluación de modelos-Debugging un algoritmo de aprendizaje">Debugging un algoritmo de aprendizaje</a></h2></div>

<p>
Para arreglar el overfitting que se produce cuando la varianza es elevada:
</p>

<ul>
<li>
Obtener más datos de entrenamiento

</li><li>
Utilizar menos características (reducir el grado del vector de pesos), pero tras un proceso de selección de aquellas más relevantes

</li><li>
Intentar aumentar el parámetro de regularización

</li></ul>
<p>
Para arregar el underfitting que se produce cuando el sesgo es elevado:
</p>

<ul>
<li>
Añadir más características

</li><li>
Añadir características polinómicas

</li><li>
Intentar reducir el parámetro de regularización

</li></ul>
<p>
En las redes neuronales:
</p>

<ul>
<li>
Las redes pequeñas tienden a producir underfitting pero son menos costosas computacionalmente

</li><li>
Las redes grandes tienen más características, por lo tanto hay una mayor probabilidad de overfitting

</li></ul>
<p>
Gestionar datos sesgados:
</p>

<ul>
<li>
Hay que ser consciente que a veces, por ejemplo en problemas de clasificación, hay categorías que con más comunes que el resto

</li></ul>
<div id="Evaluación de modelos-Medidas de evaluación"><h2 id="Medidas de evaluación" class="header"><a href="#Evaluación de modelos-Medidas de evaluación">Medidas de evaluación</a></h2></div>

<p>
La precisión y el recall son medidas de evaluación que se complementan:
</p>

<table>
<thead>
<tr>
<th>
&nbsp;
</th>
<th>
&nbsp;
</th>
<th>
Resultado
</th>
<th>
Resultado
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
&nbsp;
</td>
<td>
&nbsp;
</td>
<td>
1
</td>
<td>
0
</td>
</tr>
<tr>
<td>
Predicción
</td>
<td>
1
</td>
<td>
Verdadero positivo (TP)
</td>
<td>
Falso positivo (FP)
</td>
</tr>
<tr>
<td>
Predicción
</td>
<td>
0
</td>
<td>
Falso negativo (FN)
</td>
<td>
Verdadero negativo (TN)
</td>
</tr>
</tbody>
</table>

<ul>
<li>
Precisión = \(\frac{TP}{\text{ # positivos predichos }} = \frac{TP}{TP + FP}\)

</li><li>
Recall = \(\frac{TP}{\text{ # positivos reales }} = \frac{TP}{TP + FN}\)

</li></ul>
<div id="Evaluación de modelos-Medidas de evaluación-Balance entre precisión y recall"><h3 id="Balance entre precisión y recall" class="header"><a href="#Evaluación de modelos-Medidas de evaluación-Balance entre precisión y recall">Balance entre precisión y recall</a></h3></div>

<p>
Cuánto mayor es la precisión menor es el recall y viceversa. Entonces 
</p>

<ul>
<li>
Si queremos un modelo más preciso:
\begin{align}
\begin{cases}
\text{Predecir } 1, \text{ si } h_\Theta(x) \geq 0.7 \\
\text{Predecir } 0, \text{ si } h_\Theta(x) &lt; 0.7 \\
\end{cases}
\end{align}

</li></ul>
<p>
Entonces, la precisión es mayor ya que el número de \(FP\) es menor, pero el recall es menor, ya que el número de \(FN\) es mayor.
</p>

<ul>
<li>
Lo mismo pasa si queremos evitar falsos negativos, entonces hacemos:
\begin{align}
\begin{cases}
\text{Predecir } 1, \text{ si } h_\Theta(x) \geq 0.5 \\
\text{Predecir } 0, \text{ si } h_\Theta(x) &lt; 0.5 \\
\end{cases}
\end{align}

</li></ul>
<p>
Tal que se reduce el número de \(FN\), y se aumenta el recall, pero el número de \(FP\) es mayor, por lo que se reduce la precisión.
</p>

<p>
Entonces, para encontrar un punto de balance entre las dos medidas tenemos que seleccionar un valor límite, tal que hacemos la predicción en base a \(h_\Theta(x) \geq \text{ limite }\).
</p>

<p>
Para calibrar ese límite podemos utilizar dos métricas de evaluación:
</p>

<ol>
<li>
La media de ambas métricas: \(\frac{P + R}{2}\), funciona mal cuando \(P &gt;&gt; R\) o \(R &gt;&gt; P\), ya que el valor va a ser alto, pero no se ha encontrado un equilibrio. 

</li><li>
La puntuación \(F_1 = 2 \cdot \frac{P\cdot R}{(P + R)}\), tal que cuanto mayor sea esta puntuación mejor

<ol>
<li>
Ahora  
\begin{align}
  \begin{cases}
    F_1 \approx 0, &amp;&amp; P &gt;&gt; R \\ 
    F_1 \approx 0, &amp;&amp; R &gt;&gt; P \\ 
  \end{cases}
\end{align}

</li></ol>
</li></ol>
<p>
Para escoger el límite lo que se hace es calcular la puntuación \(F_1\) sobre el conjunto de validación cruzada, y se escoge aquel límite que ofrezca la mayor puntuación.
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>