<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Explicación de la retropropagación</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Explicación de la retropropagación"><h1 id="Explicación de la retropropagación" class="header"><a href="#Explicación de la retropropagación">Explicación de la retropropagación</a></h1></div>

<hr>

<ol>
<li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Descenso gradiente">Descenso Gradiente</a>

<ol>
<li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Descenso gradiente-Derivada del error">Derivada del Error</a>

</li></ol>
</li><li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Gradiente">Gradiente</a>

<ol>
<li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Gradiente-Capa de salida">Capa de salida</a>

</li><li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Gradiente-Retropropagación del gradiente">Retropropagación del Gradiente</a>

</li><li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Gradiente-Gradiente acumulado">Gradiente Acumulado</a>

</li><li>
<a href="Explicación de la retropropagación.html#Explicación de la retropropagación-Gradiente-Derivadas de las funciones de activación">Derivadas de las Funciones de Activación</a>

</li></ol>
</li></ol>
<hr>

<p>
Componentes:
</p>

<ul>
<li>
\(w_j^k\): peso de la neurona \(j=1...I_k\) en la capa \(k=1...H\)

</li><li>
\(a_{ij}^k=(w_j^k)^Th_i^{k-1}+b_j^k\): \(i=1...N\) (patrón), \(k=1...H\)(capa), \(j=1...I_k\) (neurona de la capa \(k\)). 

</li><li>
\(h_i^{k-1}\): salida de la capa \(k-1\) con \(I_{k-1}\) valores (uno por cada neurona \(j=1...I_{k-1}\) para cada patrón \(x_i\).

</li><li>
\(h_i^k\): salida de la capa \(k\) para cada patrón \(x_i\): \(h_{ij}=f(a_{ij}^k)\) con \(j=1...I_k\)

</li><li>
\(y_{ij}\): salida verdadera de la neurona de salida \(j\) y el patrón \(x_i\).

</li></ul>
<div id="Explicación de la retropropagación-Descenso gradiente"><h2 id="Descenso gradiente" class="header"><a href="#Explicación de la retropropagación-Descenso gradiente">Descenso gradiente</a></h2></div>

\begin{align}
\Delta w_j^k=-\mu \frac{\delta J}{\delta w_j^k}
\end{align}

\begin{align}
\Delta b_j^k=-\mu \frac{\delta J}{\delta b_j^k}
\end{align}

<p>
Para \(k=1...H\). De tal forma que se actualizan los pesos \(w_j^k\) y el <em>offset</em> \(b_j^k\) de la capa \(k\) y de la neurona \(j\).
</p>

<p>
Tenemos que la capa de salida está compuesta de \(I_H\) neuronas que se recorren con el índice \(j\). Accedemos a la salida verdadera del ejemplo \(i\) para la neurona \(j\) (\(y_{ij}\)) y restamos la salida predicha \(h_{ij}^H\) que hace referencia a la salida de la función de activación de la capa \(H\) para la neurona \(j\) y el ejemplo \(i\). La diferencia se eleva al cuadrado para obtener MSE. También se puede vectorizar restando los vectores \(y_i\) y \(h_i^H \in \mathbb{R}^j\). De esta manera obtenemos el error para el patrón \(x_i\): \(J_i\).
</p>

<div id="Explicación de la retropropagación-Descenso gradiente-Derivada del error"><h3 id="Derivada del error" class="header"><a href="#Explicación de la retropropagación-Descenso gradiente-Derivada del error">Derivada del error</a></h3></div>

<p>
Aplicamos la regla de la cadena sobre \(J_i\), ya que este depende de \(a_{ij}^k\):
</p>

\begin{align}
\frac{\delta J_i}{\delta w_j^k}=\frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta w_j^k}
\end{align}

\begin{align}
\frac{\delta J_i}{\delta b_j^k}=\frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta b_j^k}
\end{align}

<p>
Definimos:
</p>

\begin{align}
\delta_{ij}^k \equiv \frac{\delta J_i}{\delta a_{ij}^k}
\end{align}

<p>
Este indica el gradiente de la capa siguiente, para evitar tener que calcularlo.
</p>

\begin{align}
\frac{\delta a_{ij}^k}{\delta w_j^k} = h_i^{k-1}
\end{align}

\begin{align}
\frac{\delta a_{ij}^k}{\delta b_j^k} = 1
\end{align}

<p>
Debido a que el valor de \(a_{ij}^k\) es la combinación lineal de la entradas y los pesos, donde las entradas son las salidas de la capa anterior (\(k-1\)), es decir \(h_i^{k-1}\), de tal manera que:
</p>

\begin{align}
a_{ij}^k = (w_j^k)^Th_i^{k-1}+b_j^k
\end{align}

<p>
Por lo que la derivada en función de \(w_j^k\) se corresponde con \(h_i^{k-1}\) y la derivada en función de \(b_j^k\) es 1.
</p>


<div id="Explicación de la retropropagación-Gradiente"><h2 id="Gradiente" class="header"><a href="#Explicación de la retropropagación-Gradiente">Gradiente</a></h2></div>

<p>
Si sustituimos \(\delta_{ij}^k \equiv \frac{\delta J_i}{\delta a_{ij}^k}\) y \(\frac{\delta a_{ij}^k}{\delta w_j^k} = h_i^{k-1}\) en \(\frac{\delta J_i}{\delta w_j^k}=\frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta w_j^k}\) obtenemos:
</p>

\begin{align}
\Delta w_j^k=-\mu \frac{\delta J}{\delta w_j^k}
\end{align}

\begin{align}
\Delta w_j^k=-\mu \sum_{i=1}^N \frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta w_j^k} = -\mu \sum_{i=1}^N\delta_{ij}^kh_i^{k-1}
\end{align}

<p>
Hacemos los mismo para el <em>offset</em> sustituyendo \(\delta_{ij}^k \equiv \frac{\delta J_i}{\delta a_{ij}^k}\) y \(\frac{\delta a_{ij}^k}{\delta b_j^k} = 1\) en \(\frac{\delta J_i}{\delta b_j^k}=\frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta b_j^k}\) obtenemos:
</p>

\begin{align}
\Delta b_j^k=-\mu \frac{\delta J}{\delta b_j^k}
\end{align}

\begin{align}
\Delta b_j^k= -\mu \sum_{i=1}^N\frac{\delta J_i}{\delta a_{ij}^k}\frac{\delta a_{ij}^k}{\delta b_j^k}=-\mu \sum_{i=1}^N\delta_{ij}^k
\end{align}

<div id="Explicación de la retropropagación-Gradiente-Capa de salida"><h3 id="Capa de salida" class="header"><a href="#Explicación de la retropropagación-Gradiente-Capa de salida">Capa de salida</a></h3></div>

<p>
Calculamos  \(\delta_{ij}^k\) en la capa de salida (\(k=H\)), cuyo valor se va a propagar hacia las capas anteriores. Lo que vamos a calcular es \(\delta_{ij}^k \equiv \frac{\delta J_i}{\delta a_{ij}^k}\). Tenemos que la función de coste para el patrón \(i\), \(J_i\) viene definida por:
</p>

\begin{align}
J_i=\frac{1}{2}\sum_{j=1}^{I_H}(y_{ij}-h_{ij}^H)^2=\frac{|y_i-h_i^H|^2}{2}
\end{align}

<p>
Además el valor de \(a_{ij}^k\), que es la combinación lineal de las entradas (salidas de las neuronas capa anterior, \(k-1\)) y los pesos junto con el <em>offset</em>:
</p>

\begin{align}
a_{ij}^k=(w_j^k)^Th_i^{k-1}+b_j^k
\end{align}

<p>
Por lo tanto en la capa final:
</p>

\begin{align}
\frac{\delta J_i}{\delta a_{ij}^H}=\frac{1}{2}\frac{\delta (y_{ij}-h_{ij}^H)^2}{\delta (y_{ij}-h_{ij}^H)}\frac{\delta (y_{ij}-h_{ij}^H)}{\delta a_{ij}^H}
\end{align}

<p>
Donde:
</p>

\begin{align}
\frac{\delta (y_{ij}-h_{ij}^H)^2}{\delta (y_{ij}-h_{ij}^H)}=2(y_{ij}-h_{ij}^H)
\end{align}

\begin{align}
\frac{\delta (y_{ij}-h_{ij}^H)}{\delta a_{ij}^H}=\frac{\delta y_{ij}}{\delta a_{ij}^H}-\frac{\delta h_{ij}^H}{\delta a_{ij}^H}=0-f'(a_{ij}^H)
\end{align}

<p>
Ya que sabemos que \(h_{ij}^H=f(a_{ij}^H)\), por lo que:
</p>

\begin{align}
\frac{\delta h_{ij}^H}{\delta a_{ij}^H}=\frac{\delta f(a_{ij}^H)}{\delta a_{ij}^H}=f'(a_{ij}^H)
\end{align}

<p>
Una vez desarrollado todo esto sustiuimos los resultados en \(\frac{\delta J_i}{\delta a_{ij}^H}\):
</p>

\begin{align}
\frac{\delta J_i}{\delta a_{ij}^H}=\frac{1}{2}2(y_{ij}-h_{ij}^H)(-f'(a_{ij}^H))=(y_{ij}-h_{ij}^H)f'(a_{ij}^H)
\end{align}

<p>
De tal forma que:
</p>

\begin{align}
\delta_{ij}^H=\frac{\delta J_i}{\delta a_{ij}^H}=(y_{ij}-h_{ij}^H)f'(a_{ij}^H)=\epsilon_{ij}^Hf'(a_{ij}^H)
\end{align}

<p>
Donde se define \(\epsilon_{ij}^H\) como:
</p>

\begin{align}
\epsilon_{ij}^H=y_{ij}-h_{ij}^H
\end{align}

<p>
Finalmente obtenemos que el antigradiente en la última capa \(H\) viene dado por:
</p>

\begin{align}
\Delta w_j^H=-\mu \sum_{i=1}^N\delta_{ij}^Hh_i^{H-1}=-\mu\sum_{i=1}^N\epsilon_{ij}^Hf'(a_{ij}^H)h_i^{H-1}
\end{align}

\begin{align}
\Delta b_j^H=-\mu \sum_{i=1}^N\delta_{ij}^H=-\mu\sum_{i=1}^N\epsilon_{ij}^Hf'(a_{ij}^H)
\end{align}

<div id="Explicación de la retropropagación-Gradiente-Retropropagación del gradiente"><h3 id="Retropropagación del gradiente" class="header"><a href="#Explicación de la retropropagación-Gradiente-Retropropagación del gradiente">Retropropagación del gradiente</a></h3></div>

<p>
Para las capas anteriores a la capa de salida (\(k&lt;H\)):
</p>

\begin{align}
\delta_{ij}^k=\frac{\delta J_i}{\delta a_{ij}^k}=\sum_{l=1}^{I_{k+1}}\frac{\delta J_i}{\delta a_{il}^{k+1}}\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}=\sum_{l=1}^{I_{k+1}}\delta_{il}^{k+1}\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}
\end{align}

<p>
En este caso se utiliza la regla de la cadena para obtener \(\delta_{ij}^k\) de modo que se tienen en cuenta todas las combinaciones del gradiente acumulado \(\delta a_{il}^{k+1}\) con la neurona actual (\(\delta a_{ij}^k\)) donde \(l=1...I_{k+1}\), es decir se tienen encuenta todas las neuronas de la capa siguiente. 
</p>

<p>
Con grafos, la regla de la cadena se puede interpretar como todos los caminos posibles desde la capa de salida hasta la neurona \(j\) en la capa \(k\). Cada camino une cada neurona \(l\) de la capa siguiente: \(\delta_{il}^{k+1}\) (el cual ya tiene el gradiente acumulado de las capas siguientes) con una neurona \(j\) de la capa actual: \(\delta a_{ij}^k\) de la siguiente forma: \(\delta_{il}^{k+1}\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}\). Además se suman todas las combinación posibles: \(\sum_{l=1}^{I_{k+1}}\).
</p>

<p>
Esto nos permite utilizar el gradiente acumulado calculado en la capa siguiente que se propaga hacia atrás en la red neuronal, lo que evita tener que calcular \(\frac{\delta J_i}{\delta a_{ij}^k}\).
</p>

<p>
Por otro lado tenemos:
</p>

\begin{align}
a_{il}^{k+1}=(w_l^{k+1})^Th_i^{k}+b_l^{k+1}
\end{align}

<p>
Que es el cálculo de la neurona \(l\) de la capa siguiente, por lo que utiliza como entradas las salidas de la neurona de esta capa \(h_i^{k}\). Esta es la versión vectorizada del cálculo, si lo expresamos como sumatorio:
</p>

\begin{align}
a_{il}^{k+1}=\sum_{m=1}^{I_k}w_{lm}^{k+1}h_{im}^{k}+b_{lm}^{k+1}=\sum_{m=1}^{I_k}w_{lm}^{k+1}f(a_{im}^{k})+b_{lm}^{k+1}
\end{align}

<p>
De tal forma que se multiplican los \(I_k\) pesos de la capa siguiente (\(w_{lm}^{k+1}\)) con las \(I_k\) salidas de la capa actual (\(h_{im}^{k}\)) y sumamos los <em>offset</em> (\(b_{lm}^{k+1}\)). Además sabemos que \(h_{im}^{k}=f(a_{im}^{k})\). Por lo tanto:
</p>

\begin{align}
\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}=\sum_{m=1}^{I_k}(\frac{\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\delta a_{ij}^k}+\frac{\delta b_{lm}^{k+1}}{\delta a_{ij}^k})
\end{align}

<p>
La primera derivada tiene la siguiente forma:
</p>

\begin{align}
\frac{\delta(w_{lm}^{k+1}f(a_{im}^{k}))}{\delta a_{ij}^k}=w_{lm}^{k+1}\frac{\delta f(a_{im}^{k})}{\delta a_{ij}^k}
\end{align}

\begin{align}
\frac{\delta f(a_{im}^{k})}{\delta a_{ij}^k} =\begin{cases}
f'(a^k_{im})=f'(a^k_{ij}) &amp; m=j\\
0 &amp; m \ne j
\end{cases}
\end{align}

<p>
Por lo que podemos eliminar el sumatorio sobre \(m\) y la derivada sobre el <em>offset</em> ya que su valor es nulo:
</p>

\begin{align}
\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}=w_{lj}^{k+1}f'(a^k_{ij}) + 0
\end{align}

<div id="Explicación de la retropropagación-Gradiente-Gradiente acumulado"><h3 id="Gradiente acumulado" class="header"><a href="#Explicación de la retropropagación-Gradiente-Gradiente acumulado">Gradiente acumulado</a></h3></div>

<p>
Si volvemos a \(\delta_{ij}^k=\frac{\delta J_i}{\delta a_{ij}^k}=\sum_{l=1}^{I_{k+1}}\delta_{il}^{k+1}\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}\). Sustituimos \(\frac{\delta a_{il}^{k+1}}{\delta a_{ij}^k}\) obteniendo:
</p>

\begin{align}
\delta_{ij}^k=\frac{\delta J_i}{\delta a_{ij}^k}=\sum_{l=1}^{I_{k+1}}\delta_{il}^{k+1}w_{lj}^{k+1}f'(a^k_{ij})
\end{align}

<p>
Podemos extraer \(f'(a^k_{ij})\) ya que esta no depende de \(l\):
</p>

\begin{align}
\delta_{ij}^k=f'(a^k_{ij})\sum_{l=1}^{I_{k+1}}\delta_{il}^{k+1}w_{lj}^{k+1}
\end{align}

<p>
Si definimos:
</p>

\begin{align}
\epsilon_{ij}^k=\sum_{l=1}^{I_{k+1}}\delta_{il}^{k+1}w_{lj}^{k+1}
\end{align}

<p>
Tenemos que:
</p>

\begin{align}
\delta_{ij}^k=f'(a^k_{ij})\epsilon_{ij}^k
\end{align}

<div id="Explicación de la retropropagación-Gradiente-Derivadas de las funciones de activación"><h3 id="Derivadas de las funciones de activación" class="header"><a href="#Explicación de la retropropagación-Gradiente-Derivadas de las funciones de activación">Derivadas de las funciones de activación</a></h3></div>

<p>
La derivada de la función sigmoide:
</p>

\begin{align}
f'(t)=af(t)(1-f(t))
\end{align}
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>