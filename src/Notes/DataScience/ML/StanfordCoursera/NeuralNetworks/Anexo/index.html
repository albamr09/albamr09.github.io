<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/index.css">
    <!-- Custom styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="../../../../../../../src/style/custom.css">
    <title>Anexo</title>
  <script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/src/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="../Neural Networks.html">Back</a>
</p>

<div id="Anexo"><h1 id="Anexo" class="header"><a href="#Anexo">Anexo</a></h1></div>

<hr>

<div id="Anexo-Clasificación múltiple"><h2 id="Clasificación múltiple" class="header"><a href="#Anexo-Clasificación múltiple">Clasificación múltiple</a></h2></div>

<p>
Para crear una red neuronal que permita trabajar con \(c\) clases lo que hacemos es hacer que la red neuronal tenga \(c\) nodos en su capa de salida. Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/nn_ejemplo_multi.svg" alt="Ejemplo Red Neuronal Clasificación Múltiple" style="width:950px;height:550px">
</p>

<p>
De tal manera que ahora, cada salida \(y_j\) será un vector columna \(c\times1\), donde existe un valor por cada categoría, al igual que la hipótesis para el ejemplo \(j\), \(h_\Theta(x_j)\), es un vector columna \(c\times1\).
</p>

<p>
Como podemos ver, los valores de \(y_j\) indican claramente a qué clase pertenece el ejemplo \(j\) (clase 3), mientras que la hipótesis \(h_\Theta(x_j)\) ofrece, para cada clase (columna) la probabilidad de que el ejemplo \(j\) pertenezca a esa clase.
</p>

<div id="Anexo-Función de coste"><h2 id="Función de coste" class="header"><a href="#Anexo-Función de coste">Función de coste</a></h2></div>

<div id="Anexo-Función de coste-Notación"><h4 id="Notación" class="header"><a href="#Anexo-Función de coste-Notación">Notación</a></h4></div>

<p>
Como ya hemos visto en función del número de clases la salida tendrá distinta forma:
</p>

<ul>
<li>
<span id="Anexo-Función de coste-Notación-Clasificación binaria"></span><strong id="Clasificación binaria">Clasificación binaria</strong>: para cada ejemplo \(j\), \(y_j \in \{0, 1\}\), \(h_\Theta(x_j) \in \mathbb{R}\)

</li><li>
<span id="Anexo-Función de coste-Notación-Clasificación múltiple"></span><strong id="Clasificación múltiple">Clasificación múltiple</strong>: para cada ejemplo \(j\), \(y \in \mathbb{R}^c\), \(h_\Theta(x_j) \in \mathbb{R}^c\), donde \(c\) es el número de clases

</li><li>
Sea \(k\) el número de capas y \(S_i\) el número de nodos en la capa \(i\).

</li><li>
Sea \(Y=(y_{ij})\) una matriz \(c\times m\), donde \(m\) es el número de ejemplos y cada \(y_{j}\) es el vector columna \(c\times1\) de salida para el ejemplo \(j\).

</li></ul>
<p>
Definimos la función de coste como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\}
\end{align}

<p>
El primer sumatorio que va de 1 a \(m\) se encarga de calcular el coste para cada ejemplo \(j\). Mientras que el segundo sumatorio, que va de 1 a \(c\), se encarga de calcular el coste para cada nodo de salida.
</p>

<p>
Esta función se aplica sobre los \(k\) nodos en la capa de salida. 
</p>

<p>
<a href="Ejemplo Cálculo Función de Coste.html">Ejemplo Cálculo Función de Coste</a>
</p>

<div id="Anexo-Función de coste-Regularización"><h4 id="Regularización" class="header"><a href="#Anexo-Función de coste-Regularización">Regularización</a></h4></div>

<p>
Definimos la función de coste introduciendo regularización como sigue:
</p>

\begin{align}
J(\Theta) = - \frac{1}{m} \left\{ \sum_{j=1}^m \sum_{i=1}^c [y_{ij}\cdot \log(h_\Theta(x_j)_i)] + [(1-y_{ij})\cdot \log(1-(h_\Theta(x_j)_i))]\right\} + \frac{\lambda}{2m} \sum_{q=1}^k \sum_{i=1}^{S_q}\sum_{j=1}^{S_{q+1}} (\theta_{ji}^{(q)})^2
\end{align}

<p>
Antes de nada, recordar que \(S_q\) denota el número de nodos en la capa \(q\). Entonces, el primer término de la función es igual que cuando no se aplicaba regularización. Expliquemos el segundo término. La regularización, en este caso, consiste en sumar todos los pesos de la red neuronal, por lo tanto:
</p>

<ol>
<li>
Por cada capa \(q\), con \(1 \leq q \leq k\), sumamos todos los elementos de la matriz de pesos \(\Theta^{q}\), que como sabemos tiene dimensiones \(S_{q} \times S_{q-1}\) 

</li><li>
Dada la matriz \(\Theta^{(q)}\)

<ol>
<li>
Recorremos cada columna \(i\), con \(1 \leq i \leq S_{q-1}\)

</li><li>
Recorremos cada elemento \(j\) de la columna \(i\), con \(1 \leq j \leq S_{q}\)

</li><li>
Sumamos al total cada elemento de la matriz \(\Theta^{(q)}_{ji}\)

</li></ol>
</li><li>
Una vez se han sumado todas las matrices de pesos obtenemos un escalar, que multiplicamos por \(\frac{\lambda}{2m}\)

</li></ol>
<div id="Anexo-Función de coste-Múltiple Ejemplos"><h4 id="Múltiple Ejemplos" class="header"><a href="#Anexo-Función de coste-Múltiple Ejemplos">Múltiple Ejemplos</a></h4></div>

<p>
La salida de cada capa \(q\) es una matriz \(S_q \times m\), donde \(S_q\) denota el número de nodos en la capa \(q\) y \(m\) denota el número de ejemplos. 
</p>

<p>
Como vimos en nuestras figuras, donde se presentaban los cálculos sólo para un ejemplo, en cada capa \(q\) podemos mapear la salida de los \(S_q\) nodos a un vector columna \(S_q \times 1\). 
</p>

<p>
Si generalizamos esto a \(m\) ejemplos tenemos que la salida de cada capa es una matriz \(S_q \times m\). Esto se ilustra en la siguiente imagen:
</p>

<p>
<img src="https://albamr09.github.io/public/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/nn_multi_ejemplos.svg" alt="Red Neuronal con varios ejemplos" style="width:800px;height:600px">
</p>

<div id="Anexo-Retropropagación"><h2 id="Retropropagación" class="header"><a href="#Anexo-Retropropagación">Retropropagación</a></h2></div>

<p>
Vamos, ahora a explicar cómo se aplica la retropropagación. Lo primero que debemos tener en cuenta es que este proceso se basa en la misma idea de optimización que la <a href="../../Regresión Lineal.html">Regresión Lineal</a> y la <a href="../../Regresión Logística.html">Regresión Logística</a>, es decir, lo que queremos hacer es minimizar el coste, \(J(\Theta)\)
</p>

<p>
Sea \(c\) el número de nodos en la última capa, \(\theta_{it}\) el peso \(t\) del nodo \(i\) de la última capa \(k\), \(a_{ij}^{(k)}\) la salida del nodo \(i\) para el ejemplo \(j\) en la capa \(k\):
</p>
<ol>
<li>
Calculamos el gradiente de la última capa \(k\) como: \(\frac{\delta J(\Theta)}{\delta \theta_{it}^{(k)}} = \frac{\delta J(\Theta)}{\delta a_{1j}^{(k)}}\frac{\delta a_{1j}^{(k)}}{\delta \theta_{it}^{(k)}}\)

</li><li>
Calculamos el gradiente en capas intermedias utilizando la regla de la cadena como: \(\frac{\delta J(\Theta)}{\delta \theta_{it}^{(q)}} = \sum_{i=1}^{S_{(q+1)}} \frac{\delta J(\Theta)}{\delta a_{ij}^{(q+1)}}\frac{\delta a_{ij}^{(q+1)}}{\delta a_{ij}^{(q)}}\frac{\delta a_{ij}^{(q)}}{\delta \theta_{it}^{(q)}}\)

</li></ol>
<p>
Normalmente en las capas intermedias, \(q\), nos referimos al término \(\frac{\delta J(\Theta)}{\delta a_{ij}^{(q+1)}}\) como \(\Delta^{(q+1)}_{ij}\).
</p>

<p>
<a href="Explicación de la retropropagación.html">Explicación de la retropropagación</a>
</p>

<div id="Anexo-Retropropagación-Derivada de la función de coste"><h3 id="Derivada de la función de coste" class="header"><a href="#Anexo-Retropropagación-Derivada de la función de coste">Derivada de la función de coste</a></h3></div>

<p>
A continuación explicamos cómo derivar la función de coste (<span id="Anexo-Retropropagación-Derivada de la función de coste-Paso 1"></span><strong id="Paso 1">Paso 1</strong>).
</p>

<p>
<a href="Derivada de la función de coste.html">Derivada de la función de coste</a>
</p>

<div id="Anexo-Retropropagación-Capas intermedias"><h3 id="Capas intermedias" class="header"><a href="#Anexo-Retropropagación-Capas intermedias">Capas intermedias</a></h3></div>

<p>
Veamos, ahora, cómo llevar a cabo el <span id="Anexo-Retropropagación-Capas intermedias-Paso 2"></span><strong id="Paso 2">Paso 2</strong>: ¿cómo calculamos el gradiente (o lo que contribuye el peso \(it\) en el error) para los pesos de las capas intermedias?, es decir, cómo calculamos:
</p>

\[%align
\frac{\delta J(\Theta)}{\delta \theta_{it}^{(q)}} 
\]

<p>
<a href="Derivadas capas intermedias.html">Derivadas capas intermedias</a>
</p>

<div id="Anexo-Retropropagación-Ejemplo de retropropagación"><h3 id="Ejemplo de retropropagación" class="header"><a href="#Anexo-Retropropagación-Ejemplo de retropropagación">Ejemplo de retropropagación</a></h3></div>

<p>
<a href="Ejemplo de retropropagación.html">Ejemplo de retropropagación</a>
</p>

<div id="Anexo-Algoritmo"><h2 id="Algoritmo" class="header"><a href="#Anexo-Algoritmo">Algoritmo</a></h2></div>

<p>
<a href="Partes del algoritmo en python.html">Partes del algoritmo en python</a>
</p>

<div id="Anexo-Algoritmo-Notación"><h3 id="Notación" class="header"><a href="#Anexo-Algoritmo-Notación">Notación</a></h3></div>

<ul>
<li>
Época: iteración en el entrenamiento

</li><li>
Pesos (\(w_j^k, b_j^k\)): se inicializan de forma aletoria (evitar simetría) y con valores bajos.

</li><li>
Criterios de finalización

<ul>
<li>
\(J\) o gradiente de \(J\) inferior a un umbral

</li><li>
Número máximo de épocas

</li></ul>
</li><li>
Velocidad de apredizaje \(\mu\) intermedia: evita lentitud en las oscilaciones

</li><li>
Caída en mínimos locales que pueden tener \(J\) elevado. Es por ello que se ejecuta varias veces el entrenamiento y se selecciona aquel que obtenga mejor resultado.

</li><li>
Actualización de pesos patrón a patrón en lugar de tras computar el error sobre todo el dataset. Puede evitar mínimos locales y converge antes.

</li><li>
Función de activación sigmoide para clasificación o linear para regresión

</li></ul>
<div id="Anexo-Algoritmo-Pseudocódigo"><h3 id="Pseudocódigo" class="header"><a href="#Anexo-Algoritmo-Pseudocódigo">Pseudocódigo</a></h3></div>

<p>
<img src="https://albamr09.github.io/public/DataScience/ML/StanfordCoursera/NeuralNetworks/Anexo/nn_pseudocodigo.png" alt="Pseudocódigo Red Neuronal">
</p>
</div>
  

<script type="text/javascript" src="https://albamr09.github.io/src/lib/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script type="text/javascript" src="https://albamr09.github.io/src/lib/search.js" id="search" data-description="Library to perform search"></script><script type="text/javascript" id="search" data-description="Entrypoint for hightlihgting">
  $("pre").each(function (index, item) {
    $(item).html("<code>" + $(item).html() + "</code>");
  });
  hljs.highlightAll();
</script></body></html>