<html><head>
    <!-- Normal styling from vimwiki -->
    <link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style.css">
    <title>Ecuación Normal</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" id="latex_script" data-description="Support for latex"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script" data-description="Support for latex"></script><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/search.css" data-description="Styling for search"><link rel="Stylesheet" type="text/css" href="https://albamr09.github.io/style/atom-one-light.min.css" data-description="Code highlight"><link rel="icon" type="image/svg+xml" href="https://albamr09.github.io/public/icon.svg" data-description="Page icon"></head>
  <body>
    <a href="https://albamr09.github.io/" style="
        color: white;
        font-weight: bold;
        text-decoration: none;
        padding: 3px 6px;
        border-radius: 3px;
        background-color: #1e90ff;
        text-transform: uppercase;
      ">Index</a>
    <form id="search_form" class="search-form">
      <input required="" type="search" id="search_term" class="searchTerm">
      <button type="submit" class="searchButton">Search</button>
    </form>
    <div id="search-background" class="search-background">
      <div id="search-result" class="search-result-hide"></div>
      <div id="search-form-modal" class="search-form-modal">
        <form id="search-form-in-modal">
          <input required="" type="search" id="search-input-in-modal" class="search-input-in-modal" placeholder="Search whatever...">
          <button type="submit" class="searchButton">Search</button>
        </form>
      </div>
    </div>
    <hr>
    <div class="content">
<p>
<a href="index.html">Back</a>
</p>

<div id="Ecuación Normal"><h1 id="Ecuación Normal" class="header"><a href="#Ecuación Normal">Ecuación Normal</a></h1></div>

<hr>

<ol>
<li>
<a href="Ecuación Normal.html#Ecuación Normal-Descripción de los datos">Descripción de los datos</a>

</li><li>
<a href="Ecuación Normal.html#Ecuación Normal-Hipótesis">Hipótesis</a>

</li><li>
<a href="Ecuación Normal.html#Ecuación Normal-Funcion de coste">Funcion de coste</a>

</li><li>
<a href="Ecuación Normal.html#Ecuación Normal-Minimización del coste">Minimización del coste</a>

</li><li>
<a href="Ecuación Normal.html#Ecuación Normal-Anotaciones">Anotaciones</a>

</li></ol>
<hr>

<div id="Ecuación Normal-Descripción de los datos"><h2 id="Descripción de los datos" class="header"><a href="#Ecuación Normal-Descripción de los datos">Descripción de los datos</a></h2></div>

<ul>
<li>
\(X = (x_{ij})\) una matriz \(m \times (n + 1)\) donde cada \(X_{i}\) es un vector fila \(1 \times (n+1)\), que incluye los valores de todas las características para el ejemplo \(i\).
\begin{align}
X = 
\begin{bmatrix}
X_1 \\
\vdots \\
X_i \\
\vdots \\
X_m \\
\end{bmatrix} 
\end{align}

</li></ul>
<p>
Cabe destacar que \(X_{i0} = 1\), es el término independiente.
</p>

<ul>
<li>
\(\Theta = (\theta_i)\) es un vector columna \((n+1)\times 1\) donde cada \(\theta_i\) es el peso de la característica \(i\), tal que:
\begin{align}
\Theta = \begin{bmatrix} \theta_0 \\ \vdots \\ \theta_n\end{bmatrix} 
\end{align}

</li><li>
\(Y = (y_j)\) es un vector columna \(m\times 1\) donde cada \(y_j\) es la salida real para el ejemplo \(j\), tal que:
\begin{align}
Y = \begin{bmatrix} y_1 \\ \cdots \\ y_m\end{bmatrix} 
\end{align}

</li></ul>
<div id="Ecuación Normal-Hipótesis"><h2 id="Hipótesis" class="header"><a href="#Ecuación Normal-Hipótesis">Hipótesis</a></h2></div>

<ul>
<li>
Dado un conjunto de \(m\) datos, es decir matriz \(X\), de dimensiones \(m \times (n+1)\), la hipótesis se define como:
\begin{align}
h_\Theta(x) = X \cdot \Theta = 
\begin{bmatrix}
X_1 \cdot \Theta = \sum_{i=0}^{n+1} \theta_i x_{1i} \\
\vdots \\
X_j \cdot \Theta = \sum_{i=0}^{n+1} \theta_i x_{ji} \\
\vdots \\
X_m \cdot \Theta = \sum_{i=0}^{n+1} \theta_i x_{mi} \\
\end{bmatrix}
\end{align}

</li></ul>
<p>
Observa que ahora \(X\) y \(\Theta\) están colocados de forma inversa a como lo hacíamos en la <a href="Regresión Lineal.html">regresión lineal</a> y la <a href="Regresión Logística.html">regresión logística</a>. Esto es debido a que hemos transpuesto las matrices \(X\) y \(\Theta\), con respecto a como las habíamos definido en las secciones anteriores. El cálculo es el mismo.
</p>

<div id="Ecuación Normal-Funcion de coste"><h2 id="Funcion de coste" class="header"><a href="#Ecuación Normal-Funcion de coste">Funcion de coste</a></h2></div>

<p>
Se define la función de coste, \(J(\Theta)\) como:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m} (X\Theta - Y)^T(X\Theta - Y)
\end{align}

<p>
La expresión \((X\Theta - Y)^T(X\Theta - Y)\) es equivalente a \((h_\Theta(x) - y)^2\), que se utilizaba en la función de coste de la <a href="Regresión Lineal.html">regresión lineal</a>.
</p>

<div id="Ecuación Normal-Funcion de coste-Regularización"><h4 id="Regularización" class="header"><a href="#Ecuación Normal-Funcion de coste-Regularización">Regularización</a></h4></div>

<p>
Con regularización, se define la función de coste, \(J(\Theta)\) como:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m} (X\Theta - Y)^T(X\Theta - Y) + \frac{1}{2m} \lambda \Theta^T\Theta
\end{align}


<div id="Ecuación Normal-Minimización del coste"><h2 id="Minimización del coste" class="header"><a href="#Ecuación Normal-Minimización del coste">Minimización del coste</a></h2></div>

<p>
Con la ecuación normal, en lugar de actualizar el vector de pesos \(\Theta\) de forma iterativa, lo que hacemos es igualar la derivada del coste en base a los pesos a cero utilizando derivación matricial:
</p>

\[%align
\Delta_\Theta J(\Theta) = 
\begin{bmatrix}
\frac{\delta J(\Theta)}{\delta \theta_0} \\
\vdots \\
\frac{\delta J(\Theta)}{\delta \theta_i} \\
\vdots \\
\frac{\delta J(\Theta)}{\delta \theta_n} \\
\end{bmatrix}
= 
\begin{bmatrix}
0 \\
\vdots \\
0 \\
\vdots \\
0 \\
\end{bmatrix}
= 
\overrightarrow{0}
\]

<hr>

<p>
A continuación exponemos cómo se calcula la derivada:
</p>

<p>
Sustituímos la función de coste:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \Delta_\Theta \frac{1}{2m}(X \Theta - Y)^T (X \Theta - Y)
\]

<p>
Aplicamos la propiedad \((A + B)^T = A^T + B^T\) 
</p>

\[%align
\Delta_\Theta J(\Theta) =  \Delta_\Theta \frac{1}{2m}((X\Theta)^T - Y^T) (X \Theta - Y)
\]

<p>
Sacamos el factor constante de la derivada y realizamos la multiplicación:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{2m} \Delta_\Theta (X\Theta)^T(X\Theta) - (X\Theta)^TY -Y^TX\Theta + Y^TY
\]

<p>
Aplicamos la propiedad \(AB = B^TA^T\), tal que \(Y^T(X\Theta) = (X\Theta)^T((Y)^T)^T = (X\Theta)^TY\)
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{2m} \Delta_\Theta (X\Theta)^T(X\Theta) - (X\Theta)^TY - (X\Theta)^TY + Y^TY
\]

<p>
Agrupamos términos compatibles:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{2m} \Delta_\Theta (X\Theta)^T(X\Theta) - 2(X\Theta)^TY + Y^TY
\]

<p>
Como \(\Delta_\Theta Y^TY=0\):
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{2m} \Delta_\Theta (X\Theta)^T(X\Theta) - 2(X\Theta)^TY
\]

<p>
Finalmente calculamos la derivada matricial:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{2m} 2X^T(X\Theta) - 2X^TY = \frac{1}{m} X^T(X\Theta) - X^TY
\]

<hr>

<p>
Ahora igualamos la expresión obtenida a cero:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{m} [X^TX\Theta - X^TY] = 0
\]

<p>
Multiplicamos por \(m\) en ambos lados de la ecuación:
</p>

\[%align
X^TX\Theta - X^TY = 0
\]

<p>
Sumamos \(X^TY\) en ambos lados de la ecuación:
</p>

\[%align
X^TX\Theta - X^TY + X^TY= X^TY
\]

\[%align
X^TX\Theta = X^TY
\]

<p>
Multiplicamos por \((X^TX)^{-1}\) por la izquierda en ambos lados de la ecuación:
</p>

\[%align
(X^TX)^{-1}X^TX\Theta = (X^TX)^{-1}X^TY
\]

\[%align
I\Theta = (X^TX)^{-1}X^TY
\]

\[%align
\Theta = (X^TX)^{-1}X^TY
\]

<p>
De tal manera que ahora hemos calculado el vector de pesos óptimo que minimiza el coste.
</p>

<div id="Ecuación Normal-Minimización del coste-Regularización"><h4 id="Regularización" class="header"><a href="#Ecuación Normal-Minimización del coste-Regularización">Regularización</a></h4></div>

<p>
Con regularización debemos derivar la función que coste que incluye el parámetro de regularización:
</p>

\begin{align}
J(\Theta) = \frac{1}{2m} (X\Theta - Y)^T(X\Theta - Y) + \frac{1}{2m} \lambda \Theta^T\Theta
\end{align}

<p>
El primer término ya lo hemos derivado, por lo tanto procedemos a derivar el segundo término:
</p>

\[%align
\Delta_\Theta \frac{1}{2m} \lambda \Theta^T\Theta
\]

<p>
Sacamos el factor constante \(\frac{\lambda}{2m}\) fuera de la derivada
</p>

\[%align
\frac{\lambda}{2m} \Delta_\Theta [\Theta^T\Theta]
\]

<p>
Llevamos a cabo la derivada matricial:
</p>

\[%align
\frac{\lambda}{2m} 2 \Theta = \frac{\lambda}{m} \Theta
\]

<hr>

<p>
Juntamos las derivadas de ambos términos:
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{m} [X^TX\Theta - X^TY] + \frac{\lambda}{m} \Theta
\]

<p>
Sacamos \(\frac{1}{m}\) como factor común e igualamos a cero
</p>

\[%align
\Delta_\Theta J(\Theta) =  \frac{1}{m} (X^TX\Theta - X^TY + \lambda\Theta) = 0
\]

<p>
Multiplicamos por \(m\) en ambos lados de la ecuación:
</p>

\[%align
X^TX\Theta - X^TY + \lambda\Theta = 0
\]

<p>
Sumamos \(X^TY\) en ambos lados de la ecuación:
</p>

\[%align
X^TX\Theta - X^TY + X^TY + \lambda\Theta = X^TY
\]

\[%align
X^TX\Theta + \lambda\Theta = X^TY
\]

<p>
Sacamos \(\Theta\) como factor común
</p>

\[%align
(X^TX + \lambda I)\Theta = X^TY
\]

<p>
Donde \(I\) es la matriz identidad e dimensiones \((n+1) \times (n+1)\). Multiplicamos \((X^TX + \lambda I)^{-1}\) por la izquierda en ambos lados de la ecuación:
</p>

\[%align
(X^TX + \lambda I)^{-1}(X^TX + \lambda I)\Theta = (X^TX + \lambda I)^{-1}X^TY
\]

\[%align
I\Theta = (X^TX + \lambda I)^{-1}X^TY
\]

\[%align
\Theta = (X^TX + \lambda I)^{-1}X^TY
\]

<p>
De tal forma que hemos calculado el \(\Theta\) óptimo que minimiza el coste, utilizando regularización.
</p>

<hr>

<div id="Ecuación Normal-Anotaciones"><h2 id="Anotaciones" class="header"><a href="#Ecuación Normal-Anotaciones">Anotaciones</a></h2></div>

<ul>
<li>
No se debe utilizar la ecuación normal cuando el número de ejemplos \(m\) es muy grande, ya que es rendimiento del algoritmo es malo

</li><li>
Hay que tener cuidado con si las matrices son inversibles

</li><li>
Si \(m \leq n\), entonces las matrices no son invertibles.

</li><li>
Si \(\lambda &gt; 0\), entonces aseguramos la inversibilidad de las matrices.

</li></ul>
</div>
  
  <script type="text/javascript">
    $("pre").each(function (index, item) {
      $(item).html("<code>" + $(item).html() + "</code>");
    });
    hljs.initHighlightingOnLoad();
  </script>

<script src="https://albamr09.github.io/js/highlight.min.js" id="js_highlight" data-description="Support sytax highlighting on code"></script><script src="https://albamr09.github.io/js/zepto.min.js" id="zepto" data-description="Library to perform search"></script><script src="https://albamr09.github.io/js/flexsearch.bundle.js" id="flexsearch" data-description="Library to perform search"></script><script src="https://albamr09.github.io/js/search.js" id="search" data-description="Library to perform search"></script></body></html>